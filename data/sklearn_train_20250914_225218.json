[
  {
    "number":26742,
    "text":"KDtree.valid_metrics is no longer an attribute without warning\n\n### Describe the bug Prior 1.3.0, [CODE] was an attribute returning a list. This was also mentioned in the docs as showed in [URL] In 1.3.0, however, #25482 added a new method [CODE], therefore checks like [CODE] does not work anymore. From the perspective of our downstream project, this is an unexpected change of behaviour. I don't think that there's anything we can do about it now but it is a bit unfortunate. I wanted to open the issue to ensure the team is aware of it but given the situation I don't really expect a fix, unless you have some great idea how to resolve it. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] We got a list directly before. Now, we have to use [CODE]. ### Actual Results [CODE] ### Versions ```shell System: python: 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:41) [Clang 15.0.7 ] executable: \/Users\/martin\/mambaforge\/envs\/pointpats_dev\/bin\/python machine: macOS-13.4-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.0.1 setuptools: 67.4.0 numpy: 1.24.4 scipy: 1.11.0 Cython: None pandas: 2.0.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: \/Users\/martin\/mambaforge\/envs\/pointpats_dev\/lib\/libopenblas.0.dylib version: 0.3.21 threading_layer: openmp architecture: VORTEX num_threads: 8 user_api: openmp internal_api: openmp prefi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26742"
  },
  {
    "number":25433,
    "text":"Open up [CODE] and [CODE] to overriding [CODE] with an additional callable parameter [CODE]\n\n### Describe the workflow you want to enable Some people (including @betatim @ogrisel @jjerphan and I) have been devising a plugin system that would open up CODE] estimators to other external implementations, and in particular implementations with GPU backends - see [URL] . Some of the plugins we're considering can materialize the data in memory with an array library that is compatible with the Array API - namely [CuPy while still benefiting from reusing existing validation code in [CODE]. The override can be necessary in case the [CODE] method from the array library implements a superset of the array api that is necessary for the plugin, but is currently not used by [CODE] because it's not part of the array api (for instance, the [CODE] argument isn't passed to [CODE] for array libraries other than [CODE])",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25433"
  },
  {
    "number":29454,
    "text":"StackingRegressor.fit() doesn't support sample_weight when using Pipeline objects as estimators\n\n### Describe the bug sklearn 1.5.0. When using a Stacking model, fitting raises the following error when: - [CODE] argument is passed, AND - the individual estimators contain a Pipeline object instead of a single estimator I think the actual problem lies in [CODE] in line 210. There is just [CODE] used as a fit parameter. Instead it should check whether each single estimator is a Pipeline and then transforming the attribute name [CODE] to i.e. [CODE], depending on the last step of the pipeline. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Should fit the model instead of raising an error ### Actual Results ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) c:\\Users\\pfa2ba\\Documents\\dhpt\\dhpt_models.py in <module> 14 15 stack = StackingRegressor([(\"a\", mdl_a), (\"b\", mdl_b)], cv=2) ---> 16 stack.fit(X, Y, sample_weight=W) c:\\Users\\pfa2ba\\.conda\\envs\\dhpt\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py in fit(self, X, y, sample_weight) 969 _raise_for_unsupported_routing(self, \"fit\", sample_weight=sample_weight) 970 y = column_or_1d(y, warn=True) --> 971 return super().fit(X, y, sample_weight) 972 973 def transform(self, X): c:\\Users\\pfa2ba\\.conda\\envs\\dhpt\\lib\\site-packages\\sklearn\\bas...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29454"
  },
  {
    "number":27751,
    "text":"Groups in BaggingRegressor\n\n### Describe the workflow you want to enable It would be nice if we could control how random indices are created with groups in BaggingRegressor. As far as I can tell, BaggingRegressor will select sub samples u.a.r. with\/without replacement. It would be nice if we could group the data points, and have BaggingRegressor select sub samples of (optional) _groups_ u.a.r. with\/without replacement. An example of this paradigm already exists in sklearn with cross validation (e.g. KFold vs. GroupKFold) ### Describe your proposed solution One idea is to pass in a vector [CODE] of size [CODE] mapping each data point to its group. Then, [CODE] can respect groups if they exist, or default to its existing behavior if they do not. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27751"
  },
  {
    "number":30151,
    "text":"Segmentation fault in sklearn.metrics.pairwise_distances with OpenBLAS 0.3.28 (only pthreads variant)\n\n[CODE_BLOCK] [CODE_BLOCK] This happens with OpenBLAS 0.3.28 but not 0.3.27. Setting the [CODE] or [CODE] environment variable also make the issue disappear. This is somewhat reminiscent of [URL] so there may be something in OpenBLAS 0.3.28 [^1] that doesn't like [CODE] followed by [CODE]? No idea how to test this hypothesis ... this could well be OS-dependent since [URL] only happens on Linux. [^1]: OpenBLAS 0.3.28 is used in numpy development wheel and OpenBLAS 0.3.27 is used in numpy latest release 2.1.2 at the time of writing <details> <summary>Python traceback<\/summary> ``` Fatal Python error: Segmentation fault Thread 0x00007c7907e006c0 (most recent call first): File \"\/home\/lesteve\/micromamba\/envs\/testenv\/lib\/python3.12\/multiprocessing\/pool.py\", line 579 in _handle_results File \"\/home\/lesteve\/micromamba\/envs\/testenv\/lib\/python3.12\/threading.py\", line 1012 in run File \"\/home\/lesteve\/micromamba\/envs\/testenv\/lib\/python3.12\/threading.py\", line 1075 in _bootstrap_inner File \"\/home\/lesteve\/micromamba\/envs\/testenv\/lib\/python3.12\/threading.py\", line 1032 in _bootstrap Thread 0x00007c790d2006c0 (most recent call first): File \"\/home\/lesteve\/micromamba\/envs\/testenv\/lib\/python3.12\/multiprocessing\/pool.py\", line 531 in _handle_tasks File \"\/home\/...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30151"
  },
  {
    "number":26891,
    "text":"Putting it all together Pipelining is not working ( missing dependencies and plot )\n\n### Describe the issue linked to the documentation From [URL] it is missing [CODE_BLOCK] The last chart is missing the data ![image]([URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26891"
  },
  {
    "number":24429,
    "text":"\"StackingClassifier\" throws ValueError when cv = \"prefit\"\n\n### Describe the bug When I use the CODE], I get the error \"ValueError: Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable. Got prefit.\" when calling the [CODE] method ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expected to fit the model. According to the [docs or an iterable. Got prefit.\" ### Versions ```shell System: python: 3.10.2 (tags\/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] executable: c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe machine: Windows-10-10.0.22000-SP0 Python dependencies: pip: 22.2.2 setuptools: 59.8.0 sklearn: 1.0.2 numpy: 1.23.2 scipy: 1.8.0 Cython: 0.29.28 pandas: 1.4.0 matplotlib: 3.5.1 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24429"
  },
  {
    "number":28884,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Apr 26, 2024) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28884"
  },
  {
    "number":30308,
    "text":"LogisticRegression's regularization is scaled by the dataset size\n\n### Describe the workflow you want to enable Other linear models on [URL] have regularization that doesn't depend on the dataset size ### Describe your proposed solution It would be good to either change the behavior or document it very very very clearly, not only in the user guide as it is now but also in the model documentation. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30308"
  },
  {
    "number":29570,
    "text":"Try running examples in parallel during doc build\n\nWe are already using sphinx-gallery 0.17 which has added the feature to run examples in parallel see URL] See [sphinx-gallery doc. I had a quick look during the sphinx-gallery PR and it was making the doc a bit quicker locally: [URL] General directions: - configure sphinx-gallery to use 2 cores in [CODE] [CODE_BLOCK] - open a PR with [CODE] commit to do a full build - also generate the doc locally e.g. with [CODE] + [CODE] and see how much sphinx-gallery parallel settings make a difference",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29570"
  },
  {
    "number":26943,
    "text":"Kmeans : Different cluster result with same random_state beetwen two version 0.22.2 & 1.2.2\n\n### Describe the bug Hello everyone, I'm currently working on a clustering problem. To ensure result reproducibility, we initially set the random_state parameter in KMeans() to 0. However, after updating scikit-learn from version 0.22.2 to version 1.2.2, i encountered an unexpected issue. When i ran the same code with the same dataset , the results differed from our previous run. We are uncertain about the reasons behind this inconsistency and have been unable to reproduce the initial result. ### Steps\/Code to Reproduce model = KMeans(n_clusters=5, init='k-means++', tol=0.0001, random_state=0, copy_x=True, algorithm='auto' ) ### Expected Results Version 0.22.2 : Numer of cluster = 5 Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4| cluster 5 10| 20| 12| 30|45 Version 1.2.2 : Numer of cluster = 5 Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4| cluster 5 10| 20| 12| 30|45 ### Actual Results Version 0.22.2 : Numer of cluster = 5 Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4| cluster 5 10| 5| 6| 14|5 Version 1.2.2 : Numer of cluster = 5 Cluster 1 | Cluster 2 | Cluster 3 | Cluster 4| cluster 5 3| 7| 20| 8|2 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26943"
  },
  {
    "number":32168,
    "text":"Unexpected behavior when combining FunctionTransformer and pandas DataFrames\n\n### Describe the bug When using a pandas [CODE] with the [CODE] on data with the wrong column order, the column names are mixed. While this behavior might be derived from the documentation, it still felt unexpected. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The same column is returned. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.13.7 | packaged by conda-forge | (main, Sep 3 2025, 14:30:35) [GCC 14.3.0] executable: \/opt\/conda\/envs\/sklearn_burg\/bin\/python3 machine: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.7.2 pip: 25.2 setuptools: 80.9.0 numpy: 2.3.3 scipy: 1.16.1 Cython: None pandas: 2.3.2 matplotlib: None joblib: 1.5.2 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 28 prefix: libopenblas filepath: \/opt\/conda\/envs\/sklearn_burg\/lib\/libopenblasp-r0.3.30.so version: 0.3.30 threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 28 prefix: libgomp filepath: \/opt\/conda\/envs\/sklearn_burg\/lib\/libgomp.so.1.0.0 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32168"
  },
  {
    "number":30509,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Dec 22, 2024) \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev - test_euclidean_distances_extreme_values[1000000-float32-0.0001-1e-05]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30509"
  },
  {
    "number":31811,
    "text":"Bug: StackingRegressor serialization error with custom neural network regressors (TabTransformer, ANN, DNN)\n\n### Describe the bug Bug Report: StackingRegressor in scikit-learn Fails with Custom Neural Network Regressors Dear scikit-learn Maintainers, I am Dr. Mohsen Jahan, Professor of Agroecology and Instructor of Artificial Intelligence and Digital Transformation at Ferdowsi University of Mashhad, Iran. While conducting a research project on multi-objective feature selection using the NSGA-III algorithm and stacking models, I encountered an issue with the StackingRegressor implementation in scikit-learn (version 1.5.2). Specifically, this module exhibits compatibility issues with custom regression models, particularly those based on neural networks such as TabTransformerRegressor, ANNRegressor, and DNNRegressor. Issue Description When using StackingRegressor in scikit-learn with custom regression models that adhere to the standard scikit-learn API (e.g., implementing fit and predict methods) but rely on complex internal structures (e.g., based on tensorflow or pytorch), serialization or cloning errors occur. These errors manifest particularly when such models are used as regressors or meta_regressor in StackingRegressor, affecting processes like GridSearchCV or model persistence with joblib. For instance, in our project, employing a custom SimpleDNNRegressor (built with tensorflow) as the meta-regressor in StackingRegressor resulted in serialization errors. This issue was n...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31811"
  },
  {
    "number":25776,
    "text":"RFC: Breaking Changes for Version 2\n\nA while ago we talked about the possibility of a version 2 with breaking changes. Specifically, this came up in the context of SLEP006: metadata routing, but there are other things we have wanted to break which we can do all in a single release. In this issue we can talk about the possibility of such a release, and maybe a few things we'd like to include in it. I'll try to summarize the challenges of keeping backward compatibility (BC) in the context of SLEP006: - [CODE] will raise according to SLEP006 since [CODE] is not requested by any object. For backward compatibility, during the transition period, it'll only warn and assume [CODE] is requested by [CODE]. - What should the behavior of [CODE] be during the transition period? If we keep backward compatibility, it should warn (whatever the warning be), route [CODE] to [CODE], but not to the [CODE]. And that's only the case because we're keeping BC. From the user's perspective, it's very weird. - [CODE] ATM in certain methods like [CODE], [CODE], [CODE], routes [CODE] to the [CODE] of the last estimator only. And [CODE] and [CODE], [CODE] have no routing at all. Keeping this BC, proves to be challenging, and a bit nasty. ref: #24270 - In quite a few meta-estimators, we check if the sub-estimator has [CODE] in the signature of [CODE], and we pass the given sample weight to it if that's the case, and not if that's not the case. With routing, we would have a much better idea of when to pass ...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25776"
  },
  {
    "number":25050,
    "text":"Davies-Bouldin Index implementation is incomplete\n\n### Describe the workflow you want to enable The DB-Index has two parameters, p and q, where p is the power of the Minkowski norm used to compute the distance of cluster centers, while q controls the power mean for the \"radius\" \/ scatter. p=q=2 is probably the most meaningful combination. The current implementation appears to be hard-coded to p=2, q=1 ### Describe your proposed solution Add [CODE] and [CODE] parameters, default to what the current implementation uses. Allow passing known cluster centers with a [CODE] parameter, instead of recomputing them inside the function. Enhance the documentation that this index is most suited for k-means clustering, because it uses cluster centroids. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25050"
  },
  {
    "number":27561,
    "text":"MLPClassifier: Cannot turn off convergence warning without side effects\n\n### Describe the bug As described [MSC v.1934 64 bit (AMD64)] executable: []\\WPy64-31110\\python-3.11.1.amd64\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.2.2 pip: 22.3.1 setuptools: 65.5.0 numpy: 1.24.3 scipy: 1.10.1 Cython: None pandas: 2.0.1 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: []\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll version: 0.3.21 threading_layer: pthreads architecture: Haswell num_threads: 8 user_api: openmp internal_api: openmp prefix: vcomp filepath: [*]\\WPy64-31110\\python-3.11.1.amd64\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None num_threads: 8 user_ap...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27561"
  },
  {
    "number":31210,
    "text":"Issues with pairwise_distances(metric='euclidean') when used on the output of UMAP\n\n### Describe the bug When using pairwise_distances with metric='euclidean' on the output of some data from a UMAP, a [CODE] is raised. This warning is not raised if you just use pairwise_distances on some normally distributed values of the same dimension, it specifically happens when used on the output of UMAP. The warning is not raised if calling [CODE] on the same data. The warning doesn't come up with any other metric (other than euclidean family e.g. nan_euclidean etc) ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Would expect to see no RuntimeWarning (FutureWarning is expected) ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.16 (main, Feb 25 2025, 09:29:51) [Clang 16.0.0 (clang-1600.0.26.6)] executable: ...\/bin\/python machine: macOS-15.4-arm64-arm-64bit Python dependencies: sklearn: 1.6.1 pip: 25.0.1 setuptools: 65.5.0 numpy: 2.1.3 scipy: 1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31210"
  },
  {
    "number":31808,
    "text":"Handle new [CODE] that is coming in pandas 3\n\nThis issue is the result of investigating [URL] The failures in the nightlies are due to changes coming in pandas 3.0. In particular the switch to using [CODE] as the type for string columns. The old behaviour was to use [CODE]. This has a few effects: - can no longer use [CODE] because the new dtype isn't one known to numpy - selecting columns in [CODE] doesn't select the right columns anymore These are the failing tests: [CODE_BLOCK] Three of these (first one and last two) are due to using [CODE]. The other failures are due to not selecting the right columns (n.b. the way the test...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31808"
  },
  {
    "number":29621,
    "text":"mirrors-prettier pre-commit has been archived so maybe should be replaced\n\n### Describe the bug Noticed your [mirrors-prettier pre-commit]([URL] has been archived. I was going to suggest you remove and\/or look for alternative linters for the scss \/ js files. ### Steps\/Code to Reproduce Noticed this in the .pre-commit-config.yaml [CODE_BLOCK] ### Expected Results N\/A ### Actual Results N\/A ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29621"
  },
  {
    "number":25594,
    "text":"KBinsDiscretizer creates wrong bins.\n\n### Describe the bug [CODE] gives wrong bins and wrong transformed data, when the inputs contains only 2 distinct values, and [CODE]. It only produces 1 bin, which is not expected. The warning shows some bins are too small so they are merged. Note that when setting [CODE], [CODE] gives more reasonable bins and correct results. However, from the [CODE], it seems [CODE] is handled separately. ### My guess about the cause I think the root cause is inconsistent handling of [CODE] and [CODE] in fitting and transforming. In fitting, [CODE] and [CODE] are not considered when removing duplicated bin edges. But in transforming data, the first and last bin edges are replaces with [CODE] and [CODE]. Such differences obviously removes some necessary bins. Taking the case I mentioned as a example, current implementation deduplicates bin edges from [CODE] to [CODE]. Then in transforming, all real values are actually in one bin, since the bin edges are replaced with [CODE] and [CODE]. A more reasonable conversion is replacing first and last bin edges with [CODE] and [CODE] in fitting, i.e., [CODE], then removing duplicated edges, which gives [CODE]. This will give expected results. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The last [CODE] should give [CODE_BLOCK] or [CODE_BLOCK] . It depends on how to implement it correctly. ### Actual Results The last [CODE] gives [CODE_BLOCK] ### Versions ```shell System: python: 3.8.16 (default, J...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25594"
  },
  {
    "number":26303,
    "text":"PowerTransformer fails with unhelpful stack trace with all-nan feature and method='box-cox'\n\n### Describe the bug CODE] throws a difficult-to-debug error if x contains an all-nan column. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Either no error is thrown and the all-nan column is preserved, or a descriptive error is thrown indicating that there is an unfittable column ### Actual Results ``` ValueError Traceback (most recent call last) [<ipython-input-12-563273596add> ----> 1 PowerTransformer('box-cox').fit_transform(x) 4 frames \/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/utils\/_set_output.py 138 @wraps(f) 139 def wrapped(self, X, args, kwargs): --> 140 data_to_wrap = f(self, X, args, *kwargs) 141 if isinstance(data_to_wrap, tuple): 142 # only wrap the first output for cross decomposition \/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/preprocessing\/_data.py 3101 \"\"\" 3102 self._validate_params() -> 3103 return self._fit(X, y, force_transform=True) 3104 3105 def _fit(self, X, y=None, force_transform=False): \/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/preprocessing\/_data.py 3114 }[self.method] 3115 with np.errstate(invalid=\"ignore\"): # hide NaN warnings -> 3116 self.lambdas_ = np.array([optim_function(col) for col in X.T]) 3117 3118 if self.standardize or force_tran...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26303"
  },
  {
    "number":30972,
    "text":"auto clusters selection of n_clusters with elbow method\n\n### Describe the workflow you want to enable In, sklearn.cluster the KMeans algorithm. the feature suggestion is to add the elbow method cluster selection with n_cluster=\"auto\" calculates the best no of cluster based on mse add trains the models based on the return output of auto_cluster_selection() with auto as keyword in KMeans ### Describe your proposed solution to create a private method in the KMeans to calculate the no of best clusters automatically by taking the n_clusters=\"auto\" ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30972"
  },
  {
    "number":30114,
    "text":"Add differential privacy noise injection to SGDRegressor with automatic calibration\n\n### Describe the workflow you want to enable Enable differential privacy in SGDRegressor by adding noise injection with: 1. Manual noise scale setting, or 2. Automatic noise calibration from desired privacy parameter \u03b5 ### Describe your proposed solution Add parameters to SGDRegressor: CODE_BLOCK] ### Research basis Builds on: - [\"Deep Learning with Differential Privacy\" - JAX Privacy ### Benefits 1. Enables both manual noise configuration and automatic calibration 2. Calculates correct noise scale for desired privacy level 3. Interfaces naturally with the clipping parameter from #30113 Would be happy t...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30114"
  },
  {
    "number":26455,
    "text":"Incorrect formula of haversine distance\n\n### Describe the issue linked to the documentation Hi, I was wondering through documentation: [URL] On the page above you have what might be incorrect formula of haversine distance [CODE] ### Suggest a potential alternative\/fix Correct formula: [CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26455"
  },
  {
    "number":29837,
    "text":"Add float as acceptable input for n_jobs\n\n### Describe the workflow you want to enable Float may be used as possible input for n_jobs. That is, allowing selection of set percentage of the machine's CPU core count. ### Describe your proposed solution When n_jobs is a float (in the range [CODE]), the number of CPU cores can be checked using the STL or scikit-learn dependencies, with [CODE], [CODE], [CODE], etc. and [CODE] can be set as the value of self.n_jobs.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29837"
  },
  {
    "number":29098,
    "text":"Deprecate copy_X in TheilSenRegressor\n\nThe [CODE] parameter of `[CODE]` is not used anywhere and hence has no effect, so we should deprecate it.",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29098"
  },
  {
    "number":24846,
    "text":"\"X does not have valid feature names\" when training on DataFrame input with MLP\n\n### Describe the bug Bug happens only when early_stopping option is True. Looks like [CODE] and [CODE] inside of [CODE] training loop are ndarrays, but model detects column names of input [CODE] on [CODE] step. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No messages like \"UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\" ### Actual Results multiple messages like [CODE_BLOCK] ### Versions ```shell System: python: 3.10.6 (main, Nov 2 2022, 18:53:38) [GCC 11.3.0] executable: \/home\/usopp\/sinp\/mineralogy-2022\/main\/venv\/bin\/python machine: Linux-5.15.0-52-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.1.3 pip: 22.2.2 setuptools: 62.3.4 numpy: 1.23.3 scipy: 1.9.1 Cython: None pandas: 1.5.0 matplotlib: 3.6.0 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libgomp filepath: \/home\/usopp\/sinp\/mineralogy-2022\/main\/venv\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None num_threads: 12 user_api: blas internal_api: openblas prefix: libopenblas file...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24846"
  },
  {
    "number":27741,
    "text":"Make an instance of ColumnTransformer pass the common test\n\n### Describe the bug Not sure if CODE] is appropriate to use with transformers, but [CODE] inherits from [CODE], so I'm assuming so. I have a custom transformer that inherits from [CODE], and I'm trying to use [CODE] to make sure I am doing so properly. However, [CODE] itself fails on multiple tests; I'm assuming many are because creating a non-trivial [CODE] involves specifying required feature names, which seems to go against some of the tests in [CODE]. Is there a different best practice to test a custom subclass of [CODE] and\/or transformers in general? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results All tests pass. ### Actual Results [Test Results \u2014.pdf [Clang 15.0.7 ] executable: \/my_env\/bin\/python machine: macOS-13.4-arm64-arm-64bit Python dependencies: sklearn: 1.3.1 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.26.0 scipy: 1.11.3 Cython: None pandas: 2.1.1 matplotlib: 3.8.0 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/my_env\/lib\/libopenblas.0.dylib version: 0.3.24 threading_layer: openmp architecture: VORTEX ...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27741"
  },
  {
    "number":24708,
    "text":"Create a classification report using a confusion matrix as input\n\n### Describe the workflow you want to enable I had some results stored in the form of a confusion matrix for some experiments. However, I wanted to use [CODE] to easily compute all the metrics. ### Describe your proposed solution My solution consists of creating new [CODE] and [CODE] vectors and using those to then call [CODE]. [CODE_BLOCK] ### Describe alternatives you've considered, if relevant I first tried using lists and appending values, but the [CODE] version performs faster, although slightly less readable: [CODE_BLOCK] ### Additional context I was comparing two approaches:  Two separate models, each for different tasks  One single model incorporating both tasks Two compare them, I built a confusion matrix of the two separate models and concatenated them so that the final confusion matrix would be similar to the single model. I used this function to generate the classification report of the two separate models after concatenating their confusion matrices. In the case of the single model, I could use [CODE] directly.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24708"
  },
  {
    "number":25923,
    "text":"average_precision_score() and roc_auc_score() require a full list of arguments\n\n### Describe the bug These two functions, namely average_precision_score() and roc_auc_score() require a full list of arguments. Otherwise, it reports errors! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25923"
  },
  {
    "number":28488,
    "text":"Improve \"polars\" integration (error, warning & linting examples)\n\n### Describe the workflow you want to enable using polars data (DataFrame, Series) is already supported in many places which is awesome, thank you!! But in many places there are still - errors \/ crashes -> required conversion to numpy\/pandas - warnings -> requires conversion to numpy\/pandas - linting\/type problems -> requires updates to typing signalture? # Examples ## Code [CODE_BLOCK] ## Errors \/ crashes using polars Both [CODE] and [CODE] crash using polars Series with message: - [CODE]__getitem__[CODE] ### Temporary solution use - [CODE] -> works with numpy array - [CODE] -> works with pandas Series ## Warnings [CODE] creates \"UserWarnings\" using polars DataFrame with message: - [CODE] ### Temporary Solution use - [CODE] -> works with pandas DataFrame ## Linting \/ Type problems numpy arrays and pandas DataFrame have \"full support\" while polars looks a little sad \ud83d\ude06 ![image]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28488"
  },
  {
    "number":24679,
    "text":"correct SAGA weight update with prox\n\nIt seems that saga weights are being updated here with saga bit after the prox operator has been applied to SAG update [URL] Should the prox operator be applied again here after SAGA weight update? Looks like it is what is happeng in lightning [implementation]([URL]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24679"
  },
  {
    "number":27543,
    "text":"Handling 'category' for LightGBM models\n\n### Describe the bug We should be able to convert some columns in the type 'category' in a DataFrame and let the LightGBM model handle it by itself. ### Steps\/Code to Reproduce ```python import pandas as pd import numpy as np from lightgbm import LGBMClassifier from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import FunctionTransformer # Load data url = \"[URL] data = pd.read_csv(url) # Drop Name and other non-numeric non-categorical columns data = data.drop(columns=['Name']) # Define a transformer to convert specific columns to category type def convert_to_category(X): categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist() X[categorical_cols] = X[categorical_cols].astype('category') return X # Train\/test split X_train, X_test, y_train, y_test = train_test_split( data.drop(\"Survived\", axis=1), data[\"Survived\"], random_state=42 ) # Preprocessing for numerical data: standardization and missing value imputation numerical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler()) ]) # We create the preprocessor stage of final pipeline # Each transformer is a three-element tuple # (name, transformer, columns) prep...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27543"
  },
  {
    "number":27609,
    "text":"Add a version of [CODE]\/[CODE]\/[CODE] that allows input of X with missing values and [CODE].\n\n### Describe the workflow you want to enable - Select features by the percentage of missing values of X - Select features only by statistical properties of X before y is available ### Describe your proposed solution Create a version with weaker X, y checks. ### Describe alternatives you've considered, if relevant 1. [CODE_BLOCK] Not sure if this code works properly. 2. Bother to create an estimator that outputs the desired function in [CODE] and use [CODE] ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27609"
  },
  {
    "number":26438,
    "text":"RFC drop support for python 3.8 ? bump min dependencies ?\n\nThe 1.3 release being scheduled for the first half of june, it's time to think about bumping the min dependencies. Up to now we've been trying to roughly follow NEP29 according to [oldest-supported-numpy]([URL] Scipy min version has already been bumped in [URL] Pytest has also already been bumped in [URL] Joblib min version is just 1 release before the latest so we probably don't want to bump it yet. Any objection or request ?",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26438"
  },
  {
    "number":24577,
    "text":"[RFC] Modularize 'Criterion' class inside tree submodule to allow 3rd party extensions for data-based criterion\n\n### Summary There exists other criterions, where additional arguments would need to be passed in. The current [CODE] class is not actually a generic Criterion class since it explicitly assumes that the Criterion is supervised-learning based and does NOT require the data [CODE] itself. The [CODE] function explicitly passes in [CODE]. I am trying to implement and subclass [CODE] to implement Unsupervised Random Forests ([URL] which use the data within the Criterion. I would like to subclass the [CODE] class for my own [CODE]. However, the [CODE] function assumes access to a [CODE] label, but everything else about it is fine and correct. Instead I would like to define my own [CODE] function, but keep the rest of the Criterion API. This way, I can define an unsupervised criterion with the [CODE] function: [CODE_BLOCK] and I could then define the rest of the functions, while making sure I am in strict compliance with sklearn's Tree submodule. ### Proposed Solution Add a class called [CODE] and migrate all functions\/properties of the current [CODE] class, EXCEPT for [CODE] function to [CODE]. This is completely backwards-compatable and ensures 3rd party API can leverage the existing API contract for Criterions using the [CODE] class. ### Alternatives Alternatively, I can do what [CODE] does and pass in this additional data to [CODE], but this is weird because you basical...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24577"
  },
  {
    "number":29285,
    "text":"Drop Duplicates from Data Before Fitting Estimator in RFE\n\n### Describe the workflow you want to enable Currently, RFE makes a slice of the data that only includes specific features. Then, RFE fits the provided estimator to that slice. Making a slice of the data reduces the number of possible variations that can be found in the dataset, potentially leading to many duplicates being found in the dataset. Duplicates become highly prevalent when RFE needs to select a small number of features. Using a dataset with many duplicates to fit an estimator can result in misleading feature importance, bias towards the duplicated data points, and increased training time. My proposal: Drop duplicates from array slice before fitting the provided estimator in RFE so that the model doesn't overfit and provide misleading metrics. ### Describe your proposed solution Previous: [CODE_BLOCK] Proposed Solution (Using Pandas): [CODE_BLOCK] Proposed Solution (Using Numpy): [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29285"
  },
  {
    "number":31883,
    "text":"Fitting different instances of [CODE] is not thread-safe\n\n### Describe the bug Found while working on #30041. See the reproducer below. Fitting [CODE] probably relies on a shared global state in the C++ code and that introduces a race condition when fitting several models concurrently in different threads. As a result, the outcomes are randomly corrupted. [CODE] does not seem to have the problem (or at least not with its default solver). ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Nothing. ### Actual Results ```python --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) Cell In[22], line 32 23 sequential_results = [ 24 model_class(C=C, random_state=0).fit(X, y).coef_.copy() for C in C_range 25 ] 28 parallel_results = Parallel(n_jobs=4, backend=\"threading\")( 29 delayed(lambda C: model_class(C=C, random_state=0).fit(X, y).coef_.copy())(C) 30 for C in C_range 31 ) ---> 32 np.testing.assert_array_equal( 33 sequential_results, ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31883"
  },
  {
    "number":26906,
    "text":"DOC Remove some links from the list of related packages\n\nFrom the list [URL] I propose to remove - svmlight-loader Reason: Not installable via [CODE], not license - Neptune because it is not really usable without an account. Was added in #20767. - rep Reason: outdated (last commit in 2016), requires [CODE] - nolearn Reason: outdated (last commit in 2019), requires [CODE] - Lasagne: outdated - Kernel Regression Reason: outdated - LibOPF Reason: not installable via [CODE] - Spotlight Reason: not installable via [CODE] or [CODE] (only some non-standard channel) - MSMBuilder Reason: [github repo]([URL] has been archived Remarks: - mlxtend is listed twice - LightGBM should be listed BTW: We should set some standard as to what we require to be listed.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26906"
  },
  {
    "number":27236,
    "text":"BisectingKmeans - intertia per cluster\n\n### Describe the bug Hi, I have been using the sklearn package recently for some simple clustering. It appears to me that there is a typo in the BisectingKMeans class. In the [CODE] method, we need to compute the inertia per-cluster. However, in the current version of the package we have the following: [CODE_BLOCK] This will through an error unless the centers form a squared matrix, i.e. if we have the same number of clusters as we have scalar features... In order to solve this, we could simply replace the line 276 in [the corresponding file]([URL] with: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results no error should be thrown ### Actual Results IndexError: index 1 is out of bounds for axis 0 with size 1 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27236"
  },
  {
    "number":27749,
    "text":"Add handle_missing and handle_unknown options to TargetEncoder\n\n### Describe the workflow you want to enable This issue has a similar proposition then #17123, in which is discussed the addition of [CODE] and [CODE] from [CODE] in [CODE] library. The justification for it is that we should allow users to treat the NaN values as they wish, whether by imputation, algorithms robust to missing data or by any another approach. To see if this change can have a positive impact on modeling, I decided to run a Cross Validation in order to compare the performance of [CODE] allowing to return NaN values \u200b\u200band allowing to return a value (target mean). ```python from ucimlrepo import fetch_ucirepo from sklearn.ensemble import BaggingClassifier from sklearn.pipeline import Pipeline from category_encoders import TargetEncoder from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score, StratifiedKFold import matplotlib.pyplot as plt #%% adult = fetch_ucirepo(id=2) X = adult.data.features y = adult.data.targets y = y['income'].str.contains('>50K') # %% kf = StratifiedKFold(5, shuffle = True, random_state = 101) return_nan_clf = Pipeline([ ('encoder', TargetEncoder(handle_unknown='return_nan', handle_missing='return_nan')), ('classifier', BaggingClassifier( DecisionTreeClassifier( max_features = 'sqrt' ), n_estimators=100, )) ]) mean_clf = Pipeline([ ('encoder', TargetEncoder(handle_unknown='value', handle_missing='value')), ('classifier', BaggingClassifi...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27749"
  },
  {
    "number":29464,
    "text":"Boundary value problem of [CODE] and a suggestion on the document\n\n### Describe the issue linked to the documentation Hi devs of scikit-learn, I found a potential slight boundary value problem in [[CODE]]([URL] The code here is looks like: [CODE_BLOCK] The boundary value judgment of [CODE] is incomplete because 0 is not considered, although [CODE] usually takes a very small value, such as 1e-4. By the way, the related document of [[CODE]]([URL] can also be improved. In [CODE] part, the document only claims that [CODE] can be negative when removing features using [CODE]. Why not add the information in the error message to the document? This will help users better understand the relationship between [CODE] and [CODE], especially for newcomers. ### Suggest a potential alternative\/fix Maybe you can add \"tol must be positive when doing forward selection\" in the documentation and change [CODE] to [CODE] in if branch.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29464"
  },
  {
    "number":26537,
    "text":"ValueError: The covariance matrix of the support data is equal to 0 - Elliptic Envelope\n\n### Describe the bug I have been using Elliptic Envelope on a simple time series dataset with default parameters and only setting [CODE] value to [CODE]. However it throws me error like below: [CODE] Looking at the error, I referred to the documentation and the equation of [CODE] does not seem to be clear. It sayd -> [CODE]. [CODE]. The value of [CODE] and [CODE] can never be such that it will fall within 0 and 1. Could you please help in understanding and how this issue can be resolved? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results ``` File \/opt\/anaconda3\/envs\/benchmark\/lib\/python3.9\/site-packages\/sklearn\/covariance\/_elliptic_envelope.py:145, in EllipticEnvelope.fit(self, X, y) 134 def fit(self, X, y=None): 135 \"\"\"Fit the EllipticEnvelope model. 136 137 Parameters (...) 143 Not used, present for API consistency by convention. 144 \"\"\" --> 145 super().fit(X) 146 self.offset_ = np.percentile(-self.dist_, 100. * self.contamination) 147 return self File \/opt\/anaconda3\/envs\/benchmark\/lib\/python3.9\/site-packages\/sklearn\/covariance\/_robust_covariance.py:668, in MinCovDet.fit(self, X, y) 666 self.dist_ = raw_dist 667 # obtain consistency at normal models --> 668 self.correct_covariance(X) 669 # re-weight estimator 670 self.reweight_covariance(X) File \/opt\/anaconda3\/envs\/benchmark\/lib\/python3.9\/site-packages\/sklearn\/covariance\/_robust_covarianc...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26537"
  },
  {
    "number":26164,
    "text":"LinearRegression with zero sample_weights is not the same as excluding those rows\n\n### Describe the bug Excluding rows having [CODE] in [CODE] does not give the same results. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Always [CODE]. ### Actual Results [CODE_BLOCK] The print statement gives: [CODE_BLOCK] ### Versions [CODE_BLOCK] ```",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26164"
  },
  {
    "number":27117,
    "text":"Add sample_weight support to binning in HGBT\n\nUse CODE] in the binning of [CODE] and [CODE], or allow it via an option. Currently, sample weights are ignored in the [CODE]. Some more context and history summarized by @NicolasHug [here. Reading back the original PR, this was discussed extensively: > > - LightGBM implem doesn't take weights into account when binning ENH Support sample weights in HGBT #14696 (comment)](URL] Olivier and Andy seemed to be happy with it [ENH Support sample weights in HGBT #14696 (comment)]([URL] may be relevant here: it's not super clear to me how we should handle SW in an estimator that performs some sort of subsambling during the training process (as is the case here during Binning).",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27117"
  },
  {
    "number":28859,
    "text":"[CODE] fails if custom estimator implements [CODE]\n\n### Describe the bug Title. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The tests should run. ### Actual Results Parametrizing the tests fails, because [CODE] thinks it's a function tries to look up [CODE], which doesn't exist. ### Versions ```shell System: python: 3.11.6 (main, Oct 16 2023, 19:37:59) [GCC 11.4.0] executable: \/home\/rscholz\/Projects\/KIWI\/tsdm\/.venv\/bin\/python machine: Linux-6.5.0-27-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.4.2 pip: 24.0 setuptools: 69.5.1 numpy: 1.26.4 scipy: 1.13.0 Cython: None pandas: 2.2.2 matplotlib: 3.8.4 joblib: 1.4.0 threadpoolctl: 3.4.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 24 prefix: libgomp filepath: \/home\/rscholz\/Projects\/KIWI\/tsdm\/.venv\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None user_api: blas internal_api: openblas num_threads: 24 prefix: libopenblas filepath: \/home\/rscholz\/Projects\/KIWI\/tsdm\/.venv\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: Zen user_api: blas internal_api: openblas num_threads: 24 prefix: libope...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28859"
  },
  {
    "number":29073,
    "text":"Sphinx search summary disappeared from 1.5 website\n\nLooks like the sphinx search summary has gone from 1.5 website. Not crucial, but showing the context of the match is handy to decide which link is more likely to have the information we want when we use the doc search bar. Maybe due to pydata-sphinx-theme switch, maybe something else ... More context: [URL] Screenshot for 1.4 [URL] ![image]([URL] Screenshot for 1.5 [URL] ![image]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29073"
  },
  {
    "number":29364,
    "text":"Sponsors page: update with CZI \/ Wellcome Trust 2024 grant\n\n### Describe the issue linked to the documentation [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29364"
  },
  {
    "number":25031,
    "text":"MLP Classifier\n\n### Describe the issue linked to the documentation in the argument of MLP: hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,) The ith element represents the number of neurons in the ith hidden layer. we are allowed to input a tuple (a,b) for hidden_layer_sizes. But the meaning of a, b is not so clear in the document. i think it could be more specified like what is a, b is representing. depth? neurons in each layer? ### Suggest a potential alternative\/fix i think it could be more specified like what is a, b is representing. depth? neurons in each layer?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25031"
  },
  {
    "number":27427,
    "text":"[Array API] [CODE] uses [CODE] rather than [CODE]\n\n### Describe the bug stable_cumsum Cell In[6], line 1 ----> 1 stable_cumsum(arr) File ~\/mambaforge\/envs\/torch_igpu\/lib\/python3.10\/site-packages\/sklearn\/utils\/extmath.py:1214, in stable_cumsum(arr, axis, rtol, atol) 1211 xp, _ = get_namespace(arr) 1213 out = xp.cumsum(arr, axis=axis, dtype=np.float64) -> 1214 expected = xp.sum(arr, axis=axis, dtype=np.float64) 1215 if not xp.all( 1216 xp.isclose( 1217 out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=True 1218 ) 1219 ): 1220 warnings.warn( 1221 ( 1222 \"cumsum was found to be unstable: \" (...) 1225 RuntimeWarning, 1226 ) File <__array_function__ internals>:200, in sum(*args, kwargs) File \/opt\/venv\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:2324, in sum(a, axis, dtype, out, keepdims, initial, where) 2321 return out 2322 return res -> 2324 return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims, 2325 initial=initial, where=where) File \/opt\/venv\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:82, in _wrapreduction(obj, ufunc, method, axis, dtype, out, kwargs) 78 else: 79 # This branch is needed for reductions like any which don't 80 # support a d...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27427"
  },
  {
    "number":28004,
    "text":"CI is broken due to pydantic update to v2.5.3\n\n### Describe the bug Hi, We are unable to run the CI. Before: URL] ![image: File \"\/usr\/share\/miniconda\/bin\/conda-lock\", line 6, in <module> from conda_lock import main File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/conda_lock\/__init__.py\", line 3, in <module> from conda_lock.conda_lock import main File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/conda_lock\/conda_lock.py\", line 50, in <module> from conda_lock.conda_solver import solve_conda File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/conda_lock\/conda_solver.py\", line 20, in <module> from conda_lock.invoke_conda import ( File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/conda_lock\/invoke_conda.py\", line 15, in <module> from conda_lock.models.channel import Channel File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/conda_lock\/models\/__init__.py\", line 1, in <module> from pydantic import BaseModel File \"\/usr\/share\/miniconda\/lib\/python3.11\/site-packages\/pydantic\/__init__.py\", line 372, in __getattr__ module = import_module(module_name, package=package) ^^^^^^^^^^^^^^^^^^^^^^^...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28004"
  },
  {
    "number":24574,
    "text":"KNearestNeighbors dedicated API for unsupervised anomaly detection\n\n### Describe the workflow you want to enable All unsupervised anomaly detection algorithms share a similar API (e.g., IsolationForest, OneSVM) but not KNearestNeighbors... And yet, KNN is one of the most used and robust method. ([URL] Having different API makes it challenging to loop and compare different anomaly detection algorithms... Example: [URL] (KNN is not present) Today, you can detect anomalies with KNN with the code below: [CODE_BLOCK] ### Describe your proposed solution The API I propose should look like this: [CODE_BLOCK] Only 3 lines of codes and the same API as the others (e.g., IsolationForest, OneSVM) ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24574"
  },
  {
    "number":25399,
    "text":"RandomForestClassifier allows float max_samples greater than 1 without raising exception\n\n### Describe the bug When using the RandomForestClassifier from scikit-learn, the model allows passing float values greater than 1 for the max_samples parameter without raising an exception, which is unexpected. I expected the code to raise an exception, but it did not. I am using scikit-learn version 1.0.2. For below code example, notebook runs just fine, and doesn't give me any exceptions. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results At least it can throw an exception just like for integer max_samples case. ### Actual Results Code works just fine, without any exceptions. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25399"
  },
  {
    "number":29075,
    "text":"Fix version warning banner on the stable documentation page\n\n### Describe the issue linked to the documentation Currently [URL] shows the version warning banner (\"This are the docs for an unstable version\"). <img width=\"1258\" alt=\"Screenshot 2024-05-22 at 09 08 02\" src=\"[URL] I think this is happening because we haven't updated [URL] which lists all the available versions and declares which is the stable version. This file is generated by [CODE] which should run as part of the CI on [CODE]. My guess as to why the file hasn't been updated is that we have not merged a PR since releasing v1.5.0. If I run the script locally it generates the correct content for the [CODE] file. ### Suggest a potential alternative\/fix I think the fix is to merge any PR into [CODE] as this will regenerate the [CODE] file. This is because all versions of the documentation read the versions from the same URL, which is based on [CODE].",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29075"
  },
  {
    "number":31169,
    "text":"How is the progress of sklearn1.7?[help wanted]\n\n### Describe the workflow you want to enable Excuse me, is sklearn1.7.dev0 available now? How to install it? ### Describe your proposed solution none ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31169"
  },
  {
    "number":28099,
    "text":"BUG \"array-like\" in parameter validation treats sparse containers as valid inputs\n\n### Describe the bug In parameter validation there are many places where we use [CODE] so I think at least the former should not be a superset of the latter, but it is the case now. Looking at the class [CODE], it treats the input as valid as long as the input has [CODE], [CODE], or [CODE] and is not a scaler. Clearly both sparse matrices and sparse arrays satisfy this condition, though I think they should be excluded. I propose adding the constraint [CODE] to [CODE]. For more context please see #27950 which tries to extend parameter validation to the new sparse arrays. <details> <summary>Also quoting the <a href=\"[URL] page<\/a><\/summary> <p><\/p> <img src=\"[URL] width=\"80%\" \/> <\/details> ### Steps\/Code to Reproduce [CODE_BLOCK] Another example can be [CODE], where the validation for [CODE] does not include [CODE] but tests such as [CODE] are passing even though [CODE] is sparse. ### Expected Results Both should raise [CODE]. ### Actual Results No error. ### Versions ```shell System: python: 3.9.18 | packaged by con...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28099"
  },
  {
    "number":24777,
    "text":"error message improvement\n\n### Describe the workflow you want to enable Hi, I think that this error message is misleading [URL] I am basically following the example here [URL] so I have the same [CODE] from that message, it seems that the problem is [CODE], while it is actually [CODE] what do you think? ### Describe your proposed solution you could for instance swap them: \" resource can only be 'n_samples' if max_resources='auto'\" ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24777"
  },
  {
    "number":26724,
    "text":"Array API support with lazy evaluation\n\nAt the moment, our Array API integration can implicitly assume eager evaluation semantics. Furthermore we did not test our code to see how it would behave with candidate Array API implementations with lazy evaluation semantics (e.g. dask, jax, ...). The purpose of this issue is to track what would be needed to make our Array API estimator work with lazy evaluation. I think we should first investigate where this breaks, then decide whether or not we would like to support lazy evaluation semantics in scikit-learn via the Array API support and if so open a meta-issue to add common test with an estimator tag and progressively fix the Array API estimators to deal with lazy evaluation. Note that are particular point about lazy vs eager evaluation for Array API consuming libraries is being discussed here: - [URL]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26724"
  },
  {
    "number":27016,
    "text":"PoissonRegressor lbfgs solver giving coefficients of 0 and Runtime Warning\n\n### Describe the bug See the following stack exchange When fitting a Poisson Regression (without regularization) to some dummy data I encounter: - with lbfgs solver, a Runtime Warning, a non-zero intercept and all coefficients as zero - with newton-cholesky solver, coefficients as expected Some people on StackExchange have mentioned it is worth submitting an issue (there was a similar one. ### Steps\/Code to Reproduce ```python import statsmodels.api as sm import statsmodels.formula.api as smf import pandas as pd from sklearn.linear_model import PoissonRegressor from sklearn.preprocessing import ( OneHotEncoder, ) data = sm.datasets.get_rdataset('Insurance', package='MASS').data # Fit Poisson regression using formula interface formula = \"Claims ~ C(District, Treatment(1)) + C(Group, Treatment('<1l')) + C(Age, Treatment('<25')) + Holders\" model_smf = smf.poisson(formula=formula, data=data).fit() print(type(model_smf)) print(model_smf.summary()) # with sklearn OneHotEncoder X_train_ohe = OneHotEncoder(sparse_output=False, drop=[1, \"<1l\", \"<25\"]).fit(data[[\"District\", \"Group\", \"Age\"]]) X_train_ohe = pd.DataFrame(X_train_ohe.transform(data[[\"District\", \"Group\", \"Age\"]]), columns=X_train_ohe.get_feature_names_out()) X_train = pd.concat([X_train_ohe, data[[\"Holders\"]]], axis=1) y_train = data[\"Claims\"] # one-hot encode the categorical columns, and drop the baseline column # with lbfgs solver model_sklearn_lb...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27016"
  },
  {
    "number":30225,
    "text":"Label Encoder example typo\n\n### Describe the issue linked to the documentation In the [Label Encoder]([URL] documentation, the example uses an array to demonstrate the functioning of label encoding. But the arrays used in fit and transform operations are different. For the fit operation, it uses [1,2,2,6] whereas for the transform operation it uses [1,1,2,6] resulting in inconsistency. Just attaching the screen grab of the example code in the documentation. ![Screenshot 2024-11-05 094129]([URL] ### Suggest a potential alternative\/fix Change to array in [CODE] to [1,1,2,6]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30225"
  },
  {
    "number":29480,
    "text":"Instantiate tunedthresholdCV on google collab failed\n\n### Describe the bug I updated scikit-learn version into google Collab and instantiate TunedThresholdClassifierCV by importing name 'TunedThresholdClassifierCV' from 'sklearn.model_selection' but I got this error [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results merely instantiation the new version of scikit learn for TunedThresholdClassifierCV object to not fail. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29480"
  },
  {
    "number":26265,
    "text":"sklearn.tree.export_text failing when feature_names supplied\n\nfolks, I'm not sure why this works for [CODE_BLOCK] but not for [CODE_BLOCK] Can anyone help?",
    "labels":[
      "Enhancement",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26265"
  },
  {
    "number":30382,
    "text":"Gaussian Mixture: Diagonal covariance vectors might contain unreasonably negative values when the input datatype is np.float32\n\n### Describe the bug The Gaussian Mixture implementation shows numerical instabilities on single-precision floating point input numbers, that even large values of the regularization parameter reg_covar (like 0.1) cannot mitigate. More specifically, diagonal covariance elements must not be negative. However, due to the numerical instabilities intrinsic to floating point arithmetic, they might end up being tiny negative numbers that reg_covar must compensate. It turns out that, for some input float32 , the covariance can reach the unreasonable value of -0.99999979. This is because squaring float32 numbers significantly magnifies their precision errors. The proposed solution consists in converting float32 values to float64 before squaring them. Care must be taken to not increase memory consumption in the overall process. Hence, as avgX_means is equal to avg_means2, the return value can be simplified. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ```python --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Input In [132], in <cell line: 49>() 45 skgm._estimate_gaussian_covariances_diag = _optimized_estimate_gaussian_covariances_diag 48 model = GaussianMixture(n_components=2,covariance_type=\"spherical\", reg_covar=0.1) ---> 49 model.fit(np.a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30382"
  },
  {
    "number":27429,
    "text":"Sphinx cross-referencing error \"reference target not found\" in nilearn doc build with sklearn 1.3.1\n\n### Describe the issue linked to the documentation The same Sphinx cross-referencing issue as [URL] (fixed in [URL] seems to have popped up again. Since we run our nilearn doc build in nitpicky mode this causes a failure. We can work around it by just ignoring this warning so this is not urgent but just FYI. <details> <summary>Relevant warnings from nilearn doc build<\/summary> <br> ```bash \/usr\/share\/miniconda3\/envs\/testenv\/lib\/python3.9\/site-packages\/nilearn\/connectome\/connectivity_matrices.py:docstring of sklearn.utils._metadata_requests._MetadataRequester.get_metadata_routing:11: WARNING: py:class reference target not found: sklearn.utils.metadata_routing.MetadataRequest \/usr\/share\/miniconda3\/envs\/testenv\/lib\/python3.9\/site-packages\/nilearn\/connectome\/group_sparse_cov.py:docstring of sklearn.utils._metadata_requests._MetadataRequester.get_metadata_routing:11: WARNING: py:class reference target not found: sklearn.utils.metadata_routing.MetadataRequest \/usr\/share\/miniconda3\/envs\/testenv\/lib\/python3.9\/site-packages\/nilearn\/connectome\/group_sparse_cov.py:docstring of sklearn.utils._metadata_requests._MetadataRequester.get_metadata_routing:11: WARNING: py:class reference target not found: sklearn.utils.metadata_routing.MetadataRequest \/usr\/share\/miniconda3\/envs\/testenv\/lib\/python3.9\/site-packages\/nilearn\/decoding\/decoder.py:docstring of sklearn.utils._metadata_requests._Metadata...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27429"
  },
  {
    "number":25944,
    "text":"Polynomial features min degree and max degree not working properly\n\n### Describe the bug I'm trying to use the PolynomialFeatures to generate 2nd order terms and exclude linear ones. According to the documentation, this should look like this: [CODE] But when I run the fit() method on this object it throws a ValueError (see below). ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect the method to fit the dataset without throwing a ValueError. The transform() method should then return a matrix with shape (3,7) - 3 data points and 7 features (3 2nd order and 1 bias). ### Actual Results [CODE_BLOCK] ### Versions ```she...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25944"
  },
  {
    "number":29479,
    "text":"Grid search with predetermined parameter order\n\n### Describe the workflow you want to enable I often have a pipeline with expensive preprocessing, and I tune hyperparameters both for preprocessing and downstream classifier. A toy example with PCA and logistic regression: [CODE_BLOCK] In this setting, preprocessing is recomputed as many times as there are grid elements. This is very wasteful, as for given preprocessing parameters, the params of the downstream classifier can be optimized. The order would then be: [CODE_BLOCK] For logistic regression, this behavior is actually possible by using [CODE]. However, e.g. for Random Forest, it is not (to the best of my knowledge). In fact, this can be always done for any pipeline with estimator at the end, since there can only be one classifier (or estimator in general). This would probably only be relevant to grid search and halving search. For randomized search, a cache for preprocessing output would achieve similar results. ### Describe your proposed solution Additional parameter or setting to grid search, optimizing parameter order. Note that this is not equivalent to pipeline caching: [URL] Caching only saves the fitting time, whereas here I propose caching the actual transformed output and reusing it for tuning classifier hyperparameters. [URL] mentions something about caching transform output, but I haven't found confirmation of this behavior anywhere. ### Describe alternatives you've considered, if relevant This ca...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29479"
  },
  {
    "number":29558,
    "text":"RFC Should cross-validation splitters validate that all classes are represented in each split?\n\nThis is a follow-up to the issue raised in [URL] However, I recall other issues raised for CV estimator in general. So the context is the following: a CV estimator will use an internal cross-validation scheme. When we deal with a classifier, we don't have any safety mechanism in the CV to make sure that the classifier was at least trained on all classes. This could happen for two reasons on the top of the head: (i) the target is sorted and the training folds does not contain all classes and (ii) a class is potentially underrepresented and not selected. In all cases, if the [CODE] does not fail, we still obtain a broken estimator. If it breaks at [CODE] at least this is not silently giving some wrong predictions but this is not a given. However, we don't provide a direct feedback to the user of what went wrong. So I'm wondering if we should have a sort of mechanism in the CV strategies to ensure that at least all classes have been observed at [CODE] time. I don't think that we should touch the estimators because we will repeat a lot of code and fundamentally the issue is raised because of the CV strategies. NB: the same issue could happen with a simple classifier in a cross-validation to evaluate it. This is not necessarily a CV estimator.",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29558"
  },
  {
    "number":28931,
    "text":"BUG internal indexing tools trigger error with pandas < 2.0.0\n\n[#28375]([URL] triggers errors for pandas < 2.0.0, despite just using scikit-learn internal functionalities. As documented in [URL] we have pandas >= 1.1.3.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28931"
  },
  {
    "number":25669,
    "text":"OrdinalEncoder inconsistent with None and [CODE] values\n\nOrdinalEncoder treats [CODE] and [CODE] differently: [CODE_BLOCK] In case 1, [CODE] is treated as a category and encoded. In case 2, [CODE] is passed through and not encoded. Note that, if [CODE] and [CODE] appear, then [CODE] gets encoded and [CODE] gets passed through: [CODE_BLOCK] 1. We can interpret this as is a bug with [CODE], which should encode [CODE] as [CODE] because the default [CODE]. Changing the behavior will break a lot of code because encoding [CODE] has been a feature before [CODE] was introduced 2. Functionally, I think it would be useful to configure [CODE] to be it's own category, such as [CODE], which will give [CODE] the same behavior as [CODE].",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25669"
  },
  {
    "number":28324,
    "text":"[Question, Documentation] Metadata Routing, indicate metadata is required by a method\n\n### Describe the issue linked to the documentation From my understanding, there is no way to specify that some metadata is required with CODE]. Doc: [URL] It is possible to specify that some method will error if it is provided, but no converse option to error if it is not provided. I've also read [SLEP006: estimator = DummyClassifier() # True, False, None, \"sample_weight\" can't indicate that this needs sample weights estimator.set_fit_request(sample_weight=...) custom_evaluator = CustomEvalutor(est...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28324"
  },
  {
    "number":29396,
    "text":"Array API tests fail on main\n\n### Describe the bug I ran the Array API tests on main and got 10 failing tests. (Last week, with an older main and everything else the same, I had 4 failing tests.) array_api_compat==1.7.1 I only ran the cpu tests. ### Steps\/Code to Reproduce [CODE] ### Expected Results all tests pass ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.2 (main, Apr 18 2024, 11:14:27) [GCC 13.2.1 20230801] executable: \/home\/stefanie\/.pyenv\/versions\/3.12.2\/envs\/scikit-learn_dev\/bin\/python machine: Linux-6.9.5-arch1-1-x86_64-with-glibc2.39 Python dependencies: skle...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29396"
  },
  {
    "number":29807,
    "text":"make_regression always generates positive coefficients\n\n### Describe the workflow you want to enable This is my first issue, please forgive the non-standard format. I noticed that when using make_regression to generate random data, I always get positive coefficients.I read the source code and found that this situation may be caused by this code. CODE_BLOCK] [make_regression_source ### Describe your proposed solution In order to be able to generate positive and negative coefficients, modification of the value range can be considered [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29807"
  },
  {
    "number":27711,
    "text":"BUG: Buffer dtype mismatch on Windows and NumPy 2.0\n\n### Describe the bug Recent Azure CI failure for MNE-Python but this is what's failing on CIs: [CODE_BLOCK] ### Expected Results No error ### Actual Results ``` ______________________________ test_search_light ______________________________ mne\\decoding\\search_light.py:101: in fit estimators = parallel( mne\\decoding\\search_light.py:102: in <genexpr> p_func(self.base_estimator, split, y, pb.subset(pb_idx), fit_params) mne\\decoding\\search_light.py:358: in _sl_fit est.fit(X[..., ii], y, fit_params) C:\\hostedtoolcache\\windows\\Python\\3.11.6\\x64\\Lib\\site-packages\\sklearn\\base.py:1215: in wrapper return fit_method(estimator, args, *kwargs) C:\\hostedtoolcache\\windows\\Python\\3.11.6\\x64\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:343: in fit return self._fit(X, y, self.max_samples, sample_weight=sample_weight) C:\\hostedtoolcache\\windows\\Python\\3.11.6\\x64\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:478: in _fit all_results = Parallel( C:\\hostedtoolcache\\windows\\Python\\3.11.6\\x64\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67: in __call__ return super().__call__(iterable_with_config) C:\\hostedtoolcache\\windows\\Python\\3.11.6\\x64\\Lib\\site-packages\\joblib\\parallel.py:1900: in __call__ return output if self.return_generator else list(output) C:\\hostedtoolcache\\windows\\Python\\3.11....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27711"
  },
  {
    "number":28631,
    "text":"HDBSCAN error with metric cosine\n\n### Describe the bug Inconsistent HDBSCAN behavior when given a metric that is not supported by KDTree or BallTree. docs [Clang 15.0.0 (clang-1500.1.0.2.5)] executable: \/opt\/homebrew\/opt\/python@3.11\/bin\/python3.11 machine: macOS-14.3.1-arm64-arm-64bit Python dependencies: sklearn: 1.4.1.post1 pip: 24.0 setuptools: 69.0.2 numpy: 1.26.4 scipy: 1.12.0 Cython: 0.29.37 pandas: 2.2.1 matplotlib: 3.8.3 joblib: 1.3.2 threadpoolctl: 3.3.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: \/opt\/homebrew\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib version: 0.3.23...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28631"
  },
  {
    "number":27518,
    "text":"Inconsistent results with same random seed\n\n### Describe the bug I'm not sure this is a bug and I couldn't find anything in the issues archive, but I'm seeing an inconsistency between consecutive runs of kmeans.fit, even when setting the same random seed via np.random.seed or random_state. I pinned it down to multithreading: if I force self._n_threads to 1 inside _kmeans.py\/fit the inconsistency goes away. See test code below. Is this expected behavior, and is there a way to eliminate multithreading during fitting? [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results min diff: 0.0, max diff: 0.0 min diff: 0.0, max diff: 0.0 min diff: 0.0, max diff: 0.0 min diff: 0.0, max diff: 0.0 min diff: 0.0, max diff: 0.0 min diff: 0.0, max diff: 0.0 ... ### Actual Results min diff: -1.1102230246251565e-16, max diff: 2.220446049250313e-16 min diff: -2.220446049250313e-16, max diff: 1.1102230246251565e-16 min diff: -1.1102230246251565e-16, max diff: 1.1102230246251565e-16 min diff: 0.0, max diff: 0.0 min diff: -1.1102230246251565e-16, max diff: 1.1102230246251565e-16 min diff: -1.110223024...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27518"
  },
  {
    "number":24537,
    "text":"Segmentation error when calling .fit()\n\n### Describe the bug Hey all, I'm currently busy working on a solution for a classification problem using LogisticRegression from sklearn.linear_model. I'm training multiple classifiers at the same time with the same hyperparameters and only slightly different input. The largest difference is coming from the number of unique labels. I don't have any issues fitting data on a model with size 1.500.000 x 20000 with ~200 unique labels. I do however run into issues when my training data has size 1.000.000 X 11000 with ~2500 labels. I'm executing these on docker containers but after calling .fit my containers exit and get an exit code of 139 indicating a segmentation error. I'm using the saga solver. As far as I know this is not a python error but underlying cython problem and my hunch is that because of my dimensions I'm getting a stack overflow somewhere. I've already checked out the source code but did not find any conclusive answers. My question is: does anyone have an idea where this actually goes wrong and if there is a workaround? I'm running scikit-learn==1.0.2 with Python 3.7.11 ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results Nothing in the traceback. My container exits with status code 139. When running this on Jupyter my kernel exits. ### Versions ```shell System: python: 3.7.11 (default, Jul 22 2021, 16:14:15) [GCC 6.3.0 20170516] executable: \/usr\/local\/bin\/python machine: Linux...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24537"
  },
  {
    "number":28317,
    "text":"[CODE] does not support [CODE] in v1.4.0\n\n### Describe the bug Fitting a CODE] where one of the features has a [CODE] dtype will give an error: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results Stacktrace suggests it's related to [CODE] getting support for categorical dtypes in v1.4.0 <details> <summary>stacktrace<\/summary> ``` File \/anaconda\/envs\/ds_data_schemas\/lib\/python3.10\/site-packages\/sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py:558, in BaseHistGradientBoosting.fit(self, X, y, sample_weight) [556 # time spent predicting X for gradient and hessians update 557 acc_prediction_time = 0.0 --> 558 X, known_categories = self._preprocess_X(X, reset=True) [559](file:\/\/\/anaconda\/envs\/ds_data_schemas\/lib\/python3.10\/site-packag...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28317"
  },
  {
    "number":28473,
    "text":"Suggesting updates on the doc of [CODE]\n\n### Describe the issue linked to the documentation Hi, We discover an inconsistency issue between documentation and code in the class [[CODE]]([URL] As mentioned in the description of parameter [CODE]. > func: function, default=None Function to apply to y before passing to fit. Cannot be set at the same time as transformer. The function needs to return a 2-dimensional array. If func is None, the function used will be the identity function. The most relevant piece of source code looks like this: [CODE_BLOCK] The error in the code mentioned that when [CODE] is provided, [CODE] must also be provided. The constraint is not mentioned in the document. Could you please check it? ### Suggest a potential alternative\/fix Maybe you can add the constraint into the document to avoid unnecessary misuse and extra debug efforts.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28473"
  },
  {
    "number":24906,
    "text":"Add __get_item__() to FeatureUnion\n\n### Describe the bug When debugging larger pipelines for a consistent syntax we\u2019d like to reference individual sub components of [CODE] by index, slice or tuple name in the same way as the [CODE]. This way user does not need to know the types of the aggregate components. ### Steps\/Code to Reproduce for example: [CODE_BLOCK] ### Expected Results would be accessed as: [CODE_BLOCK] ### Actual Results rather than accessing as: [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24906"
  },
  {
    "number":31073,
    "text":"ValueError: Only sparse matrices with 32-bit integer indices are accepted.\n\n### Describe the workflow you want to enable The use case that triggers the issue is very simple. I am trying to compute the n-gram features of a tokenized 1M dataset (i.e., from List[str] to List[int]) and then perform clustering on the dataset based on these features. [CODE_BLOCK] However, as the n-gram size or the dataset increases, it is easy to encounter the error shown in the title. ```bash Traceback (most recent call last): 13:50:00.018 File \"<frozen runpy>\", line 198, in _run_module_as_main 13:50:00.019 File \"<frozen runpy>\", line 88, in _run_code 13:50:00.019 File \"\/mnt\/dolphinfs\/hdd_pool\/docker\/user\/hadoop-aipnlp\/INS\/ruanjunhao04\/ruanjunhao\/ndp\/ndp\/marisa_onlyclm.py\", line 432, in <module> 13:50:00.022 main() 13:50:00.022 File \"\/mnt\/dolphinfs\/hdd_pool\/docker\/user\/hadoop-aipnlp\/INS\/ruanjunhao04\/ruanjunhao\/ndp\/ndp\/marisa_onlyclm.py\", line 404, in main 13:50:00.023 clustered_data = ngram_split(train_dataset, max_dataset_size) 13:50:00.023 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 13:50:00.023 File \"\/mnt\/dolphinfs\/hdd_pool\/docker\/user\/hadoop-aipnlp\/INS\/ruanjunhao04\/ruanjunhao\/ndp\/ndp\/marisa_onlyclm.py\", line 340, in ngram_split 13:50:00.024 kmeans.fit(X_tfidf) 13:50:00.024 File \"\/mnt\/dolphinfs\/hdd_pool\/docker\/user\/hadoop-aipnlp\/ruanjunhao04\/env\/rjh\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1473, in wrapper 13:50:00.032 return fit_method(estimator, args, *kwargs) 13:50:00.032 ^^^^^^^...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31073"
  },
  {
    "number":31930,
    "text":"Docs instructions for installing LLVM OpenMP with Homebrew may need updating\n\n### Describe the issue linked to the documentation Environment variables CFLAGS, CXXFLAGS, CXXFLAGS mentioned here: [URL] may be for Intel-based Macs only. So when trying to do this: [CODE_BLOCK] I got [CODE] The reason being that [CODE] installed [CODE] here: [CODE] and not here[CODE]. ### Suggest a potential alternative\/fix Modify the env variables that I mentioned above to the right path to [CODE] for M2 macs. Please note: - I'm not sure if the variables should be updated or have the two mac versions (Intel vs M1\/M2). - I didn't test that all works for an Intel mac. - Modifying the variables to the correct path, I was able to make the new environment.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31930"
  },
  {
    "number":24612,
    "text":"\u26a0\ufe0f CI failed on Wheel builder \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24612"
  },
  {
    "number":26456,
    "text":"Haversine distance documentation\n\n### Describe the issue linked to the documentation I would like to propose to update misleading formula for haversine distance: [URL] It is not clear from the page if (x1, y1) is the first vector or (x1, x2). Thus it might be misleading how to read x1, y1, x2, y2. [CODE_BLOCK] ### Suggest a potential alternative\/fix [CODE_BLOCK]",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26456"
  },
  {
    "number":28713,
    "text":"Node Splitting Proxy Improvement\n\n### Describe the issue linked to the documentation While exploring the splitter pyx file in the library's tree folder, I discovered this [current proxy improvement]([URL] which speeds up the process of finding the best split prior to calculating the actual impurity improvement of this best estimated split. I am wondering if there could be some explanation in the documentation for the surrogate proxy improvement. And if this was documented in a paper, perhaps? We were curious about it, and we apologise if we did not see anything in the document or elsewhere that discussed it. The documentation for the proxy improvement function does not appear to be \"thorough\". It would be greatly appreciated. [CODE_BLOCK] Cheers, ### Suggest a potential alternative\/fix Either in the code itself or in the documentation, explain briefly that the process of finding the best split differs slightly from the original idea, but that the actual outcome is expected to be the same given that the best estimated split receives the original impurity improvement equation calculated on its split. If interested, we could provide such a pull request, but we would appreciate clarification to see that we are in line with your intention and actual behaviour from what we understood. Cheers.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28713"
  },
  {
    "number":29301,
    "text":"Build wheels against Numpy 2 rather than numpy development version\n\nNow that Numpy 2 has been released, I think we don't need to build against numpy-dev (we are using all dev dependencies so scipy-dev as well), i.e. we can revert [URL] i.e. I think we can just remove code like this: [CODE_BLOCK] cc @seberg who did the original PR, in case there may be a reason we want to wait a bit after Numpy 2.0 release and check whether I am not missing anything subtle.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29301"
  },
  {
    "number":24636,
    "text":"Multi-class roc_auc_score raises error when y_true is not sampled with all label of classes\n\n### Describe the bug Sometimes we would like to train or validate a multi-class classification model without using large batch size or the term n_sample in scikit-learn but with too many number of classes n_classes. Let's say n_sample < n_classes. For the example below, CODE] and [CODE]. In deep learning, huge model with high resolution input could lead to this situation easily. The [condition [GCC 10.3.0] executable: \/opt\/conda\/bin\/python machine: Linux-4.15.0-171-generic-x86_64-with-glibc2.10 Python dependencies: sklearn: 1.1.2 pip: 22.2.2 setuptools: 59.5.0 numpy: 1.22.3 scipy: 1.6.3 Cython: 0.29.28 pandas: 1.3.5 matplotlib: 3.5.2 joblib: 1.1.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libgomp filepath: \/op...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24636"
  },
  {
    "number":25762,
    "text":"BayesSearchCV default cv is wrong?\n\n### Describe the issue linked to the documentation Hi, Is there a bug in the Docs for BayesSearchCV where it says that the default value for cv is 3 but I see it using 5 as the default when I run it with no explicit value for cv? [URL] I had a quick look in the code as below, the default in the code that is used seems to be 5. cheers, Rob. --- Versions: import sklearn import skopt print(sklearn.__version__) print(skopt.__version__) 1.0.2 0.9.0 --- Docs from: [URL] cv: int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, ---- [URL] def check_cv(cv=5, y=None, *, classifier=False): \"\"\"Input checker utility for building a cross-validator. --- ### Suggest a potential alternative\/fix Change the doc to say 5 fold rather than 3 fold.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25762"
  },
  {
    "number":26930,
    "text":"Importing [CODE] forks many processes\n\n### Describe the bug I was trying to investigate why I had a lot of processes being created when running a code of mine. After a lot of investigation, I noticed that just by importing CODE] was enough to see a lot of processes in [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Nothing like that should happen. ### Actual Results ![Screenshot_20230728_145623 [GCC 10.3.0] executable: \/home\/fernando\/phd_workspace\/future-shot\/venv\/bin\/python machine: Linux-5.15.120-1-MANJARO-x86_64-with-glibc2.37 Python dependencies: sklearn: 1.3.0 pip: 23.0 setuptools: 67.1.0 numpy: 1.24.2 scipy: 1.10.1 Cython: None pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: \/home\/fernando\/phd_workspace\/future-shot\/venv\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so version: 0.3.21 threading_layer: pthreads architecture: SkylakeX num_threads: 16 user_api: openmp internal_api: openmp prefix: libgomp filepath: \/home\/fernando\/phd_workspace\/future-shot\/venv\/lib\/python3.9\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None num_threads: 16 user_api: blas internal_api: openblas prefix: libopenblas filepath: \/home\/fernando\/phd_workspace\/future-shot\/venv\/lib\/python3.9\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so version: 0.3.18 threading_layer: pthreads architecture: P...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26930"
  },
  {
    "number":24840,
    "text":"OrdinalEncoder becomes slow in presence of numerous [CODE] values\n\n### Describe the bug I want to use ordinalencoder with a feature with ~10 categories, but >99% values nan. Execution time is very slow. ~4min for a 1e5 rows. But strangely enough, if the feature is not sparsed, then fitting time is ~1s Worth mention, that if you use [CODE] the problem with sparse cat feature doesn't happen. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results execution time for sparsed column is also ~1s ### Actual Results execution time for sparsed column is ~4min ### Versions ```shell System: python: 3.9.9 (tags\/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)] executable: C:\\--\\python.exe machine: Windows-10-10.0.19043-SP0 Python dependencies: sklearn: 1.1.3 pip: 22.3.1 setuptools: 65.4.1 numpy: 1.23.4 scipy: 1.9.3 Cython: None pandas: 1.5.1 matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: vcomp filep...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24840"
  },
  {
    "number":28983,
    "text":"Saving and loading calibratedclassifierCV model (ensemble)\n\n### Describe the bug Unable to load the saved calibratedclassifierCV model to a pickle file (.pkl) trained with cv=n as that is a list of models ### Steps\/Code to Reproduce calibratedclassifier.dump('model.pkl') model= pickle.load('model.pkl') ### Expected Results Expected results - model object loaded ### Actual Results Attribute error: _CalibratedClassifier has no attribute 'estimator'. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28983"
  },
  {
    "number":28059,
    "text":"ENH: Random Forest Classifier oob scaling\/parallel\n\nMy team, working on a bioinformatics problem with high feature count (columns\/dimensions in [CODE]), noticed that the [CODE] out of bag scoring doesn't scale with [CODE]. To be fair, [CODE] clearly says what it does support, though I do wonder if the out of bag predictions under the hood might also benefit from parallel support. Someone on my team seems to have found that it does help, but implemented externally to sklearn using the exposed base estimators. I suppose it might be nice to have that internally at some point, if there are no design reasons not to? Sample reproducer code with latest stable release ([CODE]) on 16 cores\/x86_64 Linux box ([CODE]) is below the fold, and the scaling plot is underneath that. We also use far more estimators and features than that, so the delta is much greater, but the scaling trend is the main observation in any case. <details> ```python from time import perf_counter import numpy as np # sklearn 1.3.2 from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification import matplotlib matplotlib.use(\"Agg\") import matplotlib.pyplot as plt timings_oob = [] timings_base = [] feature_counts = np.linspace(10, 180_000, 20, dtype=np.int64) for feature_count in feature_counts: X, y = make_classification(n_samples=1_000, n_features=feature_count, random_state=0) for use_oob, timing_list in zip([True, False], [timings_oob, timings_base]): start = perf_counter() c...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28059"
  },
  {
    "number":25632,
    "text":"\"sklearn.utils.estimator_checks.check_transformer_data_not_an_array\" prompts error\n\n### Describe the bug the function sklearn.utils.estimator_checks.check_transformer_data_not_an_array is trying to apply X.tolist(), where X is a pd.DataFrame. This object has no method called \"tolist\" hence it prompts the following error: AttributeError: 'DataFrame' object has no attribute 'tolist' This makes the test function \"check_estimator\" pretty useless since tests can't be completed successfully. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results expected test to pass ### Actual Results test fails due to - \"AttributeError: 'DataFrame' object has no attribute 'tolist'\" ### Versions ```shell System: python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)] executable: C:\\Users\\tomer\\anaconda3\\envs\\CorrActions\\python.exe machine: Windows-10-10.0.22621-SP0 Python dependencies: sklearn: 1.2.1 pip: 22.3.1 setuptools: 65.6.3 numpy: 1.23.0 scipy: 1.9.3 Cython: None pandas: 1.5.2 matplotlib: 3.6.2 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\tomer\\anaconda3\\envs\\CorrActions\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll version: 0.3.20 threading_layer: pthreads architecture: Haswell num_threads: 8 user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\Users\\tomer\\anaconda3\\...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25632"
  },
  {
    "number":25322,
    "text":"Increase minimum Cython version to 0.29.33\n\nRequire minimum Cython version >= 0.29.33 as from this version on Cython supports [CODE] fused types with memory views, see release notes [URL] and the related issue #10624.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25322"
  },
  {
    "number":29827,
    "text":"SimpleImputer does not drop a column full of [CODE] even when [CODE]\n\nThe following code snippet lead to some surprises: [CODE_BLOCK] [CODE_BLOCK] Apparently this is something that we really wanted for backward compatibility when merging [URL] [URL] Now, I'm wondering if we should not deprecate this behaviour since the parameter [CODE] allows to control whether or not we should drop the feature entirely. So I would propose to warn for a change of behaviour when [CODE], [CODE], and that we detect that we have empty feature(s).",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29827"
  },
  {
    "number":25239,
    "text":"ColumnTransformers don't honor set_config(transform_output=\"pandas\") when multiprocessing with n_jobs>1\n\n### Describe the bug I'm trying to do a grid search with [CODE], working with pandas output, and it fails despite [CODE] I have to manually [CODE] in the ColumnTransformer for it to work. ### Steps\/Code to Reproduce #### Preparation [CODE_BLOCK] This WORKS ([CODE]): [CODE_BLOCK] This FAILS ([CODE]): [CODE_BLOCK] This WORKS again ([CODE] and force output): ```python drop = make_column_transformer(('drop', [0]), remainder='passthrough').set_output(transform='pandas') pipe = mak...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25239"
  },
  {
    "number":27987,
    "text":"[CODE] Returns Zero When All Elements Are Same\n\n### Describe the bug When using MinMaxScaler.fit_transform() from scikit-learn, if all elements in a column of data are the same, the scaler transforms these elements to zeros. This behavior might not be intuitive or desired in some cases, as users might expect a different treatment for constant columns (e.g., transforming to ones or maintaining the constant value). If this behavior is expected, feel free to close this issue! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results It might be more intuitive if constant columns are scaled to a non-zero constant value (e.g., all ones) or maintain their original value. ### Actual Results The constant column ('A') is transformed to all zeros. [CODE_BLOCK] ### Versions ```shell System: python: 3.9.17 (main, Jul 5 2023, 20:41:20) [GCC 11.2.0] executable: \/home\/guihuan\/.conda\/envs\/py39\/bin\/python machine: Linux-5.15.0-86-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 68.0.0 numpy: 1.24.3 scipy: 1.11.2 Cython: None pandas: 2.1.0 matplotlib: 3.7.2 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 32 prefix: libopenblas filepath: \/home\/guihuan\/.conda\/envs\/py39\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so version: 0.3.21 threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 32 prefix: libg...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27987"
  },
  {
    "number":24340,
    "text":"BUG: GaussianProcessRegressor.predict inplace modifies input X, when passed via kernel\n\n### Describe the bug In line 425, 426 of \/sklearn\/gaussian_process\/_gpr.py (inside the predict method) y_var is modified in place: [CODE_BLOCK] If a kernel (e.g. a custom kernel) is used that returns X when diag(X) is called (X[:,0] == var == diag(X)) this leads to the modification of the original X vector. So now X == y_var - np.einsum(\"ij,ji->i\", V.T, V). A simple but dirty fix for me is to always make and return a copy, when building a custom kernel: var = np.copy(X[:,0]). But because this error is hard to find and debug, and the documentation does not state that one should make a copy, as well user expectation is that a method call won't modify the input, i suggest the following FIX: # FIX Just replace the in place modification : [CODE] with creation of a new array: [CODE] ### Steps\/Code to Reproduce ``` import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import Hyperparameter, Kernel min_x = 0 max_x = 50 std = 0.2 stop_time = 50 nr_plot_points = 20 number_of_train_points = 5 class MinT(Kernel): def __init__(self, sigma_0=1.0, sigma_0_bounds=(0.01, 10)): self.sigma_0 = sigma_0 self.sigma_0_bounds = sigma_0_bounds @property def hyperparameter_sigma_0(self): return Hyperparameter(\"sigma_0\", \"numeric\", self.sigma_0_bounds) def __call__(self, X, Y=None, eval_gradient=False): \"\"\"Return the kernel k(X, Y) and optionally its g...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24340"
  },
  {
    "number":26999,
    "text":"[CODE] in [CODE]\n\n### Describe the bug I accidentally stumbled onto a [CODE] when executing [CODE]. I hacked into [CODE] to save both the offending [CODE] as well as the randomly generated [CODE], then cut them down to minimal shape that still exhibits the error. This data is attached below in the MCVE. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No errors ### Actual Results ``` Traceback (most recent call last): File \"...\/rep_error.py\", line 14, in <module> sklearn.manifold.smacof(dis, init=init, normalized_stress=\"auto\", metric=False, n_init=1) File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/manifold\/_mds.py\", line 329, in smacof pos, stress, n_iter_ = _smacof_single( File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/manifold\/_mds.py\", line 128, in _smacof_single dis = euclidean_distances(X) File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/metrics\/pairwise.py\", line 310, in euclidean_distances X, Y = check_pairwise_arrays(X, Y) File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/metrics\/pairwise.py\", line 156, in check_pairwise_arrays X = Y = check_array( File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py\", line 959, in check_array _assert_all_finite( File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py\", line 124, ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26999"
  },
  {
    "number":31248,
    "text":"Hangs in LogisticRegression with high intercept_scaling number\n\n### Describe the bug When using the [CODE] model with the solver set to [CODE] and specifying the [CODE] parameter, the model hangs without any clear reason. The processing time does not increase gradually with the size of the [CODE] parameter. ### Steps\/Code to Reproduce When running on my machine, the code below complete in around 7 seconds. [CODE_BLOCK] However, increasing [CODE] by just one decimal place causes the model to hang indefinitely: [CODE_BLOCK] ### Expected Results I expect the code to finish running in a reasonable time. When [CODE] is set as 1.0e+77, the program finished in around 7 sec. ### Actual Results I terminated the process by after a day, no error trace was given by the program. [CODE_BLOCK] ### Versions ```shell System: p...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31248"
  },
  {
    "number":28979,
    "text":"Documentation says scikit-learn latest versions still supports Python 3.8\n\n### Describe the issue linked to the documentation [URL] \"Scikit-learn 1.1 and later requires Python 3.8 or newer\" The latest versions of scikit-learn require Python 3.9 or newer. ### Suggest a potential alternative\/fix Specify which versions support 3.8 and which version support 3.9 or newer.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28979"
  },
  {
    "number":30954,
    "text":"QDA is not reproducible\n\n### Describe the bug We are running QDA with default hyperparameters on the same dataset, on 2 different machines (linux). We find that the results change significantly when ran on a different machine. For more details, please see this Gist: [[URL] ### Steps\/Code to Reproduce Please see the gist: [URL] ### Expected Results Please see the gist: [URL] ### Actual Results Please see the gist: [URL] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30954"
  },
  {
    "number":29127,
    "text":"t-SNE Kernel Crash\n\n### Describe the bug TSNE [CODE] leads in rare cases to a Kernel crash. I was able to reproduce it locally on my Mac, on Google Colab as well as in a GitHub Actions pipeline (Ubuntu). The error message doesn't mention a specific reason: [CODE] I came across this while working on this pull request: [URL] ### Steps\/Code to Reproduce Link to Google Colab with a detailed reproducer: [URL] ```python import pandas as pd import numpy as np from sklearn.manifold import TSNE import sklearn sklearn.show_versions() # HashGNN embeddings (dict, 64 dimensions) of a very small Graph with only 4 nodes # Something about this data will cause the t-SNE algorithm to crash the Kernel hashGNN_embeddings = {'projectName': {0: 'react-router', 1: 'react-router-dom', 2: 'react-router-native', 3: 'react-router-dom', 4: 'router'}, 'embedding': {0: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.08253173...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29127"
  },
  {
    "number":26190,
    "text":"CHAT GPT response seems to be limited to some # of lines\n\n### Describe the bug I am new user working with Chat GPT for ab out 1 1\/2 months I am generally using this to help me write python programs. It has been quite useful but makes lots of mistakes. eventually it can find the right answer if you give it a few chances and allow it time to reply. can be frustrating and scary to trust the results. I am however finding with enough effort I can get good help from this AI tool. I notice a few things which must be bugs. 1) the reply in python window seems limited by the number of lines it reply's with this makes it very difficult to keep a coherent train of thought. Second since the reply's from Chat GPT is limited even for people who pay for service asking the same question over and over and getting a partial reply wastes tome for us both. 2) Yesterday I worked for several hours and the reply's coming back were not making any sense and suddenly it thought I wanted to write a program about tic tact toe and play a game, which I did not. 3) I have noticed that in the last week or so that the chat engine seems to loos track of our conversation and expects me to re upload data we have been working on for several hours. ### Steps\/Code to Reproduce None I could provide you with chat topic ### Expected Results Print the python code we are working on with out stopping in the middle and then when asking it to finishe or not to stop it starts from beginning and generally stops at a similar ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26190"
  },
  {
    "number":29332,
    "text":"Discussion: citation file\n\n### Describe the issue linked to the documentation Any thoughts to adding a citation.cff file to this repo? Here is an example from Seaborn: [URL] This example was shared in NASA Open Science training. ### GitHub [URL] ### Suggest a potential alternative\/fix",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29332"
  },
  {
    "number":30136,
    "text":"Webpage typo\n\n### Describe the issue linked to the documentation In the first part of the [About Page]([URL] it says \"Later that year, Matthieu Brucher started work on this project as part of his thesis.\" ### Suggest a potential alternative\/fix \"Later that year, Matthieu Brucher started working on this project as part of his thesis.\"",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30136"
  },
  {
    "number":29262,
    "text":"API rename force_all_finite into ensure_all_finite in check_array ?\n\n[CODE] has several parameters that just enable a check on a property of the array, like [CODE], [CODE], ... They have no effect on the output array: they just have the effect to raise an error or not. They usually have the naming pattern [CODE] which I think is intuitive and explicit. [CODE] is another example of such behavior but doesn't follow the same naming pattern. I think it should be renamed [CODE]. - it would make the current set of params more consistent, intuitive and self explanatory. - it would allow to add new params with the naming pattern [CODE], that have a different behavior e.g. have an effect on the output array, without bringing confusion. This is for instance the case in [URL] that proposes to add [CODE]. cc @thomasjpfan",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29262"
  },
  {
    "number":31705,
    "text":"EmpiricalCovariance user guide assume_centered tip incorrect\n\n### Describe the issue linked to the documentation The [user guide documentation]([URL] for EmpiricalCovariance currently states: > More precisely, if [CODE], then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and [CODE] should be used. It doesn't make sense, however, that [CODE] would require data to be centered. Likewise, it would seem that the user would need to center the data OR use [CODE] -- not both. Additionally, it doesn't seem like there are separate training and testing data for this. ### Suggest a potential alternative\/fix I think it should read: >More precisely, if [CODE], then the data set's mean vector should be zero. If not, the data should be centered by the user, or [CODE] should be used.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31705"
  },
  {
    "number":25580,
    "text":"Proposal to change default value of n_neighbors in mutual_info_regression\n\n### Describe the bug Hi, recently I figured out that for short sequences default value of 3 is way too unstable and gives poor results. Don't know the reasons why 3 was used, my testing shows that 2 is a far more appropriate choice for some unknown reason. Please change it to 2 to reduce users' frustration ) Or maybe someone can dig deeper and find out what's causing such discrepancy, something must be not quite right there. _mutual_info_classif_ probably needs to be changed, too. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results 10 samples perfect relationship with 4 n_neighbors: mi_min=0.2811, mi_max=0.5990, corrcoef=1.0000 10 samples zero relationship with 4 n_neighbors: mi_min=0.0000, mi_max=0.2262, corrcoef=0.2706 10 samples perfect relationship with 3 n_neighbors: mi_min=0.2823, mi_max=0.8006, corrcoef=1.0000 1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25580"
  },
  {
    "number":25711,
    "text":"SequentialFeatureSelector is not working with ColumnTransformer\n\n### Describe the bug Please see the code. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error ### Actual Results ```python-traceback Traceback (most recent call last): File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3460, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"\/tmp\/ipykernel_3433582\/3663298101.py\", line 1, in <module> sfs.fit(dummy_x, dummy_y) File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/feature_selection\/_sequential.py\", line 268, in fit new_feature_idx, new_score = self._get_best_new_feature_score( File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/feature_selection\/_sequential.py\", line 299, in _get_best_new_feature_score scores[feature_idx] = cross_val_score( File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/model_s...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25711"
  },
  {
    "number":28714,
    "text":"\"NameError: name 'functools' is not defined\" running [CODE] method for a GridSearchCV class\n\n### Describe the bug I upgraded scikit-learn to latest version yesterday and rerunning an ElasticNet demo I'm getting a puzzling error message. This code works without issue in a Jupyter notebook I had saved from months ago. Working within Spyder 5.3.3, Python 3.9 and with scikit-learn v1.4.1.post1, I get the below error when running the code included. Many thanks for anyone that can shed some light. ### Steps\/Code to Reproduce ``` import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import ElasticNet x = np.array([[0.26,0.02309,0.555556,0.625,0.449275,0,0,0,0,0.349315,0.254523,0.138267,0,0.338346,0.203283,0,0,0.25,0,0.375], [0.24,0.02309,0.777778,0.5,0.971014,0.95,0.11,0.369755,0,0.212329,0.41859,0.289157,0,0,0.243361,0.333333,0,0.5,0,0.25], [0.48,0.043198,0.666667,0.5,0.876812,0.733333,0.406875,0.512675,0,0.0590753,0.408921,0.28428,0.529298,0,0.503139,0.333333,0,0.5,0.5,0.375], [0.371036,0.0313632,0.666667,0.5,0.942029,0.883333,0,0,0,0.359589,0.262009,0.145152,0.42615,0,0.334621,0,0,0.5,0.5,0.375], [0.322747,0.0136343,0.555556,0.5,0.985507,0.966667,0.1575,0.420455,0,0.078339,0.357143,0.264487,0,0,0.222598,0.333333,0,0.25,0.5,0.125], [0.3,0.038795,0.333333,0.75,0.565217,0.75,0,0.193182,0,0.267551,0.332813,0.21027,0,0,0.176968,0,0,0.5,0,0.25], [0.45,0.0562715,0.555556,0.625,0.637681,0.166667,0.0825,0.028409...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28714"
  },
  {
    "number":30237,
    "text":"BUG: Test collection for Transformer fails\n\n### Describe the bug On latest CODE] wheel things were passing yesterday but [now we now get the following ... $ mne sys_info ... \u251c\u2611 sklearn 1.6.dev0 ... $ pytest -m 'not (ultraslowtest or pgtest)' --tb=short --cov=mne --cov-report xml --color=yes --junit-xml=junit-results.xml -vv mne\/ ============================= test session starts ============================== platform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- \/opt\/hostedtoolcache\/Python\/3.12.7\/x64\/bin\/python cachedir: .pytest_cache PyQt6 6.7.1 -- Qt runtime 6.7.3 -- Qt compiled 6.7.1 MNE 1.9.0.dev108+gcc0a15c0b -- \/home\/runner\/work\/mne-python\/mne-python\/mne rootdir: \/home\/runner\/work\/mne-python\/mne-python configfile: pyproject.toml plugins: timeout-2.3.1, qt-4.4.0, cov-6.0.0 collecting ... collected 4694 items \/ 1 error \/ 70 deselected \/ 5 skipped \/ 4624 selected ==================================== ERRORS ==================================== ___________ ERROR collecting mne\/decoding\/tests\/test_search_light.py ___________ \/opt\/hostedtoolcache\/Python\/3.12.7\/x64\/lib\/python3.12\/site-packages\/pluggy\/_hooks.p...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30237"
  },
  {
    "number":28534,
    "text":"Two different versions for weighted lorenz curve calculation in the examples\n\n### Describe the issue linked to the documentation There are 2 definitions of (weighted) CODE] functions [here: [CODE_BLOCK] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28534"
  },
  {
    "number":31572,
    "text":"Documentation improvement (LogisticRegression): display a note as a note\n\n### Describe the issue linked to the documentation A note in the description of the parameter [CODE] should be displayed as a note in [LogisticRegression]([URL] just as any other note. ![Image]([URL] ### Suggest a potential alternative\/fix Change the code [in this file]([URL] See example below [here]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31572"
  },
  {
    "number":29292,
    "text":"Add Python 3.13 development wheel\n\n### Describe the workflow you want to enable Based on #29280, building sklearn on Python 3.13 succeeds and running pytest also succeeds. I want to help enable building and uploading wheels supporting 3.13 on PyPI ### Describe your proposed solution Since I'm very new in looking at CI files for this project, find someone willing to explain how wheel-building is triggered on GH. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29292"
  },
  {
    "number":27028,
    "text":"Non-metric MDS gives nonsense results on the Digits dataset\n\n### Describe the bug I am running metric and nonmetric MDS on the Digits dataset, and obtain nonsense results with [CODE]. At the same time, my understanding is that non-metric MDS is more flexible and should not yield worse results compared to metric MDS. Is my understanding wrong, or does it suggest some problems with the non-metric MDS implementation? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Non-metric MDS giving something sensible. ### Actual Results ![digits-embed-mds-nonmetric]([URL] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27028"
  },
  {
    "number":31971,
    "text":"ValueError in PLSRegression.fit() with zero-variance predictor\n\n### Describe the bug Related: [URL] When fitting a PLSRegression model, if the input array X contains a feature with zero variance (i.e., a constant column), the fit method raises a ValueError: illegal value in 4th argument of internal gesdd. This results in a division by zero when a predictor has no variance, creating NaN values likely in the intermediate matrices. These NaN values are then passed to the SciPy function, which in turn calls the LAPACK gesdd routine for SVD, causing it to crash. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The model should either fit successfully (e.g., perhaps assigning a zero weight to the zero-variance feature) or raise a more informative ValueError indicating that a predictor has zero variance. ### Actual Results We get \"ValueError: illegal value in 4th argument of internal gesdd\" ```python \/home\/user\/.local\/lib\/python3.13\/site-packages\/sklearn\/cross_decomposition\/_pls.py:99: RuntimeWarning: invalid value encountered in divide y_weights = np.dot(Y.T, x_score) \/ np.dot(x_score.T, x_score) \/home\/user\/.local\/lib\/python3.13\/site-packages\/sklearn\/cross_decomposition\/_pls.py:368: RuntimeWarning: invalid value encountered in divide x_loadings = np.dot(x_scores, Xk) \/ np.dot(x_scores, x_scores) \/home\/user\/.local\/lib\/python3.13\/site-packages\/sklearn\/cross_decomposition\/_pls.py:377: RuntimeWarning: invalid value encountered in divide y_loadings = np.dot(x_scores, yk) \/ ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31971"
  },
  {
    "number":29591,
    "text":"BaggingRegressor with fit_params with CatBoostRegressor fit(..., eval_set= ())\n\n### Describe the issue linked to the documentation How can we use the new fit_params of the BaggingRegressor to add the eval_set of Catboost or LightGBM when calling the .fit() function ? The metadata routing documentation is incomplete about this ! ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29591"
  },
  {
    "number":27508,
    "text":"Mention that DBSCAN might modify precomputed sparse distance matrix\n\n### Describe the issue linked to the documentation [DBSCAN]([URL] provides a parameter called [CODE] which can be assigned the value [CODE] so that a precomputed distance matrix can be passed to the [CODE] method. > If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and must be square. X may be a [sparse graph]([URL] in which case only \u201cnonzero\u201d elements may be considered neighbors for DBSCAN. When a sparse matrix is passed to DBSCAN, it is modified [in-place.]([URL] If the sparse matrix already had diagonal elements present, this leads to no change in the matrix. However, if the diagonal elements are not present, then extra elements are added to the sparse matrix. In the documentation, this should be mentioned explicitly. ### Suggest a potential alternative\/fix For the [CODE] param - > If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and must be square. X may be a [sparse graph]([URL] in which case only \u201cnonzero\u201d elements may be considered neighbors for DBSCAN. Note that DBSCAN modifies the sparse matrix in-place by setting the diagonal of the sparse matrix.",
    "labels":[
      "Bug",
      "help wanted",
      "Documentation"
    ],
    "label_count":3,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27508"
  },
  {
    "number":25771,
    "text":"maximal information coefficient for feature selection\n\n### Describe the workflow you want to enable maximal information coeffcient ### Describe your proposed solution maximal information coeffcient ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25771"
  },
  {
    "number":31051,
    "text":"[CODE] causes crash or misattributed features\n\n### Describe the bug If all the following hold - Using ColumnTransformer with the output container set to pandas - At least one transformer transforms 1D inputs to 2D outputs (like DictVectorizer permutes the outputs of some feature transforms making the first data point have some features from the first data point and some features from the second data point. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The following features dataframe: ||dictvectorizer__bar|dictvectorizer__baz|dictvectorizer__foo|functiontransformer__dummy_col| |---|---|---|---|---| |0|2|0|1|1| |1|0|1|3|2| ### Actual Results A crash: ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[3], line 17 11 t = make_column_transformer( 12 (DictVectorizer(sparse=False), 'dict_col'), 13 (FunctionTransformer(), ['dummy_col']), 14 ) 15 t.set_ou...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31051"
  },
  {
    "number":29905,
    "text":"Training final model with cross validation and using it to get unbiased probabilities\n\n### Describe the workflow you want to enable I want to use crossvalidation with let's say k=4 in order to get four models. That means that each sample in my dataset was used to train 3 of the four models. Thus, if I want to get a prediction for a given sample, I need to use the one model that was not trained with that particular sample. However, when I train my models and I get the four pickle files, I cannot pick the right model in an out-of-the-box way. I.e. there is no way to tell which model was not trained with that specific sample. ### Describe your proposed solution The models could be loaded in some sort of model interface, whiich would take care of picking the right model for the prediction: [CODE_BLOCK] This would mean that the model would have to somehow know that a given feature was used to train it, like: [CODE_BLOCK] Which would mean that the model would get pretty large, unless each feature gets stored in a sort of short way within the model. E.g. storing some sort of hash for the whole training dataset as an attribute. ### Describe alternatives you've considered, if relevant I can implement this myself using derived classes, a hash check. However it would be good to have it done upstream, or maybe it already exists? ### Additional context The way I understand this is done in the real world is that cross validation is used to validate the model against overtraining. Once the ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29905"
  },
  {
    "number":25590,
    "text":"Importing BaseEstimator leads to unnecessary memory usage\n\n### Describe the bug Importing CODE] from [CODE] causes a cascade of imports that leads to unnecessary memory usage (500MiB of stuff at peak, see screenshot below). ![Screenshot from 2023-02-10 19-23-53 - scipy\/sparse\/\\_\\_init\\_\\_.py:283 -> [CODE] - scipy\/sparse\/csgraph\/\\_\\_init\\_\\_.py:185 -> [CODE] - scipy\/sparse\/csgraph\/_laplacian.py:7 -> [CODE] - scipy\/sparse\/linalg\/\\_\\_init\\_\\_.py:120 -> [CODE] - scipy\/sparse\/linalg\/_isolve\/\\_\\_init\\_\\_.py:4 -> [CODE] - scipy\/sparse\/linalg\/_isolve\/iterative.py:9 -> [CODE] I guess the issue I am reporting is that importing a simple estimator base class should not cause so many unrelated imports and spike memory usage by doing so. ### Versions ```shell System: python: 3.10.8 (main, Nov 1 2022, 14:18:21) [GCC 12.2.0] executable: \/home\/jjan\/dev\/sec-certs-page\/virt\/bin\/python machine: Linux-6.0.12-arch1-1-x86_64-with-glibc2.36 Python dependencies: sklearn: 1.2.1 pip: 23.0 setuptools: 61.2.0 numpy: 1.22.3 scipy: 1.10.0 Cython: None pandas: 1.4.2 matplotl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25590"
  },
  {
    "number":25252,
    "text":"ledoit_wolf_shrinkage is not documented but still publicly available\n\n### Describe the issue linked to the documentation [CODE] is not documented, but publicly available. See discussion in #24870 ([URL] ### Suggest a potential alternative\/fix It should be either documented or made private with deprecation",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25252"
  },
  {
    "number":28549,
    "text":"Make pipeline cache ignore parameter [CODE] of transformers\n\n### Describe the workflow you want to enable Introduction sklearn's pipeline caches the output of transformers in the pipeline. The caching is based on a hash of the arguments of function CODE]. Unfortunately, the hash changes when any of the transformer's parameters change, including those that don't affect the output, for example the [CODE] parameter (there could be other ones, perhaps [CODE]?). It would be a nice feature if there would be a way to indicate to the pipeline (or if the pipeline can detect it automatically) which parameters within the transformers to ignore for caching. Use case While developing, I tend to always set a high verbosity to understand what's happening under the hood. Once I am content with the results, I turn the verbosity off. At this point, the results are already calculated and cached, but need to be recalculated because the change of parameter. Examples of affected transformers  ColumnTransformer  RFE  RFECV  SparsePCA * IterativeImputer ### Describe your proposed solution The pipeline's caching is performed here: [URL] It uses joblib's [[CODE]. I couldn't find any trivial solution without monkey patching joblib's code or programmatically changing the [CODE] parameter in the transformers self (which would lead to unexpected results for the user, for example the lack of output messages). Happy to open a PR if anyone can think of a solution. ### Describe alternatives you've considered,...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28549"
  },
  {
    "number":26573,
    "text":"Improving the descriptions in decision tree structure example\n\n### Describe the issue linked to the documentation I am suggesting two improvements to the documentation of decision trees. This is motivated by constantly having to come back to the source code to try to understand what [CODE] is. I am unsure what do the element values inside the [CODE] 3D array store? I get that at the leaf IDs, they store the average predicted probabilities. What about on the non-leaf IDs? ### Sphinx example In [URL] there is a walk-through meant to educate users\/devs on the structure of a decision tree that stems from the Cython [CODE] class. Currently, it describes: - children_left[i]: id of the left child of node i or -1 if leaf node - children_right[i]: id of the right child of node i or -1 if leaf node - feature[i]: feature used for splitting node i - threshold[i]: threshold value at node i - n_node_samples[i]: the number of training samples reaching node i - impurity[i]: the impurity at node I However, there is value, missing_go_to_left and weighted_n_node_samples as well accessible as properties of [CODE]. ### Cython docstring Moreover, the docstring in the Cython Tree class is missing a description of the missing_go_to_left property. [URL] In addition, [CODE] seems to still be included even though it is not a property anymore? Should this still be in the Cython docstring? ### Suggest a potential alternative\/fix 1. Add descriptions to the sphinx example [CODE] for [CODE], [CODE], [CODE] ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26573"
  },
  {
    "number":29061,
    "text":"TunedThresholdClassifierCV: add other metrics\n\n### Describe the workflow you want to enable I figured that I might use the new tuned thresholder to turn code like this into something that's a bit more like gridsearch with all the parallism benefits. CODE_BLOCK] This data can give me a very pretty plot with a lot of information. ![CleanShot 2024-05-21 at 10 18 42, response_method=\"predict_proba\", thresholds=200, n_jobs=-1, store_cv_results=True ) classifier_other_threshold.fit(X_train,...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29061"
  },
  {
    "number":26280,
    "text":"KernelPCA inverse transform behaves unexpectly.\n\n### Describe the bug Hi! we wanted to use inverse transform in kernel PCA. There is a parameter gamma, which is defined as 1\/num_features if gamma=None. However if gamma is provided as gamma=1\/num_features, the resultant inverse transform results is different. Why is that? How the parameter gamma is related to inverse transform? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results True True True True ### Actual Results True True True False ### Versions ```shell System: python: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] executable: \/usr\/bin\/python3 machine: Linux-5.15.0-70-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 22.0.2 setuptools: 59.6.0 numpy: 1.21.5 scipy: 1.8.0 Cython: None pandas: 2.0.0 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: us...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26280"
  },
  {
    "number":25351,
    "text":"Missing visualization tool - sklearn-evaluation\n\n### Describe the issue linked to the documentation [This issue]([URL] was closed a while ago, removing some unmaintained tools. One of them was a tool to allow out-of-the-box visualizations. My suggestion is to add [sklearn-evaluation]([URL] allowing the users an easy mechanism to plot. We were originally inspired by your plotting API. ### Suggest a potential alternative\/fix Happy to submit a PR.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25351"
  },
  {
    "number":25750,
    "text":"Differences in scalar vs vectorized predictions with [CODE]\n\n### Describe the bug I would expect that calling [CODE] with a single X matrix, or with repeated scalar evaluations rows of X should would give nearly the same result, but they don't. An example is given below. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect the error to be 0 or small - perhaps around machine [CODE], if anything. ### Actual Results [CODE] ### Versions ```shell System: python: 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:54) [Clang 13.0.1 ] executable: \/Users\/phil\/miniconda3\/envs\/skl\/bin\/python machine: macOS-12.5.1-x86_64-i386-64bit Python dependencies: sklearn: 1.1.1 pip: 22.3 setuptools: 65.5.0 numpy: 1.23.4 scipy: 1.9.3 Cython: None pandas: 1.5.1 matplotlib: 3.6.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenbl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25750"
  },
  {
    "number":24675,
    "text":"suggestion to move (copy?) binder button for example to top or right side bar\n\n### Describe the issue linked to the documentation Firstly, thanks for having binder links for examples! In the examples such as [URL] the binder button is as the bottom of the page <img width=\"1469\" alt=\"Screen Shot 2022-10-15 at 11 16 30 PM\" src=\"[URL] It can sometimes take a while to scroll to get to the button. It would great if it could be accessed quicker with the binder link added to the top or to the right side bar. ### Suggest a potential alternative\/fix I would find it beneficial if the button was at the top (as well as the bottom?) or on the right side bar. This would make it quicker for me to run an example. If the example is long then it can take a bit of scrolling to get to the binder button. I've mocked up some proposals below. I'm think the button under the title (\"A demo on Spectal Biclustering algorithm\") would look like a binder button badge as the top of a README e.g. the shield.io format [URL] It's also worth noting that the box in the right isn't accessible and I was able to scroll up to access it I understand I don't want the badge\/link to stick out like a sore thumb and not distract people are simple interested in reading though the examples. ![Screen Shot 2022-10-15 at 11 24 46 PM2]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24675"
  },
  {
    "number":30641,
    "text":"docs: TimeSeriesSplit\n\n### Describe the issue linked to the documentation In the [TSS]([URL] documentation, it states that it [CODE]. Why at fixed intervals? I don't see anything in the documentation that would indicate where the time column is in the dataframe to enforce this. ### Suggest a potential alternative\/fix remove \"fixed time intervals\", and replace it by \"over time\".",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30641"
  },
  {
    "number":31319,
    "text":"Argument order in haversine_distances for latitude\/longitude\n\n### Describe the issue linked to the documentation Hello! I frequently use sklearn.metrics.pairwise.haversine_distances or (longitude, latitude) form. From context, one can infer that the correct order is (latitude,longitude); however, it would be useful to explicitly state the order. This is my first issue submission; please let me know if there is something more that might be useful for resolution! ### Suggest a potential alternative\/fix There are two ways that this could be resolved: 1. Include a short note stating that geographic coordinates should be input as (latitude,longitude) 2. Include a comment in the example code describing the coordinate order.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31319"
  },
  {
    "number":30664,
    "text":"UX [CODE]'s naive use can lead to very confusing results\n\nThe naive use of [CODE] parameter silently leads to degenerate, noisy results when some bins have with a few data points. For instance, look at the variability obtained by displaying for calibration curve of a fitted model evaluated on various resampling with 50 data points in total using the uniform strategy when using [CODE] and the default [CODE]: ![Image]([URL] <details> [CODE_BLOCK] <\/details> This problem can easily happen with the default [CODE] if the test data is not large enough. I think this class should warn the user whenever it generates bins with lower than 10 data points per bin. A typical user will only get one of the curves above and not suspect that it's just noise without manually plotting the others by random resampling. Note that I chose a minimal test set to make the problem catastrophic above, but it can happen with larger sample sizes, in particular with the uniform strategy, in particular on imbalanced datasets. ## Updated recommendations EDIT: based on the discussion below, here are my recomme...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30664"
  },
  {
    "number":31520,
    "text":"32-Bit Raspberry Pi OS Installation Issues with UV\n\n### Describe the bug When attempting to install scikit-learn==1.4.2 - 1.6.1 on Raspberry Pi OS Lite 32-Bit (Bookworm) or Raspberry Pi OS Lit 32-Bit (Bullseye) with UV, the following error is given: [CODE_BLOCK] If I had to guess, it's that the numpy==2.0.0rc1 is the issue, but I'm not sure. Bullseye is also on Python 3.9 so the last version we can install is v1.6.1. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expect that it should install correctly without errors. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31520"
  },
  {
    "number":29805,
    "text":"DOC: Add Bioconductor's package to the list of scikit-learn Related Projects\n\n### Describe the issue linked to the documentation ### Description Add Bioconductor's package to the list of scikit-learn Related Projects: [URL] [URL] --- Hello @vjcitn & @almahmoud, Would you like to submit the PR for this one? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29805"
  },
  {
    "number":26879,
    "text":"Cirrus CI usage limits in the future\n\nIn a recent CirrusCI annoucment - 178 credits in 07\/2023 Note that Apple Silicon CPU time is 8 times more expensive than Linux for the same CPU time: [reference]([URL] I think we'll likely need to stop running ARM CI on PRs and upload ARM wheels once a week.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26879"
  },
  {
    "number":29046,
    "text":"MAINT define a single time _estimator_has and refactor code\n\nFrom past discussion, I realized that we are defining the same [CODE] in several places while it does exactly the same job and has the same semantic. I think we should do a bit of cleaning by moving this function into a submodule in [CODE]. I would probably keep this function private for the moment even thought it could be useful for developers of third-party libraries. Maybe @StefanieSenger would be interested in leading the effort since you should be familiar with this function after working on #28167?",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29046"
  },
  {
    "number":26501,
    "text":"ModuleNotFoundError: No module named 'sklearn.ensemble._bagging'\n\n### Describe the bug ModuleNotFoundError: No module named 'sklearn.ensemble._bagging' Above error i am gettting while using python3.7 . Please provide me with the right version of scikit-learn .Here i am using scikit-learn =0.21.3 ### Steps\/Code to Reproduce File \"app.py\", line 23, in <module> from speakerDiarization import dia,label File \"\/home\/jrspy\/Voice-Sentiment-Analysis-20220617T123339Z-001\/Voice-Sentiment-Analysis\/speakerDiarization.py\", line 4, in <module> from filter import main_file,main File \"\/home\/jrspy\/Voice-Sentiment-Analysis-20220617T123339Z-001\/Voice-Sentiment-Analysis\/filter.py\", line 35, in <module> model_emotion = pickle.load(modelObj) ModuleNotFoundError: No module named 'sklearn.ensemble._bagging' ### Expected Results No error is thrown ### Actual Results no error needs to be there ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26501"
  },
  {
    "number":31849,
    "text":"Extend make file to inlcude initial setup installations.\n\n### Describe the workflow you want to enable I recently made my first contribution to sklearn and found it a bit tidious to do the initial setup after cloning the repo. I think that extending the make file to include something similar to [CODE] to install the dependencies in [step 4 of the Contributing > Contributing code > How to contribute]([URL] would be benefitial. Additionnaly adding a script ot run the git commands. I'd love to implement this so please, let me know if this is something of interest! ### Describe your proposed solution Extending the make file to include something similar to [CODE] to install the dependencies in [step 4 of the Contributing > Contributing code > How to contribute]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31849"
  },
  {
    "number":30324,
    "text":"Regression in SelectorMixin in 1.6.0rc1\n\n### Describe the bug Using the estimator tag CODE] doesn't work with [CODE] in the release candidate. A first skim suggests maybe [CODE] is inconsistently expected to be [CODE] and other times [CODE]? In particular at [URL] ? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown, and the numpy array is returned unchanged. ### Actual Results ``` ValueError Traceback (most recent call last) [<ipython-input-2-d8e360602655> 18 19 my_est = MyEstimator() ---> 20 my_est.fit_transform(np.array(5, 7, np.nan, 9]).reshape(2, 2)) 7 frames [\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/utils\/_set_output.py 317 @wraps(f) 318 def wrapped(self, X, args, kwargs): --> 319 data_to_wrap = f(self, X, args, kwargs) 320 if isinstance(data_to_wrap, tuple): 321 # only wrap the first output for cross decomposition \/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/base.py 857 if y is None: 858 # fit method of arity 1 (unsupervised transformation) --> 859 return self.fit(X, *fit_params).t...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30324"
  },
  {
    "number":30935,
    "text":"The default token pattern in CountVectorizer breaks Indic sentences into non-sensical tokens\n\n### Describe the bug The default [CODE] in [CODE] is [CODE] which tokenizes Indic texts in a wrong way - breaks whitespace tokenized words into multiple chunks and even omits several valid characters. The resulting vocabulary doesn't make any sense ! Is this the expected behaviour? Sample code is pasted in the sections below ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30935"
  },
  {
    "number":31503,
    "text":"HDBSCAN performance issues compared to original hdbscan implementation (likely because Boruvka algorithm is not implemented)\n\n### Describe the bug When switching from Sklearn HDBSCAN implementation to original one from CODE] library, I've notice that Sklearn's implementation has much worse implementation. I've tried investigating different parameters but it doesn't seem to have an effect on the performance. I've created synthetic benchmark using [CODE] function. And those are my results: CPU: Ryzen 5 1600, 12 Threads@3.6Ghz RAM: 32GB DDR4 [CODE_BLOCK] ![Image og_hdbscan = OGHDBSCAN(core_dist_n_jobs=-1) sk_hdbscan = SKHDBSCAN(n_jobs=-1) RUNS = 10 def time_hdbscan(hdbscan, X, runs): times = [] for _ in range(runs): start = time.time() hdbscan.fit(X) end = time.time() times.append(end - start) return times times_og = time_hdbscan(og_hdbscan, X, RUNS) times_sk = time_hdbscan(sk_hdbscan, X, RUNS) print(\"Mean time OGHDBSCAN: \", np.mean(times_og)) print(\"Mean time SKHDBSCAN: \", np.mean(times_sk)) plt.plot(range(RUNS), times_og, label='OGHDBSCAN', marker='o') plt.plot(range(RUNS), times_sk, label='SKHDBSCAN', marker='x') plt.xlabel('Run') plt.ylabel('Time (seconds)') plt.title('HDBSCAN Timing Comparison') plt.legend() plt.sh...",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31503"
  },
  {
    "number":27057,
    "text":"Case when y_test contains a single class and y_test == y_pred.\n\n### Describe the bug It shows wrong output when we call this function So all The issue related to precision, recall , F1_score, confusion_matrix , class_likelihood_ratios is trigged Case when y_test contains a single class and y_test == y_pred ### Steps\/Code to Reproduce y_true = np.array([1, 1]) y_pred = np.array([1, 1]) print(confusion_matrix(y_true, y_pred)) ### Expected Results [[2,0], [0,0]] ### Actual Results [[2]] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27057"
  },
  {
    "number":29607,
    "text":"MinMaxScaler is not array API compliant if clip=True\n\n### Describe the bug The CODE] is [listed with the non-standard [CODE] kwarg [here]([URL] While this particular fix may be simple, I am a bit unsure how to best test for array API compliance. The [CODE] package is only implemented up to version [CODE] of the standard which didn't include [CODE] yet. I actually discovered the bug using the lazy [[CODE]]([URL] package, but it does not serve well as a reproducer here since I also run into other, earlier, issues related to eager data validation. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expected that [CODE] would just work with a standard compliant implementation. ### Actual Results The [CODE] package is lagging behind and fails with the notice that `cli...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29607"
  },
  {
    "number":28605,
    "text":"TypeError: cpu_count() got an unexpected keyword argument 'only_physical_cores'\n\n### Describe the bug I am running the KNeighbordsClassifier inside a framework of pytorch_lightning. I am fitting the model correctly, but when I try to predict new results I have an error. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] executable: \/home\/myuser\/anaconda3\/envs\/pt12\/bin\/python machine: Linux-6.5.0-21-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.4.1.post1 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.26.4 scipy: 1.12.0 Cython: 3.0.8 pandas: 2.2.1 matplotlib: 3.8.3 joblib: 1.3.2 threadpoolctl: 3.3.0 Buil...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28605"
  },
  {
    "number":24401,
    "text":"RFC bump up dependencies for 1.2\n\nThis is an issue to discuss what will be the min versions of our dependencies for the 1.2 release, targeted in november. Here's the list of our current min versions [CODE_BLOCK] Following [NEP 29]([URL] we should keep support for python 3.8 for this release, and drop support for numpy 1.19. Looking at the `[CODE]` module, we could benefit from - threadpoolctl 3.0+ released on october 2021. - scipy 1.4+, released on december 2019, (related to [URL] Please share other needs here",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24401"
  },
  {
    "number":27092,
    "text":"AttributeError: 'LogisticRegression' object has no attribute 'feature_names_in_'\n\n### Describe the bug I created two model successfully (one being Decision Tree, and the other Logistic Regression). Those were exported as pickle [CODE] files. Because I used one-hot encoding in both models and I have a lot of categories I needed to get name of features. But weird thing is that I use [CODE] for both of them and for some reason I get this error [CODE] . On the other hand this works fine for Decision Tree. I also checked the documentation for [Logistic Regression]([URL] to double check if this even exists and it does. Also, I have checked version of [CODE] by running [CODE] and indeed I have confirmed latest [CODE] version. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27092"
  },
  {
    "number":29315,
    "text":"Using [CODE] keyword argument for NumPy randomness\n\nIn SPEC7 [URL] has two goals: 1. Deprecate the use of [CODE] and [CODE] 2. Standardize the usage of [CODE] for setting seeding. For 1, according to [NEP19]([URL] I do not think NumPy wants to deprecate [CODE] because they see valid use cases. For 2, the primary reason around using [CODE] instead of [CODE] is that it is a \"better name\" for NumPy's [Random Generator]([URL] I am okay with keeping [CODE] and not have users go the pain of changing their code. Currently, scikit-learn does not support generators because we tied it to [URL] We wanted to use generators to cleanly switch to a different RNG behavior compared to RandomState. For me, I think they can be decoupled. If we tackle [URL] we can fix it for both RandomState and Generators. @scikit-learn\/core-devs What do you think of SPEC7's proposal?",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29315"
  },
  {
    "number":26596,
    "text":"calibrated classifier cv estimator is not fitted\n\n### Describe the bug fitted estimator behaves as if it is unfitted if under CalibatedClassifier CV ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results feature importance ### Actual Results > NotFittedError: This GradientBoostingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26596"
  },
  {
    "number":27037,
    "text":"StandardScaler fit_transform() does not work with list as input data when output is configured to 'pandas'\n\n### Describe the bug I have a [CODE] configured to output pandas dataframes using the [CODE] api. When [CODE] is called with data of type [CODE], it throws an error as shown below. I found that this issue does not occur when the line [CODE] in the MWE below is commented out. On further digging and looking at the traceback, this bug occurs because the [CODE] argument in the line [CODE] of the traceback is actually the bound method [CODE] of list object X (which was passed as input to [CODE]), instead of a pandas [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results no error is thrown when [CODE] is called. ### Actual Results ``` Traceback (most recent call last): File \"\/home\/nihal\/issue_check.py\", line 16, in <module> my_stand_scale.fit_transform(X=X.tolist()) File \"\/home\/nihal\/scikit-learn\/sklearn\/utils\/_set_output.py\", line 140, in wrapped data_to_wrap = f(self, X, *args, kwargs) File \"\/home\/nihal\/scikit-learn\/sklearn\/base.py\", line 948, in fit_transform return self.fit(X, fit_params).transform(X) File \"\/home\/nihal\/scikit-learn\/sklearn\/utils\/_set_output.py\", line 153, in wrapped return _wrap_data_with_container(method, data_to_wrap, X, self) File \"\/home\/nihal\/scikit-learn\/sklearn\/utils\/_set_output.py\", line 128, in _wrap_data_with_container return _wrap_in_pandas_container( File \"\/home\/nihal\/scikit-learn\/sklearn\/utils\/_set_output.py\", line 60, in _wrap_...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27037"
  },
  {
    "number":24312,
    "text":"Coordinate with other scientific python projects to use shared tools and policy to upload nightly build wheels\n\nSee the motivation in this discussion: [URL] - Draft shared Github Actions config: [URL] - New github dedicated org\/repo: [URL]",
    "labels":[
      "help wanted",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24312"
  },
  {
    "number":30540,
    "text":"Failure generating a pdf of the documentations using make latexpdf\n\n### Describe the bug Hi sklearn team and fans, I am trying to generate a pdf of the documentations to be able to read\/use sklearn documentations offline. On multiple systems ranging from Macos (ARM or AMD processors) to Ubuntu, I am facing this issue and I am unable to troubleshoot it further: ``` Configuration error: There is a programmable error in your configuration file: Traceback (most recent call last): File \"\/opt\/homebrew\/Caskroom\/miniconda\/base\/envs\/sklearn_docs\/lib\/python3.10\/site-packages\/sphinx\/config.py\", line 509, in eval_config_file exec(code, namespace) # NoQA: S102 File \"\/Users\/myself\/Documents\/sklearn_docs\/scikit-learn\/doc\/conf.py\", line 22, in <module> from sklearn.externals._packaging.version import parse File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked File \"<frozen importlib._bootstrap>\", line 945, in _find_spec File \"\/opt\/homebrew\/Caskroom\/miniconda\/base\/envs\/sklearn_docs\/lib\/python3.10\/site-packages\/_scikit_learn_editable_loader.py\", line 311, in find_spec tree = self._rebuild() File \"\/opt\/homebrew\/Caskroom\/miniconda\/base\/envs\/sklearn_docs\/lib\/python3.10\/site-packages\/_scikit_learn_editable_loader.py\", line 345, in _rebuild subprocess.run(self._build_cmd, cwd=self._build_path, env=env, stdout=subprocess.DEVNULL, check=True) File \"\/opt\/homebrew\/Caskroom\/miniconda\/base\/envs\/sklearn_docs\/lib\/pytho...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30540"
  },
  {
    "number":27151,
    "text":"RFC remove some of our examples\n\nTLDR: I think we have too many examples, in particular in - [clustering]([URL] - [ensemble]([URL] - [generalized-linear-models]([URL] This is the worst. - [model-selection]([URL] - [nearest-neighbors]([URL] - [support-vector-machines]([URL] On top, several examples don't render a plot in the gallery, e.g. [Release Highlights 1.0.0]([URL] I would like to discuss how we can improve this situation. My main worry is that, currently, a user does not find what she\/he is looking for but gets overwhelmed by the sheer amount of examples. Edit: See task list in [URL]",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27151"
  },
  {
    "number":30817,
    "text":"sample_weight is silently ignored in LogisticRegressionCV.score when metadata routing is enabled\n\n### Describe the bug I'm not sure if it is a proper bug, or my lack of understanding of the metadata routing API ;) When [CODE], the [CODE] method of a [CODE] estimator will ignore [CODE]. [CODE_BLOCK] I found it surprising, because the [CODE] method works fine when [CODE], so the same piece of code behaves differently depending on the metadata routing config. [CODE_BLOCK] If I understood the metadata routing API correctly, to make the [CODE] method [CODE] aware we need to explicitly pass a scorer that request it: [CODE_BLOCK] If it's the intended behavior of the metadata routing API, maybe we should warn the user or raise an error in the first case, instead of silently ignoring [CODE] ? ### Steps\/Code to Reproduce ```python from sklearn import set_config from sklearn.metrics import make_scorer, accuracy_score from sklearn.linear_model import LogisticRegressionCV import numpy as np rng = np.random.RandomState(22) n_samples, n_features = 10, 4 X = rng.rand(n_samples, n_features) y = rng.randint(0, 2, size=n_samples) sw = rng.randint(0, 5, size=n_samples) set_config(enable_metadata_routing=True) logreg_cv = LogisticRegressionCV() logreg_cv.fit(X, y) # sample_weight is silently ignored in logreg_cv.score assert logreg_cv.score(X, y) == logreg_cv.score(X, y, sample_weight=sw) assert not logreg_cv.score(X...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30817"
  },
  {
    "number":27808,
    "text":"TransformedTargetRegressor with Early Stopping: transforming user-supplied validation sets in fit_params, too\n\n### Describe the workflow you want to enable Many advanced regressors (CatBoost, XGBoost, LightGBM to name a few) support providing custom early stopping dataset(s) to their fit methods. Not all of them have scalar eval_fraction parameters, like GradientBoostingRegressor; even if they would, sometimes there is really a need for custom validation splitting (time-series with daily grouping, for instance). Currently, TransformedTargetRegressor only transforms the y argument, which makes training with early stopping of above-mentioned regressors impossible: they do not converge, as I have just experienced. Or even if they converge, that can theoretically happen, validating on data in different scale can hardly find the same early stopping sweetspots. ### Describe your proposed solution It would be convenient to support an extra es_fit_param_name argument to the init method of TransformedTargetRegressor. When such param is present, at the fitting time, fit_params are inspected for it. [CODE_BLOCK] Actually, for some most popular libraries relevant es_fit_param_name are known and can be used as defaults, if es_fit_param_name is not provided by the user but params nevertheless exist (can be turned off with an extra no_es_autosuggest flag). But maybe going this deep is too much. ### Describe alternatives you've considered, if relevant Applying transformer or func\/inverse_fun...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27808"
  },
  {
    "number":25117,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly_PyPy.pypy3 \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly_PyPy.pypy3 Unable to find junit file. Please see link for details.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25117"
  },
  {
    "number":30917,
    "text":"DecisionTreeClassifier having unexpected behaviour with 'min_weight_fraction_leaf=0.5'\n\n### Describe the bug When fitting DecisionTreeClassifier on a duplicated sample set (i.e. each sample repeated by two), the result is not the same as when fitting on the original sample set. This only happens for 'min_weight_fraction_leaf' specified as <0.5. This also effects ExtraTreesClassifier and ExtraTreeClassifier. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results p-values are more than \u02dc0.05 ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ] executable: \/Users\/shrutinath\/micromamba\/envs\/scikit-learn\/bin\/python machine: macOS-14.3-arm64-arm-64bit Python dependencies: sklearn: 1.7.dev0 pip: 24.0 setuptools: 75.8.0 numpy: 2.0.0 scipy: 1.14.0 Cython: 3.0.10 pandas: 2.2.2 matplotlib: 3.9.0 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30917"
  },
  {
    "number":25058,
    "text":"[CODE] to allow for partial validation in validation framework\n\nIn some PRs for adding validation to functions: [URL] or [URL] there will be double validation, by the function and another by the estimator. @adrinjalali and I share the same concern about keeping constraints in sync between the function and the estimator: [URL] [URL] The test for validation is very strict about including a constraint for every parameter. I see the purpose of having this strictness is such that when a new parameter is added, then a constraint is also included. I propose adding a [CODE] to [CODE] which is a collection of strings that states which parameters should be validated. This way when a new parameter is added, the default is to add a constraint for the new parameter, but it can be ignored by adding it to the [CODE] list. CC @jeremiedbb @glemaitre @adrinjalali",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25058"
  },
  {
    "number":31415,
    "text":"Discrepancy between output of classifier feature_importances_ with different sklearn installations\n\n### Describe the bug I am currently using [CODE] classifier [CODE] attribute on a project to rank important features from my model, and my [CODE] pipeline runs the project test-suite using instances of [CODE] and [CODE] on a remote linux host. I am experiencing some discrepancies in the output of the relevant test (for which I have provided a minimal viable reproducer below) on different machines\/installations\/sklearn versions. There are a few specific problems I am experiencing: 1. Locally, the test will pass using a binary installation of [CODE] and fail using [CODE]. With the help of my team, we have traced this error back and found the earliest failing version to be [CODE]. We suspect that the error originates from a change made in [URL] that has to do with the switch from absolute counts to store proportions in [CODE] but have not determined a root cause for the discrepancy. 2. As mentioned in (1) when running the test-suite locally on my [CODE] machine, the test will fail as described, however, when running the test on a remote linux machine, the test will pass with both sklearn versions 3. The test will fail when I build the code from source vs. from the binary distribution of [CODE] My main question is, what could be the cause of these observed discrepancies between sklearn version, installation type and environment and which output is most \"correct\"? ### Steps\/Code to ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31415"
  },
  {
    "number":25145,
    "text":"[CODE] unexpectedly upcasts numeric types in pandas Series\n\n### Describe the bug This is an unexpected (and I would argue undesirable) behavior change introduced in 1.2.0 by [URL] The issue is that [CODE] applied to a pandas series of dtype [CODE] upcasts the returned series to dtype [CODE]. I would guess that there is related upcasting behavior for other numeric dtypes. This is a change from version 1.1.3 with the potential to cause unexpected downstream failures (I found it because I tried to use the invert operator [CODE] on the series returned by [CODE], which works for [CODE] but not [CODE]). ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect the dtype to be preserved (it is preserved in [CODE]) [CODE_BLOCK] ### Actual Results The series is upcast from [CODE] to [CODE]: [CODE_BLOCK] ### Versions ```shell System: python: 3.9.14 (main, Oct 14 2022, 16:22:46) [Clang 14.0.0 (clang-1400.0.29.102)] executable: \/Users\/ben.fogelson\/.pyenv\/versions\/sklearn-bug\/bin\/python machine: macOS-12.6.1-x86_64-i386-64bit Python dependencies: sklearn: 1.2.0 pip: 22.3.1 setuptools: 65.6.3 numpy: 1.23.5 scipy: 1.9.3 Cython: None pandas: 1.5.2 matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libomp filepath: \/Users\/ben.fogelson\/.pyenv\/versions\/3.9.14\/envs\/sklearn-bug\/lib\/python3.9\/site-packages\/sklearn\/.dylibs\/libomp.dylib version: None num_threads: 12 user_api: blas internal_a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25145"
  },
  {
    "number":25825,
    "text":"GridSearchCV instant model convergence upon using many parameters\n\n### Describe the bug I am using CODE] combined with a tensorflow\/keras model wrapped using [CODE]. The image dataset used for classification can be downloaded [here: model = Sequential() if augment: model.add(data_augmentation) model.add(Rescaling(1.\/255)) model.add(Conv2D(32, (3,3), input_shape= (img_size,img_size, 3), activation = 'relu', padding = 'same')) #padding = same size output model.add(MaxPooling2D()) model.add(Conv2D(64, (3,3), activation = 'relu', padding = 'same')) model.add(MaxPooling2D()) model.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same')) model.add(MaxPooling2D()) model.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same')) ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25825"
  },
  {
    "number":30689,
    "text":"FeatureHasher and HashingVectorizer does not expose requires_fit=False tag\n\nWhile [CODE] and [CODE] are stateless estimator (at least in their docstrings), they do not expose the [CODE] tag to [CODE] as other stateless estimator. @adrinjalali Do you recall when changing the tags if there was a particular reason for those estimator to not behave the same way than others?",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30689"
  },
  {
    "number":24424,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev - test_searchcv_raise_warning_with_non_finite_score[GridSearchCV-specialized_params0-True] - test_searchcv_raise_warning_with_non_finite_score[RandomizedSearchCV-specialized_params1-True]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24424"
  },
  {
    "number":25287,
    "text":"[CODE] set in [CODE] not preserved in the Transformer object?\n\n### Describe the bug This is related to: [URL] (btw I love this enhancement!), when [CODE] is used the Transformers created within the context do not register\/memoize the transform output. This may be expected, tho I could not find that explicitly in the documentation? <hr> Could the solution be to add default init to [CODE] to capture [CODE] if set? [CODE_BLOCK] ### Steps\/Code to Reproduce This works as expected: [CODE_BLOCK] But: [CODE_BLOCK] So when [CODE] is not under [CODE] the output defaults to numpy array (default output). [CODE] constructor doesn't register the config at the construction time. This is slightly confusing because: [CODE_BLOCK] ### Expected Results As a user I would expect that `config_contex...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25287"
  },
  {
    "number":29320,
    "text":"Suggest alternative implementation of transform interface in LatentDirichletAllocation.\n\n### Describe the workflow you want to enable The current transform method in LatentDirichletAllocation normalizes the $\\gamma$ parameter from the paper. However, this normalization is not supported by the original LDA paper, and normalizing it lacks any natural interpretation. ### Describe your proposed solution Proposing the topic (assignment) frequency $\\bar{z}$ according to the paper Supervised Topic Models by David M. Blei,\\sum_{n=1}^Nz_n$$ where $z_n$ is the topic assignment, $N$ is the number of words in the documents, the sum is over all words in the document (see equation 1 in the paper). By taking expectation over the variational distribution $q$, one can compute such metric, $$\\mathbb{E}(\\bar{Z}) = \\bar\\phi=(1\/N)\\sum_{n=1}^N\\phi_n$$ (see equation 12 in the paper) Using the relation between $\\gamma$ and $\\phi_n$, one can see that $$\\mathbb{E}(\\bar{Z}) = (1\/N)(\\gamma-\\alpha)$$ where $\\alpha$ is the Dirichlet parameter of the prior document topic distribution (Attribute [CODE]) This is the proposed implementation of [CODE]. ### Reasons for such implementation: 1. Having natural interpretation: - as suggested by David M. Blei, the first author of the original paper of Latent Dirichlet allocation, this metrics can be seen as (expected) topic (assignment) frequency of the document (see paper Supervised Topic Models). By averaging the topic assignment distribution over all every words ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29320"
  },
  {
    "number":30498,
    "text":"[CODE] block is missing from [CODE] HTML repr since 1.5\n\nIn the following example, the [CODE] of [CODE] does not seem to work as I expect it: [URL] <img width=\"809\" alt=\"image\" src=\"[URL] \/> Instead I would expect a [CODE] block and a [CODE] block in the [CODE]. I think that we should check the reason and see if we can improve the rendering.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30498"
  },
  {
    "number":28259,
    "text":"RFC bump Cython minimum supported version to 3.0.8\n\nCurrently we still have Cython 0.29.33 as our minimum Cython version. We may want to decide to bump our Cython requirement to Cython >= 3.0.8. I am +1 for this given that: - [URL] needs Cython >= 3 - [URL] needs Cython >= 3.0.8 - numpy and scipy require Cython >= 3 in their [CODE] branch: [URL]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28259"
  },
  {
    "number":31222,
    "text":"SVC Sigmoid sometimes ROC AUC from predict_proba & decision_function are each other's inverse\n\n### Describe the bug Uncertain if this is a bug or counter-intuitive expected behavior. Under certain circumstances the ROC AUC calculated for [CODE] with the [CODE] kernel will not agree depending on if you use [CODE] or [CODE]. In fact, they will be nearly [CODE]. This was noticed when comparing ROC AUC calculated using [CODE] with predictions from [CODE] to using the scorer from [CODE] which appears to be calling [CODE] with scores from [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The measures of ROC AUC agree ### Actual Results ```shell AUC (roc_auc_score from predict_proba) = 0.5833 AUC (roc_auc_score from decision_function) = 0.4295 AUC ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31222"
  },
  {
    "number":26586,
    "text":"Array API support for k-nearest neighbors models with the brute force method\n\nThis issue is a sibling of a similar issue for k-means: #26585 with similar purpose but likely different constraints. In particular an efficient implementation of k-NN on the GPU would require: - [CODE] - [CODE] being discussed at: - [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26586"
  },
  {
    "number":31592,
    "text":"Compilation \"neighbors\/_kd_tree.pyx\" crashes on ARM\n\n### Describe the bug Hi. I rebuilt scikit-learn from source, but the compiler crashed. ### Steps\/Code to Reproduce [CODE_BLOCK] [CODE_BLOCK] [CODE_BLOCK] [CODE_BLOCK] ### Expected Results Build without problmes ### Actual Results ``` [205\/249] Compiling C object sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o FAILED: sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o cc -Isklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p -Isklearn\/neighbors -I..\/sklearn\/neighbors -I..\/..\/..\/pip-build-env-0jwmo4n5\/overlay\/lib\/python3.12\/site-packages\/numpy\/_core\/include -I\/usr\/local\/include\/python3.12 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c11 -O3 -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -MD -MQ sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o -MF sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o.d -o sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o -c sklearn\/neighbors\/_kd_tree.cpython-312-aarch64-linux-gnu.so.p\/sklearn\/neighbors\/_k...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31592"
  },
  {
    "number":27887,
    "text":"sklearn.linear_model.lars_path_gram ONLY accepts Xy to be of shape (n_features,) and NOT (n_features, n_targets)\n\n### Describe the bug The documentation or (n_features, n_targets)\"_. !WechatIMG671 [GCC 10.2.1 20210110] executable: \/bin\/python machine: Linux-5.10.0-26-cloud-amd64-x86_64-with-glibc2.31 Python dependencies: sklearn: 1.3.2 pip: 20.3.4 setuptools: 44.1.1 numpy: 1.25.2 scipy:...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27887"
  },
  {
    "number":29648,
    "text":"GaussianNB(priors=...) is useless\n\n### Describe the bug If I set the class priors to be very small for classes 0 and 2 and very large for class 1, I expect my predictions to be of class 1. However, I get class 0. It seems to be that [CODE] is useless. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results - [CODE] for [CODE] without priors - [CODE] for [CODE] with priors ### Actual Results - [CODE] for [CODE] without priors - [CODE] for [CODE] with priors ### Versions ```shell System: python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] machine: Linux-6.5.0-44-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.4.1.post1 pip: 24.1.2 setuptools: 68.2.0 numpy: 1.26.4 scipy: 1.12.0 Cython: None pandas: 2.2.1 matplotlib: 3.8.3 joblib: 1.3.2 threadpoolctl: 3.4.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/home\/...\/venv\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: Zen ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29648"
  },
  {
    "number":29963,
    "text":"DOC rework the example presenting the regularization path of Lasso, Lasso-LARS, and Elastic Net\n\nWe recently merge two examples and the resulting example is shown here: [URL] This example should be revisited where we should have more narrative in a tutorial-like style. Indeed, this example could explain in more details what is a regularization path and discuss the difference between Lasso and Lasso-LARS, and between Lasso and ElasticNet. Some of the experiment are really closed to the one presented in this paper: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29963"
  },
  {
    "number":28535,
    "text":"Add metrics.gini_index_score()\n\n### Describe the workflow you want to enable The Gini index of various risk models. ### Describe your proposed solution 2 sklearn examples already includes code for calculating the gini index metric [here]([URL] and [here]([URL] Related to #28534 ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28535"
  },
  {
    "number":24748,
    "text":"Add option gamma='scale' and 'auto' to RBFSampler\n\n### Describe the workflow you want to enable Right now SVM supports gamma parameter to be set to 'scale' or 'auto', which will automatically determine gamma parameter based on data, but RBFSampler does not support this. On top of that there is no check on n_components parameter, which allow users to set it to 0 and fit it without errors. It will only raise error if n_components is negative - [CODE] (this is numpy error from random.normal generation, not sklearn one). ### Describe your proposed solution Add 'scale' and 'auto' option to RBFSampler ### Describe alternatives you've considered, if relevant It is possible to calculate gamma beforehand on your own, but it become increasingly tiresome if for example someone is performing CV - you will have to use custom loop instead of [CODE] and others. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24748"
  },
  {
    "number":29917,
    "text":"[CODE] documentation for [CODE] is ambiguous\n\n[[CODE]]([URL] ### Describe the issue linked to the documentation The documentation for the [CODE] parameter to the [CODE] method of [CODE] leads to confusion. Here is the current text: > Parameters passed to the [CODE] method of the estimator, the scorer, and the CV splitter. > > If a fit parameter is an array-like whose length is equal to [CODE] then it will be split across CV groups along with [CODE] and [CODE]. For example, the [sample_weight]([URL] parameter is split because [CODE]. I was worried that this meant that [CODE] would split [CODE] up across the CV partitions, which is definitely not the right behavior. The correct behavior is to pass the [CODE] parameter unchanged to the CV splitter, e.g. [CODE]. I read through the source code and it does appear that the [CODE] parameter will get passed through unchanged to [CODE], so it looks like the behavior is correct. But we could use something in the docstring that clarifies this behavior. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29917"
  },
  {
    "number":30425,
    "text":"Make sklearn.neighbors algorithms treat all samples as neighbors when [CODE]\/[CODE]\n\n### Describe the workflow you want to enable The proposed feature is that algorithms in [CODE], when created with parameter [CODE] or [CODE], treat all samples used for fitting (or all samples to which distances are [CODE]) as neighbors of every sample for which prediction is requested. This makes sense when algorithm parameter [CODE] is not [CODE] but [CODE] or callable, distributing voting power among fitted samples unevenly. It expands which customized algorithms (that use distance-dependent voting) are available with scikit-learn API. ### Describe your proposed solution The solution: 1. allow the algorithm parameters [CODE]\/[CODE] to have the value [CODE]; 2. allow the public method [CODE] to return ragged arrays instead of 2D arrays (for the case of working on graphs instead of dense matrices); 3. make routines that process indices\/distances of neighbors of samples work with ragged arrays; 4. add the special case for the parameter being [CODE] in routines that find indices of neighbors of a sample. Examples of relevant code for k-neighbors algorithms: 1. [CODE] Add special case to return a ragged array of indices of all non-zero elements in every row (an array per row, taken directly from [CODE]). 1. [CODE] Add special case to produce [CODE] from [CODE] instead of [CODE]. 3. [CODE] In the end, where the false extra neighbor is removed for every sample, add case for a ragged array. 4. [CO...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30425"
  },
  {
    "number":28139,
    "text":"LedoitWolf.fit() crashes Jupyter Notebook\n\n### Describe the bug Hi, when i used LedoitWolf.fit(), my Jupyter Notebook crashes immediately. I am using the toy example given in the source code of LedoitWolf class ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results It is exptected to fit the estimator. ### Actual Results However kernel crashes. sklearn.show_versions() also produce a WinError. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28139"
  },
  {
    "number":25560,
    "text":"set_output API do not preserve original dtypes for pandas\n\n### Describe the bug Following issue #24182, When using the set_output with expected output to be a pandas' data frame, while converting tougher columns with different dtypes the output does not preserve the original dtype but the \"common type\" by numpy. Possible workaround is to create transformer for each column, but it doesn't feel like the right approach. In collaboration with @BenEfrati ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results We'd expect the age column to keep being an unsigned 8-bit integer. ### Actual Results the age column is now a 64-bit floating point. ### Versions ```shell System: python: 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)] executable: C:\\New folder\\venv\\Scripts\\python.exe machine: Windows-10-10.0.19044-SP0 Python dependencies: sklearn: 1.2.1 pip: 23.0 setuptools: 58.1.0 numpy: 1.24.2 scipy: 1.10.0 Cython: None pandas: 1.5.3 matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\New folder\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll version: 0.3.21 threading_layer: pthreads architecture: Haswell num_threads: 12 user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\New folder\\venv\\Lib\\site...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25560"
  },
  {
    "number":26961,
    "text":"Agglomerative clustering training error for seuclidean\/mahalanobis affinity and single linkage\n\n### Describe the bug When trying Agglomerative clustering model training with the affinity as 'seuclidean' or 'mahalanobis' and the linkage as 'single' the training fails. The same affinity values along with other linkage such as 'average' options executes for model training. There's no specification given for this issue. Also, in the code I can see the handling for the single linkage is different and there is some cython code which is not accessible. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results No error should be thrown sig ### Actual Results ![image [GCC 11.2.0] executable: \/home\/albint\/miniconda3\/envs\/myenv\/bin\/python machine: Linux-5.15.0-78-generic-x86_64-with-glibc2.17 Python dependencies: sklearn: 1.3.0 pip: 23.1.2 setuptools: 67.8.0 numpy: 1.24.4 scipy: 1.10.1 Cython: 3.0.0 pandas: 2.0.3 matplotlib: 3.7.2 joblib: 1.3.1 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/home\/albint\/miniconda3\/envs\/myenv\/lib\/python3.8\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so version: 0.3.21 threading_layer: pthreads architecture: Haswell user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/home\/albint\/miniconda3\/envs\/myenv\/lib\/python3.8\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so version: 0.3.18 threading_layer: pthr...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26961"
  },
  {
    "number":31635,
    "text":"Two bugs in [CODE]: [CODE] option\n\n### Describe the bug The function [CODE] contains two separate (but potentially interacting) bugs related to the [CODE] option. This report describes both. --- ### Bug 1: Incorrect Ordering of [CODE] Relative to Initial Point Prepending When [CODE] (the default), [CODE] attempts to simplify the ROC curve by removing intermediate points\u2014those that are collinear with their neighbors and therefore do not affect the curve's shape. However, intermediate points are dropped before the initial point [CODE] and the threshold [CODE] are prepended to the results. This causes incorrect retention of points that would otherwise be considered intermediate if the full curve were evaluated from the start. #### Example: [CODE_BLOCK] In this case, a threshold of 4 perfectly separates class 0 from class 1. The expected simplified ROC curve should be: [CODE_BLOCK] Instead, the actual output is: [CODE_BLOCK] The point [CODE] is redundant but retained, because it is evaluated before [CODE] is prepended\u2014leading to an incorrect assessment of its relevance. #### Root Cause: [CODE_BLOCK] #### Recommended Fix: Reorder the operations so that the initial point is prepended before identifying intermediate points: ```python fps, tps, thresholds = _binary_clf_curve(...) # Prepend start of curve fps = numpy.r_[0, fps] tps = numpy.r_[0, tps] thresholds = numpy.r_[numpy.inf, thres...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31635"
  },
  {
    "number":29551,
    "text":"BUG Problem when [CODE] train contains 2 classes but data contains more\n\n### Describe the bug In [CODE] when a train split contains 2 classes (binary) but the data contains more (>=3) classes, we assume the data is binary: [URL] and we only end up fitting one calibrator: [URL] Context: noticed when looking #29545 and trying to update [[CODE]]([URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expect proba to be 0 ONLY for the class not present in the train subset. ### Actual Results ``[CODE]pos_class_indices[CODE]pos_class_indices[CODE]0[CODE]pos_class_indices` [[1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [1. 0. 0.] [1. 0....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29551"
  },
  {
    "number":28280,
    "text":"Tests failing when cuda installed but no GPU is present\n\nafter doing [CODE], my tests fail with: [CODE_BLOCK] I don't think tests should ever fail for this, should they? cc @ogrisel @betatim",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28280"
  },
  {
    "number":30076,
    "text":"Error on the scikit-learn algorithm cheat-sheet?\n\n### Describe the bug In Clustering, if there are <10K samples, shouldn't yes go to Tough Luck (because there aren't enough samples), and no, go to MeanShift\/VBGMM (because there are)? ### Steps\/Code to Reproduce # N\/A ### Expected Results # N\/A ### Actual Results # N\/A ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30076"
  },
  {
    "number":31143,
    "text":"Enable exporting trained models to text files to be able to import later\n\n### Describe the workflow you want to enable [CODE_BLOCK] ### Describe your proposed solution The current recommended way I believe is to export fit (or trained) models is to serialize them using joblib, which depends on python version, joblib version and scikit-learn version too, and I presume this may lead to issues with OS and CPU architecture as well (windows or GNU Linux and x86 or ARM64). So, I request a way to export model's trained weights (or other relevant things like bins & trees for RandomForestRegressor) for it be to loaded from any scikit-learn version or python version or operating system. This is just how the big packages like [xgboost]([URL] and pytorch ([using [CODE]]([URL] and hence transformers, handle things. This would enable to change environments and platforms easily without having to train model for the new package-versions, architecture and OS again or every time an update in them is required. ### Describe alternatives you've considered, if relevant There is no alternative to everything that scikit-learn offers as of now. ### Additional context T...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31143"
  },
  {
    "number":27373,
    "text":"QuantileTransformer's default subsampling introduces artefacts for unbounded distributions\n\n### Describe the bug The default behaviour of subsampling in the QuantileTransformer introduces artefacts when the input data originates from an unbounded distribution and the transformed dataset is (significantly) larger than the fitted one. For any unbounded distribution the largest (smallest) percentiles will in expectation be underestimated (overestimated). This effect is non-trivial for the selected subsample size of 10_000. Here we draw a Multivariate Normal distribution of size 100_000 !mvn_input [CODE] ![mvn_qt_no_sampling]([URL] The effect of subsampling can be easily demonstrated by computing the difference over the subsample percentiles and the percentiles as you would estimate over the complete sample. [CODE_BLOCK] The plot below shows the mean delta per percentile over 100_000 simulations of the above: ![standard_normal_subsample_delta]([URL] ### Steps\/Code to Reproduce ```python import numpy as np import pandas as pd import scipy.stats as sps import seaborn as sns from...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27373"
  },
  {
    "number":31593,
    "text":"scikit-learn API reference on the website not rendering LaTeX correctly\n\n### Describe the bug On the API reference on the web, formulas are shown as: [CODE] Instead of <img width=\"232\" alt=\"Image\" src=\"[URL] \/> (Unless it's expected!) ### Steps\/Code to Reproduce Please see [URL] ### Expected Results I think the formulas should look like mathematical formulas, not like LaTeX: <img width=\"232\" alt=\"Image\" src=\"[URL] \/> ### Actual Results [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31593"
  },
  {
    "number":31129,
    "text":"python-version: \"3.11\" # update once build dependencies are available\n\n### Describe the issue linked to the documentation Not sure what this comment introduced by 1864117 means: [URL] Isn't it itime to update?  [URL]  [URL] ### Suggest a potential alternative\/fix Update to 3.12 or 3.13?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31129"
  },
  {
    "number":24427,
    "text":"Python 3.11 wheels\n\n### Describe the workflow you want to enable Installing scikit-learn for Python 3.11 ### Describe your proposed solution Add Python 3.11 [CODE] to cibuildwheel CI job matrix ### Additional context Likely you'll want to wait for scipy\/scipy#16851 to land",
    "labels":[
      "New Feature",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24427"
  },
  {
    "number":31123,
    "text":"BUG: Build from source can fail on Windows for scikit-learn v1.6.1 with Ninja [CODE] error\n\nLabels: [CODE], [CODE], [CODE] (Suggested) Describe the bug Scikit-learn (v1.6.1) fails to build from source on a native Windows 11 ARM64 machine using the MSYS2 ClangARM64 toolchain. The build proceeds through the Meson setup phase correctly identifying the [CODE] compiler, but fails during the [CODE] compilation phase with an error indicating it cannot create a specific, deeply nested intermediate build directory. This occurs despite successfully building other complex dependencies like NumPy (v2.2.4) and SciPy (v1.15.2) from source in the exact same environment. Pandas (v2.2.3) also builds successfully after setting [CODE] (otherwise it incorrectly selects MSVC). This suggests the issue might be specific to how scikit-learn's build structure interacts with Meson\/Ninja within this particular toolchain environment. This is related to, but distinct from, #30567 which requests pre-built wheels. This issue focuses on a specific build-from-source failure. Steps\/Code to Reproduce 1. Environment Setup:  OS: Windows 11 Pro ARM64 (via Parallels on Apple Silicon M2, or on native hardware like Windows Dev Kit 2023)  MSYS2: Latest version, updated via [CODE].  MSYS2 Environment: [CODE] shell launched.  Key MSYS2 Packages (installed via [CODE]):  [CODE] (3.12.x)  [CODE] (20.1.1)  [CODE] (20.1.1)  [CODE] (1.7.0)  [CODE] (1.12.1)  [CODE]  [CODE]  [CODE]  [CODE]  [CODE]  [CODE]  [CODE] * Project Loc...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31123"
  },
  {
    "number":26233,
    "text":"Addition of Feature That Detects And Treats Outliers As Per How The User Wishes\n\n### Describe the workflow you want to enable # Workflow ## Class There Will Be A Class In sklearn.preprocessing whose instance will be created ## Function A Function Will Be There Like fit_transform() Where User Can Pass pandas DataFrame As Argument And The The DataFrame Will Be Treated And Returned ### Describe your proposed solution # Outlier Detection Outliers can be detected by Z-Score or Interquartile Range # Outlier Treatment They can be either removed or imputed ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26233"
  },
  {
    "number":26154,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev Unable to find junit file. Please see link for details.",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26154"
  },
  {
    "number":31862,
    "text":"Ordinal Encoder Type Hints State unknown_value should be float, but this produces an error.\n\n### Describe the bug Following the type hints of the OrdinalEncoder I set the unknown_value parameter to -1.0. <img width=\"507\" height=\"146\" alt=\"Image\" src=\"[URL] \/> This produces an error when handle_unknown='use_encoded_value' as it needs and int. Should hopefully just be as easy as updating the type hints unless there is something I'm missing? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expected result would be to not get an error when following type hints. ### Actual Results An error is raised about the type of the unknown_value ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31862"
  },
  {
    "number":29463,
    "text":"DOC: Add missing solver in the doc of [CODE]\n\n### Describe the issue linked to the documentation Hi, I found a potential code-doc inconsistency issue in [[CODE]]([URL] and [[CODE]]([URL] In the description of [CODE], the document claims that Multinomial is unavailable when solver='liblinear'. However, in the code: [CODE_BLOCK] Apparently, [CODE] is unavailable not only when [CODE] but also when [CODE]. ### Suggest a potential alternative\/fix Maybe you can add \"newton-cholesky\" in the corresponding document.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29463"
  },
  {
    "number":28781,
    "text":"ColumnTransformer throws error with n_jobs > 1 input dataframes and joblib auto-memmapping (regression in 1.4.1.post1)\n\n### Describe the bug Hi, I have been trying to build a ColumnTransformer with different values in the n_jobs' parameter, but when fitting and transforming throws the error ValueError: cannot set WRITEABLE flag to True of this array. I am fitting directly a Pandas DataFrame, so not sure if that would be the problem. Thanks Best ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown ### Actual Results ``` { \"name\": \"ValueError\", \"message\": \"cannot set WRITEABLE flag to True of this array\", \"stack\": \"--------------------------------------------------------------------------- _RemoteTraceback Traceback (most recent call last) _RemoteTraceback: \\\"\\\"\\\" Traceback (most recent call last): File \\\"\/Users\/xxxx\/kaggle_2\/new_env\/lib\/python3.11\/site-packages\/joblib\/externals\/loky\/process_executor.py\\\", line 463, in _process_worker r = call_item() ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28781"
  },
  {
    "number":30221,
    "text":"RFC Remove top level indentation from changelog entry files after towncrier\n\nI just saw a PR where the changelog entry didn't have the top level indentation, i.e., it looks like this: [CODE_BLOCK] instead of [CODE_BLOCK] And made me wonder, do we really need that top level indentation in every single file? We could make our scripts \/ towncrier simply add that instead, couldn't we? cc @lesteve @glemaitre",
    "labels":[
      "Documentation",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30221"
  },
  {
    "number":29167,
    "text":"DOC Beginners Tutorial\n\nWe need to have a section in our user guide introducing new users to a few concepts similar to what was present in our basics tutorial: [URL] This can go into our \"Getting Started\" or can be something linked from \"Getting Started\" page. We should also link from our docs to the MOOC: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29167"
  },
  {
    "number":30852,
    "text":"Add a progress bar to the randomized and grid search\n\n### Describe the workflow you want to enable When working on a large hyper-parameter set, setting the verbosity of [CODE] doesn't make the CV more informative. The display should help users estimate their waiting time and take a look at their scores. This issue is particularly salient in realistic searches that can take e.g. several hours to complete. cc @glemaitre @jeromedockes @GaelVaroquaux ### Describe your proposed solution Using joblib [CODE] instead of the default [CODE] in [CODE], we could easily update a progress bar. [URL] ```diff + def progress_bar( + idx, + total, + score_name, + best_score, + best_parameters, + bar_length=30, +): + fraction = idx \/ total + filled_length = int(round(bar_length  fraction)) + + # Construct the bar with an orange filled part and a default unfilled part + orange = \"\\033[38;5;208m\" # ANSI code for orange + reset = \"\\033[0m\" # Reset color + bar = f\"{orange}{'\u2588'  filled_length}{reset}{'-'  (bar_length - filled_length)}\" + + # Print the progress bar on the same line using carriage return (\\r) + text = \" | \".join( + [ + f\"\\r[ {bar} ] {int(fraction  100)}%\", + f\"Best test {score_name}: {best_score}\", + f\"Best params: {best_parameters}\", + ] + ) + print(text, end=\"\", flush=True) + + + def _get_score_name(scorers): + scorers = getattr(scorers, \"_scorer\", scorers) + + if hasattr(scorers, \"_score_func\"): + return scorers._score_func.__name__ + + if isinstance(est := getattr(scorers, \"_estima...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30852"
  },
  {
    "number":26414,
    "text":"Calculation of splitting criteria to be executed in parallel threads\n\n### Describe the workflow you want to enable Currently the computation of splitting criteria for decision tree is single threaded, in theory this can be computed in parallel depending on features available, can the existing package be enhanced to allow multithreading\/multiprocessing. ### Describe your proposed solution A parameter to existing package where user can specify how many threads should be used to compute splitting criteria for decision tree. Even better, the package is enhanced to understand how many threads are available, how many features are required for splitting criteria at each step and the calculate threads dynamically and then compute splitting in parallel. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26414"
  },
  {
    "number":31700,
    "text":"Pipelines are permitted to have no steps and are displayed as fitted\n\n### Describe the bug Pipeline without defined steps is displayed in HTML as fitted. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Maybe empty list should not be accepted. And it should rise a ValueError with a message asking to add steps. ### Actual Results Using vscode jupyter extension: <img width=\"401\" alt=\"Image\" src=\"[URL] \/> Note: Accepting an empty list is one issue, and showing that it is fitted is another. The former occurs when a [CODE] is initialized. The latter, I believe, is a design flaw in [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31700"
  },
  {
    "number":25798,
    "text":"[CODE] can call [CODE], raising ValueError if pandas extension types are present in a pd.DataFrame [CODE]\n\n### Describe the bug At [check_array]([URL] [CODE] is determined for [CODE] objects that are pandas DataFrames by checking [CODE]. This excludes the pandas nullable extension types such as [CODE], [CODE], and [CODE], resulting in a [CODE] of [CODE]. If [CODE], then there ends up being a [call]([URL] to [CODE], which pandas will take to mean a conversion to [CODE] should be attempted. If non numeric\/boolean data is present in [CODE], this can result in a [CODE] being raised if the data has the [CODE] dtype with string data or [CODE] if the data has the [CODE] dtype with [CODE] categories. I first found this in using the imblearn [CODE] and [CODE] oversamplers, but this could happen from other uses of [CODE]. ### Steps\/Code to Reproduce Reproduction via oversamplers [CODE_BLOCK] Reproduction via check_array directly ```python import pandas as pd from sklearn.utils.validation import check_array for dtype in [\"boolean\", \"Int64\", \"Float64\"]: X = pd.DataFrame(...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25798"
  },
  {
    "number":24652,
    "text":"Full documentation build fails since set_output('pandas') was merged\n\n[URL] To reproduce locally: [CODE_BLOCK] The problem comes from the fact that examples are run in the same interpreter and may have side-effects on each other. Examples running after [CODE] will have their [CODE] set to 'pandas' and some fail because of this. Is this actually expected that some example fail if [CODE], I am not sure ... The easiest fix is probably to reset the default output at the end of [CODE]",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24652"
  },
  {
    "number":24749,
    "text":"sklearn installation error on python 3.11\n\n### Describe the bug Unable to pip install sklearn on macOS Monterey 12.6 python 3.11 It is failing when trying to prepare metadata [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error ### Actual Results ``[CODE]numpy.distutils[CODE]distutils[CODE]setuptools < 60.0` for those Python versions. For more details, see: [URL] from numpy.distutils.command.build_ext import build_ext # noqa INFO: C compiler: clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g INFO: compile options: '-c' INFO: clang: test_program.c INFO: clang objects\/test_program.o -o test_program INFO: C compiler: clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g INFO: compile options: '-c' extra options: '-fopenmp' INFO: clang: test_program.c clang: error: unsupported option '-fopenmp' clang: error: unsupported option '-fopenmp' \/private\/var\/folders\/lf\/r17346zd2gl5jn_70c...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24749"
  },
  {
    "number":29042,
    "text":"OneHotEncoder fails on missing values when Pandas uses PyArrow backend\n\n### Describe the bug A while back @thomasjpfan and @lorentzenchr contributed [URL] which enabled missing value support in [CODE] > For object dtypes, None and np.nan is support for missing values. Pandas 2.0 now supports an Arrow backend which uses [CODE] instead of either of the currently supported options ([CODE] or [CODE]) to represent its missing values. This causes [CODE] to fail ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Sklearn should work with the new Pandas Arrow backend ### Actual Results ``` Traceback (most recent call last): File \"sklearn\/utils\/_encod...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29042"
  },
  {
    "number":26061,
    "text":"Store spectral embeddings in SpectralClustering\n\n### Describe the workflow you want to enable Save the spectral embeddings used for clustering in the [SpectralClustering]([URL] class and make them accessible through an attribute, e.g. [CODE], to make easier post-processing on the clusters. ### Describe your proposed solution Optionally return the maps in the [spectral_clustering]([URL] method with a new parameter: [CODE_BLOCK] Store [CODE] attribute in the [CODE] method of the [CODE] class: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26061"
  },
  {
    "number":28587,
    "text":"[CODE] does not handle [CODE]\n\n### Describe the bug We implemented Decision Tree classifiers for a graduate course in Machine Learning. Part of my test suite compares the performance of my [CODE] to the [CODE] on the Iris dataset, with a specified amount of the data replaced with [CODE], to test the performance on a dataset with missing values. This test-module worked without issue for several weeks, that is until I updated my Debian 11 system to Debian 12, which updated python and its libraries as well. Now when I try to train the [CODE], I am receiving the following error message: [CODE_BLOCK] This is not right: as before, section 1.10.8 of the [current documentation ]([URL] on Decision Trees states that > [CODE] and [CODE] have built-in support for missing values when splitter='best' and criterion is 'gini', 'entropy\u2019, or 'log_loss', for classification or 'squared_error', 'friedman_mse', or 'poisson' for regression. For each potential threshold on the non-missing data, the splitter will evaluate the split with all the missing values going to the left node or the right node. and goes on to demonstrate these facilities using [CODE] in the sample data. Similarly, [the general documentati...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28587"
  },
  {
    "number":26369,
    "text":"SequentialFeatureSelector in backward auto mode will always remove one feature\n\n[URL] The initial value of [CODE] is incorrect if [CODE]. With the current initial value of [CODE] would always be [CODE] no matter what [CODE] is returned by [CODE], so that at least one feature would be removed in the first iteration. [CODE] needs to be set to an initial value that is the [CODE] with all the features in use if [CODE].",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26369"
  },
  {
    "number":30622,
    "text":"Validate estimators argument of VotingClassifier\n\n### Describe the workflow you want to enable [CODE] takes as input [CODE], which is expected to be [CODE]. However, if one accidentially puts in a list of estimators instead of a list of [CODE] or a single estimator, no warning\/exception is thrown and one only finds out during runtime\/fitting with the obscure error [CODE_BLOCK] As seen in some stackoverflow questions, this problem seems to occur to users other than me as well, e.g. [URL] [URL] ### Describe your proposed solution Implement validation of the estimators argument within the [CODE], e.g., [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Alternatively, we could also improve the documentation to highlight this case. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30622"
  },
  {
    "number":29521,
    "text":"NDCG in case of abscence of relevant items\n\n### Describe the bug In [CODE], there is a counterintuitive handling of the case where all true relevances are equal to zero for some samples. In this case, DCG = 0, IDCG = 0, and the whole NDCG is not defined. In [CODE] implementation it is defined as 0 and included in the averaged NDCG calculation. The least leads to strange effects, like [CODE]; moreover, it affects the metric value in non-trivial cases too. In the original 2002 paper where NDCG is proposed, it is not stated how to handle such situations, but it is clearly mentioned that [CODE_BLOCK] meaning that NDCG(y,y) must always be 1. I suggest excluding observations without relevant items and\/or throwing a warning. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results 1. ### Actual Results 0.5 ### Versions ```shell This code was not changed in 1.5, so I guess for newer versions the issue also is actual. System: python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801] executable: \/usr\/bin\/python3 machine: Linux-6.6.19-1-MANJARO-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.3.1 pip: 24.0 setuptools: 69.0.3 numpy: 1.26.4 scipy: 1.10.1 Cython: 3.0.9 pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libgomp filepath: \/home\/arabella\/.local\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29521"
  },
  {
    "number":30220,
    "text":"Missing dev changelog from the rendered website after towncrier\n\nWe should add a step to the doc build CI where we render the changelog from the existing files, and have it also under the [CODE] of the website as it was before. This also helps checking rendered changelog from the PRs. cc @lesteve @glemaitre",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30220"
  },
  {
    "number":25495,
    "text":"Feature scaling affects decision tree predictions (it shouldn't affect according to the theory)\n\n### Describe the bug data.csv, that differs. I've obtained similar situation for a number of models, so I think there is a bug in decision tree algorithm realization. ### Steps\/Code to Reproduce ```python import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor df = pd.read_csv('data.csv', header=0, sep=',') # splitting into train - val datasets df_train, df_val = train_test_split(df, test_size=0.2, random_state=42) # create dataframes for feature variables df_train = df_train.reset_index(drop=True) df_val = df_val.reset_index(drop=True) # create dataframes for target variables dy_train = df_train.proof_stress_mpa dy_val = df_val.proof_stress_mpa df_train.drop('proof_stress_mpa', axis=1, inplace=True) df_val.drop('proof_stress_mpa', axis=1, inplace=True) #scaling train features min_max_scaler = MinMaxScaler() my_features = min_max_scaler.fit_transform(df_train).reshape(-1, 1) x = pd.DataFrame(my_features, columns = ['tensile_strength_mpa']) #scaling validation features my_features = min_max_scaler.transform(df_val).reshape(-1, 1) x_val = pd.DataFrame(my_features, columns = ['tensile_strength_mpa']) # decision tree prediction for unscaled data dt1 = Decisio...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25495"
  },
  {
    "number":24434,
    "text":"Different dataset for different models when using StackingClassifier\n\n### Describe the workflow you want to enable Since stacking is a combination of different models, the different models might have different features i.e is is rather often that the same dataset does not fit different models (or it is a sub-optimal dataset for one of the models). E.g a \"LightGBM\" classifier can handle categorical values by it self, where we in LogisticRegression have to handle that in the dataset, which then (might) lead to two different datasets, and we can't just parse one dataset to [CODE] for the training. Or, an SVM would have it's data normalized, where a Decision Tree doesn't care, We can use the [CODE] to sort of work around this but being able to parse multiple datasets could be such a great option ### Describe your proposed solution Something like [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24434"
  },
  {
    "number":27991,
    "text":"sklearn.mixture.gmm is not reproducible in version 1.3.2 vs 1.2.1\n\n### Describe the bug Code using sklearn.mixture.gmm with random seed, is not returning the same result when using scikit-learn versions 1.3.2 versus 1.2.1. The reason is that the function gmm.fit() is using, in some cases, the k-means++ algorithm. This algorithm was improved to receive a new sample_weights parameter that allows different weights for samples during clustering. While gmm.fit() is using k-means++ without using this new parameter, a random object inside the function _kmeans_plusplus is randomizing differently, even when initialized with the same seed. See sklearn\/cluster\/_kmeans.py line 229 in version 1.3.2: [CODE_BLOCK] versus line 210 in version 1.2.1: [CODE_BLOCK] Even when initialized with the same seed, the center_id in both cases is not identical. Our experiments show, the moving from the function [CODE] to [CODE] is not the cause of the change. Calling r[CODE] and [CODE] for the same vector [CODE], will return the same number. It's the addition of the second argument [CODE] that changes the randomization process, even though the provided value is a uniform distribution. ### Steps\/Code to Reproduce ```python import sklearn from numpy import array from sklearn.mixture import GaussianMixture as gmm sklearn.show_versions() n_bins = 75 feature = array([[2.68317954], [0.07873421], [0.54561186], [0.56156012], [0.82741596], [1.34700796], [1.89033108], [0.56811307], [2.0302233], [0.24878048], [0.807...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27991"
  },
  {
    "number":25716,
    "text":"Is the check of strict convergence in KMeans too expensive for the benefits ?\n\n### Describe the bug In CODE] scikit-learn defines [[CODE]. But checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set [CODE] it doesn't seem necessary at all ? Checking for strict convergence seems to really help when [CODE]. With [CODE] I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at [CODE] iterations. For the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive. So I would maybe suggest the following improvements: - [ ] enable automatically the strict convergence checks only if [CODE] (or if [CODE] is \"very small\") - [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ? ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25716"
  },
  {
    "number":31185,
    "text":"BUG: examples\\applications\\plot_out_of_core_classification.py breaks with StopIteration error\n\n### Describe the bug I was building the documentation from source, following [The contributing Tutorial]([URL] When I ran the command [CODE], I noticed the following error from Sphinx build: [CODE_BLOCK] [Here is the full log of the build]([URL] It seems that the [CODE] varible is empty. Also when running the file [CODE] directly in the python environment we get the same error. ### Steps\/Code to Reproduce The simple way to reproduce is just to run the file [CODE] in the sklearn-env. 1. Activate sklearn-env 2. Supposing that you are in the folder [CODE], run: [CODE] Alternatively, you may enter the [CODE] folder, and execute ...",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31185"
  },
  {
    "number":29438,
    "text":"Wrong number of features in documentation\n\n### Describe the issue linked to the documentation In the scikit learn web page, [ [URL] section]([URL] there is a wrong number of features. ![image]([URL] ### Suggest a potential alternative\/fix It should be changed the number of features from 4 to 3 in the documentation.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29438"
  },
  {
    "number":30036,
    "text":"OneVsRestClassifier cannot be used with TunedThresholdClassifierCV\n\n[URL] When predict is called on [CODE], it calls [CODE] on the underlying classifier. If the underlying is a [CODE], it redirects to the underlying estimator instead. On the line referenced, I think that [CODE] should check if the estimator is [CODE], and if so use the [CODE] instead of 0.5",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30036"
  },
  {
    "number":24275,
    "text":"small typo on webpage\n\n### Describe the issue linked to the documentation There is a typo in [URL] _Please notice the bold font below._ > [BisectingKMeans]([URL] is more efficient than [KMeans]([URL] when the number the number of clusters is large since it only works on a subset of the data at each bisection while [KMeans]([URL] always works on the entire dataset. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24275"
  },
  {
    "number":30129,
    "text":"Doc typo\n\n### Describe the issue linked to the documentation I found a typo in the doc of OPTICS. [URL] ### Suggest a potential alternative\/fix Note 'kulsinski' is deprecated from SciPy 1.9 and will removed in SciPy 1.11. -> 'kulsinski' is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30129"
  },
  {
    "number":29303,
    "text":"Proposal to Add tau_metric as a New Classification Performance Measure\n\n### Describe the workflow you want to enable The goal is to introduce a new metric, the tau_metric, which evaluates classification accuracy through a geometric approach. This metric considers the distances from a model's predictions to a perfect and a random-guess scenario in a normalized performance space. It is applicable to both binary and multi-class classification tasks. This new metric aims to provide a more intuitive understanding of model performance, especially in cases where traditional metrics like accuracy or F1-score might not fully capture the nuances of the classification outcomes. ### Describe your proposed solution The [CODE] computes the Euclidean distances from the model's performance point (defined by the normalized true positives across each class) to both a perfect point (where all predictions are correct) and a random-guess point (representing random predictions). The score is then derived by normalizing these distances to fall within a range from 0 (worst) to 1 (best), where 1 indicates a perfect model and 0 indicates a model performing no better than random guessing. The metric will be implemented in a new function [CODE], which will: - Calculate normalized True Positive and True Negative rates for binary classifications. - Extend the calculation to handle multi-class scenarios by computing a model point for each class. - Compare these model points to theoretical perfect and rando...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29303"
  },
  {
    "number":25932,
    "text":"Random forest classifier probability is negative\n\n### Describe the bug Random forest classifier probability is negative value ### Steps\/Code to Reproduce [CODE_BLOCK] I convert scikit-learn model to ONNX and install it to Redis Ai to predict. Eventually, the predic...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25932"
  },
  {
    "number":30037,
    "text":"Implement the two-parameter Box-Cox transform variant\n\n### Describe the workflow you want to enable Currently, ony the single-parameter box-cox is implemented in sklearn.preprocessing.power_transform The two parameter variant is defined as ![]([URL] where both the parameters are to be fit from data via MLE ### Describe your proposed solution Add the two-parameter variant as a new method to sklearn.preprocessing.power_transform ### Describe alternatives you've considered, if relevant Of course, the default yeo-johnson transform can be used for negative data, but that is mathematically different ### Additional context wikipedia page: [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30037"
  },
  {
    "number":25403,
    "text":"test_spectral_embedding_two_components[float32-lobpcg] fails with scipy 1.10\n\n### Describe the bug On openSUSE Tumbleweed, when updating scipy to 1.10.0, a test starts to fail when packaging scikit-learn. ### Steps\/Code to Reproduce [CODE_BLOCK] # docker and openSUSE Tumbleweed reproducer [CODE_BLOCK] # pip reproducer [CODE_BLOCK] ### Expected Results Successful test suite. ### Actual Results ``` ============================================================ FAILURES ============================================================ _____________________________________ test_spectral_embedding_two_components[float32-lobpcg] _____________________________________ [gw1] linux -- Python 3.10.9 \/usr\/bin\/python3.10 eigen_solver = 'lobpcg', dtype = <class 'numpy.float32'>, seed = 36 @pytest.mark.par...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25403"
  },
  {
    "number":25634,
    "text":"Support nullable pandas dtypes in [CODE]\n\n### Describe the workflow you want to enable I would like to be able to pass the nullable pandas dtypes (\"Int64\", \"Float64\", \"boolean\") into sklearn's [CODE] function. Because the dtypes become [CODE] dtype when converted to numpy arrays we get [CODE]: Repro with sklearn 1.2.1 [CODE_BLOCK] ### Describe your proposed solution We should get the same behavior as when [CODE], [CODE], and [CODE] dtypes are used, which is no error: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Our current workaround is to convert the data to numpy arrays with the corresponding dtype that works prior to passing it into [CODE]. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25634"
  },
  {
    "number":27843,
    "text":"set_output doesn't work for inverse_transform method\n\n### Describe the bug Using [CODE] doesn't return a pandas dataframe for the StandardScaler's [CODE] method. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect a pd.DataFrame in return, just like with the [CODE] method. ### Actual Results A numpy array. [CODE_BLOCK] ### Versions ```shell System: python: 3.11.2 (tags\/v3.11.2:878ead1, Feb 7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)] executable: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Scripts\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.3.2 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.24.4 scipy: 1.11.3 Cython: 3.0.5 pandas: 2.1.2 matplotlib: 3.8.0 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll version: 0.3.21 threading_layer: pthreads architecture: Zen user_api: openmp internal_api: openmp num_threads: 16 prefix: vcomp filepath: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\.libs\\vcomp140...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27843"
  },
  {
    "number":32087,
    "text":"\u26a0\ufe0f CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: Sep 14, 2025) \u26a0\ufe0f\n\nCI is still failing on Linux_free_threaded.pylatest_free_threaded - test_multi_metric_search_forwards_metadata[GridSearchCV-param_grid]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32087"
  },
  {
    "number":25126,
    "text":"Wrap tabular data in a new dataclass to simplify ML pipelines\n\n### Describe the workflow you want to enable In my dreams, a new [CODE] class would simplify training and prediction to look more like [CODE_BLOCK] Note that this introduces just two data variables [CODE] and [CODE] instead of the current standard four ([CODE]). In addition, [CODE] could easily be extended to allow the above pipeline to handle related metadata such as feature weights, replacing a step like [CODE] by [CODE], for example. ### Describe your proposed solution A solution could look something like this: ``` @dataclass class Meta: \"\"\"Metadata for a Data class.\"\"\" y_cols: Optional[list[str]] = None row_weights_col: Optional[str] = None @property def y(self) -> list[str]: \"\"\"Output variable column names.\"\"\" if not self.y_cols: return [] return self.y_cols @property def columns(self) -> set[str]: \"\"\"All metadata columns.\"\"\" cols = set(self.y) if self.row_weights_col: cols.add(row_weights_col) return cols @dataclass class InferenceData: \"\"\"A data frame container that includes metadata relevant for machine learning and inference.\"\"\" df: DataFrame meta: Meta = Meta() def __post_init__(self) -> None: \"\"\"Parameter validation.\"\"\" if (self.y is not None) and (self.n_rows != len(self.y)): raise ValueError(\"Expected y to have the same number of data points as x has.\") # TODO: also validate that all columns referenced in self.meta exist in self.df etc ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25126"
  },
  {
    "number":30527,
    "text":"Feature Selectors fail to route metadata when inside a Pipeline\n\n### Describe the bug According to the metadata routing docs: - sklearn.feature_selection.RFE. ### Steps\/Code to Reproduce ```python import numpy as np import sklearn from sklearn.datasets import load_iris from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline sklearn.set_config(enable_metadata_routing=True) X, y = load_iris(return_X_y=True, as_frame=True) w = np.arange(len(X)) + 1 reg = LinearRegression().set_fit_request(sample_weight=True) pipeline_reg = LinearRegression().set_fit_request(sample_weight=True) pipeline_fs = SelectFromModel( reg, threshold=-np.inf, prefit=False, max_features=len(X.columns), ) pipeline = Pipeline( [ (\"feature_selector\", pipeline...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30527"
  },
  {
    "number":28960,
    "text":"Base function to check if the model is a clusterer (analogous to [CODE] and [CODE])?\n\n### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by aoot April 26, 2024<\/sup> According to [the note on figuring out the model type]([URL] it is recommended to use [CODE] or [CODE] function to check instead of of checking the attribute [CODE] directly. However, since the attribute [CODE] can be either [CODE], [CODE], and [CODE], are there any base function such as [CODE] to check if the model is a clusterer? Thanks for your input!<\/div> [URL] is an effort to fix this. Not sure what to think of it.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28960"
  },
  {
    "number":29040,
    "text":"\"Building from source\" instructions are outdated\n\n### Describe the issue linked to the documentation [URL] seems to be a few years old, and doesn't leverage Meson. [URL] states that Meson is experimental, but it seems to be required on [CODE]. If not installed, I get: ```bash (sklearn-dev) deepyaman@deepyaman-mac scikit-learn % pip install -v --no-use-pep517 --no-build-isolation -e . Using pip 24.0 from \/opt\/miniconda3\/envs\/sklearn-dev\/lib\/python3.9\/site-packages\/pip (python 3.9) Obtaining file:\/\/\/Users\/deepyaman\/github\/scikit-learn\/scikit-learn ERROR: Disabling PEP 517 processing is invalid: project specifies a build backend of mesonpy in pyproject.toml (sklearn-dev) deepyaman@deepyaman-mac scikit-learn % pip install -v --no-build-isolation -e . Using pip 24.0 from \/opt\/miniconda3\/envs\/sklearn-dev\/lib\/python3.9\/site-packages\/pip (python 3.9) Obtaining file:\/\/\/Users\/deepyaman\/github\/scikit-learn\/scikit-learn Running command Checking if build backend supports build_editable Checking if build backend supports build_editable ... done ERROR: Exception: Traceback (most recent call last): File \"\/opt\/miniconda3\/envs\/sklearn-dev\/lib\/python3.9\/site-packages\/pip\/_internal\/cli\/base_command.py\", line 180, in exc_logging_wrapper status = run_func(*args) File \"\/opt\/miniconda3\/envs\/sklearn-dev\/lib\/python3.9\/site-packages\/pip\/_internal\/cli\/req_command.py\", line 245, in wrapper return func(self, options, args) File \"\/opt\/miniconda3\/envs\/sklearn-dev\/lib\/python3.9\/site-packages\/pip\/_internal\/co...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29040"
  },
  {
    "number":28454,
    "text":"Very old notes in docstrings\n\n### Describe the issue linked to the documentation In [CODE] very old notes are contained in the docstrings of two functions: [URL] and [URL] These notes refer to scikit-learn 0.14.1 which is approx. 10 years old: [CODE_BLOCK] Maybe these notes are obsolet and can be removed? What is the employed strategy in scikit-learn regarding such old notes? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28454"
  },
  {
    "number":24545,
    "text":"Error when returning embedded transformers in Jupyter notebook\n\n### Describe the bug When creating a custom transformer object that includes a transformer type as an instance, a [CODE] is thrown if the object is returned at the end of a Jupyter cell. This does not cause an error in the terminal, but raises an error during the conversion to an HTML object for Jupyter notebooks. Weirdly, the object is created and returned, but an error is thrown when Jupyter attempts to display it. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown, and the HTML representation of the transformer is shown in the Jupyter cell. ### Actual Results Here's the full error traceback: ``` --------------------------------------------------------------------------- TypeError Traceback (most recent call last) File ~\/anaconda3\/envs\/prodenv\/lib\/python3.8\/site-packages\/IPython\/core\/formatters.py:973, in MimeBundleFormatter.__call__(self, obj, include, exclude) 970 method = get_real_method(obj, self.print_method) 972 if method is not None: --> 973 return method(include=include, exclude=exclude) 974 return None 975 else: File ~\/anaconda3\/envs\/prodenv\/lib\/python3.8\/site-packages\/sklearn\/base.py:631, in BaseEstimator._repr_mimebundle_(self, kwargs) 629 output = {\"text\/plain\": repr(self)} 630 if get_config()[\"display\"] == \"diagram\": --> 631 output[\"text\/html\"] = estimator_html_re...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24545"
  },
  {
    "number":29630,
    "text":"Maintenance releases for 1.1.x and 1.2.x with numpy < 2.0?\n\n### Describe the workflow you want to enable Having an environment file or requirement file with scikit-learn=1.1 or scikit-learn=1.2 will break, since neither supports numpy 2.0 but doesn't declare that. Example: [CODE_BLOCK] > ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject ### Describe your proposed solution We could do patch release for those two releases that add a numpy < 2.0 requirement to the setup.py. This is not something we've done historically much, I think, but it's not ideal if old requirement files break. If someone pinned the patch release, adding a patch release with a fix won't help, unfortunately.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29630"
  },
  {
    "number":26854,
    "text":"Exception when transforming LabelEncoder with keyword argument\n\n### Describe the bug The transform method of LabelEncoder raises an exception when providing CODE] with a keyword argument. I mentioned this in discussion #26841 but I am more and more certain this is a bug. And even if it isn't and there is some good reason it fails, the error message is completely unclear since it refers to a parameter that is not present in the method. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Normal output of the transform method. Same as when calling [CODE] (withoyt [CODE]), which in the above example would be [CODE]. ### Actual Results ![image [MSC v.1934 64 bit (AMD64)] executable: C:\\repos\\dblib\\venv\\Scripts\\python.exe machine: Windows-10-10.0.22621-SP0 Python dependencies: sklearn: 1.3.0 pip: 23.2 setuptools: 65.5.1 numpy: 1.25.1 scipy: 1.11.1 Cython: None pandas: 1.4.4 matplotlib: None joblib: 1.3.1 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 8 prefix: vcomp filepath: C:\\repos\\dblib\\venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: C:\\repos\\dblib\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll version: 0.3.23 threading_layer: pthreads architecture: SkylakeX user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: C:\\repos\\dblib\\venv\\Lib\\site-packages\\...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26854"
  },
  {
    "number":27955,
    "text":"Unable to control warning logs generated by GridSearchCV fit method when setting n_jobs to >1 for parallel processing\n\n### Describe the workflow you want to enable I am running GridSearchCV with n_jobs set to value which is > 1. The grid search is writing log of convergence and other warnings to the console. I want to control those logs so that I can write them to a logfile instead of spamming console. I tried tweaking with joblib but it didn't worked. Can you provide us feature to control logs generated by sklearn gridsearch in case of using parallelism. ### Describe your proposed solution I don't have any solution at this moment ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27955"
  },
  {
    "number":30477,
    "text":"Add missing value support for AdaBoost?\n\n### Describe the bug I am working on classifying samples in various datasets using the AdaBoostClassifier with the DecisionTreeClassifier as the base estimator. The DecisionTreeClassifier can handle np.nan values, so I assumed the AdaBoostClassifier would be able to as well. However, that does not seem to be the case, as AdaBoost gives the error CODE] when I try to use it with data containing NAs. I asked if this was the intended behavior [here: File \"C:\\Users\\pacea\\miniconda3\\envs\\j...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30477"
  },
  {
    "number":25565,
    "text":"High level documentation of the CI infrastructure\n\nAs originally discussed in [URL] I think it might be helpful to give a high level description of our CI somewhere in the doc, both for new contributors and maintainers. In particular, we should summarize the various missions of our CI: - building and testing PRs to give feedback to contributors and reviewers prior to merging (triggered by commits pushed to the PR branch on the contributor's fork), - building the stable and dev doc websites (triggered by commits pushed to the [CODE] and [CODE] branches on the main repo respectively), - building and testing scikit-learn against the development versions of its dependencies (triggered as scheduled jobs), - building, testing, and publishing nightly wheels against stable dependencies (triggered as scheduled jobs), - building, testing, and publishing release wheels to pypi.org (triggered by commits pushed to the [CODE] branch on the main repo + manually triggered upload github action workflow), Ideally this doc should only describe the high level objectives of the CI infrastructure and point the readers to the config files and the [CODE] folder for the details (which typically evolve faster than we update the doc). We should carefully use inline comments in those config files and scripts to make sure that people easily understand how they operate and how to conduct common maintenance operations (e.g. rotating secret tokens). This doc should also consolidate the list of commit messag...",
    "labels":[
      "Documentation",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25565"
  },
  {
    "number":26890,
    "text":"Handling [CODE] in encoders\n\nIt seems that we don't handle it properly CODE] in the encoder and thus differently than [CODE]. [CODE] will raise an error as in the following reproducible: [CODE_BLOCK] or [CODE_BLOCK] leading to: <details> ```pytb --------------------------------------------------------------------------- TypeError Traceback (most recent call last) File [~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:174 172 uniques_set, missing_values = _extract_missing(uniques_set) --> 174 uniques = sorted(uniques_set) 175 uniques.extend(missing_values.to_list()) File ~\/miniconda3\/envs\/dev\/lib\/python3.10\/site-packages\/pandas\/_libs\/missing.pyx:388 TypeError: boolean value of NA is ambiguous During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) [\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26890"
  },
  {
    "number":29526,
    "text":"Documentation Examples raise ValueError in utils\/sparsefuncs_fast fails.pyx\n\n### Describe the bug The example section in the docstring of [CODE] and [CODE] raise an ValueError ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK] ### Proposed solution: [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29526"
  },
  {
    "number":24634,
    "text":"HalvingRandomSearchCV error\n\n### Describe the bug [URL] when resources is the hyperparameter of estimator, the best performance is not always at its maximum, right?. so in my opinion, in this case, we should select the best in whole iter instead of the best in last iter. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24634"
  },
  {
    "number":27313,
    "text":"Implementation of Order Recursive Matching Pursuit\n\n### Describe the workflow you want to enable The sparse linear regression problem is the NP-hard problem of performing ordinary L\u00b2 linear regression with the catch that at most a fixed number of the resulting coefficients can be non-zero (or the variation in which one wants as few non-zero columns as possible while reaching a given score). Scikit-learn includes a greedy approximation algorithm for solving this problem in the form of [CODE]. Many variations on this algorithm exist; one of them (which dates back at least to the 70s. Like OMP, ORMP is an iterative algorithm which picks one column from the input at a time, while maintaining the current residual. In OMP, one picks the column whose angle with the current residual is minimal, whereas in ORMP, one chooses the column resulting in the minimal residual. That is, while the approach is still greedy, it's somewhat more to the point, choosing the column leading to the minimal objective in each step. A priori, the cost of this approach is increased computational time, but with proper care, it is possible to make a [simple implementation]([URL] that rivals the runtime performance of the existing OMP implementation, and is faster on the instances that motivated the work. ### Describe your proposed solution In our company, we needed fit accuracy greater than what could be achieved with OMP and thus implemented ORMP in a handful of different ways. The implementations are availa...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27313"
  },
  {
    "number":25496,
    "text":"Partial Dependence Plot orients differently compared to Partial Dependence values\n\n### Describe the bug The issue is that the 2D partial dependence plot from scikit-learn orients in a different way that what you would get using raw pdp values from sklearn as well. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ![image]([URL] ### Actual Results [CODE_BLOCK] ![image]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25496"
  },
  {
    "number":30131,
    "text":"LinearRegression on sparse matrices is not sample weight consistent\n\nPart of #16298. ### Describe the bug When using a sparse container like [CODE] for [CODE], [CODE] even fails to give the same coefficients for unit or no sample weight, and more generally fails the [CODE] checks. In that setting, the underlying solver is [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The [CODE] should pass. ### Actual Results [CODE_BLOCK] The test also fails for [CODE]. Note that this test and other sample weight consistency checks pass if we do not wrap [CODE] in a sparse container. ### Versions ```shell System: python: 3.12.5 | packaged by conda-forge | (main, Aug 8 2024, 18:32:50) [Clang 16.0.6 ] executable: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/bin\/python machine: macOS-14.5-arm64-arm-64bit Python dependencies: sklearn: 1.6...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30131"
  },
  {
    "number":29568,
    "text":"possible bug in sklearn.utils.compute_class_weight\n\n### Describe the bug URL] In the 'if' statement on line 85 above, the condition 'n_weighted_classes != len(class_weight)' after 'and' may cause unexpected errors. In detail, if the total length of classes minus the length of unweighted_classes is equal to the length of class_weight, the program will not throw an exception even if there are abnormal values \u200b\u200bin classes. As is shown in picture below. ![bug [MSC v.1916 64 bit (AMD64)] executable: E:\\Anaconda3\\envs\\yolov5\\python.exe machine: Windows-10-10.0.22621-SP0 Python dependencies: sklearn: 1.1.1 pip: 22.1.2 setuptools: 51.3.3 numpy: 1.22.3 scipy: 1.6.2 Cython: 0.29.30 pandas: 1.4.3 matplotlib: 3.5.2 joblib: 1.1.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: mkl prefix: mkl_rt filepath: E:\\Anaconda3\\envs\\yolov5\\Library\\bin\\mkl_rt.1.dll version: 2021.4-Product threading_layer: intel num_threads: 6 user_api: openmp internal_api: openmp ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29568"
  },
  {
    "number":31315,
    "text":"SGDRegressor is not inheriting from LinearModel\n\n### Describe the bug I wanted to rely on the base class LinearModel is not inheriting this class. However, SGDClassifier is (through LinearClassifierMixin). Is there any reason for BaseSGDRegressor ### Expected Results from sklearn.linear_model import SGDRegressor from sklearn.linear_model._base import LinearModel issubclass(SGDRegressor, LinearModel) # True ### Actual Results from sklearn.linear_model import SGDRegressor from sklearn.linear_model._base import LinearModel issubclass(SGDRegressor, LinearModel) # False ### Versions ```shell System: python: 3.10.11 (v3.10.11:7d4cc5aa85, Apr 4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)] executable: \/usr\/local\/bin\/python3.10 machine: macOS-14.4.1-arm64-arm-64bit Python dependencies: sklearn: 1.5.0 pip: 24.2 setuptools: 74.0.0 numpy: 1.26.4 scipy: 1.13.1 Cython: 3.0.12 pandas: 1.5.3 matplotlib: 3.8.4 joblib: 1.2.0 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib ...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31315"
  },
  {
    "number":28293,
    "text":"NeighborhoodComponentsAnalysis (NCA) sets incorrect [CODE] value which makes [CODE] fail if [CODE].\n\n### Describe the bug CODE] fails with the following error whenever [CODE] is set to \"pandas\": [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] is expected to return a dataframe with [CODE] columns. ### Actual Results The error lies in this line here: [URL] since [CODE] has the shape [CODE], so this line should be, for example self._n_features_out = self.components_.shape[0] because [CODE] ultimately needs to correspond to [CODE] (only then the correct number of column labels will be produced, that is 2 instead of 20). Instead, the bug could be fixed by copying the pattern found in [CODE] where [this is a property: \"\"\"Number of transformed output features.\"\"\" return self.components_.shape[0] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28293"
  },
  {
    "number":27074,
    "text":"Unable to build scikit learn on Mac-M2, python3.11 and 3.9\n\n### Describe the bug Getting below error when trying to install snips-nlu ried installing both venv and conda Also tried to install with python3.9 but failed with the same [CODE] ### Steps\/Code to Reproduce [CODE] [CODE] ### Expected Results No error is thrown ### Actual Results Error compiling Cython file: ------------------------------------------------------------ ... # Initial capacity cdef int init_capacity if tree.max_depth <= 10: init_capacity = (2  (tree.max_depth + 1)) - 1 ^ ------------------------------------------------------------ sklearn\/tree\/_tree.pyx:162:56: Cannot assign type 'double' to 'int' Error compiling Cython file: ------------------------------------------------------------ ... shape[1] = <np.npy_intp> self.n_outputs shape[2] = <np.npy_intp> self.max_n_classes cdef np.ndarray arr arr = np.PyArray_SimpleNewFromData(3, shape, np.NPY_DOUBLE, self.value) Py_INCREF(self) arr.base = <PyObject> self ^ ------------------------------------------------------------ sklearn\/tree\/_tree.pyx:1103:11: Assignment to a read-only property Error compiling Cython file: ------------------------------------------------------------ ... arr = PyArray_NewFromDescr(<PyTypeObject > np.ndarray, <np.dtype> NODE_DTYPE, 1, shape, strides, <void> self.nodes, np.NPY_DEFAULT, None) Py_INCREF(self) arr.base = <PyObject> self ^ ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27074"
  },
  {
    "number":30810,
    "text":"Windows free-threaded CPython 3.13 ValueError: concurrent send_bytes() calls are not supported\n\nNoticed in build log: # For coverage only. X, y = make_regression(n_samples=500, random_state=0) gbdt = HistGradientBoostingRegressor(loss=\"absolute_error\", random_state=0) > gbdt.fit(X, y) ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\tests\\test_gradient_boosting.py:225: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:1389: in wrapper return fit_method(estimator, args, kwargs) ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:663: in fit X_binned_train = self._bin_data(X_train, is_training_data=True) ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1178: in _bin_data X_binned = self._bin_mapper.fit_transform(X) # F-aligned array ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319: in wrapped data_to_wrap = f(self, X, args, kwargs) ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:918: in fit_transform return self.fit(X, fit_params).transform(X) ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py:234: in fit non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=\"threading\")( ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82: in __call__ ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30810"
  },
  {
    "number":24859,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly_PyPy.pypy3 \u26a0\ufe0f\n\nCI failed on Linux_Nightly_PyPy.pypy3 Unable to find junit file. Please see link for details.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24859"
  },
  {
    "number":27192,
    "text":"Update the Ledoit-Wolf covariance shrinkage methodology to include modern methods\n\n### Describe the workflow you want to enable I've been working on implementing partial correlation with basis shrinkage in Python. in particular, I've been porting R code to Python. The relevant partial correlation publications are the following:  Erb et al. 2020. > > Similarly cor.shrink computes a shrinkage estimate of the correlation matrix by shrinking the empirical correlations towards the identity matrix. In this case the shrinkage intensity is computed using estimate.lambda (Sch\\\"afer and Strimmer 2005). Additional References: Opgen-Rhein, R., and K. Strimmer. 2007. Accurate ranking of differentially expressed genes by a distribution-free shrinkage approach. Statist. Appl. Genet. Mol. Biol. 6:9. <DOI:10.2202\/1544-6115.1252> Schafer, J., and K. Strimmer. 2005. A shrinkage approach to large-scale covariance estimation and implications for functional genomics. Statist. Appl. Genet. Mol. Biol. 4:32. <DOI:10.2202\/1544-6115.1175> ### Describe your proposed solution Rpy2 wrapper which makes the compute environment very bulky and dependency heavy ### Describe alternatives you've considered, if relevant Reimplementing but outside of my expe...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27192"
  },
  {
    "number":26775,
    "text":"making mutual_info_regression more verbose\n\n### Describe the workflow you want to enable Hi, Thanks for coding up [CODE], I use it for novel space science research and it's super helpful for me. I'm currently using it to analyse quite large amounts of data, and understandably this can be quite slow. I'm wondering if it would be possible to implement an optional flag in [CODE] to make the code verbose while it's running. For example, printing out something to the terminal when it is 10%, 20%, 30% (etc) done. Just an idea - hopefully I have raised this in the correct way! Thanks, Alexandra Fogg ### Describe your proposed solution Implementation of an optional flag in [CODE] to make the function verbose, where it prints out statements at certain stages of the process, e.g. 10% done or saying that a certain stage is completed, etc. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26775"
  },
  {
    "number":25066,
    "text":"[CODE] wrongly exposes a [CODE] argument\n\n### Describe the issue linked to the documentation It's more an UX issue than a documentation issue maybe ? CODE] should not be passed to [CODE], weights of sample are not related to how far samples are from centers, which is what [CODE] computes to find labels. [CODE] output does not need [CODE] nor depends on what [CODE] is passed anyway. (another way to see it is that [CODE] is only used if [CODE] is [CODE] in the [private function (Note that [CODE] does require [CODE]) ### Suggest a potential alternative\/fix [CODE] should be removed from [CODE] signature. Since it breaks UX, it requires a deprecation cycle.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25066"
  },
  {
    "number":29539,
    "text":"Tag for identifying capability to handle non-numeric data in input\n\n### Describe the workflow you want to enable I want to be able to find out whether an estimator supports non-numeric features in the input data passed to it in fit\/transform. Example : [CODE], [CODE] supports this while [CODE] does not. ### Describe your proposed solution Setting appropriate tags for identifying the capability of the estimator. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I am trying to extend support for categorical features in [CODE] where I am now facing an issue with an sklearn adapter where the categorical support depends on the sklearn transformer passed to the adapter. Having some way to inspect the transformer and find out whether it can take categorical inputs would be useful.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29539"
  },
  {
    "number":30088,
    "text":"[CODE]\n\n### Describe the workflow you want to enable It's not just Python, there are also a lot of cool packages that import this [CODE_BLOCK] ### Additional context _No response_",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30088"
  },
  {
    "number":30442,
    "text":"Missing [CODE] in [CODE]and [CODE]\n\n### Describe the workflow you want to enable The method is currently missing in those two classes which prevent doing a loop over all Linear decomposition methods when evaluation them for denoising for instance. ### Describe your proposed solution I propose to implement the method in the class [CODE] from [URL] I have an implementation with updated tests that I will propose as PR when the issue is created. Ping to @agramfort with whom I already discussed about this. ### Describe alternatives you've considered, if relevant the documentation gives the following expale [CODE_BLOCK] which is OK but requires the user to know about [CODE] and to do that to all linear decomposition methods is tested in a loop (the others have all implemented [CODE]). Implementing the method closes a missing part of the API and would be better in my opinion. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30442"
  },
  {
    "number":32121,
    "text":"\"Improve documentation on FeatureUnion behavior with polars DataFrame output causing duplicate column names\"\n\n### Describe the issue linked to the documentation The current documentation for FeatureUnion describes its behavior with pandas DataFrames but does not mention how it behaves when used with polars DataFrames. Specifically, FeatureUnion concatenates outputs of its transformers before the set_output wrapper renames columns based on get_feature_names_out. This works fine with pandas but causes issues with polars since polars does not allow creating a DataFrame with duplicate column names. This leads to errors when using FeatureUnion with polars outputs. ### Suggest a potential alternative\/fix The documentation should mention the behavior difference when using FeatureUnion with polars DataFrames, specifically that concatenation occurs before column renaming. This can cause duplicate column names errors in polars. It would be helpful to add a warning or note about this limitation, and suggest possible workarounds, such as manually renaming columns or converting to pandas DataFrame before using FeatureUnion. Including a minimal example demonstrating the issue and how to avoid it would further improve clarity for users.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32121"
  },
  {
    "number":31899,
    "text":"Add [CODE] to [CODE]?\n\n### Describe the workflow you want to enable [CODE] has an optional [CODE] parameter, while the similar [CODE] does not. QDA is even more sensitive than LDA to covariance estimation. Would it be desirable to add the [CODE] parameter to [CODE]? ### Describe your proposed solution I can try to implement this. I would look at how it is done in [CODE], and just copy that implementation into [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31899"
  },
  {
    "number":25343,
    "text":"Typo in contributing docs\n\n### Describe the issue linked to the documentation In a previous PR when [CODE] was changed to [CODE], it also changed the word [CODE] following line: [URL] [CODE] On this line we want to change [CODE] back to [CODE]. ### Suggest a potential alternative\/fix On this line above, we just need to change [CODE] back to [CODE]!",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25343"
  },
  {
    "number":25439,
    "text":"BalancedBaggingClassifier returns \"TypeError: Cannot clone object ''deprecated''\n\n### Describe the bug Example taken from: [URL] gives error. ### Steps\/Code to Reproduce ``` from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.experimental import enable_hist_gradient_boosting # noqa from imblearn.ensemble import BalancedBaggingClassifier from sklearn.datasets import fetch_openml import pandas as pd from sklearn.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.preprocessing import OrdinalEncoder from sklearn.compose import make_column_selector as selector from sklearn.compose import make_column_transformer from sklearn.model_selection import cross_validate from imblearn.datasets import make_imbalance import sklearn print('The scikit-learn version is {}.'.format(sklearn.__version__)) df, y = fetch_openml(\"adult\", version=2, as_frame=True, return_X_y=True) df = df.drop(columns=[\"fnlwgt\", \"education-num\"]) classes_count = y.value_counts() scoring = [\"accuracy\", \"balanced_accuracy\"] ratio = 30 df_res, y_res = make_imbalance( df, y, sampling_strategy={classes_count.idxmin(): classes_count.max() \/\/ ratio}, ) index = [] scores = {\"Accuracy\": [], \"Balanced accuracy\": []} num_pipe = SimpleImputer(strategy=\"mean\", add_indicator=True) cat_pipe = make_pipeline( SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OrdinalEncoder(handle_unknown=\"use_encoded_value\", unk...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25439"
  },
  {
    "number":31804,
    "text":"DOC metadata docstrings generator has wrong indentation\n\n### Describe the issue linked to the documentation I am a maintainer of a third party package [fastcan]([URL] After I update the scikit-learn version from 1.7.0 to 1.7.1, the Sphinx document generation gives the following error. [CODE_BLOCK] The raw error log of readthedocs build can be found [here]([URL] It is suspected the error is caused by the wrong indentation in [CODE] as below. [CODE_BLOCK] ### Suggest a potential alternative\/fix The correct indentation should be as below [CODE_BLOCK] I am not sure why the official documents of scikit-learn does not have this error. However, at least for consistence with [CODE] and [CODE], which have 8 spaces indentation, [CODE] should also have 8 spaces indentation.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31804"
  },
  {
    "number":24947,
    "text":"KFold returning folds with different lengths of Train and test splits\n\n### Describe the bug The sizes of the train and test split change over different splits. For example, the size of the IRIS dataset is 150 rows. In 4-fold cross-validation, we would expect to see either 112-38 splits or 113-37 splits for all 4 folds. But actually, we are getting a mix of both. I tested this for another dataset with for 2-fold and 3-fold in which the len(data) is not divisible by 4. The same issue was there. Would be much appreciated if someone could fix it. Why I need this fix is because, in my application, I have to assume that all folds are of equal sizes for a specific reason. Thank you! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.6 (main, Oct 7 2022, 15:17:36) [Clang 12.0.0 ] executable: \/Users\/nrweerad\/opt\/anaconda3\/envs\/mlp\/bin\/python3 machine: macOS-12.4-arm64-arm-64bit Python dependencies: sklearn: 1.1.2 pip: 22.2.2 setuptools: 63.4.1 numpy: 1.23.3 scipy: 1.9.2 Cython: None pandas: 1.5.0 matplotlib: 3.6.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libomp filepath: \/Users\/nrweerad\/o...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24947"
  },
  {
    "number":28325,
    "text":"Migrate macOS arm64 wheel building and testing CI to GitHub Actions runner\n\nOur Cirrus CI account often gets rate limited because we tend to exhaust the credits allocated for free to Open Source project. This can slow down the release process quite significantly. Fortunately, GitHub Actions just introduced new M1-based runners in their offer. [URL] For convenience, here are the existing config with a relevant in-line comment with more details about our current wheel CI: [URL] and the dual section in our current Cirrus CI config. [URL] We should consider migrating to our wheel building to the new GitHub Actions macOS\/M1 workers to decrease the credit consumption of our Cirrus CI account: it would only be used for Linux\/arm64 tests and wheel building. Side note: we could also use those new for the Linux arm64 wheels via docker and not rely on Cirrus CI at all any more, but using macOS workers to build and test Linux wheels via docker might seem a bit too convoluted for our taste :)",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28325"
  },
  {
    "number":32043,
    "text":"Failed to build scikit learn with cython.\n\n### Describe the bug I run the command in [URL] , but it built failed: ``` root@dsw-1307236-5f5f447cdf-xs4m5:\/mnt\/workspace\/scikit-learn# pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true Using pip 25.2 from \/usr\/local\/lib\/python3.11\/site-packages\/pip (python 3.11) Looking in indexes: [URL] Obtaining file:\/\/\/mnt\/workspace\/scikit-learn Checking if build backend supports build_editable ... Running command Checking if build backend supports build_editable done Preparing editable metadata (pyproject.toml) ... Running command Preparing editable metadata (pyproject.toml) + meson setup --reconfigure \/mnt\/workspace\/scikit-learn \/mnt\/workspace\/scikit-learn\/build\/cp311 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=\/mnt\/workspace\/scikit-learn\/build\/cp311\/meson-python-native-file.ini The Meson build system Version: 1.9.0 Source dir: \/mnt\/workspace\/scikit-learn Build dir: \/mnt\/workspace\/scikit-learn\/build\/cp311 Build type: native build Project name: scikit-learn Project version: 1.8.dev0 C compiler for the host machine: cc (gcc 11.4.0 \"cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\") C linker for the host machine: cc ld.bfd 2.38 C++ compiler for the host machine: c++ (gcc 11.4.0 \"c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\") C++ linker for the host machine: c++ ld.bfd 2.38 Cython compiler for the host machine: cython (cython 3.1.3) Host machine cpu family: x86_64 Host machine cpu: x86_6...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32043"
  },
  {
    "number":26443,
    "text":"PowerTransformer returns inconsistent index when transform output is set globally\n\n### Describe the bug When the _global_ transform output is set to \"pandas\" via [CODE], [CODE] overrides the original index of the [CODE] with a [CODE]. This issue does not occur with other preprocessors such as [CODE] and can produce subtle side-effects. Lastly, this issue does _not_ occur when the aforementioned global configuration is removed and the transform output is set via [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results All three indexes should be identical. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:12:32) [MSC v.1929 64 bit (AMD64)] executable: C:\\Users\\jkvas\\.conda\\envs\\chiu-lab\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.2.2 pip: 23.0.1 setuptools: 67.6.0 numpy: 1.24.2 scipy: 1.10.1 Cython: None pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: mkl prefix: libblas filepath: C:\\Users\\jkvas\\.conda\\envs\\chiu-lab\\Library\\bin\\libblas.dll vers...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26443"
  },
  {
    "number":28462,
    "text":"OrdinalEncoder doesn't recognize np.nan in np.array with multiple entries\n\n### Describe the bug Hello friends, first time opening a bug report here. Don't hesitate to let me know if I should be doing this differently. When a multi-element array is passed in to transform() method, np.nan are treated as [CODE] instead of [CODE]. Single-element arrays and pandas dataframes with multiple rows seem to work fine. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28462"
  },
  {
    "number":31366,
    "text":"Gaussian Process Log Likelihood Gradient Incorrect\n\n### Describe the bug The gradient function of in the GaussianProcessRegressor Class is incorrect. This leads to inefficiencies fitting kernel (hyper) parameters. The root of the issue is in that the gradient of the kernel function is made with respect to the log of the kernel parameter. See the plot on the right showcasing the incorrect gradient currently being used in the optimization step: !Image}{\\partial \\ln(x)} \\frac{\\partial \\ln(x)}{\\partial x} = \\frac{\\partial k(x)}{\\partial x}$ $\\frac{\\partial k(x)}{\\partial \\ln(x)} \\frac{1}{x} = \\frac{\\partial k(x)}{\\partial x}$ When implementing this the correct gradient is given (middle plot). TO REPRODUCE THIS BUG - see my branch. ### Steps\/Code to Reproduce TO REPRODUCE THIS BUG - see [my branch]([URL] specifically: [sklearn\/gaussian_process\/tests\/test_gpr_loglikelihood_check.py]([URL] ### Expected Results CRRECTION AND GROUND TRUTH TANGENTS CAN BE FOUND IN [my branch]([URL] specifically: [sklearn\/gaussian_process\/tests\/test_gpr_loglikelihood_check.py](h...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31366"
  },
  {
    "number":30199,
    "text":"Add \"mish\" activation function to sklearn.neural_network.MLPClassifier and make it the default\n\n### Describe the workflow you want to enable Currently, the default activation function for CODE] is \"relu\". However, there are several papers that demonstrate better results with \"mish\" = (x \u22c5 tanh(ln\u2061(1 + e^x))) = x \u22c5 tanh(softplus(x)). Some references: 1) According to [Mish: A Self Regularized Non-Monotonic Neural Activation Function According to Optimizing cnn-Bigru performance: Mish activation and comparative analysis with Relu According to Analyzing Lung Disease Using Highly Effective Deep Learning Techniques. 4) According to Double-Branch Network with Pyramidal Convolution and Iterative Attention for Hyperspectral Image Classification Not an academic paper, but still: [URL] ### Describe your proposed solution [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Pytorch has implemented mish: [URL] However, for my small perso...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30199"
  },
  {
    "number":29294,
    "text":"ConvergenceWarnings cannot be turned off\n\nHi, I'm unable to turn off convergence warnings from [CODE]. I've tried most of the solutions from, and none of them worked (see below for actual implementations): [URL] [URL] [URL] [URL] Contrary to what the designers of the sklearn's exceptions must have thought when it was implemented, some of us actually use stdout to log important information of the host program for diagnostics purposes. Flooding it with garbage that cannot be turned off, as is in the case with cross-validation, is not ok. To briefly speak to the severity of the issue, the above sklearn-specific questions relating to suppressing warnings have been viewed ~500K times with combined ~400 upvotes, and dates back 7 years. I've tried the following ([CODE] parameter does not appear to affect the result): [CODE_BLOCK] [CODE_BLOCK] [CODE_BLOCK] [CODE_BLOCK] ```py import contextlib import os, sys @contextlib.contextmanager def suppress_stdout(): with open(os.devnull, 'w') as fnu...",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29294"
  },
  {
    "number":26857,
    "text":"DOC Link to headers mis-poisitioned\n\n### Describe the issue linked to the documentation Link to a header in a page, e.g., the 'regression' subheading: [URL] directs a position a bit further down the page from the header. Tested on firefox and chromium. ![image]([URL] Potentially this is caused by the navbar, i.e. the header would start at the top of the page if there was no navbar. This may be unique to my system and happy to close if others can't replicate. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26857"
  },
  {
    "number":29205,
    "text":"classification_report with output_dict=True leads to brittle output\n\n### Describe the workflow you want to enable When [CODE] is [CODE], the returned [CODE] structure is brittle and breaks if one of the class name is the same as one of the average metrics. Here for example one of the class is named \"accuracy\" so that it doesn't appear in the returned [CODE] . [CODE_BLOCK] [CODE_BLOCK] ### Describe your proposed solution Any unambiguous output structure, such as separating between class-wise and average metrics: [CODE_BLOCK] Or: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29205"
  },
  {
    "number":32155,
    "text":"ColumnTransformer.fit() fails on polars.DataFrame: AttributeError: 'DataFrame' object has no attribute 'size'\n\n### Describe the bug Fitting a sklearn.compose.ColumnTransformer with more than one transformer on a polars.DataFrame yields the error: > AttributeError: 'DataFrame' object has no attribute 'size'  Fitting works fine when converting the DataFrame to pandas beforehand  Fitting also works fine with a polars DataFrame for as long as only a single transformer is passed to ColumnTransformer I am using the latest stable version of sklearn (1.7.2) and polars (1.33.1). Thank you so much for looking into this! ### Steps\/Code to Reproduce ```python import polars as pl from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer ## Generate toy data (polars DataFrame) df = pl.DataFrame({ 'some_categories': list('abc'), 'some_numbers': range(3) }) print(df) shape: (3, 2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 some_categories \u2506 some_numbers \u2502 \u2502 --- \u2506 --- \u2502 \u2502 str \u2506 i64 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 a \u2506 0 \u2502 \u2502 b \u2506 1 \u2502 \u2502 c \u2506 2 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ## Define a ColumnTransformer and fit on polars df -> AttributeError preprocessor = ColumnTransformer( transformers=[ ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), ['some_categories']), ('num', 'passthrough', [\"some_numbers\"]) ]) ## Fit on polars df preprocessor.fit(df) ## AttributeError: 'DataFrame' object has no attribute 'size' ## Fit on pandas df preprocessor.fit(df...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32155"
  },
  {
    "number":29583,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Aug 01, 2024) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29583"
  },
  {
    "number":30653,
    "text":"Update videos list with recent presentations\n\nThe [presentations.rst]([URL] page has very old resources. The last video listed is from 2013, over 10 years ago. There are updated videos on the playlists here: [URL] _Originally posted by @reshamas in [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30653"
  },
  {
    "number":26617,
    "text":"DOC Introduce dropdowns in the User Guide\n\n### Describe the issue linked to the documentation Dropdowns are implemented in #26625. They can help users avoid scrolling trough large pages and can quickly get them access to the content they are interested in. ### Suggest a potential alternative\/fix Use dropdowns to hide: - low hierarchy sections such as [CODE], [CODE], etc. See for instance the subsections in [3.3.2.16 Detection error tradeoff (DET)]([URL] - in-depth mathematical details; - narrative that is too use-case specific; - narrative that may only interest users that want to go beyond the pragmatics of a given tool. Additionally: - Do not use dropdowns for the low level section [CODE], as it should stay visible to all users. Make sure that the [CODE] section comes right after the main discussion with the least possible folded section in-between. - Be aware that dropdowns break cross-references. If that makes sense, hide the reference along with the text mentioning it. Else, do not use dropdown. For more information see [Contributing to documentation]([URL] notably the \"Guidelines for writing the User Guide and other reStructuredText documents\" dropdown. This is the list of sub-modules to be addressed: - [x] [1.1. Linear Models]([URL] #26623 - [x] [~~1.2. Linear and Quadratic Discriminant Analysis~~]([URL] - [x] [~~1.3. Kernel ridge regression~~]([URL] - [x] [1.4. Support Vector Machines]([URL] #26641 - [x] [1.5. Stochastic Gradient Descent]([URL] #26647 - [x] [1.6. Near...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26617"
  },
  {
    "number":31049,
    "text":"RFC adopt narwhals for dataframe support\n\nAt least as of [SLEP018]([URL] scikit-learn supports dataframes passed as [CODE]. In #25896 is a further place of current discussions. This issue is to discuss whether or not, or in which form, a future scikit-learn should depend on [narwhals]([URL] for general dataframe support. [CODE] wide df support [CODE] less maintenance within scikit-learn [CODE] external dependency @scikit-learn\/core-devs @MarcoGorelli",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31049"
  },
  {
    "number":28574,
    "text":"Implement temperature scaling for (multi-class) calibration\n\n### Describe the workflow you want to enable It would be great to have temperature scaling available as a post-hoc calibration method for binary and multi-class classifiers, for example in [CODE]. ### Describe your proposed solution Temperature scaling is a simple, efficient, and very popular post-hoc calibration method that also naturally supports the multi-class classification setting. It has been proposed in Guo et al. (2017) with >5000 citations, so it meets the inclusion criterion: [URL] It also does not affect rank-based metrics (if the temperature is restricted to positive values) unlike isotonic regression ([URL] Moreover, it avoids the infinite-log-loss problems of isotonic regression. Temperature scaling has been discussed in [URL] I experimented with different post-hoc calibration methods on 71 medium-sized (2K-50K samples) tabular classification data sets. For NNs and XGBoost, temperature scaling is competitive with isotonic regression and considerably better than Platt scaling (if Platt scaling is applied to probabilities, as implemented in scikit-learn, and not logits). For AUC, it is considerably better than isotonic regression. Here is a simple implementation using PyTorch (can be adapted to numpy). It is derived from the popular but no longer maintained implementation at [URL] with the following changes: - using inverse temperatures to prevent division by zero errors - using 50 optimizer steps inste...",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28574"
  },
  {
    "number":26083,
    "text":"RFC\/API (Array API) mixing devices and data types with estimators\n\nRight now, if the user fits an estimator using a [CODE], but passes a [CODE] during [CODE], they get a warning due to missing feature names. The situation is only to get more complicated as we're adding support for more types via array API. Some related issues here are: - device: data during [CODE] sits on a GPU, but a CPU is used for predict (with the same data type) - types: using one type to [CODE], and use another type during [CODE]: how do we handle this both in terms of device and the type? Do we let the operator figure out if they can coerce the data into the type which can be used? - persistence: how do we let users fit on one device, but load on another device - estimator conversion: do we let users convert an estimator which is fit using one type\/device, to an estimator compatible with another type\/device? I vaguely remember us talking about some of these issues, but I don't see any active discussion. I might have missed something. Related: in a library like [CODE], you can decide which device is going to be used when you load a model's weights. cc @thomasjpfan",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26083"
  },
  {
    "number":27047,
    "text":"Inconsistency in zero_division handling between precision\/recall\/f1 and precision_recall_curve\/roc_curve related metrics\n\n### Describe the workflow you want to enable The API offers the possibility to set the behavior upon stumbling upon a zero division issue when no positive label is present in the dataset, upon computing [CODE], [CODE] or [CODE] using the keyword argument: zero_division{\u201cwarn\u201d, 0.0, 1.0, np.nan}, default=\u201dwarn\u201d Sets the value to return when there is a zero division. Notes: - If set to \u201cwarn\u201d, this acts like 0, but a warning is also raised. - If set to np.nan, such values will be excluded from the average. New in version 1.3: np.nan option was added. [CODE], [CODE], [CODE], [CODE], [CODE], despite having to compute precision or recall under the hood do not offer the same possibility. While this is unlikely to pose problem in the micro averaging setting, it becomes more likely in the sample (AP and LRAP, possibly roc_auc_score) or macro averaging setting. For instance [CODE] is not using the [CODE] or [CODE] functions: [URL] In this implementation recall=1 and precision=0 when there is no positive example. ### Describe your proposed solution Use the implemented [CODE] and [CODE] in all precision-recall and ROC curve functions. Add the same [CODE] kwarg and forward it to the [CODE] and [CODE] functions ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I think this discussion is linked to: - #24381 that is associated ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27047"
  },
  {
    "number":28666,
    "text":"AttributeError: 'OneHotEncoder' object has no attribute '_drop_idx_after_grouping'\n\n### Describe the bug Context: I am encountering an issue while using the OneHotEncoder class from scikit-learn. When trying to transform my data using the transform method, I receive the following error: AttributeError: 'OneHotEncoder' object has no attribute '_drop_idx_after_grouping' Steps to Reproduce: Import the necessary modules and classes from scikit-learn. Create an instance of the OneHotEncoder class. Call the transform method on the OneHotEncoder object with the input data. Expected Behavior: I expected the OneHotEncoder class to transform the input data into one-hot encoded format without any errors. Actual Behavior: Instead, the AttributeError is raised, indicating that the '_drop_idx_after_grouping' attribute does not exist. Additional Information: I have verified that I am using the latest version of scikit-learn. I have not directly referenced or used the '_drop_idx_after_grouping' attribute in my code, so I suspect it may be an internal issue within the scikit-learn library. Environment: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results While developing the project for web I was expecting the winning probability of a team, but I got Attribute error. ### Actual Results ```pytb AttributeError: 'OneHotEncoder' object has no attribute '_drop_idx_after_grouping' Traceback: File \"C:\\Users\\Tnluser.PG02YSJ5\\IPL_win_predictor\\pythonProject2\\.venv\\Lib\\site-packag...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28666"
  },
  {
    "number":25856,
    "text":"Sampling uncertainty on precision-recall and ROC curves\n\n### Describe the workflow you want to enable We would like to add the possibility to plot sampling uncertainty on precision-recall and ROC curves. ### Describe your proposed solution We (@mbaak, @RUrlus, @ilanfri and I) published a paper in AISTAT 2023 sigmas uncertainty grid rectangle (based on first-order approximation of the covariance matrix, with the bivariate normal distribution assumption) 4. For each of these hypothesis point in the grid, compute the test static with the observed point, called the profile log likelihood ratio (using the fact that the confusion matrix follows a multinomial distribution). 5. Plot the 3 sigmas contour (i.e. isoline) for the observed points (using Wilks\u2019 theorem stating that the profile log likelihood ratio is described asymptotically by a chi2 distribution) We have a minimal pure Python implementation: [URL] And a C++ implementation: the paper is supported by our package [ModelMetricUncertainty]([URL] which has a C++ core with, optional, OpenMP support and Pybind11 bindings. Note that this package contains much more functionality than the above notebook. The core is binding agnostic allowing a switch to Cython if ne...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25856"
  },
  {
    "number":27626,
    "text":"Isolation Forest Bug with Sparse Matrix and Contamination as Float\n\n### Describe the bug ### Environment: [CODE_BLOCK] ### Bug Summary: When using the Isolation Forest algorithm from the Scikit-learn library with a sparse matrix as input data and explicitly setting the contamination parameter as a float (rather than 'auto'), a bug is encountered that stops the algorithm during the fit operation. ### Expected Behavior: The Isolation Forest algorithm should run without errors and produce anomaly scores as expected, with the contamination level set according to the specified float value. ### Actual Behavior: When the contamination parameter is set as a float value, the Isolation Forest algorithm may encounter issues during model fitting or produce unexpected results. The bug might lead to incorrect anomaly detection and, in some cases, throw an error. ### Additional Information: The bug is not observed when the contamination parameter is set to 'auto' or when dense matrices are used. The bug's impact may vary depending on the specific version of Scikit-learn being used. This bug may be related to the handling of sparse data and the contamination parameter in the Isolation Forest implementation. ### Steps\/Code to Reproduce ### Not working with contamination as a float [CODE_BLOCK] ### Properly working with contamination = 'auto'\/not specified ```python from sklearn.datasets import make_classification from scipy.sparse import csc_matrix from sklearn.ensemble import IsolationForest...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27626"
  },
  {
    "number":31733,
    "text":"Add More Data to the RidgeCV, LassoCV, and ElasticNetCV Path\n\n### Describe the workflow you want to enable Currently, the mse_path_ is available from the above models, which lets you inspect\/plot the mse for all folds, alphas, and l1_ratios for elasticnet for instance. It would be very nice to record not only the mse in this way, but also the coefficients and possibly the in-sample\/validation score. ### Describe your proposed solution Add variables that include the coefficients and maybe the score. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31733"
  },
  {
    "number":24830,
    "text":"\u26a0\ufe0f CI failed on Linux_Docker.debian_atlas_32bit \u26a0\ufe0f\n\nCI failed on Linux_Docker.debian_atlas_32bit - test_inverse_transform[50-float32-False-10-expected_mixing_shape5]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24830"
  },
  {
    "number":28677,
    "text":"Array API support for cross_validation and friends\n\nNow that #28407 was merged, we need to adopt other cross-validation and model selection tools, starting with [CODE]. Currently it fails with: [CODE_BLOCK] ```python-traceback \/Users\/ogrisel\/code\/scikit-learn\/sklearn\/utils\/validation.py:109: UserWarning: You are comparing a array_api_strict dtype against a NumPy native dtype object, but you probably don't want to do this. array_api_strict dtype objects compare unequal to their NumPy equivalents. Such cross-library comparison is not supported by the standard. if X.dtype == np.dtype(\"object\") and not allow_nan: \/Users\/ogrisel\/miniforge3\/envs\/dev\/lib\/python3.11\/site-packages\/array_api_strict\/_indexing_functions.py:16: UserWarning: You are comparing a array_api_strict dtype against a NumPy native dtype object, but you probably don't want to do this. array_api_strict dtype objects compare unequal to their NumPy equivalents. Such cross-library comparison is not supported by the standard. if indices.dtype not in _integer_dtypes: Traceback (most recent call last): Cell In[14], line 10 cross_validate(LinearDiscriminantAnalysis(), xp.asarray(X), xp.asarray(y)) File ~\/code\/scikit-learn\/sklearn\/utils\/_param_validation.py:213 in wrapper return func(args, *kwargs) File ~\/code\/scikit-learn\/sklearn\/model_selection\/_validation.py:423 in cross_validate results = parallel( File ~\/code\/scikit-learn\/sklearn\/utils\/parallel.py:67 in __call__ return super().__call__(iterable_with_config) File ~\/mini...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28677"
  },
  {
    "number":25766,
    "text":"1.2.1: cannot build documentation without installing module\n\n### Describe the bug Looks like something is wrong and I cannot build docuemtation without installing module. ### Steps\/Code to Reproduce N\/A ### Expected Results It should be possible to build documentation out of only what is in source tree a d withoit have installed module. ### Actual Results <details> ```console ++ ls -1d lib.linux-x86_64-cpython-38 + PYTHONPATH=\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/build\/lib.linux-x86_64-cpython-38 + PBR_VERSION=1.2.1 + PDM_PEP517_SCM_VERSION=1.2.1 + SETUPTOOLS_SCM_PRETEND_VERSION=1.2.1 + \/usr\/bin\/sphinx-build -n -T -b man doc build\/sphinx\/man Running Sphinx v6.1.3 Traceback (most recent call last): File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 48, in <module> from ._check_build import check_build # noqa ModuleNotFoundError: No module named 'sklearn.__check_build._check_build' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 351, in eval_config_file exec(code, namespace) # NoQA: S102 File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/doc\/conf.py\", line 20, in <module> from sklearn.externals._packaging.version import parse File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__init__.py\", line 81, in <module> from . import __check_build # noqa: F401 File \"\/home\/tkloczko\/rpmbuild\/BUILD\/sciki...",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25766"
  },
  {
    "number":28274,
    "text":"Trigger lockfile update with a comment\n\nI've never been able to run the script to update the lock files w\/o errors. My last attempt resulted in [URL] which also didn't work, and that's after I had to install conda on my env, which I don't usually have since I use micro mamba. It would be nice to be able to trigger a bot to update lock files the same way you can re-render on conda-forge. A comment with value similar to [CODE] could update the lock files on the PR. WDYT @lesteve ?",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28274"
  },
  {
    "number":30936,
    "text":"SelectFromModel does not work when ElasticNetCV has multiple l1 ratios\n\n### Describe the bug Using [CODE] with the automatic [CODE] does not work if the [CODE] is estimated from the data, i.e., if the user provides a list of floats. ### Steps\/Code to Reproduce [CODE_BLOCK] This fails with: [CODE_BLOCK] because [CODE] calls [CODE] which returns an array with as many elements as l1 ratios. ### Expected Results Calling [CODE] should return an ndarray of str according to the best model estimating with CV. ### Actual Results ``` Traceback (most recent call last): File \"\/.venv\/lib\/python3.12\/site-packages\/IPython\/core\/interactiveshell.py\", line 3577, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"<ipython-input-12-304146ab06de>\", line 1, in <module> model.get_feature_names_out() File \"\/.venv\/lib\/python3.12\/site-packages\/sklearn\/feature_selection\/_base.py\", line 190, in get_feature_names_out return input_features[self.get_support()] ^^^^^^^^^^^^^^^^^^ File \"\/.venv\/lib\/python3.12\/site-packages\/sklearn\/feature_selection\/_base.py\", line 67, in get_support mask = self._get_support_mask() ^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/.venv\/lib\/python3.12\/site-packages\/sklearn\/feature_selection\/_from_model.py\", line 305, in _get_support_mask threshold = _calculate_threshold(estimator, scores, self.threshold) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/.venv\/lib\/python3.12\/site-packages\/skle...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30936"
  },
  {
    "number":30113,
    "text":"Add gradient clipping to SGDRegressor for stability and differential privacy\n\n### Describe the workflow you want to enable Add gradient clipping to SGDRegressor to: 1. Improve training stability when dealing with outliers or ill-conditioned data 2. Enable differentially private regression by bounding the influence of any single observation This addition would enable users to: - Train more stable models on real-world data with outliers - Implement differentially private regression with minimal additional code - Control the maximum influence of any single observation on the model ### Describe your proposed solution Add a CODE] parameter to SGDRegressor that bounds the L2 norm of gradients during training: [CODE_BLOCK] Implementation would involve: 1. Add [CODE] parameter (default: None for no clipping) 2. Add gradient clipping in the update step: [CODE_BLOCK] ### Research basis and precedent Gradient clipping is well-established: - Introduced in [\"On the difficulty of training Recurrent Neural Networks\" - Core component of differentially private SGD as formalized in [\"Deep Learning with Differential Privacy\"]([URL] (2016, [6000+ citations]([URL] Implemented in: - PyTorch: [CODE] - TensorFlow: [CODE] - JAX: [CODE] - Scikit-learn's neural networks ### Benefits 1. Stability: Prevents exploding gradients and improves convergence on ill-conditioned problem...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30113"
  },
  {
    "number":26114,
    "text":"ElasticNet does not support sparse matrices\n\n### Describe the bug Documentation says that I can use ElasticNet with [CODE], but I can't make it work with sparse. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Works fine ### Actual Results Falls check in [CODE] with [CODE] because [CODE] is [CODE] by default for [CODE]. ``` --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[34], line 4 2 y = A[:, 0] 3 regr = ElasticNet() ----> 4 regr.fit(A, y) File ~\/.e\/lib\/python3.9\/site-packages\/sklearn\/linear_model\/_coordinate_descent.py:918, in ElasticNet.fit(self, X, y, sample_weight, check_input) 907 X_copied = self.copy_X and self.fit_intercept 908 X, y = self._validate_data( 909 X, 910 y, (...) 916 y_numeric=True, 917 ) --> 918 y = check_array( 919 y, order=\"F\", copy=False, dtype=X.dtype.type, ensure_2d=False 920 ) 922 n_samples, n_features = X.shape 923 alpha = self.alpha File ~\/.e\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py:845, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name) 843 if sp.issparse(array): 844 _ensure_no_complex_data(array) --> 845 array = _ensure_sparse_format( 846 array, 847 accept_sparse=accept_sparse, 848 dtype=dtype, 849 copy=copy, 850 force_all_finite=force_all_finite, 851 ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26114"
  },
  {
    "number":31349,
    "text":"Add Multiple Kernel Learning (MKL) for Support Vector Machines (SVM)\n\n### Describe the workflow you want to enable I propose adding a Multiple Kernel Learning (MKL) to scikit-learn. MKL is a more advanced approach compared to GridSearchCV, offering a way to combine multiple kernels into a single, optimal kernel. In the worst case, MKL will behave like GridSearchCV by assigning a weight of 1 to the best kernel, but in the other cases, it will provide a weighted combination of kernels for better generalization. ### Describe your proposed solution I have already implemented a complete MKL solution for regression, binary and multi-class classification, and clustering (One-Class). This implementation includes the SimpleMKL algorithm and SumMKL (simply sums the kernels) algorithms. This implementation is available on a [previously closed pull request]([URL] ### Describe alternatives you've considered, if relevant An alternative would be to continue relying on GridSearchCV for kernel selection. However, GridSearchCV is limited to selecting only one kernel and does not consider the possibility of combining multiple kernels, which can result in suboptimal performance. MKL provides a more sophisticated approach by optimizing kernel weights, leading to better performance in many machine learning tasks.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31349"
  },
  {
    "number":25707,
    "text":"ENH HistGradientBoosting estimators should have [CODE] attribute\n\n### Describe the workflow you want to enable The practicality of using the CODE] attribute to superficially analyze global explanations of your classifier is really useful, IMHO. Right after evaluating the model, running something like [CODE_BLOCK] is the first thing I do. Furthermore, some variable selection strategies assume that your estimator has this attribute, and it seems suboptimal to me to deprive ourselves of using [CODE] with these techniques. [CODE_BLOCK] I understand the concerns raised by @ogrisel's [comment in the original PR that implemented HistGradientBoosting ### Describe your proposed solution _No response_ ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25707"
  },
  {
    "number":26912,
    "text":"Add leaky_relu to Neural Network\n\n### Describe the workflow you want to enable I would like to use the leaky_relu activation function with MLP Classifier , I thought it has already become quite popular due to its ability to be more robust to noise as compared to relu & avoid the dying relu problem. I think we should consider implementing it in sklearn.nn . I'm willing to implement this. ### Describe your proposed solution [CODE] ### Describe alternatives you've considered, if relevant None ### Additional context None",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26912"
  },
  {
    "number":29620,
    "text":"[CODE] in [CODE] classes while [CODE] is the convention in [CODE] and [CODE] classes?\n\n### Describe the issue linked to the documentation Currently most ensembling methods in [CODE] such as [bagging methods]([URL] and independent multioutput classes ([[CODE]]([URL] and [[CODE]]([URL] use the [CODE] parameter during instantiation, but [CODE] and [CODE] use the [CODE] parameter, which I think is the old convention? Or is there any other reason to continue to use [CODE] for [CODE] and [CODE]? Even the stacking methods use [CODE] the plural term as a matter of consistence. ### Suggest a potential alternative\/fix Perhaps, the class definitions and documentations for [CODE] and [CODE] needs to be updated?",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29620"
  },
  {
    "number":28400,
    "text":"More intuitive check on GaussianMixture initialization\n\n### Describe the workflow you want to enable Hey! I am using GaussianMixture and testing different initialization methods (kmeans, random, k-means++ and random_from_data). When I ran the code using [CODE] an error was produced in [CODE] [CODE], saying that responsibilities were referenced before assignment. Of course this is because I initialized the [CODE] incorrectly and typed [CODE] instead of [CODE] (without the dash). ### Describe your proposed solution It would be great if init_params was checked upon initialization of GaussianMixture: something like \"if self.init_params not in ['random', 'random_from_data', 'kmeans', 'k-means++']\" would produce an error. Alternatively (and maybe even more robust), one could add the extra else statement in _base.py _initialize_parameters to make sure the initialization technique is spelled correctly. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28400"
  },
  {
    "number":26520,
    "text":"Bug in the decision function of AdaBoost SAMME\n\nFrom the original paper ([URL] it is mentioned that the decision function of each weak-learner should have a symmetry such that the sum of the decision should be equal to zero. However, this is not the case: [CODE_BLOCK] Here, the [CODE] entry should be weighted to [CODE] instead. However, this is the case for the SAMME.R algorithm: ```python In [10]: from sklearn.ensemble import AdaBoostClassifier ...: from sklearn.datasets import make_classification ...: X, y = make_classification(n_samples=1000, n_features=4, ...: n_informative=2, n_redundant=0, ...: random_state=0, shuffle=False, n_classes=3, n_clusters_per_class=1) ...: clf = AdaBoostClassifier(n_estimators=1, random_state=0, algorithm=\"SAMME.R\") ...: clf.fit(X, y) Out[10]: AdaBoostClassifier(n_estimators=1, random_state=0) In [11]: clf.decision_function(X) Out[11]: array([[-2.65127772, 1.2727494 , 1.37852832], [ 4.84854331, -1.10251582, -3.7460275 ], [ 4.84854331, -1.10251582, -3.7460275 ], ..., [-2.65127772, 1.2727494 , 1.37852832], [-2.65127772, 1.2727494 , 1.37852832], [-2.6512...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26520"
  },
  {
    "number":32167,
    "text":"[CODE] errors with [CODE] dataframe and [CODE]\n\n### Describe the bug Having polars dataframe with [CODE] lets [CODE] crash. Maybe related to the warnings reported in this issue [URL] But here, [CODE] errors. ### Steps\/Code to Reproduce A MWE [CODE_BLOCK] ### Expected Results No error is shown. ### Actual Results ``` ...\/python3.12\/site-packages\/sklearn\/utils\/_indexing.py:259, in _safe_indexing(X, indices, axis) 248 raise ValueError( 249 \"'X' should be a 2D NumPy array, 2D sparse matrix or \" 250 \"dataframe when indexing the columns (i.e. 'axis=1'). \" 251 \"Got {} instead with {} dimension(s).\".format(type(X), len(X.shape)) 252 ) 254 if ( 255 axis == 1 256 and indices_dtype == \"str\" 257 and not (_is_pandas_df(X) or _use_interchange_p...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32167"
  },
  {
    "number":26158,
    "text":"Confusion Matrix is 1x1 instead of NxN if all labels and predicted labels are the same\n\n### Describe the bug A confusion matrix with all correctly predicted labels will generate a 1x1 confusion matrix, instead of a nxn confusion matrix (for n classes) ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26158"
  },
  {
    "number":29369,
    "text":"Erroneous optional status for y parameter in RepeatedStratifiedKFold.split\n\n### Describe the bug For context, there is a small difference in the [CODE] function between the variants of the [CODE] class: In class [CODE], the [split function]([URL] has an optional parameter [CODE] (same for class [[CODE]]([URL] In class [CODE], the same [parameter [CODE] is mandatory]([URL] because [\"Stratification is done based on the y labels\"]([URL] As expected, omitting [CODE] when calling [CODE] causes an explicit error: [CODE_BLOCK] However [[CODE]]([URL] is also a stratified variant which requires parameter [CODE], but the parameter is [erroneously left as optional]([URL] This seems due to the fact this is implemented through a general class [[CODE]]([URL] As a result, not providing [CODE] causes an unclear error message inconsistent with the one for [CODE] in the same context. ### Steps\/Code to Reproduce ```py from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, RepeatedStratifiedKFol...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29369"
  },
  {
    "number":28710,
    "text":"Misleading OpenMP warning on MacOS when building with Meson\n\n### Describe the bug Compiling on MacOS with openmp works the old way, see [URL] - [CODE] - [CODE_BLOCK] - Build [CODE] With the new meson build system, a warning is raised and scikit-learn is built without openmp: - same as above, just the last commant is [CODE] instead of [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No warning and it compiles with openmp enabled. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.7 (main, Dec 4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)] executable: \/Users\/lorentzen\/github\/python3_sklearn\/bin\/python machine: macOS-14.4-x86_64-i386-64bit Python dependencies: sklearn: 1.5.dev0 pip: 24.0 setuptools: 68.0.0 numpy: 1.26.0 scipy: 1.11.3 Cython...",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28710"
  },
  {
    "number":25078,
    "text":"check_array does not gracefully fail with pd.NA\n\nWhen a NumPy array or a pandas series contains [CODE], it will not gracefully fail. [CODE_BLOCK] The reason is that we [CODE] intend the following: ```python In [7]: s.to_numpy() != s.to_numpy() <ipython-input-7-e1263eaa78fa>:1: DeprecationWarning: elementwise comparison failed; this will raise an error ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25078"
  },
  {
    "number":28479,
    "text":"Allow [CODE] in feature selectors when estimator does Imputation\n\n### Describe the workflow you want to enable I would like to perform feature selection (e.g.: [CODE]) on data containing \"NaN\/null\" values where my estimator does the imputation in a pipeline. example [CODE_BLOCK] ### Describe your proposed solution As far as I understand this could work as is!? In the docs it says that: > This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator. As shown the estimator can deal with \"nan\" values and can compute the cv score. Is there is reason this currently is not working\/supported or can this be considered a \"bug\" that it crashes although it would work? ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28479"
  },
  {
    "number":29626,
    "text":"Add optional return of STD for kNeighboursRegressor\n\n### Describe the workflow you want to enable I would like to propose to add the option to get the standard deviation from the KNeighborsRegressor. The [CODE] function already delivers the mean, as that's the way the target is calculated, so adding the standard deviation should not be a big deal. ### Describe your proposed solution Similar to the [CODE] Function of the [CODE], a switch for the [CODE] function of the [CODE] could be implemented. This can easily be implemented by e.g. changing [CODE_BLOCK] to [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29626"
  },
  {
    "number":24430,
    "text":"[CODE] crashes with [CODE]\n\n### Describe the bug I'm trying to make a simple stacking and getting the cross validation score but an error raises: [CODE] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results It should work fine if I am understanding everything right. ### Actual Results [CODE] <details> <summary>Full traceback<\/summary> ``` --------------------------------------------------------------------------- _RemoteTraceback Traceback (most recent call last) _RemoteTraceback: \"\"\" Traceback (most recent call last): File \"\/home\/guillem.garcia\/miniconda3\/envs\/skbug\/lib\/python3.10\/site-packages\/joblib\/externals\/loky\/process_executor.py\", line 436, in _process_worker r = call_item() File \"\/home\/guillem.garcia\/miniconda3\/envs\/skbug\/lib\/python3.10\/site-packages\/joblib\/externals\/loky\/process_executor.py\", line 288, in __call__ return self.fn(self.args, self.kwargs) File \"\/home\/guillem.garcia\/miniconda3\/envs\/skbug\/lib\/python3.10\/site-packages\/joblib\/_parallel_backends.py\", line 595, in __call__ return self.func(args, kwargs) File \"\/home\/guillem.garcia\/miniconda3\/envs\/skbug\/lib\/python3.10\/site-packages\/joblib\/parallel.py\", line 262, in __call__ return [func(args, *kwargs) File \"\/home\/guillem.garcia\/miniconda3\/envs\/skbug\/lib\/python3....",
    "labels":[
      "Bug",
      "Enhancement"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24430"
  },
  {
    "number":25499,
    "text":"CalibratedClassifierCV doesn't work with [CODE]\n\n### Describe the bug CalibratedClassifierCV with isotonic regression doesn't work when we previously set [CODE]. The IsotonicRegression seems to return a dataframe, which is a problem for [CODE] in [CODE] where it tries to put the dataframe in a numpy array row [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results It should not crash. ### Actual Results ``` ..\/core\/model_trainer.py:306: in train_model cv_predictions = cross_val_predict(pipeline, ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/sklearn\/model_selection\/_validation.py:968: in cross_val_predict predictions = parallel( ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/joblib\/parallel.py:1085: in __call__ if self.dispatch_one_batch(iterator): ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/joblib\/parallel.py:901: in dispatch_one_batch self._dispatch(tasks) ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/joblib\/parallel.py:819: in _dispatch job = self._backend.apply_async(batch, callback=cb) ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/joblib\/_parallel_backends.py:208: in apply_async result = ImmediateResult(func) ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packages\/joblib\/_parallel_backends.py:597: in __init__ self.results = batch() ..\/..\/..\/..\/.anaconda3\/envs\/strategy-training\/lib\/python3.9\/site-packag...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25499"
  },
  {
    "number":26933,
    "text":"Confusion in K-means Initialization of Gaussian Mixture Model\n\n### Describe the issue linked to the documentation In 'scikit-learn\/sklearn\/mixture \/_base.py' line 115, when you refers to initialize the model parameters by K-means algorithm (note that not K-means++), you use the function KMeans from sklearn.cluster with default settings. However, when checking 'scikit-learn\/sklearn\/cluster \/_kmeans.py' line 1390, the default 'init' setting of KMeans is 'kmeans++'. So there might be a confusion. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26933"
  },
  {
    "number":29089,
    "text":"RFC Future of HalvingGridSearchCV\n\n[CODE] has been in experimental mode since its conception in 2020-09, almost 4 years ago. Things to note: - we haven't seen many issues regarding these estimators, but that is probably because we are not advertising them enough, and they're in experimental mode. - an issue such as [URL] gets very minimal traction and there are not many active maintainers who are actively maintaining that part of the codebase - the API as is, is confusing and we could work on clearing things up and improving it, which shouldn't be painful since it's still experimental and we don't need to go through deprecation cycles Now the question is, what do we think about it? Options we have: - move them out of experimental w\/o change: I don't think this is a good idea and we should probably improve things before doing so - remove from scikit-learn: they are useful estimators, but are they used? Do we think making them more prominent in our documentation would help? Do we want to do that? - improve the status quo of the estimator, improve documentation, move out of experimental: is this a priority for us? Are people here who are willing to dedicate time to work on it \/ review the work? Also, would be nice to see if they're used, and if yes, how. Maybe @amueller or @NicolasHug would have an idea here? cc @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29089"
  },
  {
    "number":24819,
    "text":"Reduce warnings in test_logistic.py\n\n### Description The tests of [CODE] in [CODE] produce currently 468 warnings (run [CODE]). This is quite a lot and should be reduced. ### Remark To explore the cause for the warnings, run [CODE].",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24819"
  },
  {
    "number":26524,
    "text":"GridSearchCV for neural networks that allows evaluation of the validation set after every epoch\n\n### Describe the workflow you want to enable I would like to be able to perform a grid search of hyperparemeters for a neural network using crossvalidation such as with GridSearchCV. I was able to set everything up in Scikit-Learn and the KerasRegressor wrapper but I then found out that my model was not evaluating on the validation set until the end of the number of epochs I set. This does not enable to track training and validation loss throughout training hence, assessing the performance of my model and taking action to e.g., use early stopping to prevent overfitting. ### Describe your proposed solution enable GridSearchCV to pass the validation data to the estimator during fitting so validation loss can be tracked throughout training. Actually, If I understand how GridSearchCV works correctly, I think that perhaps an entire new DeepLearningGridSearchCV class would be more appropriate because I think that GridSearchCV follows (at least roughly) these steps: 1. Create data folds using CV strategy provided. 2. fit training data to estimator (pipeline including scaling and model in my case) 3. evaluate on validation data. The process I was looking for would merge steps 2 and 3 so that: 1. Create data folds using CV strategy provided. 2. fit training and validation data to estimator so validation data can be used for model assessment after every epoch throughout training ### Describ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26524"
  },
  {
    "number":25000,
    "text":"Feature request: an additional config context for forcing conversion toward a specific Array API-compatible array library.\n\n### Describe the workflow you want to enable Per [URL] such workflow is possible: [CODE_BLOCK] (see [URL] ) I would like to additonally enable following workflow: [CODE_BLOCK] where [CODE] are the same in both examples, and in both examples compute has been done with [CODE]. The difference is that we pass numpy inputs and rely on the estimator to convert using the appropriate array namespace. The added value here for the user is that the [CODE] function could be used to load arbitrary inputs to the array library relying on its [CODE] method, in a way that is consistent to [CODE] requirements accross all array libraries. Currently, a user that would have data formated as, let's say, a list of lists, has two choices: - either pass the list of list directly to [CODE], but then since the list of list does not have a [CODE] attribute it will fallback to numpy array namespace - either convert first the input to [CODE], but then, unless the user re-implements methodically [CODE] with [CODE] conversion, the behavior of the conversion might differ from what [CODE] would do when converting to [CODE], and anyway reimplementing [CODE] is not very efficient to begin with. ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25000"
  },
  {
    "number":26762,
    "text":"MLPClassifier don't support target with multiclass-multioutput input.\n\n### Describe the workflow you want to enable I once expanded my [software]([URL] algorithm input datatype and found that it did not support multiclass-multioutput in the main algorithm (similar to [MLPClassifier in sklearn]([URL] Therefore, I integrated continuous-multioutput, multilabel-indicator, and all target inputs with dimension at 2d into the algorithm. [CODE_BLOCK] I use [is_target_2d]([URL] to detect the type_of_target to support any 2d input coordinated with inner criterion and outer bi-directional transform, like fit_transform and inverse_transform. ### Describe your proposed solution I think [CODE] in [CODE] should not be so robust that use 'multioutput' in self.y_type_ to filter [CODE], multilabel is restricted in the transform module due to the marking of [CODE] named y_type_ and y_is_multilabel with [output]([URL] -> The object was not fitted with multilabel input. ### Describe alternatives you've considered, if relevant I found multiclass.py and multioutput.py in the first level of scikit-learn organizational structure, and I wonder whether these...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26762"
  },
  {
    "number":31533,
    "text":"RFC: stop using scikit-learn [CODE] and instead use [CODE] directly\n\nAs discussed in [URL] our current [CODE] function brings very little value to the user: it does extra computation to check that [CODE] and raises a warning otherwise. However, in most cases, users can do nothing about the warning. Furthermore, as seen in the CI of #30878, the array API compatible libraries we test against do not have the same numerical stability behavior for [CODE] and [CODE], so it makes it challenging to write a test for the occurrence of this warning that is consistent across libraries. So I would rather not waste the overhead of computing [CODE] and just always directly call [CODE] or [CODE] and deprecate [CODE].",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31533"
  },
  {
    "number":28229,
    "text":"Trees fitted in 1.3.2 produce different outcome when evaluated in 1.4\n\n### Describe the bug We have a number of ensemble tree models in production fitted using 1.3.2 or older. Many of these models produce different outcomes when evaluated on the same data in sklearn 1.3.2 and 1.4. Analysis led me to the change in CODE] from this PR: [URL] Based on this conversation I understand that in order to support monotonicity constraint, probabilities were allowed to be outside of [0, 1] bounds as see in this diff: ![image [MSC v.1937 64 bit (AMD64)] executable: C:\\Users\\smozharov\\AppData\\Local\\miniconda3\\envs\\env311_3a\\python.exe machine: Windows-10-10.0.22631-SP0 Python dependencies: sklearn: 1.4.0 pip: 23.3.2 se...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28229"
  },
  {
    "number":25946,
    "text":"Output redirection is broken\n\n### Describe the bug I'm trying to redirect output from a script that uses Scikit-Learn to a file, as follows: [CODE] This should redirect both stdout and stderr to _mylog.log_. Most of my code uses logging statements which, as expected, are written to stderr and appear in mylog.log. Here's an example: [CODE] This works fine. What I'm trying to do is use [CODE] to tune hyperparameters for my model and (no surprise) this is taking a while. I've therefore turned verbosity up 4, as follows: [CODE_BLOCK] When output isn't redirected I get the expected output from GridSearchCV in the console as follows: ``` [CV 2\/5] END bootstrap=True, criterion=gini, max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200;, score=0.712 total time= 24.5s [CV 5\/5] END bootstrap=True, criterion=gini, max_de...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25946"
  },
  {
    "number":24274,
    "text":"Histogram GBDT can segfault if categorical contains negative categories\n\nHistogram GBDT can segfault if they contain negative categories. Indeed, we documented that they should all be positive in [CODE] but segfaulting is probably not the best error message. I triggered the problem on my Mac M1 with the following code: [CODE_BLOCK] It is not obvious that it will always segfault (it did not do that on Linux). I got the following traceback on a similar code snippet in Linux: ```pytb Traceback (most recent call last): File \"\/home\/glemaitre\/miniconda3\/envs\/dev\/lib\/python3.8\/site-packages\/joblib\/externals\/loky\/process_executor.py\", li...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24274"
  },
  {
    "number":27459,
    "text":"Use github.com\/apssouza22\/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs.\n\n### Describe the workflow you want to enable Adding this conversational UI would enable people to 'talk' directly with the backend and API requests to be carried out more effectively. RAG can help with some of the problems function calling by language models face at the moment. ### Describe your proposed solution [Chatflow]([URL] ### Describe alternatives you've considered, if relevant They're all poopo ### Additional context I'm trying to accelerate the adoption of natural language interfaces.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27459"
  },
  {
    "number":28341,
    "text":"RFC switch to Polars as the default dataframe lib in our examples\n\nWe now support polars as an output for our transformers and [CODE], but our examples use [CODE]. Our [CODE] module also returnes either a numpy array or a pandas DataFrame. I'm suggesting that we enable datasets to return a polars dataframe, and to switch our examples from pandas to polars. This has a few benefits: - Polars on users' systems takes advantage of multi-core CPUs which is the case for pretty much all users these days. So it's quite faster in most cases. - Pandas is dealing with issues related to Arrow, and even if they don't require Arrow as a required dependency, there will be behavior changes whether Arrow is installed or not at least on String Dtype. Another thing is the (in)stability issues related to Pandas API where we need to deal with deprecation warnings very often. Although I'm not sure how stable the API is on the places where we touch the API on the polars side (maybe cc @MarcoGorelli ) WDYT @scikit-learn\/core-devs @scikit-learn\/documentation-team @scikit-learn\/contributor-experience-team This is not really a core part of our library, but what we put in our examples and our default choices affect people since many learn from our examples.",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28341"
  },
  {
    "number":26402,
    "text":"SGDClassifier should use sparse representation for coefficients\n\n### Describe the workflow you want to enable [CODE] should take a [CODE] initial parameter (default false). If set to [CODE], the initial [CODE] will be sparse. For models like [CODE] (default 1M features), having a dense coefficient is very inefficient. ### Describe your proposed solution See above. ### Describe alternatives you've considered, if relevant Automatically infer [CODE]'s sparsity based on input's spasity! ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26402"
  },
  {
    "number":28993,
    "text":"MemoryLeak in [CODE]\n\n### Describe the bug repro  Repeatedly call CODE] on same size feature matrix. Or run the attached repro-script. expected  The max memory allocated is steady regardless the number of train calls. actual  Memory usage grows linear with the number of calls suggesting a memory leak. notes  I'm using [CODE], and [CODE]  Repro steps works on OSX (M1 Macbook) and on a AWS t3.large running the \"Amazon Linux\" OS.  Based on [this thread -> None: np.random.seed(42) X = np.random.rand(sample_count, feature_dim) y = np.random.randint(2, size=sample_count) if train_type == \"LR\": clf = LogisticRegression(max_iter=20) elif train_type == \"SGD\": clf = SGDClassifier(loss=\"log_loss\", max_iter=20) clf.fit(X, y) def run(reps, train_type): tracemalloc.start() with warnings.catch_warnings(): warnings.simplefilter(\"ignore\") for _ in range(reps): do_train(train_type) base, peak = tracemalloc.get_traced_memory() tracemalloc.stop() print(f\"| {reps} | {train_type} | {base\/ 10242:.2f} | {peak\/ 10242:.2f} | {(peak-base)\/ 10242:.2f}\") sample_count = 5000 feature_dim = 200 expected_memory_usage_features = sample_count  feature_dim  8 expected_memory_usage_classifier = 2  feature_dim * 8 expected_memory_usage_mb = (expected_memory_usage_featu...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28993"
  },
  {
    "number":25859,
    "text":"How to early stop in GradientBoostingClassifer?\n\nHow to track the model performance on an eval set that is provided from outside and early stop the tree building based upon the result? Currently there is the option of [CODE_BLOCK] along with [CODE_BLOCK] available in the implementation The issue with that approach: 1. Cannot use k-fold cross-validation. 2. Canont use custom metrics Currently I solve it using the following, which is kind of hacky [CODE_BLOCK]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25859"
  },
  {
    "number":26418,
    "text":"RFC Supporting [CODE]\n\n### Context SciPy is now favoring sparse arrays (i.e. [CODE] and its subclasses) over sparse matrices (i.e. [CODE] and its subclasses) to enlarge the scope of matrices to $n$-dimensional data-structures since SciPy 1.8 (see [URL] Sparse matrices now subclass their associated sparse arrays (see [URL] scikit-learn has been supporting sparse matrices but now also needs to support SciPy sparse arrays. ### Proposed solutions Ordered by preference: - Use [CODE] and the [CODE] attribute everywhere and not use [CODE] at all - Use [CODE] on private compatibility class defined to be [CODE] or [CODE] conditionally on SciPy's version (i.e. use [CODE] if available) - Rely on duck-typing or the class name to check for sparse arrays cc @ivirshup",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26418"
  },
  {
    "number":30371,
    "text":"Meson Build system error\n\n### Describe the bug I am building from source Obtaining file:\/\/\/home\/success\/Desktop\/scikit-learn Running command Checking if build backend supports build_editable Checking if build backend supports build_editable ... done Running command Preparing editable metadata (pyproject.toml) + meson setup \/home\/success\/Desktop\/scikit-learn \/home\/success\/Desktop\/scikit-learn\/build\/cp313 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=\/home\/success\/Desktop\/scikit-learn\/build\/cp313\/meson-python-native-file.ini The Meson build system Version: 1.6.0 Source dir: \/home\/success\/Desktop\/scikit-learn Build dir: \/home\/success\/Desktop\/scikit-learn\/build\/cp313 Build type: native build Project name: scikit-learn Project version: 1.7.dev0 ..\/..\/meson.build:1:0: ERROR: Unable to get gcc pre-processor defines: Compiler stdout: ----- Compiler stderr: <built-in>: internal compiler error: Segmentation fault 0x7e8c4244531f ??? .\/signal\/..\/sysdeps\/unix\/sysv\/linux\/x86_64\/libc_sigaction.c:0 0x7e8c4242a1c9 __libc_start_call_main ..\/sysdeps\/nptl\/libc_start_call_main.h:58 0x7e8c4242a28a __libc_start_main_impl ..\/csu\/libc-start.c:360 Please submit a full bug report, with preprocessed source (by using -freport-bug). Please ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30371"
  },
  {
    "number":30684,
    "text":"\u26a0\ufe0f CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jan 21, 2025) \u26a0\ufe0f\n\nCI failed on Linux_Runs.pylatest_conda_forge_mkl - test_linear_regression_sample_weights[95-True-csr_matrix] - test_linear_regression_sample_weights[95-True-csr_array]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30684"
  },
  {
    "number":28726,
    "text":"Is there any way to see alphas\/coefs\/intercept associated with all scenarios tested within ElasticNetCV\n\n### Describe the workflow you want to enable I like that ElasticNetCV outputs the MSE path for CV folds\/alphas but is there any way to similarly track associated model params (ie, coef\/intercept) for each scenario and include them as part of output. I get that it's easier to just output 'best' estimators\/params but would be useful to add granularity to allow identifying a 'sweet spot', either via MSE curve or something else, which would make outputting all params additive. ### Describe your proposed solution As described, run existing scenarios as is but instead of holding only through evaluation of 'best' model, save all model params\/outputs and return in an additional data object\/structure. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28726"
  },
  {
    "number":30181,
    "text":"DOC grammar issue in the governance page\n\n### Describe the issue linked to the documentation In the governance page at line [URL] \"GitHub\" is referred to as [CODE] However, in the other references, such as at [URL] [URL] It is correctly written as [CODE]. ### Suggest a potential alternative\/fix To maintain consistency throughout the document, we should change [CODE] to [CODE] at line no. 70 on governance page.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30181"
  },
  {
    "number":27433,
    "text":"Implement [CODE] using a sparse memory layout from the start\n\n### Describe the workflow you want to enable As discussed in #27359, [CODE] actually returns a dense numpy array (with many zero values). I think it should be possible to rewrite this code to compose operations on sparse arrays\/matrices from the start instead of allocating large dense square matrices. I have not tried myself, but I think it should be doable and it should be much more memory efficient.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27433"
  },
  {
    "number":28580,
    "text":"RFECV docstring does not state how the [CODE] attribute is ordered by\n\n### Describe the issue linked to the documentation This StackOverflow post may assume that the dictionary is sorted in descending order (i.e., the first element corresponds to the models that used ALL features, then one step less, then two steps less, etc.). However, it seems to me that the dictionary is sorted in ascending order. ### Suggest a potential alternative\/fix From my perspective, the easiest fix would be to add a few lines to the docstring. Something along the lines of: > This dictionary is sorted by the number of features in ascending order (i.e., the first element represents the models that use the least number of features, while the last element represents the models that use all available features). As an alternative, the resulting dictionary could have an additional key named [CODE] (or something along those lines) that states how many features each element in the dictionary represents.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28580"
  },
  {
    "number":27349,
    "text":"SimpleImputer fails for columns with pandas string type\n\n### Describe the bug The SimpleImputer class with strategy=\"most_frequent\" fails during fit when one of the columns of the input dataframe is of type string. The error happens here [URL] because (use X from the example below) [CODE] returns a bool ([CODE]) instead of an array of bools. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No errors thrown. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.2 (tags\/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] executable: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv310\\Scripts\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 59.8.0 numpy...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27349"
  },
  {
    "number":25231,
    "text":"sample_weight parameter in KBinsDiscretizer with kmeans strategy\n\n### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by glevv December 19, 2022<\/sup> Would adding [CODE] support when [CODE] make sense?<\/div> Could be useful to give smaller [CODE] to outliers so they will have less influence on ssq in KMeans.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25231"
  },
  {
    "number":27928,
    "text":"LASSO Solve badly when alpha is extremely small\n\n### Describe the bug There are 2 problem: - when CODE, the solver does not give a warning when it solved badly. - when [CODE] is extrimely small (like 1e-8), the solver could not find solution properly. ### Steps\/Code to Reproduce In this case, solver even do not raise a warning until [CODE] = 1e-5. [CODE_BLOCK] ### Expected Results To solve the problem when [CODE] is quite small, we should do warm up training and train the model from larger $\\alpha$ to smaller. temp solution: [CODE_BLOCK] output: [CODE_BLOCK] should we add it into .fit method? ### Actual Results when $tol=1e-4,\\alpha=0.01\/m$: [CODE_BLOCK] The loss is quite larger than the origin u, so it must be wrong. when $tol=1e-5,\\alpha=0.01\/m$: ``` Warning: Objective d...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27928"
  },
  {
    "number":27379,
    "text":"In certain cases, the results of fit_transform and transform are not identical.\n\n### Describe the bug [CODE]'s fit_transform and transform methods yield different results in specific scenarios when processing the same text data. ### Steps\/Code to Reproduce In this case, both methods yield identical results. [CODE_BLOCK] In this case, each method yields a different result. [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results ```py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict) 793 err_msg += '\\n' + '\\n'.join(remarks) 794 msg = build_err_msg([ox, oy], err_msg, 795 verbose=verbose, header=header, 796 names=('x', 'y'), precision=precision) --> 797 raise AssertionError(msg) 798 except ValueError: 799 import traceback AssertionError: Arrays are not equal Mismatched elements: 151 \/ 302 (50%) Max absolute difference: 1.66533454e-16 Max relative difference: 4.71248267e-16 x: array([[0.353388, 0.070678, 0.070678,...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27379"
  },
  {
    "number":28801,
    "text":"Bad rendering of the badge links in the README.rst file on github\n\nE.g. on: URL] You get something that looks like: ![image: [URL] ![image]([URL] I think it used to be rendered correctly on github a couple of days\/weeks ago. Looking at the source of [CODE] I cannot spot the source of the problem. Has anybody an idea?",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28801"
  },
  {
    "number":29338,
    "text":"Unable to pass categorical feature columns into DecisionTreeClassifier\n\n### Describe the bug Heyy devs of scikit-learn, While working with the famous penguins dataset to predict the species of a penguin based on a number of features, I was getting the error: [CODE] (Togersen is one of the island names) Further investigation led me to find out that sklearn does not currently support categorical feature columns as strings. I'm not sure I understand why it's possible to predict string categorical labels but not take string categorical features. Thank you and have a great day! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results ```ValueError Traceback (most recent call last) ~\\AppData\\Local\\Temp\\ipykernel_10096\\2664437121.py in ?() 2 from sklearn.tree import DecisionTreeClassifier 3 df = pd.read_csv('..\/DATA\/penguins_size.csv') # specify file path to penguins dataset 4 X, y = df.drop(columns='species'), df['species'] 5 clf = DecisionTreeClassifier() ----> 6 clf.fit(X, y) ~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py in ?(estimator, args, kwargs) 1470 skip_parameter_validation=( 1471 prefer_skip_nested_validation or global_skip_validation 1472 ) 1473 ): -> 1474 return fit_method(estimator, args, kwargs) ~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py in ?(self, X, y, sample_weight, check_input) 1005 self : DecisionTreeClassifier 1006 Fitted estimator. 1007 \"\"\" 1008 -> 1009 super()._fit( 1010 X, 1011 y, 1012 sample_weight=samp...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29338"
  },
  {
    "number":28245,
    "text":"CalibratedClassifierCV in 1.4 broke the compatibility with custom estimators that outputs float32.\n\n### Describe the bug Hi, this is an issue from xgboost forwarded here [URL] with copied code and backtrace. XGBoost outputs float32 in its inference procedure, it seems the latest version of sklearn no longer works with it. May I ask do you want to accept a PR for converting inputs to float64 inside the calibrator, or the downstream estimators should simply output float64 unconditionally? cc @jstammers ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No exception. ### Actual Results ``` Traceback (most recent call last): File \"~\\incorrect_classifier.py\", line 21, in <module> calibrator.fit(df[[\"x\"]], df[\"y\"]) File \"~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\env-vqbIi1FM-py3.10\\lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper return fit_method(estimator, args, *kwargs) File \"~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\env-vqbIi1FM-py3.10\\lib\\site-packages\\sklearn\\calibration.py\", line 403, in fit self.calibrated_classifiers_ = parallel( File \"~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\env-vqbIi1FM-py3.10\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__ return super().__call__(iterable_with_config) File \"~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\env-vqbIi1FM-py3.10\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__ return output if self.return_generator else list(output) File \"~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\env-vqbIi1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28245"
  },
  {
    "number":30283,
    "text":"Crying emoticon in \"Choosing the right estimator\" does not work for most audiences\n\n### Describe the issue linked to the documentation Contrast [URL] which uses a crying emoticon with previous versions where it said \"Not Working\" and were easier to understand. I teach a brief class on Machine Learning Crying on a weekly basis and have always used \"Choosing the right estimator\" diagram to illustrate the typical high-level process that a data scientist goes through when picking the best algorithm for their problem, and how it leads to the need of automl and hyperparameter fine tuning. This has always worked well. However, more recently there has been a change where the words \"Not working\" were replaced by a \"crying\" emoticon. At first glance, no students in class understand what that means, even younger audiences. I have to magnify it, so they see what it actually is. And when I explain it, they find it awkward. ### Suggest a potential alternative\/fix Possible solutions: 1) Replace the crying emoticon back with text. It does not have to be \"Not Working\", it could be something else such as \"needs improvement\" or 2) Replace the crying emoticon with the Hammer and Wrench emoji: [URL] Add a legend to the picture stating that it means \"not working\", \"needs improvement\", \"more work needed\". At least this emoji would be more emotionally neutral and would not be perceived as \"awkward\". 3) At least add a legend explaining that the crying emoticon means \"not working\", \"needs improvement\"...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30283"
  },
  {
    "number":27484,
    "text":"Allow LogisticRegression with lbfgs solver to control [CODE] parameter of solver\n\n### Describe the workflow you want to enable Similarly to what is mentioned on [URL] > Training an MLP regressor (or classifier) using l-bfgs currently cannot run for more than (approx) 15000 iterations. This artificial limit is caused by the call site to l-bfgs passing the MLP argument value \"max_iters\" to the argument for \"maxfun\" (maximum number of function calls), but not for \"maxiter\" (maximum number of iterations), so that no matter how large a number you pass as \"max_iters\" to train for MLP, the iterations are capped by the default value for maxiter (15000). training a LogisticRegression regressor using l-bfgs currently cannot perform more than (approx) 15000 function evaluations. This artificial limit is caused by the call site to l-bfgs passing the [CODE] argument value [CODE] to the argument for [CODE] (maximum number of iterations), but not for [CODE] (maximum number of function evaluations), so that no matter how large a number you pass as \"max_iters\" to train for LogisticRegression, the function evaluations are capped by the default value for maxiter (15000, defined on [URL] ### Describe your proposed solution When calling the l-bfgs solver, set [CODE] to be the same as [CODE], allow the user to control it. As stated on [#9274]([URL] by @daniel-perry, > Ideally you would want to pass in both a 'max_iter' and 'max_fun' argument to MLP, however 'max_fun' doesn't make sense for anythin...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27484"
  },
  {
    "number":27455,
    "text":"Results of [CODE] are sensitive to the scale of [CODE]\n\n### Describe the bug When fitting [CODE] to a dataset with imbalanced classes, [CODE] parameter seems to produce different results that depend on the scale of weights, even though the ratio of the weights is the same, e.g., 1\/2. This behaviour seems to occur only in some datasets. An MCVE is provided below. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect that the [CODE] is the same and does not depend on the scaling of class weights by a constant factor since the ratio between the classes is still 1\/2. ### Actual Results The actual output I get is this: [CODE_BLOCK] ### Versions ```shell System: python: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ] executable: \/Users\/alinacherkas\/opt\/anaconda3\/envs\/testenv\/bin\/python machine: macOS-13.5.2-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 68.0.0 numpy: 1.25.2 scipy: 1.11.1 Cython: None pandas: 2.0.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 2.2.0 Built with OpenMP: True threadpoolctl info: filepath: \/Users\/alinacherkas\/opt\/anaconda3\/envs\/testenv\/lib\/libopenblas.0.dylib prefix: libopenblas user_api: blas internal_api: openblas version: 0.3.21 num_threads: 10 th...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27455"
  },
  {
    "number":29748,
    "text":"Expose Seed in FeatureHasher and HashingVectorizer\n\n### Describe the workflow you want to enable Varying the seed of the FeatureHasher allows the user to control what inputs collide. This can allow for a better feature space either through experimentation (as a hyperparameter) or explicitly searching for a space that minimizes \"bad\" collisions ### Describe your proposed solution Add an optional \"seed\" parameter to the init of [FeatureHasher]([URL] which defaults to 0 (the current behavior, see [the underlying hashing function]([URL] The seed would be thread through to [_hashing_transform]([URL] Ditto for [HashingVectorizer]([URL] Only difference here is that the seed would be passed to the [FeatureHasher instance]([URL] This seems straightforward so I can implement this solution if it makes sense ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29748"
  },
  {
    "number":29106,
    "text":"problem with convert_sklearn and onnx opset\n\n### Describe the bug hi, i run into the following problem that convert_sklearn seems to require opset 13 but i need opset 14 to support another operator [CODE_BLOCK]: the problem is with _update_domain_version - how can i change it so it uses upset >= 14? ``[CODE]tokenizers` before the fork if possible - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false) convert_sklearn target_opset: 14 [convert_sklearn] parse_sklearn_model [convert_sklearn] convert_topology [convert_operators] begin [convert_operators] iteration 1 - n_vars=0 n_ops=2 [call_converter] call converter for 'SklearnCastTransformer'. [call_converter] call converter for 'SklearnLinearClassifier'. [convert_operators] end iter: 1 - n_vars=16 [convert_operators] iteration 2 - n_vars=16 n_ops=2 [convert_operators] end iter: 2 - n_vars=16 [convert_operators] end. [_update_domain_version] +opset 0: name='', version=13 [_update_domain_version] +opset 1: name='ai.onnx.ml', version=1 [convert_sklearn] end 14 [domain: \"\" version: 13 , domain: \"ai.onnx.ml\" version: 1 ] \/home\/ubuntu\/triton_inference_server\/export_onnx\/setfit_onnx.py:261: UserWarning: sklearn onnx max opset is 13 requested opset 14 using opset 13 for compatibility. warnings.warn( Traceback (most recent call last): File \"\/home\/ub...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29106"
  },
  {
    "number":28952,
    "text":"Add missing values and categorical features when generating datasets\n\n### Describe the workflow you want to enable I am often using random datasets (typically with make_classification). However I often find myself having to add more realistic features to the dataset: - missing data, sometime just to test the pipeline (missing at random would be fine), or sometimes to look for more complex phenomenons (missingnes not at random, possibly depending on the target) - categorical: categoricals variables often need to be handled specifically. I usually introduce categoricals with binning a continuous value, then transforming to strings. It would be nice to have both of those in datasets generation. ### Describe your proposed solution Introduce parameters to allow for generation of missing data (proportion of missingness, type of missingness - at random, not at random). Introduce parameters to allow for generation of categorical features (number of features, type of repartition in categories - even - uneven - pareto. ### Describe alternatives you've considered, if relevant I usually handle this by hand. ### Additional context Could be used to illustrate imputing techniques, encoding techniques.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28952"
  },
  {
    "number":27740,
    "text":"HalvingGridSearchCV should not care about parameter-grid layout but apparently does\n\n### Describe the bug I have two parameter-grid layouts, both specifying the exact same set of configurations. The difference is that the choices are listed in a different order, e.g., [False, True] versus [True, False]. My understanding is that this should not matter given that HalvingGridSearchCV tries all configurations on the first step. However, I get different results when using first grid versus the second. ### Steps\/Code to Reproduce ```python import numpy as np from sklearn.svm import SVC from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV #three grids with identical configuration space svc_grid_1 = dict(C=[1,2,3], gamma=['auto', 'scale'], shrinking=(True, False), kernel=['sigmoid', 'linear', 'poly', 'rbf'], max_iter = [5000, 10000, -1] ) svc_grid_2 = dict(C=[1,2,3], gamma=['auto', 'scale'], shrinking=(True, False), kernel=['rbf', 'poly', 'linear', 'sigmoid'], #reordered choices max_iter = [5000, 10000, -1] ) svc_grid_3 = dict(kernel=['sigmoid', 'linear', 'poly', 'rbf'], #reordered keys C=[1,2,3], shrinking=(True, False), max_iter = [5000, 10000, -1], gamma=['auto', 'scale'], ) ###How many different combinations? from sklearn.model_selection import ParameterGrid param_grid_1 = ParameterGrid(svc_grid_1) #a list of dictionaries, one for each combo print(f'{len(param_grid_1)=}') #144 param_grid_2 = ParameterGrid(svc_grid_2) #a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27740"
  },
  {
    "number":30692,
    "text":"Inaccurate error message for parameter passing in Pipeline with enable_metadata_routing=True\n\n### Describe the issue linked to the documentation The following error message is inaccurate: [CODE_BLOCK] This can easily be done using [CODE] as described in the documentation for sklearn.pipeline: [URL] Please consider the following example: [CODE_BLOCK] Which ou...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30692"
  },
  {
    "number":26676,
    "text":"CI: Add black version in linter bot\n\nGreat job on the linter bot @adrinjalali ! Would it be a good idea to add in the comment what the desired black (and ruff ??) versions are? I think I recall for black at least that different versions can affect formatting...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26676"
  },
  {
    "number":29043,
    "text":"Revamp the developer documentation when it comes to roll scikit-learn compatible estimator\n\nI find the documentation helping at writing a scikit-learn estimator a bit oldish: [URL] I think that we could revamp the documentation with a new look. Probably, we would like to mention what are the minimum implementation required and then go into details in the additional feature given by the mixin that we added overtime. Finally, it should be the place where we provide some documentation regarding the developer tools and manage the expectation regarding the deprecation cycle for those.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29043"
  },
  {
    "number":29439,
    "text":"Large errors when computing Euclidean pairwise distances\n\n### Describe the bug See the code below. Matrices [CODE] and [CODE] here are the same. The function [CODE] here gives the correct answer. The \"worst offender\" is large diagonal entry at [CODE]. ### Steps\/Code to Reproduce import numpy as np from sklearn.metrics import pairwise_distances from scipy.spatial.distance import cdist X=np.array([[ 1. , 1. , 1. , 1.2, 1.4, 4.4, -1. , -22. ], [ 0. , 1. , 0. , 3.2, 1.4, 4.4, -188. , -72. ], [ 1. , 1. , 0. , -0.2, 1.1, 4.4, -1. , -22. ], [ 1. , 1. , 1. , 1.2, 1.4, 14.4, -1. , -42. ]]) Y=X.copy() correct_answer = cdist(X,Y,metric='euclidean') incorrect_answer=pairwise_distances(X, Y, metric='euclidean') ### Expected Results incorrect_answer[2,2] == 0.0 or close to numerical precision ### Actual Results incorrect_answer[2,2] np.float64(3.371747880871523e-07) ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29439"
  },
  {
    "number":25982,
    "text":"Add functions for calculating log-likelihood and null log-likelihood\n\n### Describe the workflow you want to enable As a user of Scikit-learn, I want to be able to calculate the McFadden's pseudo R-squared for a binary logistic regression model for that we need log-likelihood and null log-likelihood. ### Describe your proposed solution I use the following functions and I propose to add them in the library as well. ### For Log Likelihood - [CODE_BLOCK] ### For Null Log Likelihood - [CODE_BLOCK] ### Descri...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25982"
  },
  {
    "number":30222,
    "text":"Changelog check on towncrier false positive case\n\nObserved on this PR: [URL] This run: [URL] The PR needs to add PR number to existing changelog, and changes another affected changelog, therefore there are 3 changelog files affected in the PR. However, the changelog checker complains with: [CODE_BLOCK] Which I'd say is a false positive. cc @lesteve",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30222"
  },
  {
    "number":27285,
    "text":"Weighted ridge regression regularization variable is dependent on sample weight magnitude\n\n### Describe the issue linked to the documentation When doing weighted ridge regression, the value of the regularization parameter for a particular solution is dependent on the sample weight vector due to scaling in the implementation. Scaling should either be according to the weighted average instead of just a simple multiplication, or the documentation should specify that the weight vector should sum to 1. This may affect other learners as well. As is, this is unstable in a cross validation context as the data magnitude may change between folds. ### Suggest a potential alternative\/fix Specify the weight vector should sum to one, or fix the underlying behavior to avoid the dependence.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27285"
  },
  {
    "number":27848,
    "text":"Contraction Clustering (RASTER): A very fast and parallelizable clustering algorithm\n\n### Describe the workflow you want to enable RASTER is a very fast clustering algorithm that runs in linear time, uses constant memory, and only requires a single pass. The relevant package is [CODE]. ### Describe your proposed solution RASTER has been shown to be faster than all other clustering algorithms that are part of the [CODE] package (see comparative results in the \"alternatives\" field). A detailed description of the algorithm is in [this paper]([URL] The key idea is that data points are projected onto a grid. This helper data structure that allows us to cluster data points at the desired level of precision and at a speed much faster than any other clustering algorithm we encountered in the literature. The closest comparison we were made aware of was CLIQUE, but RASTER is more efficient and, in fact, many orders of magnitude faster, which we have also shown experimentally, see Appendix B in the paper above. Plots with comparisons: <img width=\"549\" alt=\"Screen Shot 2023-11-26 at 16 10 16\" src=\"[URL] Example of adjusting the precision parameter: <img width=\"219\" alt=\"Screen Shot 2023-11-26 at 16 12 47\" src=\"[URL] Pseudo-code: <img width=\"296\" alt=\"Screen Shot 2023-11-26 at 16 06 36\" src=\"[URL] Implementation: [URL] The algorithm is furthermore parallelizable. ### Describe alternatives you've considered, if relevant We compare RASTER to 10 other clustering algorithms, and have found th...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27848"
  },
  {
    "number":30188,
    "text":"Fallback value for NaN feature during classification\n\n### Describe the workflow you want to enable In code like this: [CODE_BLOCK] where I need to predict classification probabilities from the features in the dataframe [CODE], I could have NaNs. The way things are right now, the method will raise an exception and I would have to clean the dataframe myself. ### Describe your proposed solution I would like to have something like: [CODE_BLOCK] such that when the value is nan, the probability is -1 and on inf 2. ### Describe alternatives you've considered, if relevant I have implemented this in my wrapper class: [URL] [URL] where I patch the nans with zeros and then I replace the probabilities before the return: [URL] feel free to pick up whatever you need from my code. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30188"
  },
  {
    "number":30470,
    "text":"How can I obtain the explained variance for each latent component in PLS?\n\n### Describe the workflow you want to enable How can I further obtain the explained variance for each latent component in PLS using sklearn.cross_decomposition.PLSRegression? ### Describe your proposed solution I need proposed solution ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30470"
  },
  {
    "number":28084,
    "text":"DOC Update website to [CODE]\n\nThis issue is a continuation of [#26809]([URL] and aims to migrate the scikit-learn website towards [[CODE]]([URL] As this is an ambitious goal, this issue is to track the steps of the migration. cc @lucyleeow who guided me to open this issue. #### Quick links - [The [CODE] branch]([URL] - [Tracker for upstream issues]([URL] - Live preview: [URL] #### TODO before merging into [CODE] - [CODE] is bypassed in CI for now and we need to reactivate it before merging into [CODE]. Also, we need to make sure all dependencies are documented, in particular [CODE] which was not added in the very first setup PR. See also [#28379]([URL] - Remove the [CODE] part in [CODE]. - In [CODE], change the version switcher link to [CODE]. - Remove [CODE] and update [CODE] in [CODE]: these are only useful for the old theme. - Pin higher versions of [CODE], [CODE], and [CODE]. See also [tracker for upstream issues]([URL] #### Tracking work towards the [CODE] branch - [x] [URL] - [x] [URL] - [x] [URL] - [x] [URL] - [x] [URL] - [x] [URL] - [x] [URL] - [x] [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28084"
  },
  {
    "number":29487,
    "text":"Ignore \"index\"-columns of transformations in [CODE] objects\n\n### Describe the bug I would like to be able to use a decorator in order to wrap [CODE] methods. The wrapped methods should ignore all columns in a [CODE] starting with a certain prefix, i.e. \"index\". I need to preserve the index of my data in all transformations, i.e. they should not be used in [CODE] and [CODE] calls. The simplest solution that I came up with was decorating the [CODE] (and [CODE]) methods of existing transformers. The problem is that the [CODE] API seems to have an unwanted interaction with my decorator. I am not sure how I can avoid this without having to re-implement existing transformations, so that they can appropriately handle the \"index\"-columns . As this issue lies somewhere between bug, feature request, and usage question I'd be open to (and very happy to receive) suggestions that help to solve the specific problem. Especially insights regarding the (unwanted) interaction of the decorator with the [CODE] API would be very helpful. Thanks a lot for this great library and all the work that goes into maintaining and extending it! ### Steps\/Code to Reproduce ```python import functools import polars as pl import polars.selectors as cs import sklearn.base import sklearn.preprocessing def ignore_index_columns_fit(fit_method): @functools.wraps(fit_method) def wrapper(estimator: sklearn.base.BaseEstimator, X: pl.DataFrame, y: pl.DataFrame | None = None): X = X.select(~cs.starts_with(\"index\")) if y ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29487"
  },
  {
    "number":26873,
    "text":"Nearest neighbor return structure takes significantly longer to garbage collect\n\nThe way output is structured in nearest neighbor classes (e.g. KDTree) leads to an order of magnitude greater time required to garbage collect the output, compared to actually generating it. For example: [CODE_BLOCK] This results in the output: [CODE_BLOCK] Compare this to the output of an equivalent script which uses [CODE]: [CODE_BLOCK] Though the query operation is slower using scipy, the garbage collection time for the same output is inconsequential; this seems to be due to the nested objects being lists rather than arrays. Also, I'm aware that by switching the build\/query data, the problem goes away on this contrived example - but this results in a completely different output representation. In the data I'm actually working with, swapping the two leads to a runtime approaching the garbage collect time shown above, while still having a relatively large impact from garbage collection. ```shell System: python: 3.10.12 | packaged by Anaconda, Inc. | (main, Jul 5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)] executable: env_crest\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.2.1 pip: 23.1.2 setuptools: 67.8.0 numpy: 1.23.5 scipy: 1.10.1 Cython: None pandas: 1.5.3 matplotlib: 3.7.1 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26873"
  },
  {
    "number":24575,
    "text":"Remove parsar parameter from fetch_openml()\n\n### Describe the workflow you want to enable data = fetch_openml(\"mnist_784\", as_frame=True, parser=\"pandas\") It cause error while running the sklearn\/benchmarks\/bench_mnist.py file ### Describe your proposed solution data = fetch_openml(\"mnist_784\", as_frame=True) It is running smoothly. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24575"
  },
  {
    "number":28575,
    "text":"GridSearchCV do not weight the score by the size of the fold when providing custom split for CV\n\n### Describe the bug When providing an iterable for the [CODE] arguments for GridSearchCV, if the splits have different size (as it can be the case when doing \"leave one group out\") the \"best\" score computed at the end is done as a direct average of the score for each fold, without weighting them by the number of samples in the fold. Consequently, the \"best\" estimator found is not actually the real best. For example (as seen in the example below), if there are 3 splits, of 50, 49 and 1 sample, if the split with 1 sample results in a score of [CODE] while every other samples are correctly predicted (score of [CODE]), then the final score will be [CODE] instead of [CODE]. ### Steps\/Code to Reproduce ```python from sklearn.model_selection import GridSearchCV from sklearn.neighbors import KNeighborsClassifier import numpy as np data = np.arange(100) label = data >= 99 data = data.reshape(-1, 1) indices_break = [0, 50, 99, 100] split = [np.arange(indices_break[i], indices_break[i+1]) for i in range(len(indices_break)-1)] splits = [] for i, test in enumerate(split): train = split[:i] train.extend(split[i+1:]) train = np.concatenate(train) splits.append((train, test)) model = KNeighborsClassifier() parameters = {'n_neighbors': [1, 2]} clf = GridSearchCV(model, parameters, cv=splits) clf.fit(data, label) len_split = [len(i) for i in split] res_n_1 = [] res_n_2 = [] for i in range(len(spli...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28575"
  },
  {
    "number":26307,
    "text":"KNeighborsClassifier OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata\n\n### Describe the bug when i run flask app on 56 core cpus,it show this warning and app exit, when i change \\site-packages\\joblib\\externals\\loky\\backend\\context.py can solved [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results KNeighborsClassifier.predict no bug on 56 cpus or more cpus ### Actual Results OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata; flask app exit,no Error show. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26307"
  },
  {
    "number":25588,
    "text":"Broken estimator_ attribute on some ensemble models\n\n### Describe the bug Several ensemble models raise an error when trying to access the existing [CODE] attribute. The problem is that this [CODE] tries to access [CODE], which is set by [CODE], but that method is not called by all subclasses. [URL] For [CODE] and [CODE], it's understandable IMO, but the error message could be better. For gradient boosting, [CODE] could return something useful. More as a reminder to myself, [CODE] is being rewritten in #24250 to return the estimator instead of setting it inplace. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is pri...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25588"
  },
  {
    "number":28903,
    "text":"Parameter Validation Documentation?\n\nWhile implementing a custom estimator, I noticed that the BaseEstimator class brings in a CODE] method. Looking through this repo's history, it looks like it came in back during 2022 as part of PR [URL] [CODE_BLOCK] Beyond the PR itself and a docstring in [CODE] there does not seem to be much information about this method. The string \"_validate_params\" returns no results on the [web documentation. I am curious about a few things I had trouble finding answers to: - Is there documentation on the canonical way to ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28903"
  },
  {
    "number":25481,
    "text":"Setting min_samples_split=1 in DecisionTreeClassifier does not raise exception\n\n### Describe the bug If [CODE] is set to 1, an exception should be raised according to the paramter's constraints: [URL] However, [CODE] accepts [CODE] without complaining. With scikit-survival 1.0, this raises an exception as expected: [CODE_BLOCK] I suspect that this has to do with the Intervals of the constraints overlapping. [CODE] satisfies the [CODE] constraint, whereas the [CODE] constraint should have precedence. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results No exception is raised. ### Versions ```shell System: python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0] executable: \/\u2026\/bin\/python machine: Linux-6.1.6-100.fc36.x86_64-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.3.dev0 pip: 22.2.2 setuptools: 63.2.0 numpy: 1.24.1 scipy: 1.10.0 Cython: None pandas: None matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libgomp filepath: \/\u2026\/lib\/libgomp.so.1.0.0 version: None num_threads: 16 user_api: blas internal_api: openblas prefix: libopenblas filepath: \/\u2026\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so versi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25481"
  },
  {
    "number":26482,
    "text":"Run common test for SparseCoder and FeatureUnion\n\nCurrently, the [CODE] estimator is not tested by our common test because it requires a [CODE] parameter. We should make sure to construct the instance in [CODE] and check that the estimator runs the test. I am almost sure that it is missing the [CODE] tag and the parameter validation.",
    "labels":[
      "help wanted",
      "Enhancement"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26482"
  },
  {
    "number":27623,
    "text":"DOC link benchmark results site\n\n### Describe the issue linked to the documentation Mention [URL] somewhere in our docs. I could only find it in the Readme. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27623"
  },
  {
    "number":28130,
    "text":"Expanded ColumnTransformer functionality -- transforming subsets of data\n\n### Describe the workflow you want to enable (edited) the ability to (inverse_)transform data corresponding to a subset of the ColumnTransformer's component transformations ### Describe your proposed solution Data of a smaller size can be passed in with a new keyword that identifies the relevant component transformations by name ### Describe alternatives you've considered, if relevant a function that subsets a ColumnTransformer object, including adjusting the column numbers ### Additional context In artificial intelligence applications, the researcher may want to transform an entire dataset with column groups for learning, but then transform new data corresponding just to interventions or predictions using the same transformations at a later time. Hence, the need to be able to subset a ColumnTransform or pass in only a part of the data. See #27957 for more discussion.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28130"
  },
  {
    "number":28239,
    "text":"Metadata routing breaks [CODE] with estimator that doesn't support [CODE] in fit.\n\n### Describe the bug When combining [CODE] with an estimator that doesn't have sample_weight as metadata in the [CODE] method, such as [CODE], it fails to fit. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown. ### Actual Results ``` Traceback (most recent call last): File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"<ipython-input-2-135dae0c8613>\", line 10, in <module> MultiOutputClassifier(LinearDiscriminantAnalysis()).fit(X, y) File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\multioutput.py\", line 535, in fit super().fit(X, Y, sample_weight=sample_weight, fit_params) File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper return fit_method(estimator, *args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\multioutput.py\", line 251, in fit routed_params = process_routing( ^^^^^^^^^^^^^^^^ File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\utils\\_metadata_requests.py\", line 1556, in process_routing request_routing.validate_metadata(params=kwargs, method=_method) File \"C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\utils\\_metadata_requests....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28239"
  },
  {
    "number":26128,
    "text":"HistGradientBoosting counts and sample weights\n\nRelated issues: #25210 ### Current State [CODE] and [CODE] both: - Calculate the sample size [CODE] in histograms - Use [CODE] for splitting (mostly excluding split candidates) - Save the [CODE] in the final trees and use it in partial dependence computations. ### Proposition 1. Evaluate if removing [CODE] from the histograms (LightGBM only sums gradient and hessian in histograms, no count) gives a good speed-up .(I measured a roughly 10-20% speed-up.) Edit: LightGBM uses an approximate count based on the hessian to check for min sample size. So this might not be what we want. 3. Add an option to save counts and sample weights to final trees at the very end of [CODE] (where the binned training [CODE] is still available). 4. Use partial dependence [CODE] if the above option was set, else use [CODE]. Why? #25431 concluded that adding weights to the trees is too expensive. The above proposition gives a user a clear choice: Faster training time or faster pdp afterwards.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26128"
  },
  {
    "number":31521,
    "text":"TarFile.extractall() got an unexpected keyword argument 'filter'\n\n### Describe the bug For the latest version [CODE], it can be installed with Python 3.10, but the parameter [CODE] is available starting from Python 3.12 (See: [URL] ). [URL] As a result, when I attempted to download the [CODE] dataset, an error occurred: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31521"
  },
  {
    "number":29013,
    "text":"Pyodide build broken by updating meson.build to C17\n\nScheduled Pyodide build failed today see [build log]([URL] [CODE_BLOCK] This is due to [URL] Using c11 for example instead of c17 fixes the issue. Not sure why this is happening and if this is a Pyodide issue or a more generic Meson cross-compilation issue ... <details> <summary>Full build log<\/summary> ``` ##[section]Starting: Build Pyodide wheel ============================================================================== Task : Bash Description : Run a Bash script on macOS, Linux, or Windows Version : 3.237.1 Author : Microsoft Corporation Help : [URL] ============================================================================== Generating script. Script contents: bash build_tools\/azure\/install_pyodide.sh ========================== Starting Command Output =========================== [command]\/usr\/bin\/bash \/home\/vsts\/work\/_temp\/ec9d44b4-19e2-4e87-befe-ff7e5563ae37.sh Cloning into 'emsdk'... Resolving SDK version '3.1.46' to 'sdk-releases-21644188d5c473e92f1d7df2f9f60c758a78a486-64bit' Installing SDK 'sdk-releases-21644188d5c473e92f1d7df2f9f60c758a78a486-64bit'.. Installing tool 'node-16.20.0-64bit'.. Downloading: \/home\/vsts\/work\/1\/s\/emsdk\/downloads\/node-v16.20.0-linux-x64.tar.xz from [URL] 22559772 Bytes [----------------------------------------------------------------------------] Unpacking '\/home\/vsts\/work\/1\/s\/emsdk\/downloads\/node-v16.20.0-linux-x64.tar.xz' to '\/hom...",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29013"
  },
  {
    "number":24451,
    "text":"Legend confuses lines in the logistic regression example\n\n### Describe the issue linked to the documentation The plot in the logistic regression example is incorrect. Current version from [here]([URL] looks like this: ![image]([URL] The legend in the plot is incorrect: - black dots should be the synthetic data - red line should be the logistic regression - blue line should be the linear regression ### Suggest a potential alternative\/fix from here [URL] [CODE_BLOCK] I will send a PR on this. The fix is to insert the line plot labels when calling [CODE] or similar.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24451"
  },
  {
    "number":26015,
    "text":"GaussianMixture is runnning too many computation when parameters are alredy provided\n\nIn [CODE], if a user is passing some initial values for the weights, means, and precision then there is no need to run the initialization (via kmeans or random) and to estimate the gaussian parameters. However, currently, we still run these two steps and then discard them. I think that we can make the estimator more efficient by bypassing these steps in case we already have all the necessary statistics to start the EM algorithm.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26015"
  },
  {
    "number":26518,
    "text":"DOC add __sklearn_is_fitted__ and others to Developer guide\n\n### Describe the issue linked to the documentation It would be nice to add the internal developer \"APIs\" like [CODE] to the user guide section [Developing scikit-learn estimators]([URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26518"
  },
  {
    "number":27249,
    "text":"sklearn.metrics.logAUC\n\n### Describe the workflow you want to enable Computing logAUC values. ### Describe your proposed solution $LogAUC_\\lambda=\\frac{\\sum_{i}^{where~x_i\\ge\\lambda} (\\log_{10} x_{i+1} - \\log_{10} x_i)(\\frac{y_{i+1}+y_i}{2})}{\\log_{10}\\frac{1}{\\lambda}}$ ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Practitioners are interested in early enrichment; logAUC values were created for that.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27249"
  },
  {
    "number":25193,
    "text":"StratifiedKFold and StratifiedGroupKFold for multilabel classification\n\n### Describe the workflow you want to enable Currently, these two functionalities only support binary\/multiclass classification. I am looking for similar functionalities (maybe not even k fold, a train_test_split also suffices) in multilabel classification. ### Describe your proposed solution For the multilabel version of stratification, [this]([URL] and [this]([URL] libraries seem already provided a solution. Adding grouping to them should be similar to how [CODE] adds to [CODE]. ### Describe alternatives you've considered, if relevant See above for existing libraries developed based upon Sklearn. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25193"
  },
  {
    "number":27255,
    "text":"Issue in copy to clipboard while Installing\n\n### Describe the issue linked to the documentation While I was installing scikit-learn I found a small issue when I do select Copy to Clipboard it is not copying the actual text which is \"pip install -U scikit-learn\" instead it is copying like \"python3 -m venv sklearn-venvpython -m venv sklearn-venvpython -m venv sklearn-venvsource sklearn-venv\/bin\/activatesource sklearn-venv\/bin\/activatesklearn-venv\\Scripts\\activatepip install -U scikit-learnpip install -U scikit-learnpip install -U scikit-learnpip3 install -U scikit-learnconda create -n sklearn-env -c conda-forge scikit-learnconda activate sklearn-env\" so we need to fix the documentation and when user clicks on copy to clipboard it should copy \"pip install -U scikit-learn\" Here is the link from where I found the issue: [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27255"
  },
  {
    "number":29070,
    "text":"Broken link at the 1.5.0 release page\n\n### Describe the issue linked to the documentation At the [release page]([URL] > We're happy to announce the 1.5.0 release. > > You can read the release highlights under [URL] and the long version of the change log under [URL] > > This version supports Python versions 3.9 to 3.12. The [first link]([URL] points to a missing page. Maybe it is not online yet ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29070"
  },
  {
    "number":24507,
    "text":"Support usage of [CODE] and [CODE] in cross validation\n\n### Describe the workflow you want to enable We can currently pass [CODE] and [CODE] to [CODE]s, predictors, etc., at predict time when performing \"manual\" calls. When performing cross validation, however, there is no way to pass down any params to [CODE] to use when calling [CODE]\/[CODE]. I believe this to be a limitation of the library and think all cross-validation related code should allow passing down of said params to the estimator performing the prediction. ### Describe your proposed solution Given scoring is at the core of all cross validation methods within scikit-learn, I believe it will suffice to update the scorer parent classes to accept a new arg (maybe just [CODE] given prediction probabilities is conceptually a subset of predicting in general, but I'd be open to better naming here if people have a preference, or even updating [CODE] to [CODE]) and then work backwards from there. Doing so, I can see that we'd need to update (new level of nesting indicates methods\/functions that call the parent they're nested under): - [CODE] and [CODE] - [CODE] - [CODE] - nothing needed from here as used in [CODE] and [CODE] doesn't have [CODE] or [CODE] - [CODE] - [CODE] - generic function to be used on estimator --> needs updating. not used anywhere - [CODE] - [CODE] - created inside of [CODE] --> add to fit signature - [CODE] - generic function to be used on estiamtor --> needs updating. - [CODE] - generic function to b...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24507"
  },
  {
    "number":27829,
    "text":"Different HDBSCAN clusters from scikit-learn and scikit-learn-contrib packages\n\n### Describe the bug The [CODE] functions provided by [scikit-learn-contrib\/hdbscan]([URL] and this package can give different clustering results, e.g. when using the [CODE] parameter. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] output: [CODE_BLOCK] ### Actual Results [CODE] output: [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27829"
  },
  {
    "number":31443,
    "text":"Folder\/Directory descriptions not present\n\n### Describe the issue linked to the documentation I was navigating through the codebase, trying to find source code for some algorithms. I noticed that there are no descriptions of files present within a folder, which would actually make it easier to navigate through the codebase. We can have a small readme file within folders which would describe what is present in that folder. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31443"
  },
  {
    "number":29229,
    "text":"Performance Regression in scikit-learn 1.5.0: Execution Time for ColumnTransformer Scales Quadratically with the Number of Transformers when n_jobs > 1\n\n### Describe the bug After upgrading to scikit-learn 1.5.0, we observed a significant performance regression in the ColumnTransformer when using [CODE]. The issue seems related to the IO overhead, which escalates quadratically with the number of transformers, particularly noticeable when processing Series holding Python objects like lists or strings. Below are benchmarks for running a pipeline with varying numbers of columns ([CODE]) with [CODE] across scikit-learn versions 1.4.2 and 1.5.0: ``` sklearn version: 1.4.2 and n_jobs = 1 5: Per col: 0.019380s \/ total 0.10 s 10: Per col: 0.018936s \/ total 0.19 s 15: Per col: 0.019192s \/ total 0.29 s 20: Per col: 0.019223s \/ total 0.38 s 25: Per col: 0.019718s \/ total 0.49 s 30: Per col: 0.019141s \/ total 0.57 s 35: Per col: 0.019265s \/ total 0.67 s 40: Per col: 0.019065s \/ total 0.76 s 45: Per col: 0.019170s \/ total 0.86 s sklearn version 1.5.0 and n_jobs = 1 5: Per col: 0.025390s \/ total 0.13 s 10: Per col: 0.020016s \/ total 0.20 s 15: Per col: 0.021841s \/ total 0.33 s 20: Per col: 0.020817s \/ total 0.42 s 25: Per col: 0.021067s \/ total 0.53 s 30: Per col: 0.021997s \/ total 0.66 s 35: Per col: 0.021080s \/ total 0.74 s 40: Per col: 0.020629s \/ total 0.83 s 45: Per col: 0.020796s \/ total 0.94 s sklearn version: 1.4.2 and n_jobs = 2 5: Per col: 0.243821s \/ total 1.22 s 10: Per col: 0....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29229"
  },
  {
    "number":25073,
    "text":"ValueError: \"Unknown label type: 'unknown'\" when class column has Pandas type like Int64\n\n### Describe the bug I use Pandas to load data from CSV and transform it. Pandas often parses integer columns as float, so I usually use [CODE] to bring those columsn back to int. It looks like this causes Pandas to make all integer columns [CODE]. When I try to train some Scikit-Learn models like [CODE] on such data I get error [CODE]. I think there was some effort to prevent this issue, I see it. [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect the model to be trained. ### Actual Results ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [40], line 10 8 df2.info() 9 model = sklearn.linear_model.LogisticRegression() ---> 10 model.fit( 11 X=df2.drop(columns=\"class\"), 12 y=df2[\"class\"], 13 ) File \/opt\/conda\/envs\/python3.9\/lib\/python3.9\/site-packages\/sklearn\/linear_model\/_logistic.py:1146, in LogisticRegression.fit(self, X, y, sample_weight) 1136 _dtype = [np.float64, np.float32] 1138 X, y = self._validate_data( 1139 X, 1140 y, (...) 1144 accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"], 1145 ) -> 1146 check_classification_targets(y) 1147 self.classes_ = np.unique(y) 1149 multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_)) File \/opt\/conda\/envs\/python3.9\/lib\/python3.9\/site-packages\/sklearn\/utils\/multiclass.py:200, in check_classification_targ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25073"
  },
  {
    "number":25648,
    "text":"Add the max_categories functionality from OneHotEncoder in OrdinalEncoder\n\n### Describe the workflow you want to enable I want to be able to limit the number of output levels from the OrdinalEncoder and also have a new \"unknown_value\" mapped to this same encoded level. This is motivated in the following bug report [URL] ### Describe your proposed solution The functionality should work the same as in OneHotEncoder for consistency [URL] There are three components: 1) max_categories parameter 2) min_frequency parameter 3) Option for \u2018infrequent_if_exist\u2019 in handle_unknown parameter ### Describe alternatives you've considered, if relevant I currently do this before the encoding but it is limited by lack of integration with handle_unknown ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25648"
  },
  {
    "number":27159,
    "text":"RandomForest{Classifier,Regressor} split criterion documentation\n\n### Describe the issue linked to the documentation There's no where in the documentation that explains what method is used to identify which values to consider as candidate splits. For example, for regression, an exhaustive method would be to sort each feature and then use the halfway point between each feature value. In this case there would be O(N*F) candidate splits where N=# data and F= # features. Another way would be to randomly choose K values between the min and max of each feature. Does anyone know what method is used? Could someone point me to where in the code this occurs? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27159"
  },
  {
    "number":29781,
    "text":"CI CUDA CI not running in lock-file update automated PR\n\nDiscussed in [URL] for now we need to remember to unset \"CUDA CI\" label and set it again manually on automated array API lock-file PRs like [URL] From [URL] maybe this is due to triggering a workflow from a workflow limitations, see [doc]([URL] Apparently you need a PAT rather than the default GITHUB_TOKEN when you are in this case. The last example in the doc is very similar to our use case I think: > Conversely, the following workflow uses GITHUB_TOKEN to add a label to an issue. It will not trigger any workflows that run when a label is added. I don't remember if we have a PAT for some of the CI that we could try to see whether that fixes the issue ...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29781"
  },
  {
    "number":30286,
    "text":"Cython error while installing development version on MacOS M2 chip\n\n### Describe the bug Hello - While working on #16236 , I am getting a Cython error . I pulled the latest version of Scikit Learn from the main branch . I am following the instructions here to install the development version of Scikit learn . [URL] I am not able to get rid of this error : [CODE_BLOCK] ### Steps\/Code to Reproduce ``` (sklearn-dev) gauravchawla@Gauravs-Air scikit-learn % conda activate sklearn-dev make clean pip install --editable . \\ --verbose --no-build-isolation \\ --config-settings editable-verbose=true ----------------------------------------------------------------------------------------------- (sklearn-dev) gauravchawla@Gauravs-Air scikit-learn % conda info active environment : sklearn-dev active env location : \/opt\/anaconda3\/envs\/sklearn-dev shell level : 2 user config file : \/Users\/gauravchawla\/.condarc populated config files : \/opt\/anaconda3\/.condarc \/Users\/gauravchawla\/.condarc conda version : 24.9.2 conda-build version : 24.9.0 python version : 3.12.7.final.0 solver : libmamba (default) virtual packages : __archspec=1=m2 __conda=24.9.2=0 __osx=14.2.1=0 __unix=0=0 base environment : \/opt\/anaconda3 (writable) conda av data dir : \/opt\/anaconda3\/etc\/conda conda av metadata url : None channel URLs : [URL] [URL] [URL] [URL] [URL] ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30286"
  },
  {
    "number":32146,
    "text":"Unexpected behavior of the HTML repr of meta-estimators\n\nHere's a list of some unexpected behaviors of the HTML repr of meta-estimators - [CODE] doesn't display its named steps <img width=\"164\" height=\"95\" alt=\"Image\" src=\"[URL] \/> This was maybe intentional ? In comparison [CODE] does for instance <img width=\"282\" height=\"91\" alt=\"Image\" src=\"[URL] \/> - a [CODE] in a meta-estimator doesn't render properly; it doesn't have the dashed border <img width=\"186\" height=\"120\" alt=\"Image\" src=\"[URL] \/> Another meta-estimator renders properly <img width=\"294\" height=\"116\" alt=\"Image\" src=\"[URL] \/> - transformers of [CODE] are expandable to show the selected columns, but there's no additional info. I think it should be explicit that these are the selected columns. <img width=\"274\" height=\"106\" alt=\"Image\" src=\"[URL] \/> Note that this could be fixed by [URL] - When the inner estimator of a meta-estimator is not a meta-estimator itself, it's expandable but the dropdown is not useful anymore (it's the non-html repr of the estimator basically): <img width=\"169\" height=\"129\" alt=\"Image\" src=\"[URL] \/> Now that we have the parameter table, it's not useful anymore to have the additional repr which is redundant and less informative. I'd be in favor of removing the dropdown - When the inner estimator of a meta-estimator is a meta-estimator itself, it's expandable but sometimes there's no dropdown or the dropdown is the non-html repr of the meta-estimator, and there's no parameter table....",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32146"
  },
  {
    "number":29531,
    "text":"RFE results are inconsistent between machines with ties in feature importances at threshold\n\n### Describe the bug RFE uses np.argsort on the feature_importances from the estimator, this is not repeatable across machines. This only matters when there are ties in the feature importances that overlap with the threshold. For example if the importances are [0, 2, 0, 1] and it needs to eliminate 1 feature, it may not choose the same feature to eliminate on all machines. ### Steps\/Code to Reproduce It is difficult to write code that will show this as it requires multiple machines to test; however, the following test could be added to the _rfe unit tests to check if the sort is stable, though the sort could be consistent but not stable, so this isn't enough to prove an issue. [CODE_BLOCK] ### Expected Results Expected to get the same results on all machines, being stable is not necessary though it is an easy way to enforce testable consistency. ### Actual Results Current sort does not result in a stable sort and fails the previously provided unit test. ### Versions [CODE_BLOCK] This issue is most likely to occur with zero importance features, especially at the initial steps of RFE.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29531"
  },
  {
    "number":24773,
    "text":"DOC Adjusted_rand_score not obvious in the graph\n\n### Describe the issue linked to the documentation In the example \"Adjustment for chance in clustering performance evaluation\", there are four different scores in the graph legend, but the graph shows only three lines. The blue line [CODE] seems to be very close to the green line, making it difficult to identify. <[URL] ### Suggest a potential alternative\/fix There are several ways to improve the graph, including but are not limited to: - Explain where exactly the blue line [CODE] is located in the graph. - Plot either the [CODE] or the [CODE] (green line), but not both. - Create a separate graph to showcase where the blue and green lines differ.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24773"
  },
  {
    "number":28719,
    "text":"kNN classifier - [CODE]\/[CODE] inefficient?\n\n### Describe the workflow you want to enable In a lot of cases, when calling CODE] we may want probabilities as well via [CODE]. To enable that use-case, it seems [the code, or another way of preventing duplicate computation, be difficult to implement? ### Describe alternatives you've considered, if relevant I suppose one could always just override these functions and write their own version of the classifier. Perhaps that's the advised approach for this scenario, I'm not sure. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28719"
  },
  {
    "number":28472,
    "text":"Suggesting updates on the doc of [CODE]\n\n### Describe the issue linked to the documentation Hi, We discover an inconsistency issue between documentation and code in the class [[CODE]]([URL] As mentioned in the description of parameter [CODE]. > gamma: float, default=None _Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels_. If gamma is None, then it is set to 1\/n_features. The most relevant piece of source code looks like this: [CODE_BLOCK] It seems that [CODE] will be ignored not only when [CODE] is [CODE], [CODE], [CODE] but all kernels. The same situation also happened in [CODE] and [CODE]. As mentioned in the description of parameter [CODE] and [CODE]. > degree: float, default=3 _Degree for poly kernels. Ignored by other kernels._. > coef0: float, default=1 _Independent term in poly and sigmoid kernels. Ignored by other kernels._. Could you please check it? ### Suggest a potential alternative\/fix Maybe you can reconstruct the if-else statement to cover the situation or update the doc to make it clear.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28472"
  },
  {
    "number":28585,
    "text":"Macro vs micro-averaging switched up in user guide\n\n### Describe the issue linked to the documentation Hi guys, In the \"ROC curve using micro-averaged OvR\" part of the doc ([URL] it says: \"In a multi-class classification setup with highly imbalanced classes, micro-averaging is preferable over macro-averaging. In such cases, one can alternatively use a weighted macro-averaging, not demoed here.\" I believe it should say: In a multi-class classification setup with highly imbalanced classes, macro-averaging is preferable over micro-averaging. In such cases, one can alternatively use a weighted macro-averaging, not demoed here. If correct, I believe it could spare users some confusion. Thanks for all your work, Im just trying to help :) !!! ### Suggest a potential alternative\/fix I believe it should say: In a multi-class classification setup with highly imbalanced classes, macro-averaging is preferable over micro-averaging. In such cases, one can alternatively use a weighted macro-averaging, not demoed here.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28585"
  },
  {
    "number":24540,
    "text":"Exit Code -1073741819 when doing K-means++ clustering\n\n### Describe the bug Unfortunately I am getting an exit code in Pycharm when doing clustering with k-means++. I tried nearly everything. Setup new Pycharm project try using different versions of numpy or sklearn. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results clustered buckets ### Actual Results before the clustering is finished I get an error: \"Process finished with exit code -1073741819 (0xC0000005)\" ### Versions ```shell System: python: 3.10.7 (tags\/v3.10.7:6cc6b13, Sep 5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)] executable: C:\\Users\\maron\\Desktop\\test\\venv\\Scripts\\python.exe machine: Windows-10-10.0.19044-SP0 Python dependencies: sklearn: 1.1.2 pip: 21.3.1 setuptools: 60.2.0 numpy: 1.23.3 scipy: 1.9.1 Cython: None pandas: None matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None num_threads: 32 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll version: 0.3.20 threading_layer: pthreads architecture: Zen num_threads: 24 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\scipy\\.libs\\libopenblas.PZA...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24540"
  },
  {
    "number":31889,
    "text":"We don't support [CODE] across the board as a scorer\n\nOur documentation here: def _score(self, args, kwargs): print(\"I'm logging stuff\") return super()._score(args, kwargs) def my_scorer(estimator, X, y, kwargs): print(\"I'm logging stuff in my_scorer\") return mean_squared_error(estimator.predict(X), y, kwargs) def my_metric(y_pred, y_true, kwargs): print(\"I'm logging stuff in my_metric\") return mean_squared_error(y_pred, y_true, kwargs) my_second_scorer = make_scorer(my_metric) X, y = make_classification() # this prints logs print(\"cross_val_score'ing\") cross_val_score( LogisticRegression(), X, y, scoring=MyScorer(mean_squared_error, sign=1, kwargs={}, response_method=\"predict\"), ) print(\"1. TunedThresholdClassifierCV'ing\") model = TunedThresholdClassifierCV( LogisticRegression(), # scoring=MyScorer(mean_squared_error, sign=1, kwargs={}, response_method=\"predict\"), # scoring=my_scorer, scoring=my_second_scorer, ) model.fit(X, y) print(\"2. TunedThresholdClassifierCV'ing\") model = TunedThresholdClassifierCV( LogisticRegression(), s...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31889"
  },
  {
    "number":24596,
    "text":"[CODE] is not compatible with [CODE]\n\n### Describe the bug Using the [CODE] inside a pipeline leads to errors. The possible solution could be to introduce an auxiliary parameter to the [CODE] function, similarly as it was done for [CODE]: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown, [CODE] returns [CODE] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.6 (main, Aug 9 2022, 18:38:26) [Clang 13.1.6 (clang-1316.0.21.2.5)] executable: \/reductedbin\/python machine: macOS-12.4-x86_64-i386-64bit Python dependencies: sklearn: 1.2.dev0 pip: 22.2.2 setuptools: 65.3.0 numpy: 1.23.3 scipy: 1.9.1 Cython: None pandas: None matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: False threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: \/reductedlib\/python3.10\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib version: 0.3.20 threading_layer: pthreads architecture: Haswell num_threads: 4 user_api: blas internal_api: openblas prefix: libopenblas filepath: \/reductedlib\/python3.10\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib version: 0....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24596"
  },
  {
    "number":24687,
    "text":"\u26a0\ufe0f CI failed on Linux.py38_conda_defaults_openblas \u26a0\ufe0f\n\nCI failed on Linux.py38_conda_defaults_openblas Unable to find junit file. Please see link for details.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24687"
  },
  {
    "number":28696,
    "text":"groups parameter in cross_validate not passed to inner model\n\n### Describe the bug I discovered that I am unable to perform nested cross validation using `[CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE]`` \/home\/roy\/anaconda3\/envs\/mcDestroyer\/lib\/python3.11\/site-packages\/sklearn\/model_selection\/_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"\/home\/roy\/anaconda3\/envs\/mcDestroyer\/lib\/python3.11\/site-packages\/sklearn\/model_selection\/_validation.py\", line 810, in _score scores = scorer(estimator, X_test, y_test) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/home\/roy\/anaconda3\/envs\/mcDestroyer\/lib\/python3.11\/site-packages\/sklearn\/metrics\/_scorer.py\", line 527, in __c...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28696"
  },
  {
    "number":31399,
    "text":"DOC Jupyterlite raises a ValueError when using plotly\n\n### Describe the issue linked to the documentation Running for instance [CODE] in jupyterlite raises a [CODE]. I tried adding [CODE] at the top of the notebook cell but that doesn't seem to work. As per [this post in stackoverflow]([URL] downgrading [CODE] to [CODE] solved this issue for me. ### Suggest a potential alternative\/fix Add a magic function [CODE] whenever plotly is imported.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31399"
  },
  {
    "number":30309,
    "text":"'Section Navigation' bar missing from stable documentation website on several pages\n\n### Describe the issue linked to the documentation When on the stable version of the documentation website the 'Section Navigation' header on the left side of the page remains present, but the navigation bar contents disappear. While on the dev page the feature functions as expected. It should be noted this issue is inconsistent. Some stable pages list the section navigation and work perfectly fine ([like this one here]([URL] while others do not. This presents an issue as some links take the user to the stable version and others the dev version. For example: [Present Here]([URL] [Absent Here]([URL] <img width=\"2046\" alt=\"Screenshot 2024-11-19 at 19 22 09\" src=\"[URL] <img width=\"2048\" alt=\"Screenshot 2024-11-19 at 19 22 39\" src=\"[URL] Discovered running on Chrome Browser Version 130.0.6723.117",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30309"
  },
  {
    "number":27806,
    "text":"BUG: pytest error when loading conftest (seemingly platform-specific)\n\n### Describe the bug I'm seeing errors on my Windows machine when running [CODE] (does not work with only [CODE], and does not work for directories that has [CODE]). This seems to be a platform-specific problem, since CI is not complaining. I investigated a bit and found [URL] but it doesn't look like [CODE] is planning to fix it, at least for now. I tried downgrading to [CODE] and everything worked smoothly, but in [URL] the minimum version of [CODE] has already been [CODE] for scikit-learn (due to some CI errors for [CODE]), so scikit-learn is raising error: [URL] I'm wondering if it is possible to pin [CODE] or at least relax the minimum requirement a bit to [CODE]? Or are there any other suggestions how I may resolve this issue? @glemaitre who bumped the minimum version of [CODE] to 7.1.2. Truly sorry for the inconvenience caused by my annoying Windows machine. ### Steps\/Code to Reproduce [CODE_BLOCK] or [CODE_BLOCK] Running [CODE] on a single file works correctly. ### Expected Results No error is thrown. ### Actual Results For the first example, ```pytb \u276f pytest Traceback (most recent call last): File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\Scripts\\pytest-script.py\", line 9, in <module> sys.exit(console_main()) File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 192, in console_main code = main() File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-p...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27806"
  },
  {
    "number":29157,
    "text":"TypeError when fitting GridSearchCV or RandomizedSearchCV with OrdinalEncoder and OneHotEncoder in parameters grid\n\n### Describe the bug Having both [CODE] and [CODE] inside the parameters grid to be used by the [CODE] or [CODE] results in the following error: [CODE]. ### Steps\/Code to Reproduce ```python import numpy as np import pandas as pd from sklearn import set_config from sklearn.compose import ColumnTransformer from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder set_config(transform_output=\"pandas\") # Setting seed for reproducibility np.random.seed(42) # Create a DataFrame with 1000 rows and 5 columns num_rows = 1000 data = { \"numeric_1\": np.random.randn(num_rows), # Normally distributed random numbers \"numeric_3\": np.random.randint( 1, 100, size=num_rows ), # Random integers between 1 and 100 \"object_1\": np.random.choice( [\"A\", \"B\", \"C\", \"D\"], size=num_rows ), # Random choice among 'A', 'B', 'C', 'D' \"object_2\": np.random.choice( [\"X\", \"Y\", \"Z\"], size=num_rows ), # Random choice among 'X', 'Y', 'Z' \"target\": np.random.rand(num_rows) * 100, # Uniformly distributed random numbers [0, 100) } df = pd.DataFrame(data) X = df.drop(\"target\", axis=1) y = df[\"target\"] enc = ColumnTransformer( [(\"enc\", OneHotEncoder(sparse_output=False), [\"object_1\", \"object_2\"])], remainder=\"passthrough\", verbose_featu...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29157"
  },
  {
    "number":31093,
    "text":"The covariance matrix is incorrect in BayesianRidge\n\n### Describe the bug The posterior covariance matrix in [CODE], attribute [CODE], is incorrect when [CODE]. This is because the posterior covariance requires the full svd, while the current code uses the reduced svd. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results True ### Actual Results False ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31093"
  },
  {
    "number":25755,
    "text":"Figure is shown incorrectly\n\n### Describe the issue linked to the documentation Second part of the [figure]([URL] doesn't show sampled functions ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25755"
  },
  {
    "number":24248,
    "text":"Issue with class template with 2 semi columns instead of a single one\n\n### Describe the issue linked to the documentation In the documentation, it appears that the section has 2 semi-columns instead of a single one: ![image]([URL] Not sure where it comes from. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24248"
  },
  {
    "number":31657,
    "text":"DOC Managing huntr security vulnerability reports\n\n### Describe the issue linked to the documentation ### Issues - The project receives reports from huntr that are not useful. - The reports from huntr are time consuming and use up limited maintainer resources. ### Discussion \/ Proposal - Update our SECURITY.md and proof of impact (POI) - Once POC and POI is established, can direct people to report issue via the GitHub Security Advisory: [URL] - Remove scikit-learn from the huntr bug bounty program ### Proposed text for huntr reports Draft text for huntr submissions: >The scikit-learn project is not reviewing reports submitted to huntr. Please use our SECURITY.md to submit reports. For security reports, provide both a POC (proof of concept) and POI (proof of impact). If your report is deemed impactful, you can then report it to huntr to collect a bounty. ### References - [Scientific Python SPEC]([URL] - [NumPy discussion on security]([URL] - [Dask: comment from huntr person]([URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31657"
  },
  {
    "number":30467,
    "text":"API Deprecate n_alphas in LinearModelCV\n\nIn LassoCV, ElasticNetCV, ... we have two parameters, [CODE] and [CODE], that have the same purpose, i.e. determine the alpha values to test. I'd be in favor of deprecating [CODE] and make [CODE] accept either an int or an array-like, filling both roles. I chose to keep [CODE] and not the other because [CODE] has [CODE] and no [CODE] (although [CODE] can't be an int there, maybe an enhancement to make ?), and the most recent param of this kind, [CODE] in [CODE], follows this naming pattern and fills both roles.",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30467"
  },
  {
    "number":24732,
    "text":"Improvement for Gaussian NB by rethinking the variance smoothing\n\n### Describe the workflow you want to enable ## Problem background In sklearn.naive_bayes.GaussianNB = P(c) P( x |c)\/P( x ) , where x<sub>i<\/sub> ~ N(u<sub>i<\/sub>, v<sub>i<\/sub>) However, sometimes the variance for P( x<sub>i<\/sub> |c) is zero. In other words, with label c, x<sub>i<\/sub> is a constant value in the dataset. ## Current solution in [CODE] It 's seen from the source that [CODE_BLOCK] and that [CODE_BLOCK] ### Describe your proposed solution ## The problem of the current implementation You can see, the current implementation would add a portion of the max variance of all the features(without considering the class), which makes the updated variance never being zero (unless all features are constants). This is questionable in two cases: 1. When there is no feature that has a zero variance. Well, the current algs still adds a small variance to each feature. This seems to be fair(all features are considered), but it is actually dangerous. Not all features are preprossed using something like [CODE], which means their variances can differ dramatically. Adding a big value(coming from the feature with largest variance) blindly would damage the probability computation of the feature with small variance greatly. For example, the variance of the biggest feature is 100, while other features have a variance of 1, let var_smoothing be 1, then the updated variance by current implementation would be 200 and 101-s,...",
    "labels":[
      "Bug",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24732"
  },
  {
    "number":31536,
    "text":"Improve sample_weight handling in sag(a)\n\n### Describe the bug This may be more of a discussion, but overall I am not sure what treatment of weighting would preserve the convergence guarantees for the SAG(A) solver. So far as I see it, at each update step we uniformly select some index $i_j$ such that the update steps can be generalised as: $x^{k+1} = x^{k} - \\sum_{j=1}^{k} \\alpha_{j} S(j, i_{1:k}) f'_{i_j}(x^j)$ Where $S(j, i_{1:k}) = 1\/n$ if $j$ is the maximum iteration at which $i_j$ is selected. For frequency based weighting, one could sample $i_j$ using weights as a probability, and under non-uniform sampling the SAG(A) convergence guarantees still seem to hold, (see here. ### Steps\/Code to Reproduce ```python import numpy as np from scipy.stats import kstest from sklearn.linear_model.tests.test_sag import sag, squared_dloss from sklearn.datasets import make_regression from sklearn.utils._testing import assert_allclose_dense_sparse step_size=0.01 alpha=1 n_features = 1 rng = np.random.RandomState(0) X, y = make_regression(n_samples=10000,random_state=77,n_features=n_features) weights = rng.randint(0,5,size=X.shape[0]) X_repeated = np.repeat(X,weights,axis=0) y_repeated = np.repeat(y,weights,axis=0) weights_w_all = np.zeros([n_features,100]) weights_r_all = np.zeros([n_features,100]) for random_state in np.arange(100): weights_w, int_w = sag(X,y,step_size=step_size,alpha=alpha,sample_weight=weights,dloss=squared_dloss,random_state=random_state) weights_w_all[:,random_stat...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31536"
  },
  {
    "number":29282,
    "text":"[CODE] no longer uses Hungarian method\n\n### Describe the issue linked to the documentation [CODE] on the docs is describing that the best match is found using the Hungarian method, but from what I can read, the SciPy implemantation is using the Jonker-Volgenant algorithm since v1.9 [URL] . ### Suggest a potential alternative\/fix I think the docs should reflect that, but I'm not sure about neither algorithms nor if I'm reading everything right, so I'd like some verification first before opening a pull request.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29282"
  },
  {
    "number":28619,
    "text":"Add an option handle_unknown=\"warn\" in OneHotEncoder\n\nFollow-up to [URL] It seems that it could be interested to log an eventual detection of new category during inference and issue a warning instead of silently ignoring them. Therefore, it seems reasonable to add a new option [CODE] to [CODE] that should behave as [CODE] but should issue an additional warning.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28619"
  },
  {
    "number":25336,
    "text":"sklearn.neighbors.KernelDensity bandwith estimation with \"scott\" or \"silverman\" is showing TypeError\n\n### Describe the bug I am not able to use bandwith estimation techniques \"scott\" and \"silverman\" in KernelDensity estimation in sklearn.neighbors as shown in the documentation. It is throwing TypeError. It works only when we give float values to bandwith. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results from sklearn.neighbors import KernelDensity import numpy as np X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) kde = KernelDensity(kernel='gaussian', bandwidth='scott').fit(X) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"\/home\/tony\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/neighbors\/_kde.py\", line 143, in __init__ if bandwidth <= 0: TypeError: '<=' not supported between instances of 'str' and 'int' ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25336"
  },
  {
    "number":31668,
    "text":"memory leak for QuantileTransformer\n\n### Describe the bug There is a doubling of the memory footprint when QuantileTransformer is called on a dataframe and old references to the dataframe are discarded. See repro. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Memory footprint should not exceed 4GB ### Actual Results Used memory grows to 6GB upon repetition. Df_train is replaced within the outerfunc by the scaled and transformed arr...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31668"
  },
  {
    "number":31133,
    "text":"Add sankey style confusion matrix visualization\n\n### Describe the workflow you want to enable Confusion matrices can be displayed as a colored matrix using the ConfusionMatrixDisplay, while actual amounts in each cell are represented by the size of the flows (easier to interpret quantitatively than colorscale variations). On the left size one can see the number of occurrence of each label in the ground truth data (confusion matrix row marginals). On the right side the number of occurrence of each label in the predictions (confusion matrix column marginals). Each flow represents both row-normalized (left side) and column-normalized (right side) at the same time, and could be labeled with the actual absolute number of examples in each confusion matrix cell. Interpretation is straightforward even in the multiclass case. There exist a matplotlib based implementation doing almost exactly this in the small [pySankeyBeta]([URL] package. Here is a sample from its readme: ![sankey]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31133"
  },
  {
    "number":27756,
    "text":"SystemError: initialization of beta_ufunc raised unreported exception\n\n### Describe the bug I have a 2D array of points and I want to cluster it. I have used the DBSCAN method. The main function is in C++ and the DBSCAN method is in Python. I have written an interface between both. The code is running fine but it gives a segmentation fault error when I run it to check memory leaks using Valgrind. So I debugged the Python code and found that just the import call to DBSCAN is throwing the error. My folder structure is as follows: Test: \u251c\u2500\u2500 CMakeLists.txt \u2514\u2500\u2500 src \u251c\u2500\u2500 Py_Interface \u2502 \u251c\u2500\u2500 pyhelper.hpp \u2502 \u251c\u2500\u2500 Py_Integration.cpp \u2502 \u2514\u2500\u2500 Py_Integration.h \u251c\u2500\u2500 PythonDep \u2502 \u2514\u2500\u2500 Cluster.py \u2514\u2500\u2500 main.cpp valgrind installation: [CODE] I'm using Ubuntu 20.04. ### Steps\/Code to Reproduce CMakeLists.txt [CODE_BLOCK] main.cpp [CODE_BLOCK] Cluster.py [CODE_BLOCK] Py_Integration.h [CODE_BLOCK] Py_Integration.cpp ``` #include \"Py_Integration.h\" using namespace std; void Py_Wrapper() { Py_InitializeEx(0); \/\/ Initialize the Python interpreter PyObject PythonDetectorFunction, PythonDetectorFunctionArguments; PythonDetectorFunction = PythonInitialize(\"Cluster\", \"clustering\"); if (PythonDetectorFunction == NULL) PyErr_Print(); PyObject *PythonDetectorFeatureMaps = PyObject_CallObject(PythonDetectorFunction, Pyth...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27756"
  },
  {
    "number":28313,
    "text":"Errors in Iterative Imputer Column Naming with scikit-learn 1.4 Integration\n\n### Describe the bug When attempting to support scikit-learn 1.4 in Pycaret, several bugs arose. Despite efforts to address them, including creating patches in Pycaret, issues persist, particularly regarding iterative imputation and related functionalities. The errors manifest as discrepancies in column names generated by functions, leading to failures in tests. These issues hinder seamless integration and usage of scikit-learn 1.4 within Pycaret. ### Steps\/Code to Reproduce Attempt integration of scikit-learn 1.4 with Pycaret. Execute tests, particularly those involving iterative imputation. Observe the ValueError messages indicating inconsistencies in column names. ### Expected Results Successful integration without errors or discrepancies in column names, ensuring smooth functionality of Pycaret with scikit-learn 1.4. ### Actual Results Failures in tests due to inconsistencies in column names generated by iterative imputer functions. Errors such as ValueError: The output generated by func have different column names than the one generated by the method get_feature_names_out occur, impeding proper execution of tests and integration efforts. ### Versions [CODE_BLOCK] ### More Details [URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28313"
  },
  {
    "number":30969,
    "text":"KNN tie breakers changing based on the subset of the train\n\n### Describe the bug According to [URL] Regarding the Nearest Neighbors algorithms, if two neighbors k and k+1 have identical distances but different labels, the result will depend on the ordering of the training data. I expect this is also true for KNN without going into classification, where the scope is only to find the NN without voting the class. However, this code provides me a different ordering for NN based on the selection of the train set. Please note that the last two points I removed should not change anything. Another strange behavior is that running with k=5 46 is taken, but 5 (minor index) should be selected. [CODE_BLOCK] [CODE_BLOCK] ### Steps\/Code to Reproduce ```py import pandas as pd from sklearn.datasets import fetch_file from sklearn.model_selection import train_test_split from sklearn.neighbors import NearestNeighbors url = '[URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30969"
  },
  {
    "number":31542,
    "text":"Huber Loss for HistGradientBoostingRegressor\n\n### Describe the workflow you want to enable Huber loss is available as an option for [CODE] and works great when training on data with frequent outliers (thank you!). [CODE] however does not support Huber loss, which may be required when scaling to larger datasets. ### Describe your proposed solution Add HuberLoss as an option for the [CODE] class. ### Describe alternatives you've considered, if relevant Possibly allow custom loss functions for the [CODE] ### Additional context _No response_",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31542"
  },
  {
    "number":25631,
    "text":"Inflated results on random-data with SVM\n\n### Describe the bug When trying to train\/evaluate a support vector machine in scikit-learn, I am experiencing some unexpected behaviour and I am wondering whether I am doing something wrong or that this is a possible bug. In a very specific subset of circumstances, namely: - [CODE]is used as cross-validation procedure - The SVM is used, with [CODE] and a small [CODE] such as[CODE] - The y labels are balanced (i.e. the mean of y is 0.5) The results of the trained SVM are very good on randomly generated data - while they should be near chance. If the y labels are a bit different, or the SVM is swapped out for a `[CODE][CODE]``python from sklearn import svm from sklearn.linear_model import LogisticRegression import numpy as np from sklearn.model_selection import GridSearchCV, StratifiedKFold, LeaveOneOut, KFold from sklearn.metrics import roc_auc_score, brier_score_loss from tqdm import tqdm import pandas as pd N = 20 N_FEATURES = 50 scores = [] for z in tqdm(range(500)): X = np.random.normal(0, 1, size=(N, N_FEATURES)) y = np.random.binomial(1, 0.5, size=N) if z < 10: y = np.array([0, 1] * int(N\/2)) y = np.random.permutation(y) y_real, y_pred = [], [] skf_outer = LeaveOneOut() for train_index, test_index in skf_outer.split(X, y): X_train, X_test = X[train_index], X[test_index, :] y_train, y_test = y[train_index], y[test_index] clf = svm.SVC(probability=True, C=0.01) clf.fit(X_train, y_train) predictions = clf.predict_proba(X_test)[:, 1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25631"
  },
  {
    "number":26613,
    "text":"Add Multiclass to Target Encoder\n\nAdd multi-class to target encoder, [URL] @lucyleeow is currently looking into this.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26613"
  },
  {
    "number":28337,
    "text":"Enforce [CODE] and [CODE] in [CODE] post SLEP007 implementation\n\n### Describe the workflow you want to enable I would like to propose an enhancement to the [CODE] function, particularly in light of the implementation of SLEP007. As per SLEP007, which was integrated into scikit-learn starting from version 1.1.0, estimators are expected to support [CODE] and [CODE] attributes. However, it appears that the [CODE] utility does not currently enforce the presence of these attributes. ### Describe your proposed solution I propose that the [CODE] function be updated to include checks for the existence of the [CODE] and [CODE] attributes. ### Additional context - SLEP007:[ [URL] - Version of scikit-learn where SLEP007 was implemented: 1.1.0 - Example of [CODE] not enforcing these attributes: [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28337"
  },
  {
    "number":30398,
    "text":"New example about how to implement the SuperLearner in Python\n\n### Describe the issue linked to the documentation The SuperLearner is a stacking strategy that is very used in fields like Statistics (for instance in causal inference, survival analysis etc) to obtain a good machine learning model fitted to your data without caring too much about model selection. It is implemented as an R package if needed",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30398"
  },
  {
    "number":27180,
    "text":"AttributeError: 'Flags' object has no attribute 'c_contiguous'\n\n### Describe the bug This is the error I am getting when I run my knn classifier. Everything was fine until last night and I am very puzzled to see this issue this morning. I am new to ML so please help. ```pytb AttributeError Traceback (most recent call last) Cell In[39], line 4 2 knn = KNeighborsClassifier(n_neighbors=3) 3 knn = knn.fit(X_train, y_train) ----> 4 y_pred = knn.predict(X_test) 6 # Preciision, recall, f-score from the multi-class support function 7 print(\"Classification Report\") File ~\/anaconda3\/lib\/python3.10\/site-packages\/sklearn\/neighbors\/_classification.py:246, in KNeighborsClassifier.predict(self, X) 244 check_is_fitted(self, \"_fit_method\") 245 if self.weights == \"uniform\": --> 246 if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for( 247 X, self._fit_X, self.metric 248 ): 249 probabilities = self.predict_proba(X) 250 if self.outputs_2d_: File ~\/anaconda3\/lib\/python3.10\/site-packages\/sklearn\/metrics\/_pairwise_distances_reduction\/_dispatcher.py:471, in ArgKminClassMode.is_usable_for(cls, X, Y, metric) 448 @classmethod 449 def is_usable_for(cls, X, Y, metric) -> bool: 450 \"\"\"Return True if the dispatcher can be used for the given parameters. 451 452 Parameters (...) 468 True if the PairwiseDistancesReduction can be used, else False. 469 \"\"\" 470 return ( --> 471 ArgKmin.is_usable_for(X, Y, metric) 472 # TODO: Support CSR matrices. 473 and not issparse(X) 474 and not issparse(Y) 475 #...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27180"
  },
  {
    "number":29277,
    "text":"GridSearchCV fails when parameters are arrays with different sizes\n\n### Describe the bug [CODE]. This appears to be because the code attempts to coerce the parameters into one array, and therefore fails due to the inhomogeneous shape. Note: sklearn versions - this error only occurs in recent versions of sklearn (1.5.0). Earlier versions (1.4.2) did not suffer from this issue. Note 2: the issue would be avoided if the [CODE] parameter were to be searched over (instead of the [CODE] parameter). However, it is often important to specify the knots positions directly - for example, with periodic data, as in the provided example, as the periodicity is defined by the first and last knots. In any case there are presumably other places in sklearn where arrays of different shapes can be provided as parameters and where the same issue will occur. ### Steps\/Code to Reproduce ```python import numpy as np import sklearn.pipeline import sklearn.preprocessing import sklearn.model_selection import sklearn.linear_model import matplotlib.pyplot as plt x = np.linspace(-np.pi2,np.pi5,1000) y_true = np.sin(x) y_train = y_true[(0<x) & (x<np.pi2)] x_train = x[(0<x) & (x<np.pi2)] y_train_noise = y_train + np.random.normal(size=y_train.shape, scale=0.5) x = x.reshape((-1,1)) x_train = x_train.reshape((-1,1)) spline_reg_pipe = sklearn.pipeline.make_pipeline( sklearn.preprocessing.SplineTransformer(extrapolation=\"periodic\"), sklearn.linear_model.LinearRegression(fit_intercept=False) ) spline_reg_pipe_cv...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29277"
  },
  {
    "number":27600,
    "text":"Missing assert in test_kernel_approximation.py\n\n### Describe the bug [scikit-learn\/sklearn\/tests\/test\\_kernel\\_approximation.py at main \u00b7 scikit-learn\/scikit-learn]([URL] [CODE_BLOCK] The last line [CODE] should probably be [CODE]. ### Steps\/Code to Reproduce I observed this by statically analyzing the code, but I suspect the [CODE] never fails. ### Expected Results An assertion in the unittest. ### Actual Results No assertion in the unittest ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27600"
  },
  {
    "number":24593,
    "text":"Erroneous use of chi2 in the documentation\n\n### Describe the issue linked to the documentation The documentation page features. The problem is that the example given in the documentation is the Iris data set. The use of [CODE] is therefore likely to be misleading in this context. See [this discussion]([URL] on StackExchange for further details. ### Suggest a potential alternative\/fix Either find a different use of [CODE] that is more aligned with the fact that it works with contingency matrices or explain more clearly how it could be used with continuous features.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24593"
  },
  {
    "number":29565,
    "text":"Override precompute check in LassoCV\n\n### Describe the workflow you want to enable I am trying to use precompute=True for LassoCV. To save memory, I am passing in the inputs as float32's. However, I get an error that the Gram matrix precompute didn't match the true Gram matrix, where the error is some small epsilon like 1e-5 (see photo below). ### Describe your proposed solution It would be great to override the Gram check and allow for whatever was precomputed to be used. ### Describe alternatives you've considered, if relevant ![image]([URL] ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29565"
  },
  {
    "number":29457,
    "text":"MAINT Remove scipy<1.6 specific code in QuantileRegressor and example\n\nWe don't need this code anymore since our minimum supported version is scipy 1.6: [URL] While we are at it there is likely some clode that can be removed in [CODE], for example: [URL] [URL]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29457"
  },
  {
    "number":26868,
    "text":"DOC the pipeline user guide should include same estimators to demonstrate pipeline construction\n\n### Describe the issue linked to the documentation In the [user guide for pipelines]([URL] the demonstration of the construction of a pipeline includes two examples: 1. Directly creating a new instance of [CODE]. 2. Using the utility function [CODE]. Each example uses different lists of estimators, whereas it would be clearer if both examples used the same estimators. ### Suggest a potential alternative\/fix Both examples should use the same estimators when creating a Pipeline, namely the combination of PCA and SVC as in the first example. In the file doc\/modules\/compose.rst lines 68-72 should be replaced with the following [CODE] [CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26868"
  },
  {
    "number":24392,
    "text":"StackingClassifier crashes with stack_method=\"predict\"\n\n### Describe the bug I trained a StackingClassifier with 8 estimators and logistic regression final estimator, with stacking method of \"predict\" and after the fit, when i try to do predict i get this error: Traceback (most recent call last): File \"<string>\", line 1, in <module> File \"\/root\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/ensemble\/_stacking.py\", line 627, in predict_proba return self.final_estimator_.predict_proba(self.transform(X)) File \"\/root\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/ensemble\/_stacking.py\", line 663, in transform return self._transform(X) File \"\/root\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/ensemble\/_stacking.py\", line 281, in _transform return self._concatenate_predictions(X, predictions) File \"\/root\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/ensemble\/_stacking.py\", line 99, in _concatenate_predictions X_meta.append(preds.reshape(-1, 1)) File \"\/root\/anaconda3\/lib\/python3.9\/site-packages\/pandas\/core\/generic.py\", line 5487, in __getattr__ return object.__getattribute__(self, name) AttributeError: 'Series' object has no attribute 'reshape' ### Steps\/Code to Reproduce mega_model.fit(data, y) res = mega_model.predict(data) ### Expected Results an array of predictions ### Actual Results the error ### Versions ```shell System: python: 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0] executable: \/root\/anaconda3\/bin\/python machine: Linux-4.18.0-408.el8.x86_64-x86_64-with-glibc2.28 Py...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24392"
  },
  {
    "number":29742,
    "text":"spin docs --no-plot runs the examples\n\nSeen at the EuroScipy sprint Commands run by spin: [CODE_BLOCK] Looks like our Makefile does not use SPHINXOPTS the same way as expected: Probably we have a slightly different way of building the doc [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29742"
  },
  {
    "number":24811,
    "text":"Allow directly passing in responsibilities to GMM\n\n> I think it's reasonable to extend the initialization possibilities of GMM... > Another possibility to directly pass initial responsibilities would also be interesting. Based on original comments by @jeremiedbb in [URL] It would be great if [CODE] could be directly passed into GMM models to initialize it, as the current method of passing in [CODE], [CODE], and [CODE] requires accessing several private functions hidden inside different modules if users wanted to simply use their own initializations. This feature requests builds on top of #23195 which keeps track of passing in a callable into [CODE] for initializing GMM (or future\/other mixture models). However, the issue is separate as the [CODE], [CODE], and [CODE] parameters are only defined at the GMM class level but passing in [CODE] is defined at the [CODE] class level, and is a separate effort anyway.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24811"
  },
  {
    "number":29051,
    "text":"It's really amazing!! Why are the calculation results of the AUC (recall, precision) function and the average precision score function significantly different?\n\n### Describe the bug Method 1: Precision, recall, _=Precision-Recall_curve() A=auc (recall, precision) Method 2: B=average precision score () Method 1 and Method 2 both calculate the area under the PR curve, but why are the results significantly different, i.e. a \u2260 b ### Steps\/Code to Reproduce Method 1: Precision, recall, _=Precision-Recall_curve() A=auc (recall, precision) Method 2: B=average precision score () Method 1 and Method 2 both calculate the area under the PR curve, but why are the results significantly different, i.e. A \u2260 B ### Expected Results A = B ### Actual Results A \u2260 B ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29051"
  },
  {
    "number":25611,
    "text":"Improve the visibility of the projects governance\n\n### Describe the issue linked to the documentation When I navigate to [URL] I first got to scikit-learn.org -> More -> About Us, and then there is a link to the governance. This should be improved! ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25611"
  },
  {
    "number":28492,
    "text":"Include a lower bound attribute of BaseMixture\n\n### Describe the workflow you want to enable Currently, there exists a [CODE] attribute in the [CODE] method of [CODE]. However, the entire sequence of lower bounds is not accessible, which makes a convergence analysis more difficult to a user. ### Describe your proposed solution In addition to the [CODE] attribute, create a new attribute called [CODE], which is a list of floats where each float is a lower bound set in [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Creating the list and appending to it would increase memory costs (but not by much). Is this a possible concern?",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28492"
  },
  {
    "number":27081,
    "text":"BisectingKMeans floating point exception fatal error on OSX\n\n### Describe the bug Hi, thanks for this fantastic library \ud83d\ude80 \ud83c\udf89 I believe I found a bug. In the BisectingKMeans clustering, the [CODE] function cannot be used when the to-be-predicted-data is on a different numerical scale than the fitted data. Other clustering methods like e.g., KMeans dont have this problem. In the example below, I fitted the BisectingKMeans on random data and then multiplied the data that should be predicted by 50. It causes a floating point exception error which is pretty bad because python silently exits. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A list of predicted class labels ### Actual Results Python silently exits with: [CODE_BLOCK] In Jupyter\/ipython the kernel simply dies. ### Versions ```shell System: python: 3.9.16 (main, Mar 8 2023, 04:29:44) [Clang 14.0.6 ] executable: \/Users\/jab\/miniconda3\/envs\/qondot\/bin\/python machine: macOS-10.16-x86_64-i386-64bit Python dependencies: sklearn: 1.3.0 pip: 23.0.1 setuptools: 67.8.0 numpy: 1.21.6 scipy: 1.10.1 Cython: 0.29.35 pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libomp filepath: \/Users\/jab\/miniconda3\/envs\/qondot\/lib\/python3.9\/site-packages\/sklearn\/.dylibs\/libomp.dylib version: None num_threads: 8 user_api: blas internal_api: openblas prefix: libopenblas filepath: \/Users\/jab\/miniconda3\/envs\/qondot\/lib\/python3.9\/si...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27081"
  },
  {
    "number":30652,
    "text":"Unconsistent FutureWarning when using [CODE] in [CODE]\n\n### Describe the bug Calling fit on a pipeline that includes a [CODE] step with [CODE] and [CODE] (the default value as in v1.6) raises a [CODE] Calling a cross-validation doesn't. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Warning should be raised when cross-validating as well. At least for the first internal fit. ### Actual Results Warning is not raised when cross-validating. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30652"
  },
  {
    "number":29394,
    "text":"Single-sourcing the package version?\n\nIf [CODE] is declared as a dynamic attribute in the [CODE], meson will use the one specified in [CODE]. This would avoid having to update the version in both [CODE] and [CODE]: [CODE_BLOCK]",
    "labels":[
      "Enhancement",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29394"
  },
  {
    "number":25953,
    "text":"ValueError: \"Unknown label type: 'unknown'\" when class column has Pandas nullable type like Int64\n\n### Describe the bug I often use Pandas to load data from CSV and transform it. Pandas tends to parse integer columns as floating point type, so I usually use [CODE] to bring those columns back to an integer type. By design ([URL] Pandas [CODE] converts all output columns to the corresponding nullable extension types (such as [CODE]), not \"simple\" types (such as [CODE]). When I try to train some Scikit-Learn models like [CODE] on such data I get the error [CODE]. This bug relates to already fixed #25073. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results ```py --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[96], line 7 5 df = df.convert_dtypes() 6 model = sklearn.ensemble.RandomForestClassifier() ----> 7 model.fit( 8 X=df.drop(columns=\"class\"), 9 y=df[\"class\"], 10 ) File ~\/.conda\/envs\/dis\/lib\/python3.11\/site-packages\/daal4py\/sklearn\/_device_offload.py:88, in support_usm_ndarray.<locals>.decorator.<locals>.wrapper_with_self(self, args, kwargs) 86 @wraps(func) 87 def wrapper_with_self(self, args, kwargs): ---> 88 return wrapper_impl(self, args, kwargs) File ~\/.conda\/envs\/dis\/lib\/python3.11\/site-packages\/daal4py\/sklearn\/_device_offload.py:74, in support_usm_ndarray.<locals>.decorator.<locals>.wrapper_impl(obj, args, kwargs) 72 us...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25953"
  },
  {
    "number":30213,
    "text":"Tuning [CODE] in [CODE]\n\n### Describe the workflow you want to enable In the GaussianProcessRegressor + \\sqrt{\\alpha}\\xi$, where $f(X)$ is the GP and $\\xi\\sim N(\\cdot|0, I)$. It is an important hyper-parameter, as it represents the measurement error in the target labels. Currently, the CODE] does not tune $\\alpha$. It only tunes the parameters of the kernel. ### Describe your proposed solution Would it be possible to enable the tuning of $\\alpha$ when calling [CODE]? ### Describe alternatives you've considered, if relevant Note: one could try to work around the problem by adding [CODE], but this is not equivalent to tuning $\\alpha$ and retains a different interpretation - see [here and (2.26). Here, $\\sigma_n^2$ plays the role of $\\alpha$. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30213"
  },
  {
    "number":30315,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Dec 05, 2024) \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev - test_partial_dependence_binary_model_grid_resolution[features0-10-10]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30315"
  },
  {
    "number":26895,
    "text":"OneHotEncoder linter issues on argument typing\n\n### Describe the bug The pyright lsp flags type warnings when a OneHotEncoder is initialized with categories as a list and\/or dtype specified as anything but np.float64. Code runs fine, so this maybe should not be considered a bug. It is just a slight annoyance. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The documentation indicates that categories can be a str=\"auto\" or list and dtype appears to be handled downstream by numpy, but the default is initialized as np.float64 and this is somehow confusing the lsp. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0] executable: \/home\/scott\/mambaforge\/envs\/sklearn_new\/bin\/python machine: Linux-6.4.4-zen1-1-zen-x86_64-with-glibc2.37 Python dependencies: sklearn: 1.2.2 pip: 23.2.1 setuptools: 68.0.0 numpy: 1.23.5 scipy: 1.10.1 Cython: None pandas: 2.0.2 matplotlib: 3.2.2 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libgomp filepath: \/home\/scott\/.local\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None num_threads: 12 user_api: blas internal_api: openblas ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26895"
  },
  {
    "number":29600,
    "text":"Please extend the loss-functions in SGDRegressor to allow incremental learning of Poisson \/ Gamma \/ Tweedie regressors\n\n### Describe the workflow you want to enable I would like SGDRegressor to be able to accept additional values in its 'loss' arguments to be able to incrementally train additional regressors, that are currently available in SkLearn in the form of non-incremental regressors (PoissonRegressor \/ GammaRegressor\/TweedieRegressor). ### Describe your proposed solution Add additional loss function implementations (providing gradients) to support the additional regressor types. ### Describe alternatives you've considered, if relevant Learning with L1\/L2 loss in log-space. But it doesn't work well when the label of a given sample is zero. Poisson regression handles it gracefully. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29600"
  },
  {
    "number":24877,
    "text":"Inconsistency Between GridSearchCV\/Lasso and LassoCV MSE Curve & Optimal Alpha\n\n### Describe the bug GridSearchCV with Lasso provides a different MSE curve than LassoCV for small values of alpha (less than about 0.01 in the example below). For most random seeds, they provide the same optimal alpha, but for some seeds (like below), the inconsistency leads to different values. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results I would expect the MSE curves and optimal alphas to be the same. ### Actual Results ![image [MSC v.1929 64 bit (AMD64)] executable: c:\\Program Files\\Python310\\python.exe machine: Windows-10-10.0.19045-SP0 Python d...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24877"
  },
  {
    "number":29453,
    "text":"Why 30 neighbors in 'Agglomerative clustering with and without structure'?\n\n### Describe the issue linked to the documentation This is not so much an issue as a request for explanation. I was going through the scikit-learn user guide on 'Agglomerative clustering with and without structure', which can be found at [URL] I decided to try it out by myself and included scaling of the [CODE] matrix using [CODE]. Here is where the modification occured: [CODE_BLOCK] I was surprised by the result. The connectivity constraints seem to have no effect on [CODE] case: ![fig]([URL] I realized this might be because scaling causes the nodes to come closer together, and 30 neighbors taken in the [CODE] might include cross-layer nodes. Therefore, I reduced it to 20 neighbors: [CODE_BLOCK] And then, I got better results. ![fig2]([URL] This led me to think about how 30 neighbors was decided for the docs, and if there was a way to analytically determine this for every dataset. A...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29453"
  },
  {
    "number":24772,
    "text":"IterativeImputer strange implementation of fit_transform\n\n### Describe the bug A common scenario might be to subclass [CODE] to wrap some functionality around fit\/transform\/fit_transform methods. it might look like this: [CODE_BLOCK] But if one calls this, it results in an infinite recursion because of the specific, non-intuitive way that [CODE] implements [CODE] [that is by calling [CODE]]. In particular, [CODE_BLOCK] ends up calling [CODE] which calls [CODE] which calls [CODE] again in the derived class. The remediation for the programmer of [CODE] is to read the Sklearn code and rewrite his [CODE] method as [CODE_BLOCK], which is highly non-intuitive. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No infinite recursion when overriding [CODE] method. ### Actual Results ``` --------------------------------------------------------------------------- RecursionError Traceback (most recent call last) Input In [167], in <cell line: 14>() 11 return self.transform(X,y) 13 D = Derived() ---> 14 D.fit(None) Input In [167], in Derived.fit(self, X, y) 3 def fit(self, X, y=None): ----> 4 super().fit(X,y) File ~\/anaconda3\/envs\/dvbf-dev\/lib\/python3.10\/site-packages\/sklearn\/impute\/_iterative.py:789, in IterativeImputer.fit(self, X, y) 772 def fit(self, X, y=None): ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24772"
  },
  {
    "number":27592,
    "text":"Using tqdm or progress bars while downloading datasets using [CODE]\n\n### Describe the workflow you want to enable [URL] When we fetch remote data using the function [CODE], we cannot verify that we are downloading it normally because there is no interaction action. Depending on the download environment, it may take a long time, and users may think that there is an error in the program if they do not see it. ### Describe your proposed solution I suggest that make an option to display the progress bar that we download using tqdm or other methods. We can simply implement using [urlretreve reporthook parameter]([URL] Here are some example. [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I've also suggest that adding more options: timeout, retrying.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27592"
  },
  {
    "number":25422,
    "text":"`[CODE][CODE]transform[CODE]fit` data\n\n### Describe the bug Passthrough columns appear to be dropped after running transform on the ColumnTranformer when the fit data did not have the same passthroughs. From my reading of the docstring, they should be included with the output of [CODE] as per the docstring: > By specifying `[CODE][CODE]transformers[CODE]remainder='passthrough'[CODE]``python import pandas as pd import sklearn from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder # Make a mixed-type dataset including passthrough columns df = pd.DataFrame( { \"a\": [\"a\", \"a\", \"b\", \"c\", \"d\"], \"b\": [0.577569, 0.751900, 0.722548, 0.675158, 0.676203], \"c\": [\"a\", \"b\", \"b\", \"a\", \"c\"], \"d\": [0.62, 0.75, 0.72, 0.67, 0.67], \"e\": [180.86, 182.70, 184.41, 188.70, 179.82], # add extra columns for passthrough \"passthrough_1\": [1, 2, 3, 4, 5], \"passthrough_2\": [0, 1, 0, 0, 1], } ) ct = ColumnTransformer( transformers=[ (\"ord\", OrdinalEncoder(), [\"b\", \"d\", \"e\"]), ( \"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), [\"a\", \"c\"], ), ], remainder=\"passthrough\", ) df_without = df[[x for x in df.columns if \"passthrough\" not in x]] ct.set_output(transform=\"pandas\") ct.fit(df_without) df_output = ct.transform(df) assert all(x in df_...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25422"
  },
  {
    "number":29655,
    "text":"GradientBoostingClassifier feature_importances_ is all zero\n\n### Describe the bug I'm using GradientBoostingClassifier on a rather small dataset (n=75) for classification & feature selection. I'm grid searching (in cross validation) the best hyper-parameters for my data and on some grids I get 0 importance for every feature (and on others, everything is ok). I can provide the data if needed. p.s. this is my first issue post, If more information \/ change in format is needed, please let me know ### Steps\/Code to Reproduce # code for iterative feature reduction curX = X.copy() for feature_amount in tqdm(feature_amount_lst,desc = 'Feature Selection',leave=False): model.fit(curX,y) feature_imp = pd.Series(model.feature_importances_, index = curX.columns).sort_values(ascending = False) feature_imp.name = 'importances' feature_imp = feature_imp:feature_amount] if importances_list is not None: importances_list.append(feature_imp) features_list.append(list(feature_imp.index)) curX = curX[features_list[-1]] ### Expected Results Importance array that sums to 1 ### Actual Results ![image [MSC v.1929 64 bit (AMD64)] executable: c:\\Users\\cs-lab\\AppData\\Local\\Programs\\Python\\Python39\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.5.1 pip: 24.0 setuptools: 56.0.0 numpy: 1.22.4 scipy: 1.12.0 Cython: 0.29.37 pandas: 1.3.2 matplotlib: 3.8.3 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_thre...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29655"
  },
  {
    "number":31633,
    "text":"Check [CODE] present in [CODE] in metric functions\n\nNoticed while working on [URL] Currently the following metric functions do not explicitly check that [CODE] is present in [CODE]:  [CODE]  [CODE]  [CODE]  [CODE] AFAICT all (?) other classification metrics (e.g., [CODE], [CODE]), including ranking metric [CODE] explicitly check that [CODE] is present in [CODE]: e.g. this is the error from [CODE]\/[CODE]\/[CODE] family: [CODE_BLOCK] [CODE] and [CODE] do not explicitly check this, they do warn (no error) that there are no 'positive' samples in [CODE]: [CODE_BLOCK] Similarly, for [CODE] this results in an invalid divide warning (we divide by 0): [CODE_BLOCK] [CODE] gives no error and no warning. `_validate_binary_probabi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31633"
  },
  {
    "number":25525,
    "text":"Extend SequentialFeatureSelector example to demonstrate how to use negative tol\n\n### Describe the bug I utilized the SequentialFeatureSelector for feature selection in my code, with the direction set to \"backward.\" The tolerance value is negative and the selection process stops when the decrease in the metric, AUC in this case, is less than the specified tolerance. Generally, increasing the number of features results in a higher AUC, but sacrificing some features, especially correlated ones that offer little contribution, can produce a pessimistic model with a lower AUC. The code worked as expected in sklearn 1.1.1, but when I updated to sklearn 1.2.1, I encountered the following error. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ```python-traceback $ python sfs_tol.py Traceback (most recent call last): File \"\/home\/modelling\/users-workspace\/nsofinij\/lab\/open-source\/sfs_tol.py\", line 28, in <module> pipe.fit(X, y) File \"\/home\/modelling\/opt\/anaconda3\/envs\/py310\/lib\/python3.10\/site-packages\/sklearn\/pipeline.py\", line 401, in fit Xt = self._fit(X, y, fit_params_steps) File ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25525"
  },
  {
    "number":28551,
    "text":"Implement [CODE]\n\n### Describe the workflow you want to enable I think it should be possible to implement a new method [CODE] such that: [CODE_BLOCK] ### Describe your proposed solution There might be several mathematical ways to define such a transform, in particular if when passing a [CODE] that contain real numbers that do not actually result from a spline expansion. For instance when: - [CODE] - [CODE] - [CODE]. or when all values of a given row are non-zeros at once... One possible way would be to decode based on [CODE] and then using the relative strength of neighboring spline activations to resolve ambiguities. ### Describe alternatives you've considered, if relevant The main alternative is to not implement this. The main question is probably why try to implement this in the first place? Possible use cases: - fit a GMM model on spline transformed data (to get a more axis-aligned inductive prior), generate samples in the GMM latent space and then recode those samples back into the original space. - fit PCA with a small rank on spline encoded data and then reconstruct back the projected data, - fit k-means in spline space and recode the learned centroids back in the original feature space for inspection. - idem for NMF or dictionary learning components. ### Additional context If #28043 gets merged, missing values support should also be included when using the 'indicator' strategy.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28551"
  },
  {
    "number":30079,
    "text":"[CODE]: incorrect result after merging #27412\n\n### Describe the bug When all data instances come from the same class, #27412 changed the behaviour of [CODE] to return [CODE] instead of raising an exception. The argument for the change was the consistency with PR curves. I believe that this result is incorrect, or, at least, not correct under all interpretations. Even if only the latter: it is not worth breaking backwards compatibility for a change that is a matter of discussion - in particular if the change is masking an error by returning a (dubious) \"default\". ### Arguments The issue arises when all data instances belong to the same class. While AUC is, literally, the area under the ROC curve, we interpret it as the score reflecting the quality of ranking, which is also related to the Gini index and Mann-Whitney U-statistics, as also described in sklearn documentation. - Under geometric interpretation, if all data comes from the same class, the curve may go either straight right or straight up, depending upon the class, so it can be either 0 or 1 (or 0.5), not (necessarily) 0.0. - Under statistical interpretation, the AUC is undefined. AUC is the probability that for a random pair of instances from different classes, the score assigned to the instance from the positive class is higher than the score assigned to the instance from the negative class. This measure cannot be computed for data from a single class and is thus undefined. The function should return [CODE] or raise ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30079"
  },
  {
    "number":28820,
    "text":"Race condition when building with Meson\n\nOpening this to track it in an issue rather than seeing it appear in spread-out PRs. The error looks like this: [CODE_BLOCK] or like this: [CODE_BLOCK] Some files `__...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28820"
  },
  {
    "number":30180,
    "text":"DOC grammar issue in the governance page\n\n### Describe the issue linked to the documentation In the governance page at line: [URL] there is a reference attached to \"Enhancement proposals (SLEPs).\" However, after compiling, it is displayed as \"a Enhancement proposals (SLEPs)\" which is grammatically incorrect. Page at: [URL] ### Suggest a potential alternative\/fix Fix it by updating the line with [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30180"
  },
  {
    "number":27347,
    "text":"RFC feature subsampling for tree based models\n\n### Background Currently, our (mostly tree-based) models supporting feature subsampling via [CODE] are: - [CODE], [CODE] - [CODE], [CODE] - [CODE], [CODE] - [CODE], [CODE] - [CODE], [CODE] - [CODE] - [CODE], [CODE] Soon, [CODE] and [CODE] will have feature subsampling, too, see #27139. ### Problem Statement [CODE] can be a float between 0 and 1. In this case, it means that a fraction of features is randomly selected in each split. It can also be an integer, in which case it specifies the number of features selected in each split. This means that [CODE] and [CODE] have very different effects! See [URL] Also note that for most estimators the default is [CODE] or [CODE] meaning no subsampling. The notable exception is [CODE] and [CODE] with default [CODE] (which makes sense), but the corresponding regressors still use all features (so no rf but bagged trees). This was discussed in detail in #20111. Additionally, for most estimator it means subsampling per tree split\/node. For BaggingC\/R models and IsolationForest it means per model\/iteration. ### Proposal Accompanied with a proper deprecation strategy, we could add different arguments, one for specifying fractions, one for integer numbers. ### Decision Options #### A Do we want to change the current situation? #### B Which names shall the new arguments have? ##### B.1 We could keep [CODE] for number of features and add only one new argument for fractions. Or we can actually add 2 ne...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27347"
  },
  {
    "number":26359,
    "text":"Selecting Lasso via cross-validation data leakage\n\n### Describe the issue linked to the documentation [CODE_BLOCK] Optimizing Alpha for lasso involves data leakage because standard scaling is applied on X and y in one go, instant for each fold. It is used in the example Selecting Lasso via cross-validation [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26359"
  },
  {
    "number":30240,
    "text":"Clarification on Kruskal Stress as an Optimization Target in Metric and Non-metric MDS\n\n### Describe the issue linked to the documentation I am working on research involving the optimization targets used in metric and non-metric MDS, and I have some questions regarding how scikit-learn's implementation of MDS defines and calculates stress, particularly Kruskal Stress. While reviewing the official documentation, I noticed that specific formulas for stress calculations are not explicitly provided, and I would appreciate some clarification. Non-metric MDS: My understanding is that non-metric MDS typically minimizes Kruskal Stress, defined as: ![\u5c4f\u5e55\u622a\u56fe 2024-11-08 163808]([URL] in the reduced space. Could you confirm if scikit-learn's non-metric MDS implementation uses this definition, or if it employs an alternative method? Metric MDS: Does metric MDS in scikit-learn also optimize for Kruskal Stress, or does it use a different stress formula? If a different approach is used, would it be possible to provide some insight or references on the stress function applied here? ### Suggest a potential alternative\/fix Documentation Clarification: It would be incredibly helpful if the documentation could include specific details on the stress formulas used in both metric and non-metric MDS. This addition would help researchers and users better understand the theoretical underpinnings of the algorithm in scikit-learn. Thank you very much for your guidance and clarification on these points. You...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30240"
  },
  {
    "number":27358,
    "text":"DOC Improve description of the Nystroem method in the user guide\n\n### Describe the issue linked to the documentation We currently have a rather short and shallow description of the [Nystroem Method for Kernel Approximation]([URL] ### Suggest a potential alternative\/fix I think we can expand the user guide to first include an intuitive explanation of the method, and then a more mathematical explanation of the implementation, as done in [this stackexchange post]([URL] by @djsutherland.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27358"
  },
  {
    "number":26321,
    "text":"Duality gap computation in covariance.GraphicalLasso yields negative values.\n\n### Describe the bug The computation of the duality gap in [CODE] of [CODE] uses the definition from [CODE]. However, their duality gap is expressed given a _feasible_ dual variable. In the implementation, it is applied to a primal variable that doesn't necessarily satisfy the dual variable's feasibility constraints. This results in potentially negative values for the duality gap. The primal problem reads $$P(\\Theta) = \\min_\\Theta \\varphi(\\Theta) + \\nu(\\Theta)$$ with $$\\varphi(\\Theta) = -\\log \\det (\\Theta) + \\langle S, \\Theta \\rangle \\text{ and } \\nu(\\Theta) = \\Vert \\Lambda \\odot \\Theta \\Vert_1$$ The Fenchel-Rockafellar dual problem is then given by $$D(W) = \\min_{W} \\varphi^{\\star} (W) + \\nu^{\\star} (-W)$$ $$= \\min_{W} -\\log \\det(S - W) - p \\quad \\text{s.t. } \\quad \\vert W_{ij} \\vert < \\lambda_{ij}$$ We can then compute the duality gap as follows: $$G(\\Theta, W) = P(\\Theta) + D(W)$$ Only for a feasible $W$ can we actually take $\\theta = (S - W)^{-1}$ and thus simplify the duality gap expression and fall back on the implemented formula: $$G(\\Theta) = \\langle S, \\Theta \\rangle + \\Vert \\Lambda \\odot \\Theta \\Vert_1 - p$$ @mathurinm ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The expected results would be positive values for the duality gap at all iterations. ### Actual Results Here are the duality gaps for the first 4 iterations : [CODE_BLOCK] ### Versions ```shell System: python: 3.1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26321"
  },
  {
    "number":30478,
    "text":"[CODE] incorrectly swaps the weights for the positive and negative classes\n\n### Describe the bug See the example below. CODE] is set to [CODE], so with the class weights applied, [CODE] should be [CODE] for the positive class and [CODE] should be [CODE] for the negative class. But after adding some logging, we can see the [CODE] and [CODE] variables [here [Clang 18.1.8 ] executable: \/opt\/homebrew\/Caskroom\/miniforge\/base\/envs\/momatrader-intelligence\/bin\/python machine: macOS-14.7.1-arm64-arm-64bit Python dependencies: sklearn: 1.5.2 pip: 24.3.1 setuptools: 75.5.0 numpy: 1.26.4 scipy: 1.14.1 Cython: None pandas: 2.2.3 matplotlib: 3.9.3 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: op...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30478"
  },
  {
    "number":24685,
    "text":"Example of code that needs to be updated\n\n### Describe the issue linked to the documentation [URL] The code example on this link does not work properly because it uses a discarded parameter. ![image]([URL] ### Suggest a potential alternative\/fix Update the use of parameters in the code to comply with the newer versions",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24685"
  },
  {
    "number":30194,
    "text":"Rename [CODE] to [CODE]\n\nLooking through all our estimators, none of them have the word \"Estimator\" besides [CODE] and [CODE]. I think we can shorten the meta-estimator name to [CODE]. CC @adrinjalali @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30194"
  },
  {
    "number":26864,
    "text":"Is this behavior a bug?: f1_score([1,0,0], [0,1,0], zero_division=1.0) returns 1.0 not 0.0\n\n### Describe the bug Thank you for the update in [URL] In scikit-learn 1.3.0, there seems to be a bug in the [CODE] argument of the [CODE]. It appears to be degrading comparing with scikit-learn 1.2.2. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Returns 0.0 not 1.0. In scikit-learn 1.2.2 [CODE_BLOCK] --- [CODE_BLOCK] - precision 0.0 - recall 0.0 - -> f1_score should be 0.0 to show model's low performance ### Actual Results Returns 1.0 --- Other case (OK) [CODE_BLOCK] ### Versions ```shell System: python: 3.9.16 (main, Jan 3 2023, 22:16:38) [Clang 14.0.0 (clang-1400.0.29.202)] executable: \/...\/sci13\/bin\/python machine: macOS-12.6.6-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.2 setuptools: 68.0.0 numpy: 1.25.1 scipy: 1.11.1 Cython: None pandas: None matplotlib: None joblib: 1.3.1 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 10 prefix: libomp filepath: \/...\/sci13\/lib\/python3.9\/site-packages\/sklearn\/.dylibs\/libomp.dylib version: None user_api: blas internal_api: openblas num_threads: 10 prefix: libopenblas filepath: \/...\/sci13\/lib\/python3.9\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib version: 0.3.23 threading_layer: pthreads architecture: armv8 user_api: blas internal_api: openblas ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26864"
  },
  {
    "number":30713,
    "text":"Error in [CODE] multiclass when one of the classes is missing in [CODE].\n\n### Describe the bug Hello, I encountered an error with the [CODE] in the multiclass setting (i.e. when [CODE] has shape (n, k) with k >= 3) when one of the classes is missing from the [CODE] labels, even when giving the labels through the [CODE] argument. The error disappear when all the classes are present in [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.8 | packaged by conda-forge | (main, Dec 5 2024, 14:19:53) [Clang 18.1.8 ] executable: \/Users\/alexandreperez\/dev\/lib\/miniforge3\/envs\/test\/bin\/python machine: macOS-15.2-arm64-arm-64bit Python dependencies: sklearn: 1.6.1 pip: 24.3.1 setuptools: 75.8.0 numpy: 2.2.2 scipy: 1.15.1 Cython: None pandas: None matplotlib: None joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30713"
  },
  {
    "number":27531,
    "text":"NearestNeighbors.kneighbors returns inaccurate distance\n\n### Describe the bug Using neighbors.NearestNeighbors I noticed that when finding an exact match, kneighbors _sometimes_ returns a distance > 0. (Although the values I've seen so far have been pretty small ~1e-8 to 1e-9) At first I thought this was a floating point precision problem, but spacial.distance - which the documentation for NearestNeighbor implies it uses - never displays this problem in my testing. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Ideally, distance should always be accurate, i.e. 0 between two identical elements. If this is infeasible, I would suggest updating the documentation to warn users about this and encourage them to recalculate the distance with [CODE] instead of relying on the returned value if accuracy is important. ### Actual Results ``` 0->0: kneigbors' distance: 0.0 spacial.distance.euclidean: 0.0 metrics.pairwise.euclidean_distances: [[0.]] 1->1: kneigbors' distance: 1.862645149230957e-09 spacial.distance.euclidean: 0.0 metrics.pairwise.euclidean_distances: [[0.]] Process finished wi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27531"
  },
  {
    "number":25727,
    "text":"wrong display\n\n### Describe the bug why is it not displaying properly? ### Steps\/Code to Reproduce from sklearn.metrics import classification_report classification_report(y_test,y_pred, target_names=['normal','Toxicity']) ### Expected Results No error ### Actual Results ![image]([URL] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25727"
  },
  {
    "number":30189,
    "text":"[CODE] on empty array raises [CODE]\n\n### Describe the bug I understand that the imputer requires at least one sample to fit. There is no reason for it not to return an empty array on [CODE] though. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results An empty array of shape [CODE]. ### Actual Results [CODE_BLOCK] ### Versions ```s...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30189"
  },
  {
    "number":29355,
    "text":"Shape may mismatch in [CODE] if set [CODE] with array-like input and some features contain all missing values\n\n### Describe the bug In all Imputers, features with all missing values will be droped if [CODE] is set to [CODE]. However, in [CODE], if we set [CODE] and [CODE] with array-like input, it might raise an error indicate that the shape mismatch. We can manually reduce the elements in [CODE] and [CODE] after we check if there is any features with all missing values, I wonder if this can be done automatically. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error raised. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:38:11) [Clang 14.0.6 ] executable: \/Users\/xxf\/miniconda3\/envs\/sklearn-env\/bin\/python machine: macOS-14.5-arm64-arm-64bit Python dep...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29355"
  },
  {
    "number":26948,
    "text":"The copy button on install copies an extensive comman including env activation\n\n### Describe the issue linked to the documentation [URL] Above link will lead you to the sklearn downlanding for link . when you link copy link button it will copy [CODE] instead of [CODE] if this is the issue so please issue i want to create a pull request for it and tell in which file this issue reside Thanks ### Suggest a potential alternative\/fix By resoving above issue",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26948"
  },
  {
    "number":27105,
    "text":"Feature selection estimator class params does not update with pipeline class params for Gridsearch\n\n### Describe the bug In a pipeline which has a step of feature selection (SelectFromModel with XGB estimator) and a step of XGB class, when Grid Search is used for hyperparameter tuning of a param in XGB, only one step of the XGB param is update even another step of XGB model is cloned as shown in below code example. How could the XGB parameter value of both steps be updated at the same time? ### Steps\/Code to Reproduce ``` from xgboost import XGBClassifier from sklearn.base import clone from sklearn.datasets import make_classification from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import SelectFromModel from sklearn.model_selection import GridSearchCV, train_test_split X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=0, random_state=0, shuffle=False) X_train, X_test, y_train, y_test = train_test_split(X, y) param_grid = { 'selectfrommodel__estimator__colsample_bytree': [i\/10.0 for i in range(8, 10)], } ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27105"
  },
  {
    "number":29650,
    "text":"Expand build from source docs for debugging with meson\n\nFrom [URL] and [URL] > Could you please open a follow-up PR that expands either our \"build from source\" documentation or our documentation on \"how to debug\/profile\" to explain how to switch between \"release\" and \"debugoptimized\" via by using a pip commandline flag and explains the expected impact (in terms of binary size and ability to use a debugger\/profiler for native code). > In particular it would be interesting to see the impact of this switch when using a profiler such as linux perf (see this page for Python 3.12 specific integration) on a Python script that relies heavily on native code (e.g. fitting HistGradientBosttingClassifier which is mostly Cython). > And similarly check that it works as expected for py-spy's support for native extension profiling:",
    "labels":[
      "Documentation",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29650"
  },
  {
    "number":30007,
    "text":"Upgrade free-threading CI to run with pytest-freethreaded instead of pytest-xdist\n\nThere is a new kid on the block that should help us find out whether scikit-learn and its dependencies can be reliably considered free-threading compatible: URL] Let's try to adopt it in scikit-learn. Here is a possible plan: - first run the tests locally a few times and see if they are tests (or a set of interacting tests) that cause a crash or a failure, open an issue for each of them, possibly upstream and then mark them as skipped under free-threading builds with a reference to the issue in the \"reason\" field; - then upgrade our nightly free-threading scheduled CI run to use [CODE]. Any comments @lesteve @jeremiedbb @ngoldbaum? EDIT: anyone interested in getting hands on the first item can find this resource useful: [URL] EDIT 2: there is also the [pytest-run-parallel - [ ] benchmark the use of threading, in particular when we we expect nested parallelism between Python threads (e.g. in [CODE] with the \"threading\" backend of joblib) and BLAS or OpenMP native threading in the underlying estimators. - [ ] communicate results on a blog post \/ pydata presentation \/ social media.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30007"
  },
  {
    "number":30913,
    "text":"Typo in _k_means_lloyd.pyx\n\n### Describe the issue linked to the documentation I noticed that in the lloyd_iter_chunked_sparse function of _k_means_lloyd.pyx, there is a potential typo in the comment for handling an empty array. It reads (starting on line 280): \"An empty array was passed, do nothing and return early (before attempting to compute n_chunks). This can typically happen when calling the prediction function of a bisecting k-means model with a large fraction of outiers.\" ### Suggest a potential alternative\/fix I'd like to propose editing the file to read \"large fraction of outliers\" instead of \"large fraction of outiers\". Let me know what you think!",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30913"
  },
  {
    "number":24269,
    "text":"\u26a0\ufe0f CI failed on macOS.pylatest_conda_forge_mkl \u26a0\ufe0f\n\nCI failed on macOS.pylatest_conda_forge_mkl Unable to find junit file. Please see link for details.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24269"
  },
  {
    "number":31672,
    "text":"ENH Add clip parameter to MaxAbsScaler\n\n### Describe the workflow you want to enable Add a [CODE] parameter to [CODE] that will allow for clipping values that exceed the maximum value seen during the training stage. ### Describe your proposed solution Similar to [CODE], but in this case it will clip [-1, +1]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I'm not sure if it is possible to implement it without breaking sparsity of the inputs, which is the main problem.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31672"
  },
  {
    "number":26658,
    "text":"Automatic bandwidth calculation valid only for normalized data\n\n### Describe the bug [CODE] supports automatic (optimal) bandwidth calculation via [CODE] and [CODE]. The algorithm computes the appropriate observation-weighted bandwidth factors (proportional to nobs^0.2) but does not adjust for the standard deviation or interquartile range of the dataset. Roughly, the algorithm should scale the dataset's standard error by the algorithmic bandwidth factors. See, e.g., [Wikipedia]([URL] The implementation in [CODE] is correct. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Automatic SKLearn bandwidth curve should approximately match SciPy bandwidth curve, roughly the shape of the underlying data histogram. ### Actual Results Automatic SKLearn bandwidth curve generates a flat PDF. ### Versions ```shell System...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26658"
  },
  {
    "number":30058,
    "text":"DOC broken image link in user guide due to removal of example\n\n### Describe the issue linked to the documentation The image at the bottom of Section 3.5.1 on [URL] is broken, which I believe is due to the removal of some example in #29936. We may want to rework or remove the last part. ### Suggest a potential alternative\/fix NA",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30058"
  },
  {
    "number":29409,
    "text":"DOC Correct lower bound for adjusted rand index in User Guide\n\n### Describe the issue linked to the documentation The lower bound for the adjusted Rand Index is described as -1 in the User Guide, whereas the docstring says -0.5. It has been discussed in #8166 that -0.5 is correct, we should correct it also in the User Guide to avoid confusion. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29409"
  },
  {
    "number":28554,
    "text":"Interactive code examples\n\n### Describe the issue linked to the documentation I think the [CODE] docs would be even better if the code examples were interactive (while still being lighter and more reader-friendly than full-featured notebooks). Then people could change the code and see it reflected in the output and visualizations. ### Suggest a potential alternative\/fix Here is an example of how this can be done: [URL] If you're interested, I'll be happy to send you a PR to backport this into the docs.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28554"
  },
  {
    "number":27430,
    "text":"Catching deprecation warnings from examples\n\nUp-to-now, we never check deprecation warnings that are raised when executing our examples. I assume that we could scrap the RST generated file to find such warning: [CODE_BLOCK] What I am wondering is exactly how do we want to use it with the CI. Supposedly, it should be only CircleCI but should target only PRs?",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27430"
  },
  {
    "number":24728,
    "text":"How observations with sample_weight of zero influence the fit of HistGradientBoostingRegressor\n\n### Describe the bug Hello, I am trying to exclude some training observations by giving them a weight of zero through the \"sample_weights\" argument. As I understand it, observations with a weight of 0 do not influence the training, so changes in their values should not affect the resulting model. However, this is not what I see if I train two models, each one with different values in the samples with weight of zero. Does anyone know how exactly the algorithm implements sample weight inside the code? Thanks a lot! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results array([10.02028706, 10.36184929, 9.45997232, 9.18761327, 9.93495853]) array([10.02028706, 10.36184929, 9.45997232, 9.18761327, 9.93495853]) ### Actual Results array([10.02028706, 10.36184929, 9.45997232, 9.18761327, 9.93495853]) array([10.06163207, 10.17065387, 9.04865117, 9.15543545, 9.92940014]) ### Versions ```shell System: python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0] executable: \/home\/ubuntu\/anaconda3\/envs\/skforecast\/bin\/python machine: Linux-5.15.0-1022-aws-x86_64-with-glibc2.17 Python dependencies: sklearn: 1.1.0 pip: 22.1.2 setuptools: 63.4.1 numpy: 1.23.0 scipy: 1.9.1 Cython: None pandas: 1.4.0 matplotlib: 3.5.0 joblib: 1.1.0 threadpoolctl: 3.1.0 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24728"
  },
  {
    "number":24378,
    "text":"DOC: new example on feature engineering for time-series forecasting with prediction intervals\n\nFor EuroScipy 2022, I gave a tutorial on how to use pandas-engineered lagged and windowed features for time series forecasting with scikit-learn regressors. Here is the notebook: - [URL] I think it might be worth investing some effort to reuse some of that material to turn it into a tutorialish example for the gallery (and cross-link it with the time-related feature example on the same dataset). Note that this example probably covers too many thinks and we should probably focus on a trim down version. For instance, by removing the experiment with MAPIE that I don't find particularly conclusive at the moment (would need to speed more time to find how to make MAPIE output heteroscedastic prediction intervals on this data in particular). I find that the discussion on sktime to be informative to go beyond the pure-scikit-learn approach which I found out to be limitting in retrospect as explained towards the end of the tutorial. \/cc @lorentzenchr who expressed interest. Also \/cc @ArturoAmorQ who might be interested in working on such a contribution.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24378"
  },
  {
    "number":25333,
    "text":"Read only buffer in cross_val_score with sparse matrix.\n\n### Describe the bug When calling [CODE] with a sparse data matrix [CODE] and a [CODE] with [CODE], there is a weird interaction with joblib and memmapping that makes the buffer from [CODE] read-only, breaking the cython code for the tree construction but it is weird as it only appears with the [CODE] function, and not when calling the classifier alone, while [CODE] for the cross val function so joblib should not enter the play here... ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Working code ### Actual Results ``` ValueError: All the 5 fits failed. It is very likely that your model is misconfigured. You can try to debug the error by setting error_score='raise'. Below are more details about the failures: -------------------------------------------------------------------------------- 5 fits failed with the following error: joblib.externals.loky.process_executor._RemoteTraceback: \"\"\" Traceback (most recent call last): File \"\/home\/temp\/.local\/miniconda\/lib\/python3.10\/site-packages\/joblib\/externals\/loky\/process_executor.py\", line 428, in _process_worker r = call_item() File \"\/home\/temp\/.local\/miniconda\/lib\/python3.10\/site-packages\/joblib\/externals\/loky\/process_executor.py\", line 275, in __call__ return self.fn(self.args, self.kwargs) File \"\/home\/temp\/.local\/miniconda\/lib\/python3.10\/site-packages\/joblib\/_parallel_backends.py\", line 620, in __call__ return self.func(args, kwargs) File \"\/home\/temp\/.local\/mini...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25333"
  },
  {
    "number":30197,
    "text":"Exception on rendering html empty pipeline\n\n### Describe the bug Rendering empty pipeline to html fails, and just simply displaying an empty pipeline fails on IPython\/Jupyter. See upstream IPython issue: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] or just in IPython: [CODE_BLOCK] ### Expected Results Don't raise in the repr_handler. ### Actual Results ``` Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"\/Users\/bussonniermatthias\/miniconda3\/envs\/arm64\/lib\/python3.12\/site-packages\/sklearn\/utils\/_estimator_html_r...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30197"
  },
  {
    "number":29692,
    "text":"Add Diebold Mariano test for distinguishing forecasts\n\n### Describe the workflow you want to enable I would like to be able to compare whether one forecast is statistically better than another. ### Describe your proposed solution Under certain conditions, the Diebold-Mariano test achieves this. There's an example in Python [here]([URL] ### Describe alternatives you've considered, if relevant I'm not sure there are alternatives to this. ### Additional context In time series forecasting, we often want to know which forecast performs better. This test puts the difference in performance on a firm statistical footing.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29692"
  },
  {
    "number":27505,
    "text":"Impact of class weights in LogisticRegression\n\n### Describe the issue linked to the documentation The impact of class weights and the exact objective function with (all kinds of) weights for [CODE] should be mentioned in the user guide. Importantly, the scale of weights interact with the (anti-) penalty strength [CODE]. Proof that this is confusing: #27455 ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27505"
  },
  {
    "number":27152,
    "text":"DOC \"Copy to clipboard\" doesn't copy multiline instructions\n\n### Describe the issue linked to the documentation For instance in [Getting Started]([URL] ![image]([URL] The pasted code is: [CODE_BLOCK] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27152"
  },
  {
    "number":26817,
    "text":"Scorer not working on ClassifierChain\n\n### Describe the bug ClassifeirChain fails when trying to make scoring predictions. The reason is: ClassifeirChain's attribute [CODE] is a list and not a np.array as the [CODE] function expects. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown. ### Actual Results [CODE_BLOCK] ### ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26817"
  },
  {
    "number":31587,
    "text":"Can't create exe-file with this module\n\n### Describe the bug [CODE_BLOCK] Project output will not be moved to output folder ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expected to create exe-file with this module imported using pyInstaller ### Actual Results [CODE_BLOCK] Project output will not be moved to output folder ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31587"
  },
  {
    "number":31149,
    "text":"BUG: Build from source fails for scikit-learn v1.6.1 on Windows 11 with Visual Studio Build Tools 2022, Ninja subprocess error\n\n### Describe the bug First of all, thank you guys for the fantastic job with Sklearn. I'm trying to build from source to start contributing to the project, but it ended with me bringing more issues to you. After struggling for some days with this problem, I'm seeking help. Maybe if you have some clue or workaround, I could open a Pull Request with the solution for this. I am following the guidelines for Contributing with Scikit-learn, as well as the creation of the Python virtual environment and the installation of the packages [CODE]. But in the step of building from source using the [CODE], the build breaks when Compiling C objects after some [C4090 warnings]([URL] throwing an [CODE] from a subprocess of [CODE]. This seems related\/similar to issue #31123. Despite not being the same problem, if we find a solution, it may work for both issues. ### Steps\/Code to Reproduce I have tried the steps using [CODE] and also [CODE] in different versions of Python: pip :{3.10.11, 3.12.7} conda:{ 3.13.2} to check if it was a problem with Python\/pip itself. 1. Environment Setup: - OS: Windows 11 Pro, Version 24H2, OS build 26100.3476 - System type: 64-bit operating system, x64-based processor...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31149"
  },
  {
    "number":28824,
    "text":"RFC Trigger a copy when copy=False and X is read-only\n\nHighly related to #14481 and maybe a little bit to [URL] My understanding of the [CODE] parameter of estimators is \"allow inplace modifications of X\". When avoiding a copy is not possible (X doesn't have the right dtype or memory layout for instance), a copy is still triggered. I believe that X being read-only is a valid reason for still triggering a copy. My main argument is that the user isn't always in control of the permissions of an input array within the whole pipeline. Especially when joblib parallelism is enabled, which may create read-only memmaps. We've have a bunch of issues because of that, the latest being [URL] And it's poorly tested because it requires big arrays which we try to avoid in the tests (although joblib 1.13 makes it easy to trigger with small arrays). I wouldn't make [CODE] always trigger a copy when X is read-only because the semantic of the [CODE] param of [CODE] is not the same as the one of estimators. We could introduce a new param in check_array, like [CODE] ? - Estimator has no copy param (i.e.) doesn't intend to do inplace modification: [CODE] - Estimator has copy param: [CODE] It could also be a third option for copy in check_array: True, False, \"if_readonly\": - Estimator has no copy param: [CODE] - Estimator has copy param: [CODE]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28824"
  },
  {
    "number":28370,
    "text":"[Bug, 1.5 nightly] [CODE] broken by #28256\n\n### Describe the bug [CODE] crashes when [CODE] when no metadata is passed. This was caused by the follow PR which changed #28256 which removed the [CODE] inside of [CODE]. I have created a PR in #28371 Issue comes from: [URL] This issue comes from the fact that [CODE] can return two different objects, an [CODE] or a [CODE] which behave slightly different with respect to attribute access. [CODE_BLOCK]  [[CODE]]([URL]  [[CODE]]([URL] ### Steps\/Code to Reproduce ```python import sklearn from sklearn.datasets import load_iris from sklearn.dummy import Dum...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28370"
  },
  {
    "number":25896,
    "text":"Support other dataframes like polars and pyarrow not just pandas\n\n### Describe the workflow you want to enable Currently, scikit-learn nowhere claims to support [pyarrow]([URL] or [polars]([URL] And indeed, [CODE_BLOCK] errors with [CODE_BLOCK] ### Describe your proposed solution scikit-learn should support those dataframes, maybe via the [python dataframe interchange protocol]([URL] In that regard, a new option like [CODE] would be nice. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Some related discussion came up in #25813.",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25896"
  },
  {
    "number":26801,
    "text":"HDBSCAN Ongoing Work\n\n## Introduction This is a (hopefully) exhaustive list of ongoing\/future work for HDBSCAN. These have all been discussed and are considered wanted, but some still require thorough investigation (especially heuristic evaluations). ## Priority List The higher priority items appear earlier in this list. - [ ] [URL] - [x] [URL] - [ ] Finalize [CODE] API to avoid writing custom dispatcher - [ ] Support [CODE] in Cython implementation for sparse matrices - [x] [URL] - [ ] [Reintroduce [CODE] algorithm]([URL] (removed in [URL] - [ ] Implement PWD backend for weighted [CODE] in medoid calculation - [ ] Investigate PWD backend for [CODE] functions in [CODE] - [ ] Investigate PWD backend for [CODE] - [ ] Benchmark KD vs Ball Tree efficiency - [ ] Add consistent threading semantics to enable [CODE], e.g. in [CODE] - [x] Improve partition strategy in [CODE] (cf. [URL] - [ ] Add support for [CODE] values when [CODE] and [CODE] is sparse.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26801"
  },
  {
    "number":25729,
    "text":"Issue with check_transformer_general\n\n### Describe the bug [CODE] does not work well when sklearn is configured with set_output as pandas. To reproduce it, using sklearn version at least 1.2.0: [CODE_BLOCK] The cause of the error is the following: In [CODE], there is this code: [CODE_BLOCK] And in [CODE] this code is executed at the end of the method: [CODE_BLOCK] The code is valid only if X is a numpy array. I think several solutions are valid, maybe the most simple will be checking if the type of X is a dataframe and, if so, converting it to numpy. --- In case you are wondering why this is an issue, it's because for a third party library we are considering using pandas dataframe as inputs and outputs always, and hence forcing the setting [CODE] in the __init__.py. The problem we are facing with this approach is that the tests of our estimators are raising this exception. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results -------------------------------...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25729"
  },
  {
    "number":30958,
    "text":"Request: base class with HTML repr but without being an 'Estimator'\n\n### Describe the workflow you want to enable Creating third-party packages that offer objects that are meant to be passed to estimators, but which aren't estimators themselves. ### Describe your proposed solution Would be nice if there could be some class similar to [CODE] that would offer pretty printing, HTML representations, and so on; but without needing to be an estimator (e.g. without having metadata routing and similar). This could be used for example as a base class for objects that are meant to be passed as constructor arguments to actual estimators, and which are thus desirable to show with a pretty-printed form when visualizing estimators. For example, something like parameterizable probability distributions offered as objects in third-party packages that are meant to be passed to estimators from said third-party packages. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30958"
  },
  {
    "number":30238,
    "text":"Missing format string arguments\n\nThis assertion error string is not properly formatted as the 2 format arguments [CODE] and [CODE] are missing: [URL] [CODE_BLOCK] should become [CODE_BLOCK]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30238"
  },
  {
    "number":31323,
    "text":"Add train_validation_test_split for three-way dataset splits\n\n### Describe the workflow you want to enable Enable the user to divide the dataset into 3 parts (train, validation and test) instead of only two (train and test) using only one method. This would present a more elegant solution than using the method train_test_split twice. [CODE_BLOCK] ### Describe your proposed solution Add a new method called train_test_validation_split where the dataset is divided into train, validation and test set. The arguments would be the same as the train_test_split method with the additional val_size, similar to test_size and train_size but for the validation set. ### Describe alternatives you've considered, if relevant Using train_test_split twice works, but having a dedicated train_validation_test_split function would be cleaner and more concise. ### Additional context Using a validation set helps avoiding both overfitting aswell as underfitting.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31323"
  },
  {
    "number":31318,
    "text":"[CODE] raised by [CODE] with [CODE] that outputs a pandas [CODE] in scikit-learn version 1.6\n\n### Describe the bug Hello, I'm currently working with scikit-learn version 1.6, and I encountered a regression that wasn't present in version 1.4. The following minimal code computes two features \u2014 the cumulative mean of age and weight grouped by id. Each transformation function returns a pandas.Series: When I run this code with scikit-learn 1.6, I get the following error: After investigation, I found that the issue occurs because each transformer returns a Series, not a DataFrame. If I update the functions to return DataFrame objects instead, the error disappears. Interestingly, in scikit-learn 1.4, the same code works correctly even when the functions return Series. Do you have any explanation for why this changed between version 1.4 and 1.6 ? Thanks in advance for your help! ### Steps\/Code to Reproduce ```python import pandas as pd from sklearn.pipeline import FunctionTransformer, FeatureUnion import numpy as np def compute_cumulative_mean_age(df: pd.DataFrame) -> pd.Series: return ( df[\"age\"] .astype(float) .groupby(df[\"id\"]) .expanding() .mean() .droplevel(level=\"id\") .reindex(df.index) .rename(\"cumulative_mean_age\") ) def compute_cumulative_mean_weight(df: pd.DataFrame) -> pd.Series: return ( df[\"poids\"] .astype(float) .groupby(df[\"id\"]) .expanding() .mean() .droplevel(level=\"id\") .reindex(df.index) .rename(\"cumulative_mean_weight\") ) def compute_features(df: pd.DataFrame) -> ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31318"
  },
  {
    "number":29122,
    "text":"Pre-Built Dataset-Specific Pipelines for scikit-learn\/ML Beginners\n\n### Describe the workflow you want to enable I would be fine allowing anyone else to contribute along with this series of pre-built pipelines or simply being the first to make one for one of the already included datasets in scikit-learn. ### Describe your proposed solution I think that a series of already built pipelines with some basic preprocessing and other steps should be made available for those that are new scikit-learn or machine learning in general. Essentially, some example pipelines will be created and tuned for various datasets that scikit-learn already offers. These should be easily downloadable through some mean, maybe as pickle files so that newcomers can instantly deploy simple models on common datasets. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Having already built pipelines would allow newcomers to easily deploy them and see the results that well-crafted learning models can have. This would hopefully minimize the amount of discouraged individuals who find the starting learning curve of developing machine learning models difficult and encourage them to continue their path.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29122"
  },
  {
    "number":29715,
    "text":"LocallyLinearEmbedding : n_neighbors <= n_samples\n\n### Describe the bug Minor bug in [CODE]'s parameter validation: [URL] The [CODE] condition contradicts the error message in the case that [CODE]. So you get a message like [CODE_BLOCK] which doesn't make sense. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results n\/a ### Actual Results ```python-traceback --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[1119], line 8 4 X = np.random.randn(3, 5) 6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0]) ----> 8 embedder.fit_transform(X) File ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/utils\/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, args, kwargs) 311 @wraps(f) 312 def wrapped(self, X, args, kwargs): --> 313 data_to_wrap = f(self, X, args, kwargs) 314 if isinstance(data_to_wrap, tuple): 315 # only wrap the first output for cross decomposition 316 return_tuple = ( 317 _wrap_data_with_container(method, data_to_wrap[0], X, self), 318 data_to_wrap[1:], 319 ) File ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, kwargs) 1466 estimator._validate_params() 1468 with config_context( 1469 skip_parameter_validation=( 1470 prefer_skip_nested_validation or global_skip_validation 1471 ) 1472...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29715"
  },
  {
    "number":27306,
    "text":"ConfusionMatrixDisplay does not correctly change text color when confusion matrix contains NaN\n\n### Describe the bug In our specific usecase we generate a Confusion Matrix using our own software to be passed on to ConfusionMatrixDisplay. Due to the nature of our needs this confusion matrix could contain one or more NaN values (as we want to make it explicit that this combination does not exist instead of replacing NaN by 0). When passing a confusion matrix to ConfusionMatrixDisplay and CODE] is set to True the [CODE] method is unable to properly colour the text of the included values. This is due to the fact that the [CODE] variable ends up being NaN: [URL] Both [CODE] and [CODE] propagate NaN as per the [Numpy docs. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results Expected result would be a confusion matrix plot with correctly coloured text in the cells, similar to this example without a NaN value: ![Unknown-1 [GCC 12.3.0] executable: \/opt\/conda\/bin\/python machine: Linux-6.1.21-v8+-aarch64-with-glibc2.35 Python dependencies: sklearn: 1.3.0 p...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27306"
  },
  {
    "number":26336,
    "text":"No attribute classes_ during multi-class scoring\n\n### Describe the bug Regression we hit in MNE-Python's [CODE] run and bisected locally: [URL] Local bisect suggests the culprit is #26037 by @glemaitre It's entirely possible we're doing something wrong at the MNE-Python end but this code has worked for years :shrug: cc @agramfort who might also know the problem quickly ### Steps\/Code to Reproduce Not really minimal, but after cloning MNE-Python you can do: [CODE_BLOCK] And it will fail. Can try to whittle it down more if it would help... ### Expected Results No error ### Actual Results ```pytb mne\/decoding\/tests\/test_base.py:323: in test_get_coef_multiclass_full scores = cross_val_multiscore(time_gen, X, y, cv=cv, verbose=True) ... mne\/decoding\/base.py:567: in _score score = scorer(estimator, X_test, y_test) \/opt\/hostedtoolcache\/Python\/3.11.3\/x64\/lib\/python3.11\/site-packages\/sklearn\/metrics\/_scorer.py:411: in _passthrough_scorer return estimator.score(args, *kwargs) mne\/decoding\/search_light.py:581: in score score = parallel( \/opt\/hostedtoolcache\/Python\/3.11.3\/x64\/lib\/python3.11\/site-packages\/joblib\/parallel.py:1085: in __call__ if self.dispatch_one_batch(iterator): \/opt\/hostedtoolcache\/Python\/3.11.3\/x64\/lib\/python3.11\/site-packages\/joblib\/parallel.py:901: in dispatch_one_batch self._dispatch(tasks) \/opt\/hostedtoolcache\/Python\/3.11.3\/x64\/lib\/python3.11\/site-packages\/joblib\/parallel.py:819: in _dispatch job = self._backend.apply_async(batch, callback=cb) \/opt\/hostedtoolcache\/P...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26336"
  },
  {
    "number":28253,
    "text":"ridge regression, objective function\n\n### Describe the workflow you want to enable add another option to define the objective function in another way. currently, the objective function is defined as ||y - Xw||^2_2 + alpha  ||w||^2_2, but when the number of observations, n, is huge. The l2 penalty plays a tiny role, sometimes the objective function needs to be ( ||y - Xw||^2_2)\/n + alpha  ||w||^2_2. ### Describe your proposed solution I think the solution is straightforward, at least for some solvers. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28253"
  },
  {
    "number":29524,
    "text":"GaussianMixture takes very long in pathological cases\n\n### Describe the workflow you want to enable In general, fitting a GaussianMixture works well and quickly (~1s). However in certain cases it takes very long, even though the data set is not very big. A simple example that takes almost a minute (5.5 minutes of CPU clock time on my computer) follows. [CODE_BLOCK] ### Describe your proposed solution I do not know enough to propose a solution. However this doesn't seem to be desirable behaviour. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29524"
  },
  {
    "number":27776,
    "text":"HDBSCAN's [CODE] parameter has no effect\n\n### Describe the bug I am trying to apply HDBSCAN to a dataset in order to find clusters with a certain maximum size (e.g. 5), but the max_cluster_size parameter is not working (i.e. the result contains clusters bigger than 5). As an example, I will generate the same dataset of the tutorial \"Demo of HDBSCAN clustering algorithm: if ax is None: _, ax = plt.subplots(figsize=(10, 4)) labels = labels if labels is not None else np.ones(X.shape[0]) probabilities = probabilities if probabilities is not None else np.ones(X.shape[0]) # Black removed and is used for noise instead. unique_labels = set(labels) colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] # The probability of a point belonging to its labeled cluster determines # the size of its marker proba_map = {idx: probabilities[idx] for idx in range(len(labels))} for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = [0, 0, 0, 1] class_index = np.where(labels == k)[0] for ci in class_index: ax.plot( X[ci, 0], X[ci, 1], \"x\" if k == -1 else \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=4 if k == -1 else 1 + 5 * proba_map[ci], ) n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) preamble = \"True\" if ground_truth else \"Estimated\" ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27776"
  },
  {
    "number":24754,
    "text":"add [CODE] attribute to [CODE]\n\n### Describe the workflow you want to enable The docs for CODE] in the [CODE] say: > feature_names_in_ : ndarray of shape ([CODE],) Names of features seen during :term:[CODE]. Only defined if the underlying estimator exposes such an attribute when fit. Several pipeline-friendly classes already have this attribute, e.g. [CODE], [CODE], [CODE]. It would be helpful if [CODE] also had this attribute. ### Describe your proposed solution My proposed solution is to collect and store the [CODE] during fitting using the [[CODE]. Howe...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24754"
  },
  {
    "number":24795,
    "text":"URLs of citeseer or citeseerx should be updated\n\n### Describe the issue linked to the documentation Files pointed by the [CODE] or [CODE] URLs are no longer reachable. I think the site has updated its routes to the files. The good news is that there are only 25 places in 17 files to be modified ![image]([URL] ### Suggest a potential alternative\/fix For example, the URL in [CODE] [CODE]_ should be replaced with [URL] or [URL] ![image]([URL] ![image]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24795"
  },
  {
    "number":25970,
    "text":"Allow NearestNeighbor radius_neighbors to accept an array for the [CODE] keyword argument\n\n### Describe the workflow you want to enable In KDTree and BallTree, there is a way to pass in a radius array that queries nearest-neighbors for each sample point for different radii. - BallTree: [URL] - KDTree: [URL] However, in [CODE] class, the [URL] function only allows a single radius. In my current application, I am first computing the distance to the say Kth nearest-neighbor, [CODE] for each sample point in some space, and then for each sample point, computing the number of neighbors within that radius in some other spaces: [CODE_BLOCK] ### Describe your proposed solution Allow [CODE] parameter in [CODE] to be an array of shape [CODE], which would have a similar functionality as the KDTree and BallTree. Presumably, this would be possible for BruteForce as well, although maybe computationally more expensive. ### Describe alternatives you've considered, if relevant Alternatively, I showed how users would loop through samples just to pass in a different radius....",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25970"
  },
  {
    "number":27342,
    "text":"ENH Add [CODE] parameter to [CODE]\n\n### Describe the workflow you want to enable Add a [CODE] parameter to [CODE] to enable the user to specify which label should be the positive class when the target is binary. ### Describe your proposed solution Add a [CODE] parameter that is passed to the [CODE] [CODE] parameter. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Ref: [URL] cc @ogrisel",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27342"
  },
  {
    "number":27127,
    "text":"DOC Add permalinks to dropdown headers\n\n### Describe the workflow you want to enable With addition of dropdowns, you can no longer click on them to get a permalink to the header (and manually adding the [CODE] to the end of the URL will not take you to the header. Related: [URL] ### Describe your proposed solution Potential solution in #26872 but I think we want a non-javascript solution. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27127"
  },
  {
    "number":29028,
    "text":"Issue with int32\/int64 dtype with NumPy 2.0\n\nThe conda-forge build caught the following error: [URL] It boils down to the function missing the [CODE] fused type. However, I'm not sure that our current CI would have caught the issue at any point since we would need a Windows with the latest [CODE] version (here this is even with [CODE] since NumPy is not released yet). We should have this fix included in 1.5 final release.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29028"
  },
  {
    "number":29195,
    "text":"Mistake on \"Choosing the right estimator\" in latest documentation\n\n### Describe the issue linked to the documentation On the latest stable version (1.5.0) of the documentation, on the [Choosing the right estimator]([URL] section of the sklearn tutorials, there is an error in the fluxogram: ![image]([URL] This arrow should start on the \"predicting a quantity\", like the following image from the fluxogram present on version 1.4.2 of the documentation ![image]([URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29195"
  },
  {
    "number":31010,
    "text":"RFC Make all conditional\/optional attributes raise a meaningful error when missing\n\nRelated: [URL] [URL] Right now accessing attributes which are added to the instances when a method is called (like [CODE] in [CODE]) before they're created, raises a simple python [CODE]. This is not only on our estimators, but also sometimes on other objects such as display objects. Since we've had issues \/ confusions before, I was wondering if we'd want to introduce meaningful error messages when somebody tries to access an attribute which is not there yet, and we can tell them why it's not there. Like, [CODE] or [CODE] In terms of UX, that to me is a very clear improvement, but I'm not sure if we want to add the complexity. We can certainly find ways to make it easier to implement via some python magic, to reduce\/minimise the boilerplate code. cc @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31010"
  },
  {
    "number":27507,
    "text":"adding uncertainty quantifier in the \"predict\" function for DecisionTreeRegressor\n\n### Describe the workflow you want to enable The \"n_node_samples\" in \"tree_\" attribute tracks the number of samples in the leafs. But there should be a easier way of using this quantity for general users. I hope the following feature can be added: when calling the \"predict\" function, there is an option that allows the user to directly output number of training samples used for each prediction. This helps quantify the uncertainty in the prediction. This is not far from what scikit-learn already has, but it would be helpful for general users. ### Describe your proposed solution This should be fairly easier for scikit-learn developers. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27507"
  },
  {
    "number":30411,
    "text":"Make [CODE] in [CODE] a callable with the [CODE] and [CODE] as the parameters\n\n### Describe the workflow you want to enable CASE 1: I use a \"pipeline\" approach with [CODE] and [CODE], and I want to use [CODE] which is a number array now. As [CODE] chooses the arbitrary set of features each time, I don't have any ability to provide the proper monotonic flags in the pipeline. CASE 2: Similar, but with [CODE] and [CODE]. The later allows to specify a map of monotonic rules in [CODE]. But if [CODE] chooses to omit the fields then [CODE] fails saying the fields in the monotonic rules are missing in the [CODE]. CASE 3: The documentation to [CODE] says: > The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built. But in a fully automated functional \"pipelined\" trainer how would I know the dataset is small beforehand? CASE 4: I know I don't want to lose time on checking too small or to big learning rates if the set of features selected by [CODE] has or has not some specific fields. Or the training set itself has or has not some features. ### Describe your proposed solution Right now the documentation says: \"param_grid: dict or list of dictionaries\" scikit-learn needs to allow to specify a callback for [CODE] as: [CODE_BLOCK] which will be called before [CODE] If it is possible for the \"CASE 1\" I would be able to do that (considering the [CODE] is a Pandas datafram...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30411"
  },
  {
    "number":30056,
    "text":"LinearSVC does not correctly handle sample_weight under class_weight strategy 'balanced'\n\n### Describe the bug LinearSVC does not pass sample weights through when computing class weights under the \"balanced\" strategy leading to sample weight invariance issues cross-linked to meta-issue #16298 ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown ### Actual Results ``` AssertionError: Not equal to tolerance rtol=1e-10, atol=1e-10 Mismatched elements: 20 \/ 20 (100%) Max absolute difference among violations: 0.00818953 Max relative difference among violations: 0.10657042 ACTUAL: array([[ 0.157045, -0.399979, -0.050654, 0.236997, -0.313416], [-0.038369, -0.169516, -0.239528, -0.164231, 0.29698 ], [ 0.069654, 0.250218, 0.268922, -0.065565, -0.195888], [-0.117921, 0.185563, 0.005148, 0.006144, 0.130577]]) DESIRED: array([[ 0.157595, -0.401087, -0.051018, 0.23653 , -0.313528], [-0.041687, -0.169006, -0.243102, -0.16373 , 0.302628], [ 0.065096, 0.245549, 0.260732, -0.061577, -0.188419], [-0...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30056"
  },
  {
    "number":27620,
    "text":"sklearn PCA rotates a single vector\n\n### Describe the bug The issue we recently discovered is that sklearn PCA rotates the input when only a single variable is fed into the model. I am aware there are infinite rotations when there is a single vector fed into the model, however, the output from PCA should intuitively make sense. So I suggest hard coding this scenario (which I guess already done in R) ### Steps\/Code to Reproduce [CODE_BLOCK] ### output: [CODE_BLOCK] as we see here the value 4.5 is corresponding to the smallest value in the set (1) and the value -4.5 is corresponding to the highest value in the input set (10). ### Expected Results: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.4 | packaged by Anaconda, Inc. | (main, Jul 5 2023, 13:47:18) [MSC v.1916 64 bit (AMD64)] executable: e:\\Users\\*\\miniconda3\\python.exe machine: Windows-10-10.0.22621-SP0 Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 68.0.0 numpy: 1.24.3 scipy: 1.11.1 Cython: ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27620"
  },
  {
    "number":28474,
    "text":"Suggesting updates on the doc of [CODE]\n\n### Describe the issue linked to the documentation Hi, We discover an inconsistency issue between documentation and code in the class [[CODE]]([URL] As mentioned in the description of parameter [CODE]. > validation_fraction: float, default=0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. _Only used if early_stopping is True._ However, I did not see any constraint in the source code or even in the parent class. Could you please check it? ### Suggest a potential alternative\/fix Maybe you can update the doc to make it clear.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28474"
  },
  {
    "number":31781,
    "text":"Documentation may be inaccurate regarding deprecation of [CODE] in LogisticRegression\n\n### Describe the issue linked to the documentation In the documentation for [CODE] under [CODE], there is a [note:]([URL] \"Deprecated since version 1.5: [CODE] was deprecated in version 1.5 and will be removed in 1.7. \" However, I think this will be removed in version 1.8, based on this PR: [URL] ### Suggest a potential alternative\/fix Change the docs to 1.8 version - if that is correct.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31781"
  },
  {
    "number":27782,
    "text":"Floating Point Precision in RandomForestClassifier is only [CODE] despite [CODE]\n\n### Describe the bug When training the classifier, the precision is only about CODE]. This is true even when the [CODE] is of data type [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ![image [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-5.15.120+-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 23.1.2 setuptools: 67.7.2 numpy: 1.23.5 scipy: 1.11.3 Cython: 3.0.5 pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 2 prefix: libopenblas filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-742d56dc.3.20.so version: 0.3.20 threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 2 prefix: libg...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27782"
  },
  {
    "number":25773,
    "text":"DOC: roc_auc_score wrong description of [CODE] parameter with [CODE] value\n\n### Describe the issue linked to the documentation Documentation of [CODE] for [CODE] parameter ([lines 442-443]([URL] of [URL] clearly says that: > For multiclass targets, [CODE] is only implemented for [CODE] This is false because function [CODE] always throws a [CODE] error [CODE] ([lines 681-684]([URL] but it works flawlessly for [CODE]. ### Suggest a potential alternative\/fix Therefore, the documentation of [CODE] should be changed to: > For multiclass targets, [CODE] is only implemented for [CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25773"
  },
  {
    "number":30147,
    "text":"average_precision_score not working as expected\n\n### Describe the bug When compute AP with average_precision_score, I get unexpected results. The y_scores (output from the models) are very low for positive samples, so my AP should be very low. Instead I get a perfect 1.0 AP score. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results An AP score close to 0. ### Actual Results AP score: 1.0 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30147"
  },
  {
    "number":29836,
    "text":"Incorrect calculation of Precision and Recall score from\n\n### Describe the bug The values calculated for the precision and recall seems to be in opposite of each other. ### Steps\/Code to Reproduce [CODE_BLOCK] [CODE_BLOCK] Accuracy: 0.7762237762237763 Precision: 0.6326530612244898 Recall: 0.6888888888888889 ### Expected Results The values for Precision and Recall scores are incorrect. The commands from the library returns the values that are opposite of each other. ### Actual Results [CODE_BLOCK] Accuracy: 0.7762237762237763 Precision: 0.6888888888888889 Recall: 0.6326530612244898 ### Versions ```shell System: python: 3.12.5 (tags\/v3.12.5:ff3bc82, Aug 6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)] executable: c:\\Users\\Ming Xian\\AppData\\Local\\Programs\\Python\\Python312\\python.exe machine: Windows-11-10.0.22631-SP0 Python dependencies: sklearn: 1.4.1.post1 pip: 24.2 setuptools: 69.2.0 numpy: 1.26.4 scipy: 1.12.0 Cython: 3.0.11 pandas: 2.2.1 matplotlib: 3.9.1.post1 joblib: 1.3.2 threadpoolctl: 3.4.0 Built with OpenMP: True threadpoolctl info: user_api: openm...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29836"
  },
  {
    "number":31738,
    "text":"Present parameters and attributes sorted alphabetically to make it easier to find them on the documentation pages.\n\n### Describe the issue linked to the documentation ## Example On documentation page [URL] the parameters are listed out of order, with \"hidden_layer_sizes\" being shown at the top, followed by \"activation\", that should be the first parameters among the three visible on this screenshot. The \"solver\" parameter is kind of better positioned than the other two, but it's actually not well positioned at all, because after it we have the \"alpha\" parameter, which should be at the top of the list since it starts with \"a\". \"batch_size\" should appear after the parameters that start with \"a\", and so on. <img width=\"992\" height=\"862\" alt=\"Image\" src=\"[URL] \/> ### Suggest a potential alternative\/fix Sort the parameters and attributes alphabetically by name before presenting them on the documentation pages.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31738"
  },
  {
    "number":26766,
    "text":"CalibratedClassifierCV fails silently with large confidence scores\n\n[URL] With default [CODE], [CODE] may fail silently, giving an AUC score of 0.5, apparently when the classifier outputs large confidence scores. This happens in particular with [CODE]. You may question the quality of [CODE]; that's a separate question. But if you change [CODE] to [CODE], then AUC is correct, showing that despite suspiciously large confidence scores, [CODE] does rank the samples correctly. Code to reproduce: [CODE_BLOCK] Expected output: [CODE_BLOCK] The logistic calibration code appears to fail silently at this line [URL] Because [CODE], the caller does not know that [CODE] has failed. If you turn on the warning, then you can see that it fails without one iteration, so that logistic calibration outputs constant probability. ```python from math import log from scipy.special import expit, xlogy from sklearn.utils import column_or_1d from scipy.optimize import fm...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26766"
  },
  {
    "number":27690,
    "text":"scikit learn project runnable on pycharm but not on vscode?\n\n### Describe the bug Hello, I recently created a python project using scikit learn on PyCharm. First, I followed the sample code on official website [CODE] and moved on to rest of the code. Then I tried to run it on vscode, I set the interpreter to its venv, same as it was in pycharm, but i got the following error: > Traceback (most recent call last): > from sklearn import linear_model > ModuleNotFoundError: No module named 'sklearn' which is weird because i have never seen this erron on pycharm. any idea what is going wrong? ### Steps\/Code to Reproduce just include [CODE] in your code, assume you have installed the packages, both vscode and pycharm should compile and you wont see any error at this stage ### Expected Results expected to run whatever the rest of your code is ### Actual Results get an error > Traceback (most recent call last): > from sklearn import linear_model > ModuleNotFoundError: No module named 'sklearn' ### Versions ```shell System: python: 3.10.2 (tags\/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] executable: .\\venv\\Scripts\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.3.2 pip: 23.3.1 setuptools: 60.2.0 numpy: 1.22.2 scipy: 1.11.3 Cython: None pandas: 1.4.1 matplotlib: 3.5.1 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: C:\\Use...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27690"
  },
  {
    "number":26306,
    "text":"[CODE] ignores the [CODE] if it's an estimator\n\n### Describe the bug When using [CODE] on a [CODE], it sets the output to its sub-transformers but it ignores the transformer defined in [CODE]. This issue causes the following [CODE] to fail when gathering the results: [URL] Thus not gathering the final result correctly. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] executable: ...\/bin\/python machine: Linux-5.15.0-71-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 23.1.2 setuptools: 65.5.1 numpy: 1.24.3 scipy: 1.10.1 Cython: None pandas: 2.0.1 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: ...\/lib\/python3.10\/si...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26306"
  },
  {
    "number":24901,
    "text":"Inconsistent results with HalvingGridSearchCV\n\n### Describe the bug [CODE] does not match top [CODE] in[CODE]. ### Steps\/Code to Reproduce Please see [URL] ### Expected Results Expect best params to be top ranked result. ### Actual Results The output of [CODE] is not within the top 4 [CODE] in[CODE]. ### Versions [CODE_BLOCK]",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24901"
  },
  {
    "number":26595,
    "text":"UX: Enhance the HTML displays\n\n### Describe the workflow you want to enable When I interact when non-advanced users a recurrent difficulty for them is finding information and understanding what is going on. ### Describe your proposed solution I think that we can guide users with better html displays. In general, what would be desirable is to give ways to the users to access all the information that an estimator knows about itself, but avoiding to add any lengthy computation during fit. Of course the difficulty of any UX which is that adding more information leads to crowding, and thus the UX needs to be kept light and focused. I propose to do changes in an iterative way, adding one feature after the other. Here are the ideas that I have in mind:  [ ] Display the result of \"get_params\" (not visible by default, either folded or in a hover)  [x] #21266  [x] Add a link to the API documentation. This link would be inferred from the version of the module, the import path and the name of the class. For instance sklearn.cluster._spectral.SpectralClustering would lead to [URL] . Note that we will have to apply heuristics such as dropping the last modules in the path if they are private. Also, we will have to be careful to cater for non scikit-learn classes inheriting from our BaseEstimator, and thus define an override mechanism and probably check that the imported module corresponds to the one for which the path was defined  [x] Display in a light way whether the estimator has been fi...",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26595"
  },
  {
    "number":29455,
    "text":"StackingRegressor doesn't take estimators created via make_pipeline\n\n### Describe the bug sklearn 1.5.0. Using the shortcut [CODE] triggers an error while using the explicit [CODE] instantiation works (see code example) ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Should fit the model instead of raising an error ### Actual Results ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) c:\\Users\\pfa2ba\\Documents\\dhpt\\dhpt_models.py in <module> 15 16 stack = StackingRegressor([(\"a\", mdl_a), (\"b\", mdl_b)], cv=2) ---> 17 stack.fit(X, Y)#, sample_weight=W) c:\\Users\\pfa2ba\\.conda\\envs\\dhpt\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py in fit(self, X, y, sample_weight) 969 _raise_for_unsupported_routing(self, \"fit\", sample_weight=sample_weight) 970 y = column_or_1d(y, warn=True) --> 971 return super().fit(X, y, sample_weight) 972 973 def transform(self, X): c:\\Users\\pfa2ba\\.conda\\envs\\dhpt\\lib\\site-packages\\sklearn\\base.py in wrapper(estimator, args, kwargs) 1471 ) 1472 ): -> 1473 return fit_method(estimator, args, kwargs) 1474 1475 return wrapper c:\\Users\\pfa2ba\\.conda\\envs\\dhpt\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py in fit(self, X, y, sample_weight) 199 # all_estimators contai...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29455"
  },
  {
    "number":24449,
    "text":"The loss squared_error is not supported for SGDRegressor\n\n### Describe the bug Hi I'm using SGDRegressor for OLS regression but encountered this error: ValueError: The loss squared_error is not supported. ### Steps\/Code to Reproduce from sklearn.linear_model import SGDRegressor clf = SGDRegressor(loss=\"squared_error\") X = het_data[:, [1,3,4,5,7,8,9]] Y = het_data[:, 6] clf.fit(X, y) # het_data is the prepared data ### Expected Results No error ### Actual Results --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-99-aec03d97b9d6> in <module> ----> 1 clf = SGDRegressor(loss=\"squared_error\") 2 ''' 3 1. average price: np.mean(temp['features']['Item_NCurrentPrice'])\/100 4 2. weighted price: temp['features']['Item_NCurrentPrice']@click_propensity[min(12, len(temp))]\/100 5 3. logged price: np.mean(np.log(temp['features']['Item_NCurrentPrice']))\/100 ~\/opt\/anaconda3\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py in inner_f(args, kwargs) 61 extra_args = len(args) - len(all_args) 62 if extra_args <= 0: ---> 63 return f(args, kwargs) 64 65 # extra_args > 0 ~\/opt\/anaconda3\/lib\/python3.8\/site-packages\/sklearn\/linear_model\/_stochastic_gradient.py in __init__(self, loss, penalty, alpha, l1_ratio, fit_intercept, max_iter, tol, shuffle, verbose, epsilon, random_state, learning_rate, eta0, power_t, early_stopping, validation_fraction, n_iter_no_change, warm_start, average) 1580 power_t=0.25, early_stop...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24449"
  },
  {
    "number":30896,
    "text":"Kmeans Elkans deteriorates with different cores settings\n\n### Describe the bug Currently I'm trying to run Kmeans Elkan on a large-scale dataset. I have tried to run it with 2 configuration: 8-thread setting and 16-thread setting. While the former one seems to work normally, the running time for the later surge surprisingly. I do not understand why this behavior happens (I have tried with different datasets but have not encountered any issue like this one). Link to datasets: [URL] CPU Info [CODE_BLOCK] Please kindly help me to check this one. Thank you so much for your consideration. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Running with command [CODE_BLOCK] R...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30896"
  },
  {
    "number":27590,
    "text":"Error in joblib forking when using RandomForestClassifier\n\n### Describe the bug When using [CODE] joblib backend, a grid search with the RandomForestClassifier causes an error from joblib. The error complains that the system doesn't support forking, but MacOS does. Running a very close example with either a different classifier or the \"loky\" (default) joblib backend runs without issue. ### Steps\/Code to Reproduce Errors: [CODE_BLOCK] Does not error using a different joblib backend: [CODE_BLOCK] Also does not error using a different classifier: ``` import joblib import numpy as np from litho...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27590"
  },
  {
    "number":25380,
    "text":"SVC and OneClassSVM fails to fit or have wrong fitted attributes with null sample weights\n\n### Describe the bug SVC().fit(X, y, w) fails when the targets y are multiclass and the sample_weights w zero out one of the classes.  Dense X produces incorrect arrays for support_, n_support_, and dual_coef_ attributes, some with wrong dimension.  Sparse X errors out when trying to construct sparse dual_coef_ from incompatible arguments. A warning is emitted (e.g., \"class label 0 specified in weight is not found\"), but it does not indicate that the arrays on the trained SVC object are incorrect. Seems to be a case that was not tested by PR #14286. ### Workaround Replace the zero weights (or negative weights) with very small values like 1e-16. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The fitted attributes should be [CODE_BLOCK] assuming the 'arbitrary' values in dual_coef_ are set to zero. ### Actual Results For dense X, the fitted attributes are actually [CODE_BLOCK] For sparse X it raises [CODE_BLOCK] with traceback ``` sklearn\/svm\/_base.py:252, in fit(self, X, y, sample_weight) --> 252 fit(X, y, sample_weight, solver_type, kernel, random_seed=seed) sklearn\/svm\/_base.py:413, in _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed) --> 413 self.dual_coef_ = sp.csr_matrix( 414 (dual_coef_data, dual_coef_indices...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25380"
  },
  {
    "number":30304,
    "text":"Factor out [CODE]\n\n### Describe the workflow you want to enable At the moment, SKL creates the [CODE] at run time [here]([URL] That makes it difficult to test properly if an an object is an instance of [CODE]. ### Describe your proposed solution Could we factor this class definition out of the [CODE] function? ### Describe alternatives you've considered, if relevant _No response_ ### Additional context [CODE] doesn't use local variables, so it doesn't benefit from the closure within [CODE].",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30304"
  },
  {
    "number":29587,
    "text":"Truly parallel execution of pairwise_kernels and pairwise_distances\n\n### Describe the workflow you want to enable Both [CODE] and [CODE] functions call [CODE] function, which is (contrary to its name) not parallel as it enforces the threading backend. Therefore, these functions are terribly slow, especially for computationally expensive user-defined metrics. I understand that the reasons for the threading backend are possibly large memory demands and data communication overhead but I suggest a different approach. Also, the documentation for these functions talks about parallel execution and processes which is currently simply not true. ### Describe your proposed solution The memory and data communication issues can be reduced by a smarter distribution of the input data to individual processes. Right now, only [CODE] is sliced in the [CODE] function which is suboptimal for parallel processing. Both [CODE] and [CODE] should be sliced to lower the demands for multiprocessing. For example for 100x100 [CODE] and [CODE] distributed to 100 processes, we have to copy 100+1 inputs to every process when slicing only [CODE] while only 10+10 when slicing both [CODE] and [CODE]. As a result, multiprocessing can be allowed. Also, joblib does automatic memmapping in some cases. Alternatively, at least the documentation for [CODE] and [CODE] should be corrected. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29587"
  },
  {
    "number":30493,
    "text":"DBSCAN AttributeError: 'NoneType' object has no attribute 'split'\n\n### Describe the bug I am trying to use DBSCAN to do clustering on a normalized np.ndarray (571,128) named all_encodings. I use VSCode on Mac M1. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results DBSCAN to cluster properly. ### Actual Results ``` --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[42], [line 8 6 display(all_encodings.shape, type(all_encodings)) 7 plt.scatter(all_encodings:,0],all_encodings[:,4]) ----> [8 dbscan_cluster_model.fit(all_encodings) File ~\/anaconda3\/lib\/python3.11\/site-packages\/sklearn\/base.py:1151, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, *kwargs) 1144 [1146]([URL] with config_context( [1147]([URL] skip_parameter_validation=( [1148]([URL] prefer_skip_nested_validation or global_skip_validation [1149]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30493"
  },
  {
    "number":26960,
    "text":"Add test weighted huber in (old) gradient boosting\n\nThe case of Huber loss function with sample weights is not tested, see [URL] i.e. [CODE_BLOCK]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26960"
  },
  {
    "number":28928,
    "text":"Allow to use prefitted SelectFromModel in ColumnTransformer\n\n[CODE_BLOCK] yields: ```python-traceback Traceback (most recent call last) File ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:349, in SelectFromModel.fit(self, X, y, fit_params) 348 try: --> 349 check_is_fitted(self.estimator) 350 except NotFittedError as exc: File ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1461, in check_is_fitted(estimator, attributes, msg, all_or_any) 1460 if not _is_fitted(estimator, attributes, all_or_any): -> 1461 raise NotFittedError(msg % {\"name\": type(estimator).__name__}) NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator. The above exception was the direct cause of the following exception: NotFittedError Traceback (most recent call last) Cell In[1], line 22 13 clf.fit(X[feature_selection_cols], y) 14 ct = ColumnTransformer( 15 [( 16 'SelectFromModel', (...) 20 remainder='passthrough', 21 ) ---> 22 ct.fit(X, y) File ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:717, in ColumnTransformer.fit(self, X, y) 699 \"\"\"Fit all transformers using X....",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28928"
  },
  {
    "number":29133,
    "text":"FEAT Allow the vector-form representation of symetric distance matrices as input\n\n### Describe the workflow you want to enable I would like to calculate the upper triangle of a distance from [CODE] instead of the redundant and memory intensive version from [CODE] which has $0.5 * (N^2 - N)$ values and use this as input to [CODE] ### Describe your proposed solution [CODE_BLOCK] In addition to the original functionality (or ideally replacing): [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Using the redundant square form but this requires a lot more memory that isn't necessary. ### Additional context It may be worthwhile creating a [CODE] object like [URL]",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29133"
  },
  {
    "number":25401,
    "text":"PartialDependence categorical not working with missings\n\n### Describe the bug Since version 1.2, the partial dependence plot can deal with categorical features - fantastic! We encountered a small but annoying issue when there are missing values in the categoricals: - Missing values in numeric features are handled, but not shown (okay) - Missing values in categoricals raise an error ### Steps\/Code to Reproduce [CODE_BLOCK] - A missing value in the numeric feature seems no problem (even if the nan level is not shown in the plot) - The code works when not adding missing values ### Expected Results Either a plot without nan level or a plot with separa...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25401"
  },
  {
    "number":31988,
    "text":"Different Results on ARM and x86 when using [CODE]\n\n### Describe the bug When using [CODE] with [CODE] with [CODE], I am seeing significant discrepancies in floating point results between ARM Macs and x86 Macs\/Linux machines. This discrepancy goes away when I downgrade to [CODE] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect the resulting [CODE] to be equal to [CODE] or ``` [0.4999999999999999, 0.52, 0.52, 0.5399999999999999, 0.44000000000000006, 0.52, 0.4600000000000001, 0.5599999999999998, 0.52, 0.52, 0.5, 0.5399999999999999, 0.54, 0.55...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31988"
  },
  {
    "number":24615,
    "text":"Inconsistent Results For Logistic Regressions across multiple computers\n\n### Describe the bug Hey all - I'm working on teaching some students some logistic regressions and noticed different computers can produce slightly different intercepts\/coefs. At first I thought it was maybe environment differences, but I have been able to reproduce the variances when accounting for the various packages\/interpreters. I can create a new environment on computer 1 (windows 10, intel 8th gen CPU), computer 2 (windows 10, intel 11th gen CPU), and a coworker's M1 MacBook - all three produce different results. I thought maybe minute differences in Numpy's Openblas or MKL could be the culprit but that yielded the same varying results. I've tried not splitting the data, different random states, different C values, different solvers... For the purposes of reproducibility, I ran this is exact code & uploaded the data to Google sheets: [CODE_BLOCK] All versions of all other packages are held constant. I received t...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24615"
  },
  {
    "number":28697,
    "text":"\u26a0\ufe0f CI failed on Linux_nogil.pylatest_pip_nogil \u26a0\ufe0f\n\nCI failed on Linux_nogil.pylatest_pip_nogil Unable to find junit file. Please see link for details.",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28697"
  },
  {
    "number":24315,
    "text":"log_loss giving nan when input is np.float32 and eps is default\n\n### Describe the bug When input has values that are numpy array of np.float32, 1-eps (with default eps=1e-15) results in 1.0, and log_loss() when calculating log(1-p) with p=1.0 results in nan. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results not nan ### Actual Results with eps=1e-15 (default): nan [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24315"
  },
  {
    "number":30782,
    "text":"_py_sort() returns ValueError on windows with numpy 1.26.4 but works correctly with numpy 2.x\n\n### Describe the bug _py_sort() returns ValueError with numpy 1.26.4 but works correctly with numpy 2.x. I have created 2 different conda envs with different numpy versions from conda-forge: [CODE_BLOCK] and [CODE_BLOCK] In each of the envs, I essentially reproduced [URL] test_sort_log2_build test that shows different behavior. This works correctly with numpy 2, but with numpy 1.26.4 it returns: [CODE_BLOCK] ### Steps\/Code to Reproduce In fact, this is just a copy of test_sort_log2_build test: [CODE_BLOCK] ### Expected Results [CODE_BLOCK] This is the normal behavior of the test in case numpy 2: [CODE_BLOCK] ### Actual Results [CODE_BLOCK] This behavior is reproduced if the test is run with numpy 1.26.4 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30782"
  },
  {
    "number":30305,
    "text":"RFC deprecation warnings only when user is affected\n\nWhile reviewing [URL] I realised we're raising deprecation warnings to users, while most of them are not affected by the change, since the change only occurs when a division by zero is happenings. So I was wondering about our deprecation warning policy. In many cases, most users might not be affected at all, and we'd be asking them to set the value of a parameter explicitly while the parameter doesn't affect their code at all, and make their code more verbose unnecessarily. So not raising the warning for them, would be nice in this case. The down side is that we might be changing some behavior, which although not affecting the user, the user might rely on it, or they might only be affected very close to the deprecation cycle ends, not giving them much time to react. But if this is the case, they still will have time to react since they get the warning. WDYT? cc @StefanieSenger @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30305"
  },
  {
    "number":28780,
    "text":"[CODE] need [CODE] even if [CODE] returns DataFrame\n\n### Describe the bug Trying to call [CODE] for [CODE] for which [CODE] is configured raises error that advises to use [CODE]. But this doesn't change anything. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] like following | | feature 1 square | feature 1 cubic | feature 2 square | feature 2 cubic | |---:|-------------------:|------------------:|-------------------:|------------------:| | 0 | 1 | 1 | 9 | 27 | | 1 | 4 | 8 | 16 | 64 | | 2 | 9 | 27 | 25 | 125 | | 3 | 16 | 84 | 36 | 216 | | 4 | 25 | 125 | 49 | 343 | ### Actual Results ``[CODE]func[CODE]get_feature_names_out[CODE]get_feature_names_out` returned: ['feature 1 square'...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28780"
  },
  {
    "number":29099,
    "text":"RFC module location in API table for the API reference page\n\nI was looking at the API documentation on the new website. I was first not convinced to not have a different table for each module but at the end, if the search bar and the left navigation bart, I think this is just a matter to get use to it. However, I thought that it might be less surprising to have the full module name appearing above the \"Object\" column instead of the current location (caption of the Description column). I find it more straightforward to see that the class\/function is belonging to a certain module and I was thinking that these two information could be next to each other. Here, it is just a print screen of the current rendering to understand my remark. <img width=\"911\" alt=\"image\" src=\"[URL] @Charlie-XIAO Do you think this is feasible or this is just complex to achieve?",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29099"
  },
  {
    "number":29395,
    "text":"Assign (aka [CODE] in comment) workflow broken for non maintainers\n\nSee [URL] Edit: so actually this works as a maintainer but not as a normal user. It is more useful as a normal user ... maybe a permission thing that was changed at one point to be read (and not write) for security reasons? [CODE_BLOCK]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29395"
  },
  {
    "number":26878,
    "text":"It would be nice if TargetEncoder could apply itself to a subset of columns which it has encoded.\n\n### Describe the workflow you want to enable Once a TargetEncoder has been fitted, when you go to transform another data set (e.g., in a production job after the models have been trained), the TargetEncoder.transform(X) will insist that X be the same columns as it was fitted on. However, it's possible that the new X has only a subset of the X-columns used during fitting. One reason (well, _my_ reason) is that a feature selection step could occur between fitting and production, and potentially many features become irrelevant and likely won't exist when TargetEncoder is asked to .transform() the production data. Internally to TargetEncoder, each feature's encoding parameters are independent of the other features' (I've only checked this for non-hierarchical columns; if a hierarchy is provided I don't know what happens). So it should be safe to have TargetEncoder apply the transform only to the subset of columns, and not fail with \"Unexpected input dimension\". ### Describe your proposed solution If a TargetEncoder has been fitted on features A,B,C, allow that encoder to .transform() any subset of A,B,C without requiring all of them. ### Describe alternatives you've considered, if relevant I've looked at ColumnTransformer briefly, but it appears that I need to know the column names ahead of time. In my particular environment, the underlying data changes shape often and the column na...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26878"
  },
  {
    "number":28632,
    "text":"[Bug] [CODE] subclasses not decoratable\n\n### Describe the bug Greetings, wondrous CODE] maintainers. This is @leycec, the maintainer of @beartype \u2013 a third-generation hybrid static-runtime type-checker with far too many hyphens in its description. Who even knows what that means at this point. The point is that I recently [fielded an issue exhibiting this issue: ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No exception is raised. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.2 (main, Mar 9 2024, 18:58:45) [GCC 13.2.1 20240113] executable: \/usr\/bin\/python3.12 machine: Linux-6.1.67-gentoo-x86_64-AMD_Athlon-tm-_II_X2_240_Processor-with-glibc2.38 Python dependencies: sklearn: 1.3.2 pip: 24.0 setuptools: 69.1.1 numpy: 1.26.4 scipy: 1.12.0 Cython: 3.0.8 pandas: 2.2.0 matplotlib: 3.8.3 joblib: 1.3.2 threadpoolctl: 3.3.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28632"
  },
  {
    "number":28695,
    "text":"\u26a0\ufe0f CI failed on Check Manifest \u26a0\ufe0f\n\nCI is still failing on Check Manifest",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28695"
  },
  {
    "number":27621,
    "text":"euclidean_distances with float64 x,y and float32 xx and yy\n\n### Describe the bug When running CODE] I think it is possible to get to [this [GCC 12.2.0] executable: \/home\/ageorgiou\/projects\/test\/venv\/bin\/python machine: Linux-6.2.12-1-MANJARO-x86_64-with-glibc2.38 Python dependencies: sklearn: 1.3.1 pip: 22.2.1 setuptools: 63.2.0 numpy: 1.26.1 scipy: 1.11.3 Cython: None pandas: None matplotlib: None joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threa...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27621"
  },
  {
    "number":26621,
    "text":"Reporting that the EM algorithm didn't converge for a single initialization\n\n### Describe the bug Should the code L273-L280 [URL] be indented under the for loop? [URL] ### Steps\/Code to Reproduce Have a look at the bug description. ### Expected Results We should get a warning for every initialization that fails. ### Actual Results Currently it does it only for the last initialization as L273-L280 is not indented. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26621"
  },
  {
    "number":27340,
    "text":"A welcome bot\n\nShall we add a welcome bot with a nice and pleasant message as: ![image]([URL] [URL] It gave me a cheesy feel-good moment, and I like these. More seriously, I think that such a both can help - Keep positive vibes - Manage expectations \/ give early hints about how to move a PR forward",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27340"
  },
  {
    "number":28191,
    "text":"ENH: use the sparse-sparse backend for computing pairwise distance\n\nFirst reported in: URL] We have a regression in [CODE] with sparse matrix from 1.1.X to 1.3.X. A code sample to reproduce: [CODE_BLOCK] #### 1.1.X [CODE_BLOCK] #### [CODE] [CODE_BLOCK] #### Small benchmark ![image results_1_1 = p...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28191"
  },
  {
    "number":25874,
    "text":"make_classification: allow shuffling of samples only\n\n### Describe the workflow you want to enable At present, [CODE] allows shuffling both the samples and the features. I would like to create a sample dataset with some categorical features, and I can do that by thresholding some of the features. I have set some of the features to be informative and some useless, so I must ensure that the features are not shuffled so I know which ones are which before doing the thresholding. But I would still like to shuffle the samples. ### Describe your proposed solution I think the simplest solution is to allow the [CODE] parameter to take four different values: [CODE], [CODE] (behaving as at present), [CODE] (shuffle the samples only), [CODE] (shuffle the features only). An alternative would be introducing a new [CODE] parameter, taking the values [CODE] (default: ignore this parameter), [CODE], [CODE] (overriding the [CODE] parameter). But I think this alternative is more confusing. ### Describe alternatives you've considered, if relevant The alternative, which is what I am currently doing, is to manually shuffle the samples myself. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25874"
  },
  {
    "number":24663,
    "text":"Decision function for kernel SVM not reproducible.\n\n### Describe the bug I want to calculate the decision function through the trained svm model, but the decision function I implement based on the mathematical expression + b $$ !pic (140, 4) (10, 4) 1.0 [-4.24105215 -4.38979215 -3.524...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24663"
  },
  {
    "number":28350,
    "text":"GridSearchCV with PCA returns [CODE] masked array\n\n### Describe the bug I noticed this while looking into [URL] The dtype of the [CODE] is [CODE], which means that the pandas object which is then created is of dtype [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results int64 ### Actual Results object ### Versions ```shell System: python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] executable: \/home\/marcogorelli\/tmp\/.venv\/bin\/python3.10 machine: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.4.0 pip: 22.0.2 setuptools: 59.6.0 numpy: 1.26.3 scipy: 1.11.4 Cython: None pandas: 2.2.0 matplotlib: 3.8.2 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: \/home\/marcogorelli\/tmp\/.venv\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: SkylakeX user_api: openmp internal_api: openmp num_threads: 16 prefix: libgomp filepath: \/home\/marcogorelli\/tmp\/.venv\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: \/home\/marcogorelli\/tmp\/.venv\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so version: 0....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28350"
  },
  {
    "number":30357,
    "text":"HTML display rendering poorly in vscode \"Dark High Contrast\" color theme\n\n### Describe the bug When I use vscode, I use the \"Dark High Contrast\" theme, as my eyes are tired. In this mode, some of the estimator names are not visible in the HTML display ### Steps\/Code to Reproduce Execute the following code in a vscode (for instance a cell) [CODE_BLOCK] ### Expected Results With the \"Dark (Visual Studio)\" theme, the result is: ![image]([URL] ### Actual Results However, with the \"Dark High Contrast\", the result is ![image]([URL] Note that the title of the enclosing meta-estimator, here \"Pipeline\", is not visible ### Versions [CODE_BLOCK] ```",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30357"
  },
  {
    "number":25421,
    "text":"There is no raise expectation\n\n### Describe the issue linked to the documentation There is no raise expectation in baysius model when the prediction's feature is not in the data trained, but just index out of range. But there's raise value error in fitting function <img width=\"303\" alt=\"image\" src=\"[URL] ### Suggest a potential alternative\/fix add raise valueerror when the inputted prediction's feature is not trained (input) in the fittings this took me three hours to find that bug",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25421"
  },
  {
    "number":28366,
    "text":"nested columntransformers missing _columns with metadata_routing enabled\n\n### Describe the bug When calling fit on a nested column transformer within a pipeline an AttributeError is raised. When fit is called, [CODE] is invoked which in turn calls _iter and zips with self._columns. But _columns is not realized yet, as _validate_column_callables has yet to be called. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results ```python Traceback (most recent call last): File \"c:\\Users\\gravesee\\projects\\dsgtools\\bug.py\", line 13, in <module> column_transformer.fit(df) File \"c:\\Users\\gravesee\\projects\\dsgtools\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 860, in fit self.fit_transform(X, y=y, params) File \"c:\\Users\\gravesee\\projects\\dsgtools\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 273, in wrapped data_to_wrap = f(self, X, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"c:\\Users\\gravesee\\projects\\dsgtools\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper return fit_method(estimator, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"c:\\Users\\gravesee\\projects\\dsgtools\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 910, in fit_transform routed_params = process_routing(self, \"fit_transform\", params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"c:\\Users\\gravesee\\pro...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28366"
  },
  {
    "number":27708,
    "text":"Iris Dataset Wrong Values.\n\n### Describe the bug There are three incorrect values in the Iris dataset, as follows: (Instances from: [URL] In Row 36, the 4th feature is recorded as 0.2 instead of 0.1. In Row 39, the 2nd feature is noted as 3.6 instead of 3.1. In Row 39, the 3rd feature is documented as 1.4 instead of 1.5. From Original Iris Dataset the rows are 35 ### Steps\/Code to Reproduce iris = datasets.load_iris() X = iris.data y = iris.target ### Expected Results Row 36 (35 from Original Dataset): 4.9, 3.1, 1.5, 0.1, Iris-setosa Row 39 (38 from Original Dataset): 4.9, 3.1, 1.5, 0.1, Iris-setosa ### Actual Results Row 36 (35 from Original Dataset): 4.9, 3.1, 1.5, 0.2, Iris-setosa Row 39 (38 from Original Dataset): 4.9, 3.6, 1.4, 0.1, Iris-setosa ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27708"
  },
  {
    "number":25696,
    "text":"CalibratedClassifierCV fails on lgbm fit_params\n\nHi, I'm trying to use CalibratedClassifierCV to calibrate the probabilities from a LGBM model. The issue is that when I try CalibratedClassifierCV with eval_set, I get an error ValueError: Found input variables with inconsistent numbers of samples: [43364, 1] which is caused by check_consistent_length function in validation.py The input to eval set is [X_valid,Y_valid] where both X_valid,Y_valid are arrays with different shape. Since this format is a requirement for LGBM eval set [URL] I am not sure how will I make the check_consistent_length pass in my scenario. Full code is given below: [CODE_BLOCK] X_train.shape = (43364, 152) X_valid.shape = (43364,) Y_train.shape = (43364, 152) Y_valid.shape = (43364,)",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25696"
  },
  {
    "number":31931,
    "text":"Allow common estimator checks to use [CODE]\n\n### Describe the workflow you want to enable I'd like to be able to use [CODE] ### Describe your proposed solution We use [CODE] internally when generating all the estimator + check combinations. I think we could pass [CODE] there to make it a failure for a test, that is marked as xfail, to pass. [URL] I think we want to make this behaviour configurable, so we need a new parameter for [CODE], something like [CODE] with the option to set it to [CODE]\/[CODE]. I'd set the default to [CODE] so that not setting it does not override the setting in [CODE] (to be checked if this actually works). If you are using [CODE] to control strict mode then not passing [CODE] to [CODE] should not change anything. ### Describe alternatives you've considered, if relevant I tried layering [CODE] on top of [CODE] but that doesn't seem to work. [CODE_BLOCK] ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31931"
  },
  {
    "number":27876,
    "text":"HDBSCAN: Remove centroids_ attribute from API documentation\n\n### Describe the issue linked to the documentation The API documentation of CODE] on the [scikit-learn website. ### Suggest a potential alternative\/fix [CODE] should be removed from the attributes entry of the [scikit-learn website]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27876"
  },
  {
    "number":26220,
    "text":"RFC Suggesting HistGradientBoosting in RandomForest and GradientBoosting pages\n\nRight now we have this in the GradientBoosting API page: > sklearn.ensemble.HistGradientBoostingClassifier. After LogisticRegression and LinearRegression, RandomForest is the third most visited page among models we have. And I think we'd agree that in most cases users can probably be better off using HGBT models instead. Right now users compare random forests with xgboost, while they could be using HGBT. So my question is, should we add a message on forest\/tree pages regarding HGBT, and have the statement a bit bolder than just saying \"it's probably faster\"? @amueller had done quite a bit of analysis on this, maybe we could link to those?",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26220"
  },
  {
    "number":29722,
    "text":"Make [CODE] and [CODE] react the same way to [CODE]\n\n### Describe the workflow you want to enable Currently [CODE] accepts [CODE] as input, in which case it returns prediction for all samples in the training set based on the nearest neighbors not including the sample itself (consistent with [CODE] behavior). However, [CODE] does not accept [CODE] as input. This is inconsistent and should arguably be harmonized: [CODE_BLOCK] ### Describe your proposed solution My proposed solution is to make [CODE] behave the same as [CODE]. As explained in [URL] the necessary fix requires changing only two lines of code. ### Additional context As explained in [URL] this would be a great feature, super useful and convenient for computing LOOCV accuracy simply via [CODE]. Using [CODE] where [CODE] is the training set used in [CODE] gives a biased result because each (training set) sample gets included into its own neighbors.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29722"
  },
  {
    "number":27957,
    "text":"Standard \"Total Variance\" Scaler\n\n### Desired feature A preprocessor that removes the mean for each feature, and then scales the total variance of the dataset, rather than the variance of each feature, to 1. ### Proposed Solution A new preprocessor that operates like StandardScaler but automatically scales total-variance instead of the variance of individual feature ### Possible Alternatives A new input parameter for StandardScaler that allows the user to set the variance of each feature, or which allows the user to identify groups of features to be considered individual \"macro\" features ### Additional context Intended use case for situations where more than one feature (column) is associated with the same data-concept (like when including multiple points in space for sea surface temperature in the Pacific and also multiple points in space for sea level pressure in the Atlantic)",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27957"
  },
  {
    "number":29546,
    "text":"CI Investigate timeout in no-OpenMP build with Meson 1.5\n\n[URL] > So the no-OpenMP build still times out ... from the [diff]([URL] Meson 1.5 could be the culprit maybe \ud83e\udd14 ... > > One thing that is weird is that OpenMP is now detected in the no-OpenMP build, so at the very least this build does not serve its purpose and needs to be adapted. Probably this as a side-effect of [mesonbuild\/meson#13350]([URL] see see [build log]([URL] > > [CODE_BLOCK] Once this is fixed\/worked-around, the temporary Meson<1.5 pin can be removed: [URL]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29546"
  },
  {
    "number":25444,
    "text":"Incorrect test test_validation.py::test_cross_validate?\n\n### Describe the bug I wanted to learn more about how cross_validation is implemented in sklearn and came across what I think is a faulty or confusing test. The test test_cross_validate in model_selection\/tests\/test_validation.py creates two datasets, one regression model and one classification model. The test loops over the dataset x model combinations and fits and scores the evaluation metrics. In the loop est is used to represent the model under consideration (Lasso or SVC). However, in that process, clone(reg).fit(...) is called, so that the Lasso regression is fit and evaluated in both iterations, also for the classification data. If I'm not mistaken, this should be clone(est).fit(...), to train and evaluate the SVC classifier. However, using clone on est actually fails the test, which I did not expect given the comment \"It's okay to evaluate regression metrics on classification too\". To me this shows that the test is not testing what it is supposed to be testing. ### Steps\/Code to Reproduce Run the test test_cross_validate() ### Expected Results The test should use the regressor on the regression data and the classier on the classification data, while passing the test. ### Actual Results The test uses the Lasso regressor on both the regression data and the classification data, while passing the test. ### Versions ```shell System: python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25444"
  },
  {
    "number":26415,
    "text":"Incorrect initialization of [CODE] from [CODE] in the [CODE] method\n\n### Describe the bug When passing CODE] to a [CODE] model, a user expects to resume training the model from the provided precision matrices, which is done by calculating the [CODE] from [CODE] in the [_initialize^{-1}=(L^{-1})^{T}L^{-1}=UU^{T}$$ Given the covariance matrix $\\Sigma$, applying the Cholesky decomposition to it gives rise to a lower-triangular matrix $L$, and then we use back-substitution to calculate $L^{-1}$ from $L$, and finally the [CODE] can be calculated from $U=(L^{-1})^{T}$, which is an upper-triangular matrix $U$. This is correct for calculating [CODE] from [CODE]. However, when resuming training, the [_initialize]([URL] method calculates [CODE] from [CODE] by directly conducting the Cholesky decomposition of [CODE], which is not correct. The error can be simply verified by the fact that the resultant [CODE] is a lower-triangular matrix which should be an upper-triangular matrix. In fact, what we need is a $UU^{T}$ decomposition. The correct ma...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26415"
  },
  {
    "number":26444,
    "text":"Improvements in documentation and tests for perceptron classifier\n\n### Describe the issue linked to the documentation The documentation and tests for the Perceptron classifier [CODE_BLOCK] require enhancements to improve clarity, completeness, and usability. Currently, the documentation lacks detailed explanations of the classifier's usage, parameter descriptions, and examples illustrating its functionality. This hinders users and contributors in understanding the Perceptron classifier's behavior and effectively utilizing it for their tasks. ### Suggest a potential alternative\/fix To address these issues, it is recommended to enhance the documentation and tests for the Perceptron classifier in scikit-learn. The documentation can be improved by providing comprehensive explanations of the classifier's purpose, underlying algorithm, parameter details, and their impact on the model's behavior. Additionally, practical code examples that showcase the classifier's usage with different options and datasets would greatly benefit users in understanding and applying it correctly. Furthermore, the existing test suite should be expanded to cover a wider range of scenarios, including edge cases, corner cases, and potential pitfalls. This will help ensure the robustness and reliability of the Perceptron classifier implementation. By strengthening the test coverage, potential issues and regressions can be identified and resolved, resulting in a more stable and trustworthy classifier.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26444"
  },
  {
    "number":25976,
    "text":"Incorrect documentation for SplineTransformer.include_bias - the opposite is true\n\n### Describe the issue linked to the documentation Documentation says: > include_bias: bool, default=True > If True (default), then the last spline element inside the data range of a feature is dropped. As B-splines sum to one over the spline basis functions for each data point, they implicitly include a bias term, i.e. a column of ones. It acts as an intercept term in a linear models. [URL] But it seems it's exactly the opposite, with [CODE] I get 3 columns, with [CODE] I get 2 columns: [CODE_BLOCK] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25976"
  },
  {
    "number":26869,
    "text":"RFC Cap uppper versions of dependencies\n\nWe've been seeing quite a few issues where people try to install scikit-learn in an environment for which there are no wheels and therefore pip trying to build the package from the source distribution, but failing. Making scikit-learn work with new versions of dependencies, including python is not trivial, and there are no guarantees for the package to work correctly even if [CODE] somehow magically manages to build from the sdist. I propose we set the higher limit of our dependencies to whatever's released at the time of the release, so that when somebody tries to do [CODE] in 2-3 years from now, it either fails or downgrades the dependencies to the right ones. The same thing for supporting versions of python. We can also stop putting sdist out there. If the users want to build the project, they probably can build it from github. cc @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26869"
  },
  {
    "number":25505,
    "text":"Bisecting Kmeans fails to bisect a certain cluster\n\n### Describe the bug Hi all, I'm using the [CODE] to perform a clustering, and it worked for a range of k values, until it failed at k=9 (I don't think the k-value is important though). The issue seems to be that it failed to split a cluster into 2, but got 2 identical centers instead. More details are given below. ### Steps\/Code to Reproduce I'm afraid I can't provide a minimal reproducible example without relying on external data. I've uploaded the training data to an empty repo: [URL] The only python script in the repo has only a few lines, but I'm assuming the [CODE] data file is at the same folder as the [CODE] script. The code to reproduce the bug is: [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results Below is the Traceback info: [CODE_BLOCK] Printing ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25505"
  },
  {
    "number":26838,
    "text":"TargetEncoder has no way to select the columns being processed\n\n### Describe the workflow you want to enable [CODE] currently appears to try and look at the entire X dataframe. That's usually not what I want. What I want to do is: just target encode one feature, or perhaps a small number of features in a dataset with many features. Additionally, I want to be able to choose whether to keep the original columns that were target encoded, or drop them. ### Describe your proposed solution [CODE] should have an argument indicating which columns (features) it should process. The default there could simply be all columns. The argument should allow the user to specify the features by column index \/ array index, or by column name. It should have another option, allowing the user to choose whether they want to keep the original (untransformed) features, or drop them. There should be a third option there, indicating an optional suffix to be added to the names of the transformed columns. If the original columns are dropped, the suffix could be the null string (no suffix). If the original columns are kept, the default suffix could be some reasonable string such as [CODE], and the user should be able to override it. ### Describe alternatives you've considered, if relevant I can probably conjure the right combination of column selection in [CODE] plus a bunch of other transformers, but it's unnecessarily complex. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26838"
  },
  {
    "number":26842,
    "text":"[CODE] fails when checked after doing [CODE] on the instance\n\n### Describe the bug CODE] fails with a method when [CODE] is called([CODE]), but doesn't fail when it is not called. I was doing the test on a custom estimator that I had written and after fail, I wanted to check the behaviour on an estimator that is part of sklearn and that's when I get this error. So, I will adopt the fix that I get from this issue. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown by [CODE] ### Actual Results ```pytb --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) [<ipython-input-12-1f43d30ea78e> 10 my_stand_scale.fit_transform(X=cal_house.drop(columns='MedHouseVal', axis=1), y=cal_house.loc:,'MedHouseVal']) 11 ---> 12 check_estimator(estimator=my_stand_scale, generate_only=False) 2 frames [\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/utils\/estimator_checks.py 631 for estimator, check in checks_generator(): 632 try: --> 633 check(estimator) 634 except SkipTest as exception: 635 # SkipTest is thrown when pandas c...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26842"
  },
  {
    "number":31512,
    "text":"Add free-threading wheel for Linux arm64 (aarch64)\n\n### Describe the workflow you want to enable I am a maintainer for the third-party package fastcan, and found scikit-learn missing a wheel for Linux arm64 (aarch64) on PyPI. I would like to have the official release wheel rather than building it from source. ### Describe your proposed solution I tested scikit-learn on my own fork, and the free-threading wheel for Linux arm64 (scikit_learn-1.8.dev0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl) can be successfully built. So I suppose that wheel is just mistakenly missed. Just add that wheel in wheel.yml should be fine. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31512"
  },
  {
    "number":26392,
    "text":"NMF fit transform without updating H should not require the user to input \"n_components\"\n\nThe [CODE] function of the [CODE] module has the option to set [CODE], where the H matrix is left constant. the private method [CODE] is called by the exposed [CODE] function. In a scenario I've encountered, the user provides the H matrix, meaning the number of components is known a-prior, and there is no reason for the algorithm to run the lines [CODE_BLOCK] and raise an error later in the [CODE] [URL]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26392"
  },
  {
    "number":27973,
    "text":"Bug in utils\/multiclass.py\/_ovr_decision_function\n\n### Describe the workflow you want to enable Dear scikit learn developpers, I think the implementation of [CODE] in utils \/multiclass.py doesn't work properly when the parameter [CODE] is probability. While as the documentation suggests, it can be a probability . [CODE_BLOCK] The problem is the following two lines of codes [CODE_BLOCK] In this context, there is a binary classifier for class [CODE] vs [CODE]. And [CODE] is the positive class. If [CODE] is \"decision_function\", then it works. Because if \"decision funtion\" is negative, it means the classifier thinks the negatve class [CODE] is more possible. And the [CODE] will increase the [CODE] of [CODE], and decrease the [CODE] of [CODE]. However, if [CODE] is \"probability\", it doesn't work. Because probability is always greater than zero. So the [CODE] of [CODE] will always decrease, even when [CODE] is more likely to happend (prob of [CODE] < 0.5). ### Describe your proposed solution \"decision_function\" is centered at 0, while \"probability\" is centerd at 0.5. These two cases should be handled seperately. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27973"
  },
  {
    "number":31498,
    "text":"Doc website incorrectly flags stable as unstable\n\n### Describe the bug Current website gives: ![Image]([URL] I tried having a look on how to fix this, but went in a rabbit hole that the version switcher is generated by \"list_versions.py\" in the circle-ci scripts and this exceeded the time that I have. IMHO, such automation is over-engineered and does not make things more reliable, as we are seeing currently ### Steps\/Code to Reproduce Go to [URL] ### Expected Results Not having the banner on top ### Actual Results The banner of the top of the website displays ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31498"
  },
  {
    "number":29252,
    "text":"[BUG] [CODE] is wrong (edge case)\n\n### Describe the bug Hi, It seems that on the edge case when there are equal scores, [CODE] or not. I tried illustrating this example below (keep in mind that this behaviour is speculated by myself, I didn't go through any of [CODE] code). ![roc_auc_bug]([URL] Any feedback is welcome. Cheers! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] or [CODE] ### Actual Results [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29252"
  },
  {
    "number":25486,
    "text":"circle ci's push_doc.sh fails at git push step\n\nI am investigating with the recent failures when deploying the build doc from circle ci. - [URL] I did an interactive run on circle ci and reproduced the various steps of the [CODE] script until the failing git push which I ran in verbose mode: ``` [...] circleci@a8e707caff89:~\/scikit-learn.github.io$ export GIT_SSH_COMMAND=\"ssh -v\" circleci@a8e707caff89:~\/scikit-learn.github.io$ git push OpenSSH_8.2p1 Ubuntu-4ubuntu0.2, OpenSSL 1.1.1f 31 Mar 2020 debug1: Reading configuration data \/etc\/ssh\/ssh_config debug1: \/etc\/ssh\/ssh_config line 19: include \/etc\/ssh\/ssh_config.d\/.conf matched no files debug1: \/etc\/ssh\/ssh_config line 21: Applying options for  debug1: Connecting to github.com [140.82.114.4] port 22. debug1: Connection established. debug1: identity file \/home\/circleci\/.ssh\/id_rsa type -1 debug1: identity file \/home\/circleci\/.ssh\/id_rsa-cert type -1 debug1: identity file \/home\/circleci\/.ssh\/id_dsa type -1 debug1: identity file \/home\/circleci\/.ssh\/id_dsa-cert type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ecdsa type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ecdsa-cert type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ecdsa_sk type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ecdsa_sk-cert type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ed25519 type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ed25519-cert type -1 debug1: identity file \/home\/circleci\/.ssh\/id_ed25519_sk type -1 debug1: identity file \/hom...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25486"
  },
  {
    "number":30078,
    "text":"svcmodel.fit(X_train,y_train) on GPU? we need native GPU mode for scikit-learn\n\n### Describe the workflow you want to enable svcmodel.fit(X_train,y_train) on GPU? we need native GPU mode for scikit-learn ### Describe your proposed solution svcmodel.fit(X_train,y_train) on GPU? we need native GPU mode for scikit-learn ### Describe alternatives you've considered, if relevant svcmodel.fit(X_train,y_train) on GPU? we need native GPU mode for scikit-learn ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30078"
  },
  {
    "number":26285,
    "text":"radius_neighbors incorrect behavior\n\n### Describe the bug The [CODE] function is intended to return neighbors within a given distance. However, the following example seems to behave differently. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The only point in Y that lies within distance 2 of X[i] is Y[i]. However, [CODE] returns multiple points for some elements of X. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: pyt...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26285"
  },
  {
    "number":26925,
    "text":"[DOCS] Missing values are now supported in Decision Trees\n\nv1.3 of scikit-learn introduced some missing value support as evident in the same documentation [file]([URL] later on but it still states in the beginning of the dos that missing values are [\"not supported in this module\"]([URL] I think it would be fine to just remove that sentence or reference the section explaining the compatibility with missing values.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26925"
  },
  {
    "number":30413,
    "text":"Identical branches in the conditional statement in \"svm.cpp\"\n\n### Describe the bug File svm\/src\/libsvm\/svm.cpp, lines 1895-1903 contain the same statements. Is it correct? ### Steps\/Code to Reproduce if(fabs(alpha[i]) > 0) { ++nSV; if(prob->y[i] > 0) { if(fabs(alpha[i]) >= si.upper_bound[i]) ++nBSV; } else { if(fabs(alpha[i]) >= si.upper_bound[i]) ++nBSV; } } ### Expected Results none ### Actual Results none ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30413"
  },
  {
    "number":29253,
    "text":"\u26a0\ufe0f CI failed on Linux_free_threaded.pylatest_pip_free_threaded (last failure: Jun 14, 2024) \u26a0\ufe0f\n\nCI failed on Linux_free_threaded.pylatest_pip_free_threaded - test_minibatch_sensible_reassign[34]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29253"
  },
  {
    "number":29679,
    "text":"Arguments in train_test_split not being recognised.\n\n### Describe the bug When using the train_test_split function, arguments such as \"test_size\" and \"random_state\" are not being recognized, generating an unexpected keyword argument TypeError. ### Steps\/Code to Reproduce [CODE_BLOCK] with x and y being PyTorch tensors ### Expected Results The split occurs without an error, that is, the arguments are correctly recognised. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29679"
  },
  {
    "number":32099,
    "text":"DecisionTreeRegressor with absolute error criterion: non-optimal split\n\n### Describe the bug While working on fixing the issue [URL] I noticed that in some cases, the current implementation of [CODE] doesn't not find the optimal split in some cases, when sample weights are given. It seems to only happen with a small number of points, and the chosen split is not too far from the optimal split. My PR for [URL] will fix this one too. I'm openning this issue only to document the current behavior. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Chooses a split that minimizes the AE. ### Actual Results Prints: [CODE_BLOCK] Showing the chosen split is not optimal. ### Versions ```shell System: python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ] executable: \/home\/arthur\/dev-perso\/fast-mae-split\/.venv\/bin\/python machine: Linux-6.14.0-29-generic-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.7.1 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32099"
  },
  {
    "number":27977,
    "text":"Routing metadata to the [CODE] used by a scorer\n\n### Describe the workflow you want to enable I would like to pass sample properties to the response method (eg [CODE]) called by a scorer. For example, the [CODE] package has a [CODE] estimator which needs (in addition to X and y) the [CODE] argument both for fit and predict. AFAICT I can pass arguments to the score function (the metric), but not to the response method of the estimator. [CODE_BLOCK] This also applies when using a scorer indirectly, for example in [CODE] ### Describe your proposed solution Maybe the scorers could have a method like [CODE] or [CODE] to specify which parameters should be forwarded to the response method? ### Describe alternatives you've considered, if relevant _No response_ ### Additional con...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27977"
  },
  {
    "number":24745,
    "text":"[RFC] Always convert lists of lists of numbers to numpy arrays during input validation.\n\n### Describe the workflow you want to enable Transformers and Estimators accept list of lists of numbers as valid for inputs like [CODE]. Yet, when it comes to access to some basic attributes of the datasets (like the shape and the dtype which are present for numpy array) or to reach the best performances (e.g. be able to use Cython implementation which only operates on continuous buffers of memory), list of lists of numbers structure is inconvenient. Also lists of lists really are used for simple examples (such as doctests) but are unlikely used in practice. ### Describe your proposed solution I propose changing inputs validation to always convert list of list of numbers to their associated natural numpy array. In this context: - lists of lists of Python [CODE] will be converted to 2D numpy array of [CODE] - lists of lists of Python [CODE] will be converted to 2D numpy array of [CODE] - a [CODE] will be raised if leaf element aren't numbers - a [CODE] will be raised if internals list have different length (the case of ragged array) There might be some cost and maintenance complexity in converting list of lists to numpy array. Changes mostly need be made in: - [CODE]: [URL] - [CODE]: [URL] ### Describe alternatives you've considered, if relevant Continue supporting list of lists of numbers and introduce utility functions to be able to get basic attributes of the datasets which this struct...",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24745"
  },
  {
    "number":27814,
    "text":"RandomForestRegressor having problem with integer-values targets: The type of target cannot be used to compute OOB estimates\n\n### Describe the bug When having: - RandomForestRegressor - Multiple targets - integer values only (e.g., 1.0, 2.0, 3.0, ...) in the targets - oob_score=True The check in [CODE] will raise the error: [CODE_BLOCK] because [CODE] misclassifies the target as multiclass instead of continuous when integer values are reported. This is a bug because (1) I explicitly requested for a Regressor and (2) the classes are clearly to many to be a classification problem. I could solve the problem by perturbing just a bit one value for each target, e.g.,: [CODE_BLOCK] but I would like to work out a more definitive fix in the program. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results RandomForestRegressor(oob_score=True, random_state=42) ### Actual Results ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[5], line 12 4 df = pd.DataFrame({ 5 \"feat1\": [1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10], 6 \"feat2\": [2, 6, 8, 1, 3, 5, 7, 9, 4, 10], 7 \"target1\": [4.0, 6.0, 7.0, 3.0, 5.0, 4.0, 6.0 ,7.0 ,8.0 ,9.0], 8 \"target2\": [5.0, 5.0, 6.0, 7.0, 3.0, 4.0, 10.0,6.0,6.0,7.0], 9 }) 11 rf = RandomForestRegres...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27814"
  },
  {
    "number":26583,
    "text":"\u26a0\ufe0f CI failed on Wheel builder \u26a0\ufe0f\n\nCI failed on Wheel builder",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26583"
  },
  {
    "number":28093,
    "text":"Add verbose option to sklearn.inspection.permutation_importance()\n\n### Describe the workflow you want to enable Add the verbose parameter to sklearn.inspection.permutation_importance. This will improve the user experience by allowing them to evaluate how long the process will take, which is especially useful when working models with a high number of features. ### Describe your proposed solution My solution would be to add \"verbose\" as a paramter in sklearn.inspection.permutation_importance and than add verbose inside the function like this: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context As someone who uses the library regularly, I was wondering if there's a reason why the 'verbose' parameter hasn't been implemented yet. I actually changed it locally and have been using it like this ever since!",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28093"
  },
  {
    "number":25159,
    "text":"scikit-learn 1.2.0 compiled against numpy ABI version 0x10 but this version of numpy is 0xf\n\n### Describe the bug Using numpy 1.22.0 from my OS (Fedora 37, x86_64), I get this error on [CODE]: [CODE_BLOCK] Does scikit-learn really need [CODE]? If so, the metadata needs updated. Or is something else going on? I found these older issues: #7527, #10839, but as far as I can tell they are unrelated. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results no error is thrown ### Actual Results ``` import sklearn --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) File __init__.pxd:942, in numpy.import_array() RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) Cell In [1], line 1 ----> 1 import sklearn File ~\/.local\/lib\/python3.11\/site-packages\/sklearn\/__init__.py:82 80 from . import _distributor_init # noqa: F401 81 from . import __check_build # noqa: F401 ---> 82 from .base import clone 83 from .utils._show_versions import show_versions 85 __all__ = [ 86 \"calibration\", 87 \"cluster\", (...) 128 \"show_versions\", 129 ] File ~\/.local\/lib\/python3.11\/site-packages\/sklearn\/base.py:17 15 from . import __version__ 16 from ._config import get_config ---> 17 from .utils import _IS_32BIT 18 from .utils._set_output import _SetOutputMixin 19 from .utils._tags import ( 20 _...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25159"
  },
  {
    "number":25980,
    "text":"Improving documentation discoverability for Displays\n\nLooking at the documentation (e.g [URL] I just realised that the examples reported in the displays are the ones invoking [CODE]. However, we request to use [CODE] and [CODE]. So we don't show enough examples or even not the ones that we want people to use. @lesteve You know better than me sphinx-gallery. Do you know how we could solve this issue? I was thinking that for those specific classes, we could make a new template derived from [CODE] where we alter: [CODE_BLOCK] to show the examples of the [CODE] and [CODE] methods instead?",
    "labels":[
      "Enhancement",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25980"
  },
  {
    "number":26326,
    "text":"Add a facility that allows random forest classifiers to be combined after training\n\n### Describe the workflow you want to enable In a federated environment, I have federation elements that build private random forest classifiers, which I would like to combine after the fact into a single random forest. ### Describe your proposed solution See the \"alternatives\" section. ### Describe alternatives you've considered, if relevant Stacking might suffice as a work-around, although I'd like to avoid that. As a throw-away experiment, simply concatenating all the constituent decision tree estimators into a common estimators_ array (and adjusting the count) seems to work superficially, but clearly isn't good practice. In addition, this approach can fail, if, say, we try to combine random forest instance #1 which has classes_ of [dog, cat] and forest #2 which has classes_ of [cow, dog, cat]. To address that concern, I looked at forcing the union of all possible classes (over all the forests) into the resultant combined forest, and the underlying trees. This appears to work at some level, but doesn't handle misshapen oob_decision_function_ which is shaped according n_classes_. Another approach to dealing with classes_ heterogeneity is to make sure each federation forest is exposed to the full gamut of potential classes during training. (Even then, one worries about the order of the elements found in classes_ : [dog,cat] vs [cat,dog]). It appears that classes_ is constructed before any boo...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26326"
  },
  {
    "number":28629,
    "text":"Make RFE\/RFECV preserve pandas dataframes\n\n### Describe the workflow you want to enable Hi! I am currently using xgboost with some categorical features. To get that to work the categorical features have to be marked as such in the pandas dataframe: [CODE_BLOCK] The RFE(CV) implementation converts any input to a numpy array that can only contain numeric values which blocks me from using them with xgboost because the dtype information get's lost along the way. So my proposal: make RFE and RFECV preserve pandas dataframes so the estimator that is used still has access to this information. ### Describe your proposed solution I was able to get this to work with the following quick-and-dirty changes: [CODE_BLOCK] The second step is a hack of course and would have to be replaced depending on the input of X. With these changes and a [CODE] i got the optimization to work. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context [URL] - similar problem with catboost.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28629"
  },
  {
    "number":31717,
    "text":"SimpleImputer fails in \"most_frequent\" if incomparable types only if ties\n\n### Describe the bug ### Observed behavior When using the \"most_frequent\" strategy from SimpleImputer and there is a tie, the code takes the minimum values among all ties. This crashes if the values are not comparable such as [CODE] and [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect the Imputer to have a consistant behavior not depending on whether or not a tie is presente. Namely:  Run whether or not values are comparable  Crashes if values are not comparable, wheter there is a tie or not. Note that the code claims to process data like [CODE] but [CODE] only processes numeric values since scipy 1.9.0, it therefore crashed on this example and redirect the user toward [CODE]: ``[CODE]a[CODE]np.unique`....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31717"
  },
  {
    "number":29440,
    "text":"Suggesting updates on the doc of [CODE]\n\n### Describe the issue linked to the documentation Hi, We are an academic team of software engineering researchers from a university working on automated program analysis techniques to improve API documentation quality, ultimately contributing to improving data science software development practices. we would like to keep anonymity for the purpose of double-blind paper reviewing. We discover an inconsistency issue between documentation and code in the class [CODE]. Otherwise, we just use [CODE]. However, the most relevant source code snippet looks like this: [CODE_BLOCK] Apparently, the condition (_n_components <= n_classes_) shown in the document are different from the condition (_n_components <= min(n_features, n_classes - 1)_) actually executed in the code. ### Suggest a potential alternative\/fix Maybe you can update the documentation to avoid unnecessary misunderstanding.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29440"
  },
  {
    "number":26460,
    "text":"Pandas set_output Development and Customer Transformers Best Practice\n\n### Describe the issue linked to the documentation The set_output feature is amazing. However, many of my pipelines have sklearn style custom transformers. It would be really useful to have some best practices on how to write custom transformer objects so that they maintain the functionality of the set_output API. The closest I have found is this: [URL] but while an excellent answer on stackoverflow, it would be great to see some offical best practices on it. I think it would be a really useful for more advanced pipelines. Many thanks. Awesome work \ud83d\ude80 ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26460"
  },
  {
    "number":27931,
    "text":"ENH support for missing values in ExtraTrees\n\n### Describe the workflow you want to enable Inspired by [URL] I think that support for missing values for ExtraTrees regressor and classifier should\/could also be provided. ### Describe your proposed solution I think a foundational work is already provided by @thomasjpfan in [URL] and besides tests and documentation to enable nan handling it is enough to modify [CODE]: For [CODE] add method: [CODE_BLOCK] For [CODE] add method: [CODE_BLOCK] I've run the code locally, and it appears to be functioning as expected. However, I must emphasize that my testing was not exhaustive, and I might have overlooked some obvious aspects. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27931"
  },
  {
    "number":31359,
    "text":"Documentation improvement for macOS Homebrew libomp installation\n\n### Describe the issue linked to the documentation The current documentation in [CODE] under the \"macOS compilers from Homebrew\" section provides environment variable examples using the path [CODE]. While this is correct for Intel-based Macs, Homebrew on Apple Silicon (arm64) Macs installs packages, including [CODE], to [CODE]. This can lead to confusion and build issues for users on Apple Silicon hardware who follow the documentation to install from source. The documentation will improve from mentioning that [CODE] is often installed as \"keg-only\" by Homebrew, which is why explicitly setting these paths is necessary. Homebrew's own output ([CODE]) often provides guidance on the necessary [CODE] and [CODE]. ### Suggest a potential alternative\/fix The documentation could be updated to: 1. Mention the different Homebrew base paths for Intel ([CODE]) and Apple Silicon ([CODE]). 2. Update the example environment variable settings to reflect the [CODE] path as a common case for Apple Silicon, or provide instructions for users to identify and use the correct path for their system. 3. Optionally, We could briefly explain the \"keg-only\" nature of [CODE] from Homebrew and how it relates to needing these environment variables.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31359"
  },
  {
    "number":29627,
    "text":"Performance Degradation in FeatureUnion with String Columns when concatenate the outputs of the transformers\n\n### Describe the bug I am experiencing significant performance degradation when using FeatureUnion in a Pipeline with DataFrames that include string columns set to be concatenated in the passthrough, the execution time is notably slower. ### Steps\/Code to Reproduce ``` import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.datasets import make_classification from sklearn.pipeline import FeatureUnion, Pipeline from sklearn.preprocessing import StandardScaler from sklearn.compose import ColumnTransformer from joblib import parallel_backend class CustomStandardScaler(BaseEstimator, TransformerMixin): def __init__(self, columns=None): self.columns = columns self.scaler = None def fit(self, X, y=None): self.scaler = StandardScaler() self.scaler.fit(X[self.columns]) return self def transform(self, X): X_scaled = X[self.columns].copy() X_scaled = self.scaler.transform(X_scaled) return X_scaled def fit_transform(self, X, y=None): self.fit(X[self.columns], y) X_scaled = self.scaler.transform(X[self.columns]) return X_scaled def set_output(self, *, transform=None): pass def benchmark_feature_union(n_samples, n_steps, n_features, n_string_features=0): X, y = make_classification(n_samples=n_samples, n_features=n_features, random_state=42) X = pd.DataFrame(X) X.columns = [f'feature_{i}' for i in range(n_features)] num_cols = X....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29627"
  },
  {
    "number":25725,
    "text":"Doubled prefix operators \"not\" and \"~\" should not be used python:S2761\n\n### Describe the bug Only found one instance but should be worth improving since is in tutorial sections. Calling the not or ~ prefix operator twice might be redundant: the second invocation undoes the first. Such mistakes are typically caused by accidentally double-tapping the key in question without noticing. Either this is a bug, if the operator was actually meant to be called once, or misleading if done on purpose. Calling not twice is commonly done instead of using the dedicated \"bool()\" builtin function. However, the latter one increases the code readability and should be used. ### Steps\/Code to Reproduce a = 0 b = False c = not not a # Noncompliant d = ~~b # Noncompliant ### Expected Results Same functional result, true\/false but improve readability. ### Actual Results Syntax and readability issue. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25725"
  },
  {
    "number":30212,
    "text":"Missing documentation on ConvergenceWarning?\n\n### Describe the issue linked to the documentation Hi! I was looking to know more about the convergence warning, I found this link? Shouldn't this page say so if it's the case? ![image]([URL] ### Suggest a potential alternative\/fix If not deprecated, it would be nice to put the link directly in this page to the new page.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30212"
  },
  {
    "number":30707,
    "text":"Add sample_weight support to QuantileTransformer\n\n### Describe the workflow you want to enable Would be good to get sample_weight support for QuantileTransformer for dealing with sparse or imbalanced data, a la [#15601]([URL] [CODE_BLOCK] ### Describe your proposed solution As far as I know it would just require adding the weight argument to the quantiles_ computation in np.nanpercentile. [CODE]supports sample_weight and with strategy='quantile', encode='ordinal' this behavior can be achieved but it is much, much slower. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30707"
  },
  {
    "number":31441,
    "text":"Regression error characteristic curve\n\n### Describe the workflow you want to enable Add more fine-grained diagnostic, similar to ROC or Precision-Recall curves, to regression problems. It appears that this library has a lot of excellent tools for classification, and I believe it would benefit from some additional tools for regression. ### Describe your proposed solution Compute Regression Error Characteristic (REC) 1] curve - for each error threshold the percentage of samples whose error is below that threshold. This is essentially the CDF of the regression errors. Its function is similar to that of ROC curves - allows comparing performance profiles of regressors beyond just one summary statistic, such as RMSE or MAE. I already implement a pull-request: [URL] Screenshot from the merge request: ![Image curves, proposed [2], which plot over-prediction vs under-prediction, are a different form of diagnostic curves for regression. They may also be useful, but I think we should begin from somewhere, and I belive it's better to begin from REC, both because the paper has more citations, and because it turned out to be very useful for me at work, and I believe it can be similarly useful to other scientists. ### Additional context References --- [1]: Bi, J. and Bennett, K.P., 2003. Regression error characteristic curves. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 43-50). [2]: Hern\u00e1ndez-Orallo, J., 2013. ROC curves for regression. Pattern Rec...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31441"
  },
  {
    "number":26565,
    "text":"Dropout implementation\n\n### Describe the workflow you want to enable I am coming from Tensorflow models, and there is a neural network important feature that is not implemented yet: the dropout rate. ### Describe your proposed solution When reading across internet, you can find a useful link explaining the drop out feature and how to implement it in the code: [URL] ` # Creating a custom MLPDropout classifier from sklearn.neural_network import MLPClassifier from sklearn.neural_network._stochastic_optimizers import AdamOptimizer from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS from sklearn.utils import shuffle, gen_batches, check_random_state, _safe_indexing from sklearn.utils.extmath import safe_sparse_dot import warnings from sklearn.exceptions import ConvergenceWarning class MLPDropout(MLPClassifier): def __init__( self, hidden_layer_sizes=(100,), activation=\"relu\", *, solver=\"adam\", alpha=0.0001, batch_size=\"auto\", learning_rate=\"constant\", learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=1e-4, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8, n_iter_no_change=10, max_fun=15000, dropout = None, ): ''' Additional Parameters: ---------- dropout : float in range (0, 1), default=None Dropout parameter for the model, defines the percentage of nodes to remove at each layer. ''' self.dropout = dropout...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26565"
  },
  {
    "number":29951,
    "text":"RFC Expose [CODE] with a more flexible API\n\nxref: [URL] Right now we have the [CODE] which seems private since it has the leading underscore. We're refactoring tests and making them more modular and much nicer to deal with, but still there are going to be cases where an estimator developer might want to skip a few tests, and not a whole category. So the proposal here is to rename [CODE] to [CODE] (with a deprecation cycle of one release?), and also add the ability for the developers to set the whether the tests should fail, warn, or be skipped\/xfailed. There's also the question of granularity: do we want to set the [CODE] to be set on the estimator level, or for each test? Some alternatives could be: # Option 1 [CODE_BLOCK] # Option 2 [CODE_BLOCK] # Option 3 [CODE_BLOCK] cc @scikit-learn\/core-devs since it's public \/developer API RFC",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29951"
  },
  {
    "number":30750,
    "text":"MiniBatchKMeans not handling sample weights as expected\n\n### Describe the bug Following up from PR #29907, we realised that when passing sample weights any resampling should be done with weights and replacement before passing through to other operations. MiniBatchKMeans has a similar bug where minibatch_indices are not resampled with weights but instead weights are passed on to the subsequent minibatch_step which returns resulting in sample weight equivalence not being respected (i.e., repeating and weighting a sample n times behave the same with similar outputs). ### Steps\/Code to Reproduce ```python from sklearn.cluster import MiniBatchKMeans, KMeans import matplotlib.pyplot as plt from scipy.stats import kstest,ttest_ind from sklearn.datasets import make_blobs import numpy as np rng = np.random.RandomState(0) centres = np.array([[0, 0, 0], [0, 5, 5], [3, 1, 1], [2, 4, 4], [100, 8, 800]]) X, y = make_blobs( n_samples=300, cluster_std=1, centers=centres, random_state=10, ) # Create dataset with repetitions and corresponding sample weights sample_weight = rng.randint(0, 10, size=X.shape[0]) X_resampled_by_weights = np.repeat(X, sample_weight, axis=0) y_resampled_by_weights = np.repeat(y,sample_weight) predictions_sw = [] predictions_dup = [] predictions_sw_mini = [] predictions_dup_mini = [] prediction_rank = np.argsort(y)[-1:] for seed in range(100): ## Fit estimator est_sw = KMeans(random_state=seed,n_clusters=5).fit(X,y,sample_weight=sample_weight) est_dup = KMeans(random_...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30750"
  },
  {
    "number":30334,
    "text":"ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n\n### Describe the bug I will be succinct. I am training a binary classification dataset on \"rain\" or \"not rain\". This is a binary target. Yet scikit-learn throws an error stating that it's not binary. Is this expected behavior \/ what am I missing? !samp Cell In75], line 1 ----> 1 final_dct = model_selection_kfold(model_lst, rain_ml_fin, \"RainTomorrow_Yes\") Cell In[74], line 32, in model_selection_kfold(models, df, dep_var) 29 scores_dct[str(model)][\"model\"] = model 30 scores_dct[str(model)][\"preds\"] = preds 31 scores_dct[model] = { ---> 32 'precision':metrics.precision_score(preds, y_test), 33 'recall':metrics.recall_score(preds, y_test), 34 'accuracy':metrics.accuracy_score(preds, y_test), 35 'f1':metrics.f1_score(preds, y_test), 36 'train':clf.score(x_train, y_train), 37 'test':clf.score(x_test, y_test), 38 'cv':cv_score 39 } 41 print('\\n') 42 print('The model ', model, 'had the following Classification Report') File [~\\Languages\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213 207 try: 208 with config_context( 209 skip_parameter_validation=( 210 prefer_skip_nested_validation or global_skip_validation 211 ) 212 ): --> 213 return func(args, kwa...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30334"
  },
  {
    "number":24905,
    "text":"Inconsistency between GridSearchCV 'roc_auc' scoring and roc_auc_score metric\n\n### Describe the bug I am using GridSearchCV and obtaining different ROC_AUC scores. From my understanding of the documentation and source code, passing 'roc_auc' as a scorer uses the metric.roc_auc_score function. However, passing in the metric scorer and the string result in two different scores (0.833 and 1.0). I have replicated the metric score by manually doing the gridCV steps including cross-validation and classifying. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results Both AUCs should be the same as they are both using the same scoring function. ### Actual Results ![Screen Shot 2022-11-12 at 5 49 57 PM [GCC 7.5.0] executable: \/root\/anaconda3\/envs\/py10\/bin\/python machine: Linux-5.15.35-1-pve-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.1.2 pip: 22.2.2 setuptools: 65.5.0 numpy: 1.22.3 scipy: 1.7.3 Cython: None pandas: 1.4.4 matplotlib: 3.5.3 joblib: 1.1.1 threadp...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24905"
  },
  {
    "number":26026,
    "text":"[CODE] score (or maximum of F1\/Fbeta)\n\n### Describe the workflow you want to enable The maximum of F1 across thresholds is a well-studied metric and it is both robust and valid in binary and multilabel classification problems. Basically, it can be computed with [CODE], as shown below, for binary problems. For multilabel problems, I'm not sure if there is an efficient way to do this without looping thru all labels. ### Describe your proposed solution For binary: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26026"
  },
  {
    "number":30271,
    "text":"partial_dependence errors when given only two grid points\n\n### Describe the bug In the nightly build, when given only two grid points, the partial dependence function incorrectly thinks it is dealing with a binary output and tries to drop one of them in an attempt to fetch only the positive class. The offending line is here: [URL] This entire check is unneeded because our call to [CODE] above now selects for the positive class in binary models. I intend to correct this as part of [URL] but can split that fix off as well. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown and a partial dependence output is computed with two values. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:24:20) [Clang 17.0.6 ] executable: \/Users\/XXXX\/miniconda3\/envs\/sklearn-test\/bin\/python machine: macOS-14.7....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30271"
  },
  {
    "number":28406,
    "text":"PCA supports sparse now, docs suggest otherwise.\n\n### Describe the issue linked to the documentation The latest release notes for 1.4 and 70x lower memory usage. Based on [Alexander Tarashansky]([URL] implementation in [scanpy]([URL] [#18689]([URL] by [Isaac Virshup]([URL] and [Andrey Portnoy]([URL] However, once you go to the [PCA docs]([URL] it still says this. > Notice that this class does not support sparse input. See [TruncatedSVD]([URL] for an alternative with sparse data. ### Suggest a potential alternative\/fix I guess that one sentences can just be removed now? I can whip up a PR if folks agree.",
    "labels":[
      "help wanted",
      "Enhancement",
      "Documentation"
    ],
    "label_count":3,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28406"
  },
  {
    "number":30905,
    "text":"Unclear information in Explained variance\n\n### Describe the issue linked to the documentation Hi, the text in Explained variance page is somewhat unclear, so I want to propose a clearer text. On line 1005, the detail says this: > \"The Explained Variance score is similar to the R^2 score, with the notable difference that it does not account for systematic offsets in the prediction. Most often the R^2 score should be preferred.\" ### Suggest a potential alternative\/fix I propose to change it like this: > \"The Explained Variance score is similar to the R^2 score, with the notable difference that R^2 score also accounts for systematic offsets in the prediction (i.e., the intercept of the linear function). This means that R^2 score changes with different systematic offsets, whereas Explained Variance does not. Most often the R^2 score should be preferred.\"",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30905"
  },
  {
    "number":24867,
    "text":"Logisitic Regression: Questions\n\n### Describe the workflow you want to enable Hi all, I found an exact fomrula on my own playing with maths for the logisitc regression. I do not know if it is implemented yet for logisitc regression in sklearn. Sorry if it is ### Describe your proposed solution [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24867"
  },
  {
    "number":29434,
    "text":"CI Unpin matplotlib<3.9 in doc build\n\nIn URL] we pinned [CODE] see in particular [URL] This is a DeprecationWarning in matplotlib 3.9 turned into error in the CI: [CODE_BLOCK] 3 examples fail (DeprecationWarning turned into error): they have been fixed in #29471. - [x] [CODE] - [x] [CODE] - [x] [CODE] See [build log: [CODE_BLOCK]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29434"
  },
  {
    "number":26413,
    "text":"Ability to specify depth of a decision tree\n\n### Describe the workflow you want to enable There is no parameter currently in decision tree package (both regression and classification) to specify the depth of a tree. There are parameters to limit the number of nodes (maximum nodes) and minimum number of leaves for each node but there is no parameter to specify minimum number of nodes I want in the tree ### Describe your proposed solution I am not expert in object oriented programming but you could grow the tree as deep as having two points for each leaf node. If you add a parameter to specify minimum number of leaf nodes for a tree, it gives control to user to grow tree. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26413"
  },
  {
    "number":29065,
    "text":"GridSearchCV.score: support multiple scoring metrics\n\n### Describe the workflow you want to enable [CODE] supports multiple scoring metrics using the [CODE] parameter. However, this only applies to [CODE], not to [CODE]. I would like to use these same scoring metrics to evaluate on the test set as well. ### Describe your proposed solution This change would require changing the return type of [CODE] depending on whether [CODE] is a list of metrics. Other than that, it should be easy to iterate over [CODE] and compute the test accuracy on the same set of metrics used during scoring. ### Describe alternatives you've considered, if relevant The only alternative right now is to manually loop over [CODE] and compute the metrics yourself using [CODE] instead of [CODE]. ### Additional context As an example, the last line in the following code block should return scores for all [CODE] metrics: [CODE_BLOCK]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29065"
  },
  {
    "number":29503,
    "text":"pruning trees\n\n### Describe the workflow you want to enable I would like a more general [CODE] function which would allow the user to specify criteria on pruning a DecisionTree _posthoc_, i.e. without refitting it. Criteria could be minimum leaf samples per class, min variance, etc... Ideally, this function could be applied to each tree in a random forest as well. ### Describe your proposed solution A recursive function that works its way up from the leaves and prunes the tree. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29503"
  },
  {
    "number":24988,
    "text":"NEW FEATURE: MarginalSumsRegression\n\n### Describe the workflow you want to enable Adding Marginal Sums as a regression estimator. Marginal Sums are used in actuarial science to construct a multiplicative estimator: f1  f2  ...  fn  b = y with fi being a factor for every feature and b being a base value (the mean of the target variable). ### Describe your proposed solution Currently working on a possible implementation in #24989 ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Actuarial science is working with big data for a long time and there are a lot of established methods to calculate premiums. Unfortunately, they are often implemented in proprietary languages, that are around for decades (e.g. SAS). The change towards open source is very hard. Especially, if you can't reproduce your existing models in the open source framework. The proposed new feature is one of these methods. Adding this to sklearn would allow actuarial scientists to port existing models into sklearn and compare them with more modern estimators (e.g. GradientBoostingRegressor). Additionally teaching in actuarial science often requires lightweight\/demo versions of said proprietary tools. I believe that unis should always be able to teach in open source languages.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24988"
  },
  {
    "number":27259,
    "text":"New clustering metrics\n\n### Describe the workflow you want to enable Scikit-learn defines three popular metrics for evaluating clustering performance when there are no ground-truth cluster labels: sklearn.metrics.silhouette_score: X, labels = check_X_y(X, labels) le = LabelEncoder() labels = le.fit_transform(labels) n_samples, _ = X.shape n_labels = len(le.classes_) check_number_of_labels(n_labels, n_samples) extra_disp, intra_disp = 0.0, 0.0 mean = X.mean(axis=0) for k in range(n_labels): cluster_k = X[labels == k] mean_k = cluster_k.mean(axis=0) extra_disp += len(cluster_k) * ((mean_k - mean)  2).sum() intra_disp += ((cluster_k - mean_k)  2).sum() return np.log(extra_disp \/ intra_disp) def ball_hall(X, labels): X, labels = check_X_y(X, labels) le = LabelEncoder() labels = le.fit_transform(labels) n_samples, _ = X.shape n...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27259"
  },
  {
    "number":28937,
    "text":"Allow for multiple scoring metrics in [CODE]\n\n### Workflow In its current state, [CODE] only allows for a single scoring metric. In my opinion, calculating multiple scores on each model using k <= K features would be extremely valuable. For example, if I wanted to study how the precision and recall metrics of a binary classifier evolve as I feed less and less features to a model, I would have to run [CODE] twice: one with [CODE] and another with [CODE]. This is inefficient, as it implies running RFECV twice instead of once. The [CODE] attribute of [CODE] returns one rank per metric used to evaluate each combination of hyperparameters. Replicating this behavior in [CODE] would be extremely helpful. ### Proposed solution #### Notation - K is the number of folds used for cross-validation. - P is the total number of features available. - p is the number of features tried at each step. That is, an integer such that [CODE] <= p <= P. - m is one of M performance metrics passed by the user (e.g., 'precision'). #### Solution User can pass a list of strings representing M [predefined scoring metrics]([URL] and at each step, the algorithm stores the performance metric of the k models trained with p <= P features. The [CODE] attribute of the resulting [CODE] would now include the following keys for each metric m and fold k: - [CODE] - [CODE] - [CODE] - [CODE] #### Example [CODE_BLOCK] ##### Considerations It is likely that [CODE] will differ from [CODE] for any pair of performance metric...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28937"
  },
  {
    "number":31708,
    "text":"Frisch-Newton Interior Point Solver for Quantile Regression\n\n### Describe the workflow you want to enable Hi @ scikit-learn devs! Over at pyfixest have collected some benchmarks [here]([URL] - the FN solver seems to outperform the scikit default solver by an order of a magnitude. <img width=\"1362\" height=\"534\" alt=\"Image\" src=\"[URL] \/> Would you be interested in a PR that adds the FN solver as a new estimation method to the quantile regression class? We've also implemented algorithms from [Chernozhukov et al ]([URL] can drastically speed up estimation of the entire quantile regression process. All the best, Alex ### Describe your proposed solution I open a PR and add a new solver \"fn\" to [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Related to [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31708"
  },
  {
    "number":31360,
    "text":"Describe [CODE] API, expose [CODE], or expose [CODE]\n\n### Describe the issue linked to the documentation TL;DR: Metadata routing for scoring could either use a base class or documentation of how to write [CODE]. Currently the [Metadata Estimator Dev Guide]([URL] has examples of a metadata-consuming estimator and a metadata-routing estimator. However, the metadata routing is also designed for scorers and CV splitters which may or may not be estimators. Fortunately, [CODE] exposes [CODE], which like [CODE], subclasses [CODE]. Unfortunately, ~there's no base class for scorers.~ the base class for scorers, [CODE], is not public. I don't understand how to string together the relevant methods that should be a part of [CODE], The current workaround is to simply subclass [CODE], even if I'm not making an estimator, or to subclass [CODE], even though its not part of the public API. ~Or use [CODE] to pin the kwargs when instantiating the meta-estimator, rather than in [CODE]~ My use case is for scoring a time-series model where the data generating mechanism is known to the experiment, but not the model, and I need to compare the fit model to the true data generating mechanism. I understand how to use a custom scorer in [CODE], and the [metadata API]([URL] explains how meta estimators like [CODE] can pass additional arguments to my custom scorer. ### Suggest a potential alternative\/fix  publicly exposing [CODE]  publicly exposing [CODE] * Document how to write the [CODE] methods. It loo...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31360"
  },
  {
    "number":29032,
    "text":"Improve [CODE] diagram representation\n\n### Describe the workflow you want to enable Currently, using multiple CODE] in a pipeline leads to an uninformative view: [CODE_BLOCK] ![image A sample implementation might be look like this: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29032"
  },
  {
    "number":29495,
    "text":"GroupKFold inconsistent under ties in group sizes.\n\n### Describe the bug Due to the use of argsort (a non stable sort withthout the stable parameter introduced in numpy 2.0), GroupKFold is not always reproducible when there are ties in group sizes. ### Steps\/Code to Reproduce You may need to run this on different machines, but you can reproduce this issue with even less code than GroupKFold by simply testing [CODE]. [CODE_BLOCK] Both times with numpy version: 1.26.4 machine a: [CODE] machine b: [CODE] ### Expected Results Technically a stable sort isn't required, no need to maintain the original ordering. However a consistent sort result is required for reproducibility. Stable sort is an easy way to achieve this that is already built into numpy. ### Actual Results Both times with numpy version: 1.26.4 machine a: [CODE] machine b: [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29495"
  },
  {
    "number":26310,
    "text":"SimpleImputer.strategy = 'random'\n\n### Describe the workflow you want to enable SimpleImputer.strategy = 'random' will randomly choose a non-NaN value. ### Describe your proposed solution Find a list of non-NaN indices. Randomly pick one index for each NaN. ### Describe alternatives you've considered, if relevant Pandas: [CODE_BLOCK] ### Additional context Let's say you're in the following setting: GOAL: Iteratively tighten a hyperparameter grid.  You have a broad hyperparameter grid  Some hyperparameter choices disable (i.e. make NaN) other hyperparameter choices.  You do random selection of grid points and get the loss.  You make _isnan a value, true or false, for params that can be NaN.  You impute the missing values for NaNs.  You use decision tree regression to figure out what hyperparameter choices are best. You tighten your grid iteratively using the above. However, one downside is that some of the splits will be misleading, if they involved imputed NaNs. (The split on this hyperparameter value could suggest a particular threshold OR that the governing hyperparameter that created this NaN is important.) A way to reduce this confusion is using random choice when imputing.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26310"
  },
  {
    "number":27506,
    "text":"Test failure in i686 with version 1.3.1\n\n### Describe the bug During the build of scikit-learn for Fedora Linux, I'm obtaining an error runing the tests in i686. The test that fails is: [CODE] ### Steps\/Code to Reproduce In a i686 machine [CODE_BLOCK] ### Expected Results Test passes ### Actual Results ``` sklearn\/tree\/tests\/test_export.py::test_graphviz_toy FAILED [ 93%] =================================== FAILURES =================================== ______________________________ test_graphviz_toy _______________________________ def test_graphviz_toy(): # Check correctness of export_graphviz clf = DecisionTreeClassifier( max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2 ) clf.fit(X, y) # Test export code contents1 = export_graphviz(clf, out_file=None) contents2 = ( \"digraph Tree {\\n\" 'node [shape=box, fontname=\"helvetica\"] ;\\n' 'edge [fontname=\"helvetica\"] ;\\n' '0 [label=\"x[0] <= 0.0\\\\ngini = 0.5\\\\nsamples = 6\\\\n' 'value = [3, 3]\"] ;\\n' '1 [label=\"gini = 0.0\\\\nsamples = 3\\\\nvalue = [3, 0]\"] ;\\n' \"0 -> 1 [labeldistance=2.5, labelangle=45, \" 'headlabel=\"True\"] ;\\n' '2 [label=\"gini = 0.0\\\\nsamples = 3\\\\nvalue = [0, 3]\"] ;\\n' \"0 -> 2 [labeldistance=2.5, labelangle=-45, \" 'headlabel=\"False\"] ;\\n' \"}\" ) assert contents1 == contents2 # Test plot_options contents1 = export_graphviz( clf, filled=True, impurity=False, proportion=True, special_characters=True, rounded=True, out_file=None, fontname=\"sans\", ) contents2 = ( \"digraph Tree {\\n\" 'node [shape=box, style=\"fil...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27506"
  },
  {
    "number":28367,
    "text":"unit test failures on x64 darwin and scipy 1.12\n\n### Describe the bug after upgrading from scipy 1.11.4 -> 1.12 x64 darwin build fails with multiple unit test failures. [URL] [raw log]([URL] ```console =========================== short test summary info ============================ FAILED cluster\/tests\/test_mean_shift.py::test_parallel[float64] - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED decomposition\/tests\/test_dict_learning.py::test_sparse_encode_shapes_omp - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED decomposition\/tests\/test_dict_learning.py::test_dict_learning_reconstruction_parallel - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED decomposition\/tests\/test_dict_learning.py::test_dict_learning_lassocd_readonly_data - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED decomposition\/tests\/test_dict_learning.py::test_sparse_coder_parallel_mmap - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED decomposition\/tests\/test_dict_learning.py::test_cd_work_on_joblib_memmapped_data - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED ensemble\/tests\/test_bagging.py::test_parallel_classification - joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed... FAILED ensemble\/tests\/test_bagging.py::test_parallel_regression - joblib.externals.l...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28367"
  },
  {
    "number":28260,
    "text":"ColumnTransformer output unexpected prefixed feature names from FunctionTransformer() step\n\n### Describe the bug The following code demonstrates that when [CODE] is present as a step in [CODE], the feature names output are all prefixed with the name from the last step 'C__'. For example, column 'x1' is output as 'C__x1' for 3 times. When 'FunctionTransferformer' is _not_ present as a step, the feature names are corrected prefixed by the name in each steps. For example, column 'x1' is output as 'A__x1' and 'C__x1' respectively, as expected. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ``` C__x1 C__x2 C__x3 C__x1 C__x2 C__x3 C__x1 C__x2 C__x3 0 1 10 100 1 100 10000 -1.224745 -1.224745 -1.224745 1 2 20 200 4 400 40000 0.000000 0.000000 0.000...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28260"
  },
  {
    "number":27753,
    "text":"help support\n\n### Describe the bug i'm using pycaret with scikit-learn but it only uses scikit-learn 1.2.2. could anyone send a pull-request with changes to support scikit-learn 1.3 ? [URL] ### Steps\/Code to Reproduce . ### Expected Results . ### Actual Results . ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27753"
  },
  {
    "number":26695,
    "text":"New function to export output from Decision Tree: [CODE]\n\n### Describe the workflow you want to enable When building a Decision Tree, I'd like the ability to export the internals of the tree as a Pandas Data frame. This function would operate syntactically similar to the current internal functions: - [[CODE]][export_text] - [[CODE]][export_graphviz] However, instead, the output would be as a pandas DataFrame object. [export_text]: [URL] [export_graphviz]: [URL] ### Describe your proposed solution Classification Example We can set up our model as follows: [CODE_BLOCK] And when we can print the model as follows: [CODE_BLOCK] ![image]([URL] And the text output: [CODE_BLOCK] Then, once build, we should be able to export the model as a DataFrame like this: [CODE_BLOCK] With ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26695"
  },
  {
    "number":30545,
    "text":"Allows finer costs to be taken into account in learning\n\n### Describe the workflow you want to enable I am generally trying to take into account costs in learning. The set-up is as follows: a statistical learning problem with usuall X and y, where y is imbalanced (roughly 1% of ones). I also have costs matrices C (see below). Scikit learn usually offers wights parameters where you can set up weights matching imbalance. So the weights are depending on the target. Assigning weights will transform the log loss into weighted log loss as seen below. $\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$ $\\text{Weighted Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} w_{y_i} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$ As you see the weights $w$ is constant on each classes and only depends on $y_i$. I am generally looking for specifying the weights in terms of errors costs. More specifically, I have costs matrixes C associated with: - $c_{1,1}$, cost associated with True Positives (correctly identified positives) - $c_{0,1}$, cost associated with False Positives (Type 1 error) - $c_{1,0}$, cost associated with False Negatives (Type 2 error) - $c_{0,0}$, cost associated with True Negatives (correctly identified negatives) With three sub-cases: 1) $c_{y_i,1,1}, c_{y_i,0,1}, c_{y_i,1,0}, c_{y_i,0,0}$ depends only on classes, typically I have classifications costs for each classes (8 parameters in total) 2) $c_{...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30545"
  },
  {
    "number":31110,
    "text":"\u26a0\ufe0f CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Mar 31, 2025) \u26a0\ufe0f\n\nCI failed on Linux_Runs.pylatest_conda_forge_mkl - Test Collection Failure",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31110"
  },
  {
    "number":31164,
    "text":"Fix ConvergenceWarning in [CODE] example\n\nThis issue is about addressing a [CODE] that occurs when running the [CODE]example in CI (also when building the documentation locally). The example creates three plots. The last use case on a classification of DNA sequences throws a [CODE] related to the [CODE] defined in a custom kernel when fitting. It seems that the lower bound is pushed resulting in the lack of convergence. This occurs with the setting [CODE] in the custom kernel. Even setting [CODE] results in the same warning: [CODE_BLOCK] Lowering the bound further with [CODE] results in a different warning stemming from [CODE]: [CODE_BLOCK] It would be preferable to resolve this so the example can be build without displaying warnings. While being at the example, other small improvements are welcome (for instance fixing the typo in \"use of kernel functions that operates\" (the s in operates)).",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31164"
  },
  {
    "number":29784,
    "text":"Big problem with scikit-learn on Python311 when installing (FreeBSD)\n\n### Describe the bug [long log.txt]([URL] [URL] ### Steps\/Code to Reproduce After [CODE] [URL] ### Expected Results Just install and use with other ### Actual Results problem ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29784"
  },
  {
    "number":31032,
    "text":"[CODE] should error\/warn when all sample weights 0\n\n### Describe the bug Noticed while working on #29431 ### Steps\/Code to Reproduce See the following test: [URL] ### Expected Results Error or warning should probably be given. You're effectively asking for a quantile of a empty array. ### Actual Results When all sample weights are 0, what happens is that [CODE] (as in the index of desired observation in array is the) is [CODE] (the last item). We should probably add a check and give a warning when [CODE] is all zero cc @ogrisel @glemaitre ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31032"
  },
  {
    "number":25954,
    "text":"Allow users to have custom PDP\/ICE axes\n\n### Describe the workflow you want to enable Currently, the CODE] function [here that would accept an array of values. The code would then avoid the call to [CODE] and use these points directly (with some validation). ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Passing in custom values is nice to have in many situations when you want to control the PDP\/ICE lines - for instance comparing against different datasets or reproducing the same plot exactly.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25954"
  },
  {
    "number":27964,
    "text":"Correct scale back for PLS regression coefficients\n\n### Describe the bug In [CODE], PLS regression coefficients are calculated in class [CODE] (starts at line 165). In this class, when [CODE], data are scaled (on line 265). In that case, the resulting regression coefficients need to be scaled back to the original scale, such that they represent the relationship between the original X and y. The way this scale back is done on line 360, is wrong: [URL] This is wrong because rescaling is done by only adjusting for the [CODE]. The correct formula is to scale back as [CODE]. It is easy to verify the latter for ordinary least squares regression as they will match exactly. As PLS is only rotationally invariant, the rescaled coefficients from this proposal will not match exactly, but they will be much closer to coefficients from unscaled data than the present version. Example code given below. ### Steps\/Code to Reproduce [CODE_BLOCK] Simulate Data [CODE_BLOCK] Test both options for OLS regression [CODE_BLOCK] [CODE] and [CODE] are identical Now test both options for PLS using internal scaling [CODE_BLOCK] Off! Now the proposed solution ```python PLSms = PLSRegression(n_components=2, scale...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27964"
  },
  {
    "number":27783,
    "text":"AgglomerativeClustering unexpected clustering\n\n### Describe the bug When clustering the provided data set with sklearn.clusters.AgglomerativeClustering, we receive an unexpected output. Clusters are not formed as expected. For details see below. ### Steps\/Code to Reproduce How to reproduce: <img width=\"778\" alt=\"image\" src=\"URL] 1. Open new juypter notebook 2. Using pandas read the clustering.csv in seperate cell and store in dataframe df. 3. Run following code in seperate cell [CODE_BLOCK] 4. Now change the parameter n_cluster to n_cluster=6 and run the cell above again. You should see following plot. Looking at the yellow and purple clusters the clustering seems to be buggy. ![image [Clang 15.0.0 (clang-1500.0.40.1)] executable: \/opt\/homebrew\/opt\/python@3.11\/bin\/python3.11 machine: macOS-14.0-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 68.2.2 numpy: 1.25.2 scipy: 1.11.2 Cython: None pandas: 2.0.3 matplotlib: 3.8.1 joblib: 1.3.2 threadpool...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27783"
  },
  {
    "number":29663,
    "text":"[CODE] gives HTTP Error 403 Forbidden\n\n### Describe the bug This was also recently reported on StackOverflow System: python: 3.9.4 (tags\/v3.9.4:1f2e308, Apr 6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)] executable: C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\python.exe machine: Windows-10-10.0.19041-SP0 Python dependencies: sklearn: 1.5.1 pip: 24.1 setuptools: 69.5.1 numpy: 1.26.4 scipy: 1.13.1 Cython: 0.29.32 pandas: 2.2.2 matplotlib: 3.9.0 joblib: 1.3.2 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None num_threads: 16 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll version: 0.3.23.dev threading_layer: pthreads architecture: Zen num_threads: 16 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.27--3aa239bc726cfb0bd8e5330d8d4c15c6.dll version: 0.3.27 threading_layer: pthreads architecture: Zen ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29663"
  },
  {
    "number":27557,
    "text":"sklearn.cluster.AgglomerativeClustering - input weights\n\n### Describe the workflow you want to enable In its current form, AgglomerativeClustering with ward linkage (I haven't looked into other linkages) doesn't allow the user to input a weight vector for the observations, and they are all treated by default as equal. By comparison, hclust in R does allow the user to input a weight vector. This allows for example to restart a clustering from an intermediate state, or to compute clustering based on groups of observations which don't all contain the same number of observations. Edit: sorry I missed the labels so they were put automatically, and I don't see how to edit them... ### Describe your proposed solution From what I looked into the codebase, I suspect it is as simple as being able to provide a vector for moments_1 (sklearn.cluster._agglomerative.py, line 334) instead of setting it to 1. However I'm not a dev and I'm really unsure about how to use all the test environment and things for a PR on this. There may be be API consistency issues as well with other calls, and I never dealt with that kind of thing. [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27557"
  },
  {
    "number":32178,
    "text":"Trees: impurity decrease calculation is buggy when there are missing values\n\n### Describe the bug In decision trees (both classif. and regression), the impurity decrease calculation is sometimes wrong when there are missing values in X. This can lead to unexpectedly shallow trees when using [CODE] to control depth. This was discovered by investigations started by this issue: #32175 ### Steps\/Code to Reproduce [CODE_BLOCK] The last print shows that in approx. half of the cases, the tree has only one leaf (i.e. no split). ### Expected Results Chaning 0 by nan should have no impact on the tree construction in this example. The tree should always have one split (and hence two leaves). ### Actual Results In approx. half of the cases, the tree has only one leaf (i.e. no split). ### Versions ```shell System: python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ] executable: \/home\/arthur\/dev-perso\/scikit-learn\/sklearn-env\/bin\/python machine: Linux-6.14.0-29-generic-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.8.dev0 pip: None setuptools: 80.9.0 numpy: 2.3.3 scipy: 1.16.2 Cython: 3.1.3 pandas: None matplotlib: 3.10.6 joblib: 1.5.2 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libscipy_op...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32178"
  },
  {
    "number":26494,
    "text":"Return training loss from LogisticRegression\n\n### Describe the workflow you want to enable Currently there seems to be no way to retrieve the loss (or change in loss) when training a logistic regression model ([CODE]). If one changes the verbosity then this can be seen from the terminal, but also cannot be retrieved from stdout. This makes it unfeasible to actually plot training curves from logistic regression models. ### Describe your proposed solution Add [CODE] as an attribute, same as e.g. [CODE] or [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26494"
  },
  {
    "number":28864,
    "text":"BUG: Issue building from source on MacOS Python 3.11\n\n### Describe the bug I have followed the steps mentioned at [URL] but it did not work and giving me the following error. I want to contribute to the community but I am not able to get my development environment ready. Any help\/pointer is highly appreciated. ### Steps\/Code to Reproduce [CODE] ### Expected Results pip install should succeed without error ### Actual Results ``[CODE]find[CODE]include[CODE]exclude[CODE]src-layout[CODE]py_modules[CODE]packages` with a list of names To find more information, look for \"package discovery\" on setuptools docs. error: subprocess-exited-with-error \u00d7 python setup.py egg_info did not run successfully. \u2502 exit code: 1 \u2570\u2500> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. full command: \/Users\/tuhinsharma\/.virtualenvs\/scikit-learn\/bin\/python -c ' exec(compile('\"'\"''\"'\"''\"'\"' # This is <pip...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28864"
  },
  {
    "number":24249,
    "text":"Docstring typo \ud83d\udcdc\n\n[URL] Typo makes docstring unclear and ambiguous: [CODE_BLOCK] I suggest: [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24249"
  },
  {
    "number":30106,
    "text":"Reduce redundancy in floating type checks for Array API support\n\n### Describe the workflow you want to enable While working on #29978, we noticed that the following procedure is repeated across most regression metrics in [CODE] for the Array API: [CODE_BLOCK] To reduce redundancy, it would make sense to incorporate the [CODE] logic directly into the [CODE] function. This would result in the following cleaner implementation: [CODE_BLOCK] ### Describe your proposed solution We could introduce a new function, [CODE], defined in the obvious way. This approach would enable us to utilise the existing tests in [CODE] with minimal changes. ### Describe alternatives you've considered, if relevant We could modify the original [CODE] function, but this would require carefully reviewing and updating the relevant tests in [CODE] to ensure everything remains consistent. ### Additional context This is part of the Array API project #26024. ping: @ogrisel cc: @glemaitre, @betatim, @adrinjalali.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30106"
  },
  {
    "number":28827,
    "text":"mean_squred_error giving wrong results\n\n### Describe the bug I have recently noticed a bug in the implementation of mean_squared_error in sklearn.metrics. The current implementation of the function basically calculates the MSE as follows: [CODE_BLOCK] Which is reasonable in most cases, but may return wrong results in cases that the type of [CODE] and [CODE] has a low bit count, for example [CODE] ranging from 0 to 254. The reason for that is that when doing the calculation using arrays of types like [CODE], it is very likely that overflows will occur (which are not reported in any way!) resulting in wrong results. To resolve this [CODE] and [CODE] should first be casted to a [CODE] big enough so overflows will not occur with reasonable errors, such as [CODE]. For example: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expected result is 256 as (0 - 16)2 = 256 ### Actual Results The result of mean_squared_error is 0 ### Versions ```shell System: python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-6.1.58+-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.4.2 pip: 23.1.2 setuptools: 67.7.2 numpy: 1.25.2 scipy: 1.11.4 Cython: 3.0.10 pandas: 2.0.3 matplotlib: 3.7.1 joblib: 1.4.0 threadpoolctl: 3.4.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 2 prefix: libopenblas filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-5007b6...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28827"
  },
  {
    "number":30781,
    "text":"[CODE] fails [CODE]\n\n### Describe the bug [CODE] was added to [CODE] in 0.24 but [CODE] was not removed from [CODE]. (Noticed while trying to fix an unrelated problem in [CODE]) ### Steps\/Code to Reproduce On main, remove [CODE] from [CODE] and run [CODE] - in particular the check that sample weights of one's is the same as [CODE] fails ### Expected Results No error ### Actual Results ``` name = 'median_absolute_error' @pytest.mark.parametrize( \"name\", sorted( set(ALL_METRICS).intersection(set(REGRESSION_METRICS)) - METRICS_WITHOUT_SAMPLE_WEIGHT ), ) def test_regression_sample_weight_invariance(name): n_samples = 50 random_state = check_random_state(0) # regression y_true = random_state.random_sample(size=(n_samples,)) y_pred = random_state.random_sample(size=(n_samples,)) metric = ALL_METRICS[name] > check_sample_weight_invariance(name, metric, y_true, y_pred) sklearn\/metrics\/tests\/test_common.py:1558: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ sklearn\/metrics\/tests\/test_common.py:1458: in check_sample_weight_invariance assert_allclose( _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ actual = array(0.36388614), desired = array(0.35997069), rtol = 1e-07, atol = 0.0, equal_nan = True err_msg = 'For median_absolute_error sample_weight=None is not equivalent to sample_weight=ones', verbose = True def assert_allclose(...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30781"
  },
  {
    "number":27540,
    "text":"SelectKBest shouldn't raise if k > n_samples\n\n### Describe the workflow you want to enable Let's say I want to build a logistic regression model with at most 50 features. I could do that with something like this: `[CODE][CODE][CODE][CODE][CODE]` does? ### Describe alternatives you've considered, if relevant In a pipeline like the above, there's no work-around as far as I can see. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27540"
  },
  {
    "number":31731,
    "text":"[CODE] deprecation warning for [CODE] and [CODE] arguments\n\n### Describe the bug When upgrading to scipy 1.16, fitting a LogisticRegression raises a deprecation warning: [CODE_BLOCK] The [documentation page of scipy.minimize]([URL] mentions this double deprecation. ### Steps\/Code to Reproduce [CODE] [CODE_BLOCK] ### Expected Results No deprecation warning ### Actual Results See above ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31731"
  },
  {
    "number":29508,
    "text":"Add \"ensure_positive\" to check_array for non-negative value validation\n\n### Describe the workflow you want to enable Adding an option to [CODE] to the [CODE] function. Currently, to ensure that an input array contains only positive values [CODE] is used. Most users then either use the [CODE] right after [CODE], or create a custom [CODE] function that contains both checks. Additionally, the new [estimator_checks]([URL] in the case of the [CODE] tag, require a [CODE] to be raised if negative values are found in X. This will also help simplify making a new estimator more compliant easier, just with using [CODE], as a large number of users already do. ### Describe your proposed solution Adding an option to [CODE] to the [CODE] function, that contains the [CODE] functionality. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29508"
  },
  {
    "number":28046,
    "text":"Log Loss gradient and hessian returns NaN for large negative values\n\n### Describe the bug The private [CODE] gradient and hessian returns [CODE] for large negative values of [CODE]: - [CODE] - [CODE] Only the [CODE] returns the correct gradient. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results gradient = -1 and hessian = 0 ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28046"
  },
  {
    "number":31311,
    "text":"Reference CalibrationDisplay from calibration_curve's docstring in a \"See also section\"\n\n### Describe the issue linked to the documentation Enrich documentation like proposed in #31302 for calibration_curve's ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31311"
  },
  {
    "number":32072,
    "text":"LogisticRegressionCV intercept is wrong\n\n### Describe the bug The intercept calculated by [CODE] is wrong. A bit related to #11865. ### Steps\/Code to Reproduce [CODE_BLOCK] It is also not related to the freedom to add a constant to coefficients: Probabilities are invariant under shifting all coefficients of a single feature j for all classes by the same amount c: [CODE] See [CODE_BLOCK] ### Expected Results The [CODE] should reproduce the same result as the selected on from [CODE]. ### Actual Results AssertionError: Not equal to tolerance rtol=1e-05, atol=0 Mismatched elements: 1 \/ 1 (100%) Max absolute difference among violations: 0.01538293 Max relative difference among violations: 0.04377435 ACTUAL: array(0.336031) DESIRED: array(0.351414) ### Versions ```shell System: python: 3.12.9 (main, Feb 4 2025...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32072"
  },
  {
    "number":28443,
    "text":"ENH Plots for partial dependence and ICE of categoricals\n\n### Describe the workflow you want to enable Having support of categorical features in [CODE] and [CODE] is great. However, I would like to challenge the current visualization as bar plot. - Bars take a lot of space. - They suggest that 0 has a special meaning. - They look dominant when a mix of categoricals and non-categoricals are plotted in the same plot. Visualizating them as dots and\/or lines would be neater in most cases. This would also fix the issue that \"individual\" ICE curves of categoricals are not implemented. There, we would simply use the same visualization (lines) as with non-categoricals. ### Describe your proposed solution Replace bar plots by dots and\/or lines. If individual curves are drawn, simply plot lines as for non-categoricals. Centered ICE curves of multiple models would look like this: ![image]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28443"
  },
  {
    "number":26887,
    "text":"infinity in split nodes\n\n### Describe the bug !image DT.fit(X_train, y_train) export_text(DT) ### Expected Results There shouldn't infinity. ### Actual Results in image. ### Versions ``[CODE]`...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26887"
  },
  {
    "number":25249,
    "text":"Cannot increase verbosity of SGDRegressor fit\n\n### Describe the bug Increasing verbosity of SGDRegressor triggers an error from cython. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Logs from the fitting are available for debugging. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.9.15 (main, Nov 9 2022, 18:08:21) [Clang 13.1.6 (clang-1316.0.21.2.5)] executable: \/Users\/ludwik\/.pyenv\/versions\/3.9.15\/en...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25249"
  },
  {
    "number":30072,
    "text":"Add TQDM progress bar to .fit\n\n### Describe the workflow you want to enable Add TQDM progress bar to .fit [CODE_BLOCK] ### Describe your proposed solution Add TQDM progress bar to .fit ### Describe alternatives you've considered, if relevant Add TQDM progress bar to .fit ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30072"
  },
  {
    "number":32090,
    "text":"Unpickling ColumnTransformer fitted in 1.6.1 fails in 1.7.1 with AttributeError: _RemainderColsList\n\n### Describe the bug Summary A [CODE] pickled with scikit-learn 1.6.1 cannot be unpickled with 1.7.1 (and other versions > 1.6.1). The unpickling fails before any method call with: [CODE_BLOCK] This makes it impossible to load persisted pipelines across these versions when ColumnTransformer was used. ### Steps\/Code to Reproduce ## Run it with scikit-learn==1.6.1 [CODE_BLOCK] ## Run with scikit-learn > 1.6.1 [CODE_BLOCK] ### Expected Results A ColumnTransformer fitted and persisted in 1.6.1 can be loaded in 1.7.1 and used normally (e.g., transform), or\u2014if cross-version unpickling is intentionally unsupported\u2014clear guidance in release notes and\/or a compatibility shim to avoid a hard failure on import. ### Actual Results Unpickling fails immediately with AttributeError (below), seemingly because a private helper [CODE] referenced in the pickle no longer exists \/ was moved in [CODE] in 1.7.x. `At...",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32090"
  },
  {
    "number":24281,
    "text":"Feature Request - Parallel Coordinates Plot for GridSearch result analysis\n\n### Describe the workflow you want to enable GridSearch result are hard to analyze expecially when CODE] is very large. The current documentation show usages of: - [matrix_plot\/pivot. - line plot relationship and by taking multiple line when can visualize at least 2D relationship, but this became mess for more. - box plot. Same don't integrate weel, not matplotlib stack and need even more conversion work and a web server (or cloud account :() - Handmade matplotlib implementation. Matplotlib compatible, but at the time we don't have any easy to use implementation from common package. + with [CODE] format compatibility. ### Describe your proposed solution I presently...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24281"
  },
  {
    "number":27503,
    "text":"Cannot save any model\n\n### Describe the bug Hi, Hope everything is going well. I have been having issues saving any model either using pickle or joblib getting this error: [CODE] When using Skops, I am able to save the model, but when loading it back I get this error: [CODE] I used Skop's module get_untrusted_types to see what is being saved and I see at the beginning of the model a line with the main as the following [CODE] I wonder if this is a normal behaviour or if this is an issue either on my end or on my environment, because I had been able to save models normally until yesterday when I ran into this issue. I just gave up on trying different ways on fixing it. Thanks Best ### Steps\/Code to Reproduce import joblib joblib.dump(hgbc_model, '.\/models\/hgbc_test.joblib') ### Expected Results No error ### Actual Results ``` { \"name\": \"PicklingError\", \"message\": \"Can't pickle <function <lambda> at 0x28bf58fe0>: it's not found as __main__.<lambda>\", \"stack\": \"--------------------------------------------------------------------------- PicklingError Traceback (most recent call last) \/Users\/xxxx\/kaggle_2\/streamlit\/cv\/pages\/diamonds_st\/diamonds.ipynb Cell 110 line 3 <a href='vscode-notebook-cell:\/Users\/alejandrodelgado\/kaggle_2\/streamlit\/cv\/pages\/diamonds_st\/diamonds.ipynb#Y235sZmlsZQ%3D%3D?line=0'>1<\/a> import joblib ----> <a href='vscode-notebook-cell:\/Users\/alejandrodelgado\/kaggle_2\/streamlit\/cv\/pages\/diamonds_st\/diamonds.ipynb#Y2...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27503"
  },
  {
    "number":27088,
    "text":"Wrong infrequent categories and error in OrdinalEncoder\n\n### Describe the bug When I manually set the numpy object to [CODE] in OrdinalEncoder, I got wrong [CODE]. If I run [CODE], then I got an error. See the code below. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ``` onehot [array(['c', 'a'], dtype=object)] ordinal [array(['b', 'c'], dtype=object)] (0, 0) 1.0 (1, 0) 1.0 (2, 1) 1.0 (3, 1) 1.0 (4, 2) 1.0 Traceback (most recent call last): File \"tt.py\", line 17, in <module> print(ode.fit_transform(X)) File \"\/Users\/xxf\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/utils\/_set_output.py\", line 140, in wrapped data_to_wrap = f(self, X, args, kwargs) File \"\/Users\/xxf\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 915, in fit_transform return self.fit(X, fit_params).transform(X) File \"\/Users\/xxf\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/utils\/_set_output.py\", line 140, in wrapped data_to_wrap = f(self, X, args, kwargs) File \"\/Users\/xxf\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/preprocessing\/_encoders.py\", line 1573, in transform X_int, X_mask = self._transform( File \"\/Users\/xxf\/miniconda3\/lib\/python3.8\/site-packages\/sklearn\/preprocessing\/_encoders.py\", line 236, in _transform self._map_infrequent_cate...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27088"
  },
  {
    "number":30099,
    "text":"Inconsistency between lars_path documentation and behavior in code\n\n### Describe the issue linked to the documentation While using the CODE] function from the [CODE] module, I came across a confusing behavior that seems to contradict the documentation. According to the [documentation for [CODE] Input data. Note that if X is None then the Gram matrix must be specified, i.e, cannot be None or False . However, when I passed [CODE] and provided the [CODE] matrix, I encountered the following error in the code: [CODE_BLOCK] This directly contradicts what the documentation suggests, as I expected lars_path to work with X=None as long as the Gram matrix was given, but instead, I got a ValueError. Could you help to look into this issue? Thank you for the attention! ### Suggest a potential alternative\/fix I would suggest to update the documentation to align with the current behavior of the code.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30099"
  },
  {
    "number":26063,
    "text":"Use [CODE] as default dtype for [CODE] instead of [CODE]\n\n### Describe the workflow you want to enable [CODE] should use as [CODE] the [CODE] by default instead of [CODE] as it is currently. I don't see the reasoning behind using [CODE], this if anything only causes memory explosions and potentially (?) even slows down the computation. ### Describe your proposed solution Make [CODE] as the default [CODE] parameter of [CODE] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26063"
  },
  {
    "number":26452,
    "text":"Reference of partial_fit of MLP\n\n### Describe the issue linked to the documentation In MLPClassifier model, I can not find reference about partial_fit() function. Could you add the reference into the document that where this function comes from or just a progress by yourself? ### Suggest a potential alternative\/fix You could add the reference in your official document about how the partial_fit() function works.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26452"
  },
  {
    "number":28055,
    "text":"Infinite Loop in K-means when relocating empty clusters\n\n### Describe the bug Relocating empty clusters in Kmeans is not working as expected in this edge case, where : - There is duplicate entries. - The number of clusters is equal to the number of entries. - Very particular initial positions. Kmeans is stuck in a infinite loop, and the only way to end it is the max number of iterations. I have suggestions to improve the relocation of empty clusters : - When selecting an entry to fill an empty cluster (the entry that has max_dist), we should not select an entry in a cluster that contains only 1 entry. - After each relocation of an empty clusters, Centers needs to be updated. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results We should only perform 1 iteration [CODE_BLOCK] ### Actual Results Kmeans is stuck in an infinite loop [CODE_BLOCK] ### Versions ```shell System: python: 3.9.0 machine: Windows-10 Python dependencies: sklearn: 1.3.2 pip: 20.2.3 setuptools: 49.2.1 numpy: 1....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28055"
  },
  {
    "number":26936,
    "text":"Create a way to transform the target variable within a custom sklearn transformer.\n\n### Describe the workflow you want to enable It is not uncommon that the target variable in the raw dataset is not in the ideal format to be fitted in the estimator: - In multiclass classification, we may need to apply a custom encoding. - In regression, we may want to scale the target. It is critical (good practice) to keep all data transformation within the sklearn pipeline. This will ensure that the model can accept the raw features and target as input when performing streaming predictions. If all transformations are not concentrated in the sklearn pipeline, the input data for online requests will need to pass through a preprocessing pipeline first adding a lot of unnecessary complexity. If the transformation in this preprocessing pipeline needs to be stateful (learn its parameter from the training dataset through fit) the creation of such preprocessing pipeline becomes even more complicated. ### Describe your proposed solution Enable a way for a Transformer to be able to change the target variable and return it forward as y. The default behavior for a transformation should not change nor return why: - If the transformer doesn't return y (default) then can assume that y did not change. - If it returns X, y we should replace the old y with the returned y. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26936"
  },
  {
    "number":32174,
    "text":"Fix rendering of D2 Brier score section in User Guide\n\n_This is an issue for a contributor who has worked with rst and sphinx documentation before, or who wants to spend 10 hours to learn on the task. It is not suitable for ai agents._ The newly added section about D2 Brier score (added via #28971) doesn't render correctly in the User Guide. In the 1.8 dev version documentation it renders as [CODE_BLOCK] We probably need to use .[CODE] like in the section above. Maybe @elhambb, do you want to take care of it? Also @star1327p or @EmilyXinyi, if that's not too boring for you.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32174"
  },
  {
    "number":28026,
    "text":"ValueError: buffer source array is read-only in check_estimator\n\n### Describe the bug I am trying to make a scikit-learn estimator [CODE] based on Python wrapper [CODE] for C++ library [CODE] (yes :sweat_smile:). [CODE_BLOCK] Possibly related issues: - [URL] - [URL] ### Steps\/Code to Reproduce ```python from sklearn.datasets import dump_svmlight_file import sklearn import numpy as np class FMClassifier(sklearn.base.BaseEstimator): def __init__(self): super()....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28026"
  },
  {
    "number":25532,
    "text":"[CODE] is inconsistent with [CODE] when using [CODE]\n\n### Describe the bug Although the metric CODE] is already removed from the documentation, [CODE] function still allows its usage. When used, the input arrays are converted into boolean. This brings inconsistency with the counterpart function [CODE] and [CODE] from [CODE] (note that [CODE] [has been completely removed. In scipy's CODE] and [CODE], the metric [CODE] is [considered a synonym [GCC 10.2.1 20210110] executable: \/usr\/local\/bin\/python3 mach...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25532"
  },
  {
    "number":24364,
    "text":"Deprecated is_first_col() function\n\n[URL] The is_first_col function was deprecated in Matplotlib 3.4 #40714 ([URL] Could you change the function by get_subplotspec().is_first_col() instead? Thanks! Josu",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24364"
  },
  {
    "number":30623,
    "text":"Bad color choice in Prediction Intervals for Gradient Boosting Regression\n\n### Describe the issue linked to the documentation The first plot in the example Prediction Intervals for Gradient Boosting Regression which makes it near-impossible to differentiate, at least on my device. !sphx_glr_plot_gradient_boosting_quantile_001.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30623"
  },
  {
    "number":31719,
    "text":"What are the coefficients returned by Polynomial Ridge Regression (or any regression)?\n\n### Describe the issue linked to the documentation I asked and answered a question about Regression in Stack Overflow and Java (no libraries). That was not straightforward. I eventually found the coefficients under [CODE] after someone pointed that out. Even the answers I got on Google indicated they were under the attribute [CODE] but I couldn't find them though I thought I had read the docs sufficiently. As explained at the link above, I expected coefficients for an equation: [CODE]. If I looked at the coefficients under the attribute [CODE] I got: [CODE], from which I assumed that meant that [CODE],[CODE], etc. It turned out that was only partially true, [CODE] are correct but [CODE] is not. [CODE] is actually the interecept which is another attribute [CODE]. At least, that is how I got things to work (code below for an example) Question: what is the first element in the coefficients from Polynomial Ridge Regression or have I completely misunderstood? ``` import pandas as pd import warnings # regression libs from sklearn.pipeline import make_pipeline from sklearn.linear_model import Ridge from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures # useful initializations warnings.filterwarnings('ignore') p = [0, 10, -20, .30] # Create fake data using the preceding coefficients with some noise def regr_noise(x, p): mu = np.random.uniform(0,50E6) return (p[...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31719"
  },
  {
    "number":29850,
    "text":"[CODE] accepts [CODE] in fitted estimator, but should raise or warn\n\n### Describe the bug When we pass a fitted estimator into [CODE] it will fit this estimator again on the given train-validation splits. However, users can pass [CODE] to the fitted estimator without being warned that it is not taken into account. This might also be the case for other splitters. I have not tested. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The scores with or without [CODE] passed to the estimator's fit method are the same. Passing [CODE] has no effect. Thus, the user should be warned about this fact or the estimator should raise. The message can maybe contain a hint to use the metadata routing API. ### Actual Results nothing happens ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29850"
  },
  {
    "number":29902,
    "text":"ImportError: cannot import name 'InconsistentVersionWarning' in sklearn.exceptions\n\n### Describe the bug The error message \"ImportError: cannot import name 'InconsistentVersionWarning'\u201c occurs when there is an attempt to import the sklearn ### Steps\/Code to Reproduce import sklearn ### Expected Results successful import ### Actual Results ImportError: cannot import name 'InconsistentVersionWarning' from 'sklearn.exceptions' (\/path\/to\/sklearn\/exceptions.py) ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29902"
  },
  {
    "number":29347,
    "text":"DOC Broken link: Working with Text Data Tutorial Setup\n\n### Describe the issue linked to the documentation In the setup section of the Working With Text Data, so both of these instructions are broken. ### Suggest a potential alternative\/fix Either put the required data back in the repository somewhere, or adjust the tutorial so that there is no longer any references to this setup. The only parts of the tutorial which really require the setup are the exercises, which refer to running on the dataset created from [CODE].",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29347"
  },
  {
    "number":24432,
    "text":"StackingRegressor does not allow classifiers as estimators (and vice versa)\n\n### Describe the workflow you want to enable I'm facing an ordinal regression problem. In these kind of problems, both a classifier or a regressor can be used as a naive approach. I'm trying to stack both of them, however sklearn does not allow me to do that. The error is raised from the next line: [URL] ### Describe your proposed solution I think that behaviour is a bit strict and limits some use cases like mine. A simple solution would be raising a warning instead of an error given that everything can work properly even if one of the estimators in a stack is not of the same kind of the stack itself. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24432"
  },
  {
    "number":29340,
    "text":"Support Vector Machines - Multi-class classification --> wrong default approach\n\n### Describe the issue linked to the documentation Documentation on main page for Support Vector Machines ([URL] seems to be outdated. Under 1.4.1.1 it says: _SVC and NuSVC implement the \u201cone-versus-one\u201d approach for multi-class classification._ The documentation of SVC ([URL] correctly states 'ovr' as default for 'decision_function_shape': _decision_function_shape{\u2018ovo\u2019, \u2018ovr\u2019}, default=\u2019ovr\u2019_ ### Suggest a potential alternative\/fix Revision of the documentation with updated default approach for SVC.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29340"
  },
  {
    "number":25258,
    "text":"Matthews correlation coefficient gives wrong results for single-class result\n\n### Describe the bug for a single class input, (ie perfect correlation) MCC returns 0, when it should return 1. since MCC is a special case of pearson correlation, it should fall back to pearson when appropriate ### Steps\/Code to Reproduce from sklearn.metrics import matthews_corrcoef y_true = [1, 1, 1, 1] y_pred = [1, 1, 1, 1] matthews_corrcoef(y_true, y_pred) Out [9]: 0.0 ### Expected Results output should be 1 ### Actual Results output is 0 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25258"
  },
  {
    "number":27629,
    "text":"Please provide option to set unknown_values during test time to same as encoded min_frequency in OrdinalEncoder(Infrequent categories)\n\n### Describe the workflow you want to enable It seems that OneHotEncoder has a parameter for setting[CODE] but the same is missing in OrdinalEncoder . Currently [CODE] and the value encoded by setting the parameter [CODE] seems to be different. There is always workaround to figure out the encoded value on [CODE] and pass the same to [CODE] but I think having something similar to OneHotEncoder's parameter [CODE] seems intuitive as we would want to treat unseen values as infrequent ones. Not sure if this feature already exists and I'm missing it somehow. ### Describe your proposed solution Implement parameter option similar to OneHotEncoder's parameter [CODE] where unknown (unseen values during training) get similar encoding as happened for infrequent_categories during training. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27629"
  },
  {
    "number":26301,
    "text":"Support custom callable in SimpleImputer\n\n### Describe the workflow you want to enable Is it possible to support any aggregation function as [CODE] in [CODE]? The only thing we will need to check is that function returns one non-null value (aggregating) consistently (not random). ### Describe your proposed solution Support [CODE] as valid [CODE] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context This could be an easier and more elegant solution for simple imputer strategies requests that are appearing from time to time.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26301"
  },
  {
    "number":26441,
    "text":"Human Readable Rules for Decision Tree\n\n### Describe the workflow you want to enable Decision Tree should give the human readable if then rules apart from tree plot and structure results. ### Describe your proposed solution I want to add a function to extract the rules from a decision tree in a simple \"If\/Then\" rules. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26441"
  },
  {
    "number":28829,
    "text":"scikit-learn cannot be built with OpenMP support.\n\n### Describe the bug I would like to install TrackPal however it is indicated that \"scikit-learn cannot be built with OpenMP support\". ### Steps\/Code to Reproduce ```shell (base) ASUS@dyn3175-229 ~ % pip install TrackPal Collecting TrackPal Obtaining dependency information for TrackPal from [URL] Using cached TrackPal-1.2.0-py3-none-any.whl.metadata (2.8 kB) Requirement already satisfied: numpy in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (1.24.3) Requirement already satisfied: pandas>=1.0.4 in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (2.0.3) Requirement already satisfied: scikit-image in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (0.20.0) Collecting scikit-learn==0.21.1 (from TrackPal) Using cached scikit-learn-0.21.1.tar.gz (12.2 MB) Preparing metadata (setup.py) ... done Requirement already satisfied: tifffile in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (2023.4.12) Requirement already satisfied: tqdm in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (4.65.0) Requirement already satisfied: scipy in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (1.11.1) Requirement already satisfied: statsmodels in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (0.14.0) Requirement already satisfied: matplotlib in .\/anaconda3\/lib\/python3.11\/site-packages (from TrackPal) (3.7.2) Collecting rdp (from TrackPal) Using cached rdp-0.8-py3-none-any.whl Colle...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28829"
  },
  {
    "number":30702,
    "text":"CI Use explicit permissions for GHA workflows\n\nCodeQL scanning is nudging us towards using explicit permission, see [URL] Once this is done we could in principle set the default workflow permissions to read as recommended by GitHub. [Settings]([URL] ![Image]([URL] ~This is an excuse to try out sub-issues \ud83d\ude05~ Apparently you can not add PR as sub-issue oh well \ud83d\ude13 ?",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30702"
  },
  {
    "number":32095,
    "text":"Using [CODE] with multiple pytest workers leads to race\n\n### Describe the bug When using [CODE] with several workers to run a test suite that uses [CODE] as a fixture ([CODE]X.shape=(5902, 68435) y.shape=(5902,)[CODE]pytest-xdist[CODE]shutil.move[CODE]NamedTempFile[CODE]fname + '.part'[CODE]check_or_create(path)[CODE]python your_file.py <n_procs>` it reproduces a different version (I think) of the problem. [CODE_BLOCK] Make sure to delete the 20newsgroups file(s) fro...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32095"
  },
  {
    "number":29603,
    "text":"DOC Update advanced installation instructions from macOS\n\nI think we should kind of wait for the dust to settle on [URL] but I am pretty sure that with the improvements in OpenMP detection in Meson 1.5 [URL] you don't need to set any environment variables and that our [macOS installation doc]([URL] can be simplified. We may be setting environment variable in our CI as well, this is worth a look if we can remove them. cc @EmilyXinyi if you feel like working on it at one point. I guess it's good to note that this is a positive side-effect of moving away from setuptools to Meson: some things just work better out of the box. I am certainly slightly biased but I am personally convinced that the cost of switching was worth it. Future will tell if I was wrong but I am reasonably confident about this :wink:.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29603"
  },
  {
    "number":27470,
    "text":"Error inside Pyodide with sklearn.utils.sparsefuncs on scipy sparse arrays and int64 indices\n\nTo reproduce, paste the following snippet in [Pyodide stable console]([URL] [CODE_BLOCK] The same snippet works fine with [CODE] instead of [CODE]. Traceback: [CODE_BLOCK] This was noticed when running the tests inside Pyodide [URL] The best solution seems to use [CODE] and [CODE] that have been introduced in scipy 1.11 and keep our code for scipy<1.11 which does not have the issue....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27470"
  },
  {
    "number":28996,
    "text":"Enhancement: Add Summary Output for Linear Regression Models\n\n### Describe the workflow you want to enable While scikit-learn excels in predictive modeling, users often need detailed statistical summaries to interpret their regression results. I propose we develop options for users wanting comprehensive statistical reports for models such as LinearRegression(), without impacting model performance. ### Describe your proposed solution Modular Design: Introduce optional modules or mixins for secondary features. Users can enable them explicitly when needed. Feature Flags: Allow users to toggle specific functionalities. Lazy Evaluation: Compute secondary features only when requested. ### Describe alternatives you've considered, if relevant While statsmodels provides comprehensive summaries (including p-values!), having an integrated solution within scikit-learn would be valuable. The synergy between the two libraries benefits users seeking both prediction and statistical inference. Using the existing metrics is inconvenient -- I often find myself copying the same code across projects for printing out all the evaluations. Statisticians would appreciate the full summary output. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28996"
  },
  {
    "number":24288,
    "text":"Add plotting AUC plotting tools to \"plot_roc\" example\n\n### Describe the issue linked to the documentation The following example: [URL] does not use sklearn.metrics.plot_roc_curve or any of the related plotting tools. ### Suggest a potential alternative\/fix We should either: - Reference the corresponding functionality on this example - [preferably]use this functionality on this example, which might require improving it, if writing the corresponding example is clunky with our plotting utilities.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24288"
  },
  {
    "number":27846,
    "text":"\u26a0\ufe0f CI failed on Ubuntu_Atlas.ubuntu_atlas (last failure: Aug 28, 2025) \u26a0\ufe0f\n\nCI is still failing on Ubuntu_Atlas.ubuntu_atlas - test_float_precision[33-MiniBatchKMeans-dense]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27846"
  },
  {
    "number":24662,
    "text":"Can I show the chart in the browser ?\n\n### Describe the workflow you want to enable <img width=\"350\" alt=\"image\" src=\"[URL] Hi! Can I show the chart in the browser ? Thanks ### Describe your proposed solution LOL ### Describe alternatives you've considered, if relevant _No response_ ### Additional context ``",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24662"
  },
  {
    "number":29870,
    "text":"Publish Python 3.13 wheels on PyPI for 1.5.2\n\n### Describe the workflow you want to enable Hello, Could you please release CPython 3.13 manylinux wheels on PyPI? Python 3.13.0~rc2 has already been released and there will be no ABI changes even for bug fixes at this point. It will help projects starts using scikit-learn from the day the final candidate is released. Python 3.13 is also a main Python in Fedora 41 which will be released in October, so we'd like to enable seamless [CODE] experience to our users. ### Describe your proposed solution Publishing the wheels on PyPI ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29870"
  },
  {
    "number":26056,
    "text":"Typo in HistGradientBoosting documentation | no_interaction should be no_interactions\n\n### Describe the issue linked to the documentation In documentation, the [CODE] field of [HistGradientBoostingRegressor]([URL] mentions [CODE] as an allowed value. This throws an error, as it needs to be [CODE]. Same thing in [CODE] ### Suggest a potential alternative\/fix [CODE] -> [CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26056"
  },
  {
    "number":25595,
    "text":"About using feature_Selection many times\n\n### Describe the workflow you want to enable I will state my question first. When using a pipeline that combines a variable selection method with multiple estimators. If I evaluate them with cross_validation, is there a good way not to evaluate fit for variable selection every time? [CODE_BLOCK] In the code above, the same selector in another pipeline repeats the same calculation. This repeat can become a big problem when the number of estimators to be evaluated increases and when the computational complexity of the selector increases. Is there a good way to avoid this repeat? ### Describe your proposed solution If the method to solve the above problem is not implemented in the current sklearn, I would like to know a good method. ### Describe alternatives you've considered, if relevant - ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25595"
  },
  {
    "number":28108,
    "text":"BUG Wrong error raised for OneVsRestClassifier with a sub-estimator that doesn't allow partial_fit\n\n### Describe the bug When using the [CODE] with a sub-estimator, that doesn't have [CODE] implemented, a misleading error message is shown: [CODE]. Though, [CODE] does implement [CODE], but the underlying estimator doesn't. There is an appropriate error raising already implemented in [URL] [CODE_BLOCK] But it's not run, because the AttributeError from the [CODE] decorator\/descriptor thing pops up earlier. I'd like to learn about that, repair that and add a test to make sure this doesn't happen again by accident. I will also check if other methods from the multiclass classifiers are also affected. Is it alright if I go ahead with this? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] ### Actual Results [CODE]. ### Versions ```shell System: python: 3.10.6 (main, Oct 10 2022, 12:43:33) [GCC 9.4.0] executable: \/home\/stefanie\/.pyenv\/versions\/3.10.6\/envs\/scikit-learn_dev\/bin\/python machine: Linux-5.15.0-91-generic-x86_64-with-glibc2.31 Python dependencies: sklearn: 1....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28108"
  },
  {
    "number":25578,
    "text":"Support pandas nullable dtypes for scoring metrics\n\n### Describe the workflow you want to enable I would like to be able to pass data with the nullable pandas dtypes ([CODE], [CODE], and [CODE]) into sklearn metrics such as [CODE], [CODE], and [CODE] (and more) even if the data does not contain any nans. Currently, they result in one of several errors: - If [CODE] and [CODE] are both nullable types: [CODE] - it only one of [CODE] or [CODE] is nullable and the other is non nullable : [CODE] - Some metrics such as [CODE] result in a different error when [CODE] is nullable: [CODE] Repro with sklearn 1.2.1 and pandas 1.5.3: [CODE_BLOCK] ### Describe your proposed solution Sklearn should recognize the pandas nullable dtypes as the correct type of target for their scoring metrics like it does with the non nullable dtypes. ### Describe alternatives you've co...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25578"
  },
  {
    "number":26347,
    "text":"RandomForestRegressor() producing mainly constant forecast results over time-series data\n\n### Describe the workflow you want to enable Let's say I have dataset as a single feature and count as Label\/target to predict within the following pandas after splitting data with the following strategy for 274 records: split data into training-set + validation-set] [Ref. CODE_BLOCK] I tried default pipeline as well as optimized one by tuning Hyper-parameters to get optimum results by equipping the RF pipeline with [GridSearchCV(): {r2_score(test['count'], rf_pipeline2.predict(X_test))}\") print(f\"r2 (opt.): {r2_score(test['count'], rf_pipeline2o.predict(X_test))}\") #r2 (defaults): 0.025314471951056405 #r2 (opt.): 0.07593841572721849 img Full code for reproducing the example: # Load the time-series data as dataframe import numpy as np import pandas as pd import matplotlib.pyplot as plt df = p...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26347"
  },
  {
    "number":24328,
    "text":"DOC deprecate is updating a docstring against numpydocs rules\n\n### Describe the issue linked to the documentation [CODE] is updating the docstring within [CODE] This causes the numpydocs to fail during the test with the error: [CODE] After discussing with @glemaitre and @ogrisel : the [CODE] should be removed from [CODE] ensuring that the appropriate depracation message appears in all the docstrings using the [CODE] decorator at the moment. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24328"
  },
  {
    "number":27368,
    "text":"allow uniform intialization for BayesianGaussianMixture with dirichlet_process wight prior\n\n### Describe the workflow you want to enable I would like to have a deterministic version of the Bayesian Gaussian Mixture Model ### Describe your proposed solution I propose allowing for unifrom responsibility initialization. This is possible when using the dirichlet process weight prior, because it is inherently asymmetric with the only assuption that there is an order to the cluster sizes ### Describe alternatives you've considered, if relevant random initialization is non-deterministic which tends to introduce unwanted noise in the process ### Additional context I have manually hacked in uniform initialzation, which is rather simple. it should likely be forbidden if the prior is not a dirichlet process but a symmetric dirichlet distribution",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27368"
  },
  {
    "number":30888,
    "text":"RFC Write an explicit rule about bumping our minimum dependencies\n\nRoughly a year ago, SPEC0. - non pure-Python dependencies (numpy, scipy, pandas, etc ...): in each December release they are bumped to the minimum minor release that has wheels for the minimum Python version. - pure Python dependencies: in each release (December and June) bump to the most recent minor release older than 2 years old - we expect that exceptions may arise, although hopefully not too often, for example security or critical bug fixes ### Rationale - we want a simple rule - we don't want to be even more conservative that what we have been doing historically - in an ideal world, we would want to try to avoid requiring newer versions if there is not a \"good reason\" too, although there is some tension between having a \"simple rule\" and this bullet point ### Proposed plan we didn't bump our dependency in 1.6 so we would bump them in 1.7 (June 2025) and start the regular December version bump in December 2025. This is what it would look like for the next 4 scikit-learn releases ([CODE] column is the age of the min Python at the time of the scikit-learn release, and similarly for other dependencies). Python: ``` scikit-learn scikit-learn-date python python-date-diff 0 1.7 2025-06-01 3.10 3.660274 1 1.8 2025-12-01 3.11 3.106849 2 1.9 2026-06-01 3.1...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30888"
  },
  {
    "number":31885,
    "text":"[CODE] is not thread-safe\n\nThis was discovered while running: [CODE_BLOCK] before including the fix pushed to #30041 under [URL] I suspect the problem is that the built-in Platt scaling implementation of the vendored C++ code base of libsvm that uses a singleton pseudo random generator. Therefore, seeding the shared RNG state from competing threads prevents getting reproducible results and hence the test failure.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31885"
  },
  {
    "number":26612,
    "text":"DOC Outdated note at top of [CODE]\n\n### Describe the issue linked to the documentation The top of [[CODE]]([URL] says: [CODE_BLOCK] I think [CODE] has changed to [CODE] although the deloy step doesn't seem to use the [CODE] key, so potentially this note could be removed altogether? Ref: [URL] ### Suggest a potential alternative\/fix Amend [CODE] to [CODE] or remove this part altogether.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26612"
  },
  {
    "number":24238,
    "text":"AttributeError: 'NoneType' object has no attribute 'split'\n\n### Describe the bug I am using SMOTE to sample a binary classification dataset (churn or not). For multi-label, it works, but when I use the same function on a binary dataset it fails with the following error: ``` AttributeError: 'NoneType' object has no attribute 'split' Full traceback: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-9-2fc28e7f2344> in <module> 12 return data, target 13 ---> 14 data, target = SMOTE_oversample(churn, 'Churn_Yes') <ipython-input-9-2fc28e7f2344> in SMOTE_oversample(df, dep_var) 8 oversample = SMOTE() 9 ---> 10 data, target = oversample.fit_resample(data, target) 11 12 return data, target ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\imblearn\\base.py in fit_resample(self, X, y) 81 ) 82 ---> 83 output = self._fit_resample(X, y) 84 85 y_ = ( ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\imblearn\\over_sampling\\_smote\\base.py in _fit_resample(self, X, y) 322 323 self.nn_k_.fit(X_class) --> 324 nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:] 325 X_new, y_new = self._make_samples( 326 X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0 c:\\python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py in kneighbors(self, X, n_neighbors, return_distance) 761 ) 762 if use_pairwise_distances_reductions: --> 763 results = PairwiseDistancesArgKmin.compute( 764 X=X, 765 Y=self._fit_X, ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24238"
  },
  {
    "number":28297,
    "text":"Getting HTTPError: HTTP Error 403: Forbidden when trying to load California Housing dataset\n\n### Describe the bug When trying to load the dataset I get an error. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results Dataset loads ### Actual Results ``` HTTPError Traceback (most recent call last) [\/var\/folders\/wx\/mz49j6yd5514yjn5k60sk6900000gn\/T\/ipykernel_16344\/1379907178.py 6 7 X_train_full, X_test, y_train_full, y_test = train_test_split( ~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/datasets\/_california_housing.py 133 This dataset consists of 20,640 samples and 9 features. 134 \"\"\" --> 135 data_home = get_data_home(data_home=data_home) 136 if not exists(data_home): 137 makedirs(data_home) [~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/datasets\/_base.py]([URL] in _...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28297"
  },
  {
    "number":30242,
    "text":"Missing link to how to install dev version on the top bar\n\nWe used to have a link to \"how to install this dev version via nightly releases\" when the user explores the [CODE] version of the website. But now we have: ![image]([URL] I'm not sure how to put it back. @Charlie-XIAO maybe?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30242"
  },
  {
    "number":27350,
    "text":"Problem in function \"balanced_accuracy_score\" calculation.\n\n### Describe the bug Hi all I am facing a problem in reproducing the [CODE] function. Many resources converges on the equation of balanced accuracy as we can see in the link [URL] By definition it is: Sensitivity or True Positive Rate = $\\Large \\frac{TP}{TP + FP}$ Specificity or True Negative Rate = $\\Large \\frac{TN}{TN + FN}$ Balanced Accuracy = $\\Large \\frac{Sensitivity + Specificity}{2}$ Simulating results by hand and using the built-in function the answers diverges. ### Steps\/Code to Reproduce Let's simulate 100 data points where TP = 30, TN = 40, FP = 12 and FN = 18. Then, we calculate the True Positive and Negative Rate (Sensitivity and Specificity). Finally, the calculate the result of accuracy and balanced_accuracy. [CODE_BLOCK] If we extract the TP, TN, FP, FN from the simulated predictions, we have the right values: ```python tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel() sensitivity = tp \/ (tp + fp) specificity = tn \/ (tn + fn) balanced_acc = (sensitivity + specificity) \/ 2 print(\"Balanced Accuracy:\", balance...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27350"
  },
  {
    "number":31571,
    "text":"Several Doc improvement for whats_new\n\n### Describe the issue linked to the documentation I found some bugs or unclear areas that need further improvement in several versions of whats_new documentation. ### v1.5.0](URL] However, class[CODE] doesn\u2018t seem to have the [CODE] method (refer to [class PLSSVD](URL] I recommend to make it clear that the [CODE] and [CODE] are Parameters (like the previous item), otherwise it will be misleading to know whether they are parameters or methods. ### [v1.4.0]([URL] The full qualified name of function [CODE] should be [CODE]. ### [v1.3.0]([URL] - \"The parameter log_scale in the class :class:[CODE] has been deprecated in 1.3 and will be removed in 1.5....",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31571"
  },
  {
    "number":28389,
    "text":"Apparent mismatch between possible arguments for [CODE] in the base stochastic gradient class\n\n### Describe the bug Raised in URL] The [CODE] parameter in [[CODE] [is constrained]([URL] to non-negative integers or boolean values: [CODE]. This seems to be at odds with [CODE] seemingly meaning [CODE] which contradicts the typical truth-evaluation of [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions This is in the main branch right now.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28389"
  },
  {
    "number":31546,
    "text":"Regression in [CODE] with [CODE] and [CODE] after upgrading to v1.7.0\n\n### Describe the bug Hello. Recently, after upgrading to scikit-learn v1.7.0, I encountered an issue when using [CODE] with the [CODE] keyword argument. Specifically, the following error is raised: [CODE_BLOCK] However, in v1.6.0, everything works fine. After further investigation, it seems this issue was introduced by PR #29797, where both [CODE] and [CODE] are passed to [CODE] unconditionally, without explicit conflict handling: [URL] Additionally, when setting [CODE] in multiclass classification scenarios, the decision boundary is no longer shown as it was in v1.6.0. It appears that this regression is due to the switch in v1.7.0 to always using a cmap to plot the entire decision surface in multiclass scenarios. Here are the visual differences: - v1.6.0 with [CODE]: ![Image]([URL] - v1.7.0 with the same code: ![Image]([URL] ## Suggestion To preserve backward compatibility and expected behavior: - Check for mutual exclusivity of [CODE] and [CODE] and raise a clear warning\/error; - Retain the old behavior when [CODE]. I'd be happy to open a PR to help address this regression if the core team is supportive. ### Steps\/Code t...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31546"
  },
  {
    "number":31776,
    "text":"Documentation Bug: Warning about \"unstable development version\"\n\n### Describe the issue linked to the documentation When browsing the scikit-learn documentation, I selected a stable version (e.g., 1.7.0) from the versions. However, I still see the warning banner at the top of the page: This is documentation for an unstable development version. This is a bit confusing, as I'm clearly viewing a stable release. <img width=\"1748\" height=\"830\" alt=\"Image\" src=\"[URL] \/> ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31776"
  },
  {
    "number":25298,
    "text":"RMSE using mean_squared_error does not return the correct value\n\n### Describe the bug The RMSE value obtained from mean_squared_error function with the squared parameter set to false return a different value compared to manually root the MSE obtained by the same function ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results 37384.461953883205 193.3506192229112 193.3506192229112 193.3506192229112 ### Actual Results 37384.461953883205 78.94719343148691 193.3506192229112 193.3506192229112 ### Versions ```shell System: python: 3.9.12 (tags\/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)] executable: c:\\Users\\SRI-LAB-05\\AppData\\Local\\Programs\\Python\\Python39\\python.exe machine: Windows-10-10.0.22621-SP0 Python dependencies: sklearn: 1.2.0 pip: 22.3.1 setuptools: 58.1.0 numpy: 1.24.1 scipy: 1.9.3 Cython: None pandas: 1.5.2 matplotlib: 3.6.2 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\Users\\SRI-LAB-05\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None num_threads: 20 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\SRI-LAB-05\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll version: 0.3.21 threading_layer: pthreads architecture: Haswell num_threads: 20 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25298"
  },
  {
    "number":29241,
    "text":"[CODE] with boolean [CODE] that include [CODE] fails on [CODE] when [CODE] is boolean\n\n### Describe the bug An CODE] that was fitted on a [CODE] with boolean columns that include [CODE] will fail when transforming a boolean [CODE] due to a mismatch in the [CODE]s when calling [CODE]. Since [CODE] has no [CODE] [CODE], there is an attempt to call [CODE], which fails because [CODE] _does_ have an [CODE] [CODE]. As far as I can tell, this can be fixed by casting the [CODE] of [CODE] in [[CODE] Cell In[1], line 10 7 o.fit_transform(x) 9 y = pd.DataFrame({'a': [True, True, False]}) ---> 10 o.transform(y) File ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/utils\/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, args, kwargs) 293 @wraps(f) 294 def wrapped(self, X, args, kwargs): --> 295 data_to_wrap = f(self, X, args, kwargs) 296 if isinstance(data_to_wrap, tuple): 297 # only wrap the first output for cross decomposition 298 return_tuple = ( 299 _wrap_data_with_container(method, data_to_wrap[0], X, self), 300 data_to_wrap[1:], 301 ) File ~...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29241"
  },
  {
    "number":29009,
    "text":"Incorrect documented output shape for [CODE] method of linear models when [CODE] > 1\n\n### Describe the issue linked to the documentation For some classes under [CODE] such as [CODE], [CODE], [CODE], and a bunch of others, the documentation for the [CODE] method states that it returns [CODE_BLOCK] However, this is incorrect when the model is fitted with [CODE] with shape [CODE], i.e. [CODE] > 1, in which case the returned array should have shape [CODE]. ### Suggest a potential alternative\/fix The documentation for the return value of the [CODE] method should be [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29009"
  },
  {
    "number":27819,
    "text":"Is it a good idea to have different definitions of cluster radius for BIRCH?\n\n### Describe the workflow you want to enable I want to enable different definitions of cluster radius, i.e., CODE], for [CODE] clustering algorithm implemented in scikit-learn. The cluster radius, i.e., [CODE] is arguably one of the most important parameters for [CODE] to decide how to cluster data, and it is now defined as the root mean squared value of Euclidean distances between data points and centroid in each cluster. (see [codes here![Screenshot 2023-11-20 at 10 41 22 PM]([URL] The above unreasonable behavior is because of the definition of cluster radius, i.e., [CODE], which I think should have more definitions to choose from or change to a more reasonable one. ### Describe your proposed solution The cluster radius could be defined as the maximum allowed Euclidean distance from centroid to each data point in the cluster. This definition will solve the problem described above by making each cluster a shell-like shape with a \"radius\", instead of an arbitrary shape containing data points with distance to centroid larger than threshold. I think there could be more than one way to define this threshold, and I'm interested to know if there is any suggestions of easy-to-implement defini...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27819"
  },
  {
    "number":26826,
    "text":"Add an example showcasing the HistGradientBoosting...\n\n### Describe the issue linked to the documentation The documentation is not putting forward the HGBT very much. People are not guided to it, while it is arguably the most useful model in scikit-learn One paradigmatic problem is that on the front-page we have a nice visual on regression, but it point to an AdaBoost example :crying_cat_face: !image. It should then (in separate section) showcase a few other nice features, for instance: - Quantile regression - Support of missing values - Support of categorical values - Monotonicity constraints There are already examples on the various features that I listed above. Ideally, I like not to add examples (we have too many). I'm not sure whether we should merge these in (not convinced, as the corresponding examples are probably too detailed) or point too them. I would also make sure that this example is linked to from a bunch of places (for instance from the various examples on gradient boosting, just to make sure that people do find the HGBT) Also, we should link from the HGBT docs to this example and vice-versa - [ ] Do the example - [ ] Replace the landing-page figure and link to example - [ ] Cross-link in the documentation - [ ] Cross-link in other examples cc @ArturoAmorQ",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26826"
  },
  {
    "number":30391,
    "text":"CI Use CIBW_ENABLE rather than CIBW_FREE_THREADED_SUPPORT in wheels builder\n\nIt seems like this is about CIBW_FREETHREADED_SUPPORT and CIBW_PRERELEASE_PYTHONS. There may be some complications for CIBW_PRERELEASE_PYTHONS which we are using for Windows minimal docker image. > Added a new CIBW_ENABLE\/enable feature that replaces CIBW_FREETHREADED_SUPPORT\/free-threaded-support and CIBW_PRERELEASE_PYTHONS with a system that supports both. In cibuildwheel 3, this will also include a PyPy setting and the deprecated options will be removed. ([#2048]([URL] Is it relevant for us @lesteve ? _Originally posted by @jeremiedbb in [URL]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30391"
  },
  {
    "number":31267,
    "text":"Change the default data directory\n\n### Describe the workflow you want to enable It's not a good practice to put files directly into the home directory. ### Describe your proposed solution A more common way is to put them into the standard cache directories recommended by operating systems: | OS | Path | | -- | ---- | | Linux | [CODE] (if the env var presents) or [CODE] | | macOS | [CODE] | | Windows | [CODE] ([CODE]) | ### Describe alternatives you've considered, if relevant Put into [CODE] for all operating systems. Though not being standard, it's still better than the home dir. ### Additional context [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31267"
  },
  {
    "number":27172,
    "text":"incorrect intercept in LinearRegression when [CODE]?\n\n### Describe the bug The intercept is incorrectly computed when using sample weights and [CODE]. The docs only say that [CODE] may be overwritten by setting this flag but the intercept also changes. ### Steps\/Code to Reproduce import numpy as np from sklearn.linear_model import LinearRegression rng = np.random.default_rng(20) n, p = 50, 3 X = rng.standard_normal((n, p)) Y = rng.standard_normal(n) + 2 + X @ rng.standard_normal(p) W = rng.uniform(0, 1, size=(n,)) lm_nocp = LinearRegression(copy_X=False) lm_nocp.fit(X, Y, sample_weight=W) lm_cp = LinearRegression(copy_X=True) lm_cp.fit(X, Y, sample_weight=W) print('checking with weights', np.allclose(lm_nocp.intercept_, lm_cp.intercept_)) print(lm_nocp.intercept_, lm_cp.intercept_) lm_nocp.fit(X, Y) lm_cp.fit(X, Y) print('checking without weights', lm_nocp.intercept_ == lm_cp.intercept_) print(lm_nocp.intercept_, lm_cp.intercept_) ### Expected Results checking with weights True 1.6871821183020286 1.6871821183020286 checking without weights True 1.874959984635478 1.874959984635478 ### Actual Results checking with weights False 1.8599658087359068 1.6871821183020286 checking without weights False 1.7039080345298554 1.874959984635478 ### Versions ```shell System: python: 3.10.12 (main, Jul 5 2023, 15:02:25) [Clang 14.0.6 ] executable: \/Users\/jtaylo\/anaconda3\/envs\/pyglmnet\/bin\/python machine: macOS-13.5-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 68...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27172"
  },
  {
    "number":30563,
    "text":"[CODE] optimization by early elimination of bad performing configurations\n\n### Describe the workflow you want to enable [CODE] currently tries every configuration k-times (of [CODE]). But the bad performing configurations could be tried less than k-times. This could decrease the training time\/resources when using [CODE] by about 30% (depending on the variance of the scores). So for example a configuration with a score of [CODE] in 1 out of 5 folds can't perform better than a configuration with [CODE] as scores and therefore the other 4 folds of the former mentioned configuration can be ignored. ### Describe your proposed solution Therefore I propose the following scheme: 1. Do 1 out of [CODE] rounds for every configuration. 2. Do a round for the configuration which scored best in the previous round(s) 3. If all [CODE] rounds were applied to a configuration, eliminate all configurations which scores combined with the best possible score for the missing rounds are lower than the best finished score. Step [CODE] can also be applied when a new step is done and the highest possible overall score for this configuration is lower than the overall score of the best finished configuration. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30563"
  },
  {
    "number":25740,
    "text":"Verbose for Gaussian Process regression\n\n### Describe the workflow you want to enable I would like to monitor the progress of my fit function for a Gaussian Process. ### Describe your proposed solution if verbose >= 1 print the parameters \"theta\" and the log marginal likelihood. Maybe offer the option to return the log likelihood with a dict for the tried thetas. - In general any verbose would be very welcome. To get a rough estimate of the progress. Please and thank you. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25740"
  },
  {
    "number":29509,
    "text":"An inconsistency between the document of [CODE] and code implementation\n\n### Describe the issue linked to the documentation Hi, I may find a potential condition missing in [CODE] or primal (regularized) formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. Corresponding part found in the source code: [CODE_BLOCK] Apprarently, the relationship between [CODE] and [CODE] are checked without the condition of l2 [CODE]. ([CODE] && [CODE] && [CODE]) can still pass the [CODE] function. Could you check it? ### Suggest a potential alternative\/fix Perhaps we can modify the judgment condition.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29509"
  },
  {
    "number":31364,
    "text":"Tfidf no genera los cluster correctos para oraciones con poco significado y palabras repetidas\n\n### Describe the workflow you want to enable Dado el sigueinte csv: texto,categoria \"el gato el gato el gato el gato el gato\",\"gato\" \"el perro el perro el perro el perro el perro\",\"perro\" \"la casa la casa la casa la casa la casa\",\"casa\" \"el avi\u00f3n el avi\u00f3n el avi\u00f3n el avi\u00f3n el avi\u00f3n\",\"avi\u00f3n\" \"la playa la playa la playa la playa la playa\",\"playa\" \"el gato el perro el gato el perro el gato\",\"mezcla\" \"el perro el gato el perro el gato el perro\",\"mezcla\" \"la playa la casa la playa la casa la playa\",\"mezcla\" Al usar tfidf con stop words y ngramas el cluster de la ultima oracion de nuestro csv no lo agrupa en el cluster correcto, que en este caso deberia estar con la oracion 6 y 7 ### Describe your proposed solution Podemos mencionar las limitaciones con textos repetidos en la documentacion o mejorar los calculos para poder manejar textos con poco significado semantico y palabras repetidas. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31364"
  },
  {
    "number":24885,
    "text":"Extend [CODE] to validate arguments which are type parameters (i.e. classes, built-in types, generic types, etc.)\n\n### Describe the workflow you want to enable Main idea is to add [CODE] option somewhere in [CODE]. Working on #24862 makes me realize we need an [CODE] checker to simplify parameter validation. For example when the parameter (argument to function) is itself a type parameter. Currently, if a function takes in a [CODE] as a parameter, such as [CODE] or [CODE], then the type validator for this if the accepted contained DTypes are numeric, currently one would have to find out all the numeric dtypes and write a rather clunky: [CODE_BLOCK] Since [CODE] (or other abstract classes from [CODE]) won't work., i.e. [CODE] is [CODE], one would have to enumerate all numeric like class types. This also won't work with [CODE] or third party dtypes. The workflow I want to enable is the following when writing the validators: [CODE_BLOCK] The former will pass all manner of good generic conditions without being overly (or impossibly) wordy. ### Describe your proposed solution add the following to [CODE] (but also see the alternative, maybe both should be in?): [CODE_BLOCK] also add the required code to [CODE] in the same script. ### Describe alternatives you've considered, if relevant A possible alternative is to make a generic class which leverages the Typing methods made available by third parties like numpy. Something like ```python class TypeChecker(_Constraint): def __init__(s...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24885"
  },
  {
    "number":32154,
    "text":"GridSearchCV cannot obtain the optimal parameter results.\n\n### Describe the bug When I used GridSearchCV to obtain the optimal parameters, I found that different cross-validation folds produced inconsistent results. ### Steps\/Code to Reproduce [CODE_BLOCK] > auc: 0.8859 best_params is 50 [CODE_BLOCK] > auc: 0.8725 best_params is 80 ### Expected Results Both results above should be identical. >auc: 0.8859 best_params is 50 ### Actual Results Between the two parameter lists, we should obtain the parameters with the best AUC, not inconsistent ones. ### Versions ```shell System: python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0] executable: \/work\/users\/suny\/mm\/.venv\/bin\/python machine: Linux-5.15.0-143-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.7.2 pip: 25.2 setuptools: 80.9.0 numpy: 2.2.6 scipy: 1.15.3 Cython: None pandas: 2.3.2 matplotlib: 3.10.6 joblib: 1.5.2 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 64 prefix: libscipy_openblas filepath: \/work\/users\/suny\/mm\/.venv\/lib\/python3.10\/site-packages\/numpy.libs\/libscipy_openblas64_-56d6093b.so version: 0.3.29 threading_layer: pthreads ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32154"
  },
  {
    "number":24760,
    "text":"Install fails on python 3.11\n\n### Describe the bug While installing scikit-learn, installation fails with setuptools error. If setuptools version is downgraded to setuptools==58.2.0 from setuptools==65.5.0(current stable), installation works fine. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error Thrown. ### Actual Results ``[CODE]numpy.distutils[CODE]distutils[CODE]setuptools < 60.0` for those Python versions. For more details, see: [URL] from numpy.distutils.command.build_ext import build_ext # noqa INFO: C compiler: clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g INFO: compile options: '-c' INFO: clang: test_program.c INFO: clang objects\/test_program.o -o test_program INFO: C compiler: clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g INFO: compile options: '-c' extra options: '-fopenmp' INFO: clang: test_program.c clang: error: unsupported option '-fopenmp' clang: error: unsup...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24760"
  },
  {
    "number":29629,
    "text":"plot_tree fails with ValueError Invalid RGBA argument\n\n### Describe the bug When using CODE] with [CODE] (so the nodes are colored), one sometimes gets a [CODE] such as [CODE_BLOCK] The same [CODE] will work fine if [CODE], and draw a decision tree. Below is an example. I attach two files with the dump of the DecisionTree and the list of columns used. Thank you. ### Steps\/Code to Reproduce [CODE_BLOCK] [_br.dmp [GCC 11.3.0] executable: \/mnt\/c\/Workspace\/py-pip-workspaces\/pert.ai.main\/.venv\/bin\/python machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.5.1 pip: 24.2 setuptools: 65.5.0 numpy: 1.26.4 scipy: 1.12.0 Cython: None pandas: 2.2.2 matplotlib: 3.9.1 joblib: 1.3.2 threadpoolctl: 3.3.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/mnt\/c\/Workspace\/py-pip-workspaces\/pert.ai.main\/.venv\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: SkylakeX user_api: blas internal_api: openblas num_...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29629"
  },
  {
    "number":28314,
    "text":"Request to update \"Choosing the Right Estimator\" Graphic (scikit-learn algorithm cheat sheet)\n\n### Describe the issue linked to the documentation As seen here: [URL] One of the \"tough luck\" paths that go through the clustering section appear to say this is the case when there are >10k samples. ### Suggest a potential alternative\/fix However, with modern computational hardware, and the optimized implementation of DBSCAN in Scikit-learn, it appears that it may be helpful to recommend DBSCAN as a possible solution for datasets containing <100K or even <1M datapoints for clustering in reasonable amounts of time on CPU.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28314"
  },
  {
    "number":26140,
    "text":"RandomForest not passing feature names to trees and creating warnings.\n\n### Describe the bug I fit a decision forest with training data that includes feature names. When I call predict_proba on the forest everything is fine. When I call rf.estimators_[0].predict_proba it will warn that it was not trained with feature names. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No warnings ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26140"
  },
  {
    "number":27272,
    "text":"MultiOutputRegressor _ BUG\n\n### Describe the bug [CODE_BLOCK] ### Steps\/Code to Reproduce from sklearn.multioutput import MultiOutputRegressor ### Expected Results import issue ### Actual Results from sklearn.multioutput import MultiOutputRegressor leads to : ImportError: cannot import name '_check_fit_params' from 'sklearn.utils.validation' ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27272"
  },
  {
    "number":28943,
    "text":"MAPE approaching infinity with RandomForestRegressor\n\n### Describe the bug When using the current version of scikit-learn for learning a Random Forest Regressor ([URL] on the same dataset on which the same kind of model was learned in 2021, the mean absolute percentage error (MAPE) behaves in a completely different way. In particular, the models learned in 2021 had a MAPE of the order of magnitude of 10^-2, now the MAPE is of the order of magnitude of 10^16 or +inf. Other error metrics (MAE, RMSE, Accuracy) do not show this difference on the same data. I suppose something's wrong with the computation of the MAPE in combination with sklearn.ensemble.RandomForestRegressor. The computation of the MAPE with other regressors (SVR, MultilayerPerceptron), both with the stable version of the library and the 2021 version, is correct on the same dataset. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A value less or equal to 1 for MAPE. ### Actual Results mape: 3726411741284014,0 or even mape:inf ### Versions ```shell System: python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-6.1.58+-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 23.1.2 s...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28943"
  },
  {
    "number":28321,
    "text":"TimeSeriesSplit Train Size formula correction\n\n### Describe the issue linked to the documentation The documentation states: > The training set has size [CODE] in the [CODE]th split, with a test set of size [CODE] by default, where [CODE] is the number of samples. The equation for the training set looks like it is flawed. Given a split with: [CODE_BLOCK] The previous code generates a list similar to this: [CODE_BLOCK] I tried using the formula in the documentation to determine the size of the training set at a given iteration: [CODE_BLOCK] ### Suggest a potential alternative\/fix So the correction to the formula should be that the training set size is equal to: [CODE] instead of: [CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28321"
  },
  {
    "number":26277,
    "text":"Support [CODE] in Hist-GBDT estimators and categorical features with high cardinality\n\nAs originally sketched in [URL] there might be a way to enable support for arbitrary high values of [CODE] for both categorical and numerical features. This may not be super critical for numerical features, but this would enable categorical features of arbitrary cardinality, which is desirable. The rough idea is to internally map an input categorical feature into multiple binned features (probably [CODE] features) and to update the [CODE] and the predictors to treat that group of features as a single feature.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26277"
  },
  {
    "number":28410,
    "text":"Provide stubs for TargetEncoder\n\n### Describe the workflow you want to enable Being able to get autocompletions and type checking for sklearn.preprocessing.TargetEncoder ### Describe your proposed solution I noticed that TargetEncoder was failing to be recognized by pylance\/pyright. I assume that being a relatively new class some stub is missing. ### Describe alternatives you've considered, if relevant Enabling kernel-based completions. ### Additional context I reported it to [URL] Maybe you're in a better position than me to quickly create the missing stub. If that's the case, could you lend a hand with [URL] Thanks",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28410"
  },
  {
    "number":31659,
    "text":"Metadata not routed to transformers in pipeline during cross validation.\n\n### Describe the bug When using a pipeline with transformers in combination with cross validation, it seems that metadata is not correctly routed to the transformers during prediction. I would expect, that if [CODE] is set, that this is honored when calling predict on the pipeline. Edit: At least according to the code this is a known limitation. Although I couldn't find an issue tracking the progress on this. [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results ``` 1.7.0 --- Cross validation --- Received metadata='Some metadata' # Fit Received metadata='Some metadata' # Predict Received metadata='Some metadata' Received metadata='So...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31659"
  },
  {
    "number":24381,
    "text":"Inconsistency in AUC ROC and AUPR API\n\n### Describe the bug When only one class is present on the groundtruth. The function [CODE] throws an [CODE] and exits while the [CODE] returns [CODE]. I feel that both functions should return similar output in this case. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results auc_roc = roc_auc_score(y_true, prob) >> throws [CODE] aupr=-0.0 ### Actual Results Both should have similar outputs ### Versions ```shell Python dependencies: sklearn: 1.1.2 pip: 22.0.2 setuptools: 59.6.0 numpy: 1.23.2 scipy: 1.9.1 Cython: None pandas: 1.4.3 matplotlib: 3.5.3 joblib: 1.1.0 threadpoolctl: 3.1.0 Built with OpenMP: True ...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24381"
  },
  {
    "number":25929,
    "text":"Visual improvements for ROC and precision-recall plots\n\n### Describe the workflow you want to enable Hello, I would like to suggest the following improvements: - [x] (1) Add x and y axis limit: [0, 1], in sklearn axes currently start at ~-0.1 (#26366) - [x] (2) Modify the plotting frame: either remove the top and right lines to see the curve better when values are close to 1, or plot the frame with a dotted line (#26367) - [x] (3). Fix aspect ratio to squared, since the two axes are the same scale. (#26366) - [x] (4). Add title (not planned) ### Describe your proposed solution Hello, While working on ROC and precision-recall plots, I came up with the following suggestions: 1. Add x and y axis limit: [0, 1], in sklearn axes currently start at ~-0.1 2. Modify the plotting frame: either remove the top and right lines to see the curve better when values are close to 1, or plot the frame with a dotted line 3. Fix aspect ratio to squared, since the two axes are the same scale. 4. Add title [CODE_BLOCK] ### Example 1 Edge case #### PR Before: ![image]([URL] After: ![image]([URL] #### ROC Before: ![image]([URL] After: ![image]([URL] ### Example 2 - Edge ...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25929"
  },
  {
    "number":27873,
    "text":"RFC Unify old GradientBoosting estimators and HGBT\n\n### Current situation We have the unfortunate situation to have 2 different versions of gradient boosting, the old estimators ([CODE] as well as the new ones using binning and histogram strategies similar to LightGBM ([CODE]. This makes advertising the new ones harder, e.g. #26826, and also result in a larger feature gap between those two. Based on discussions in #27139 and during a monthly meeting (maybe not documented), I'd like to call for comments on the following: #### Proposition Unify both types of gradient boosting in a single class, i.e. the old names [CODE] and make them switch the underlying estimator class based on a parameter value, e.g. [CODE] ([CODE]->old classes, integer->new classes). Note that binning and histograms are not the only difference. ### Comparison #### Algorithm The old GBT uses Friedman gradient boosting with a line search step. (The lines search sometimes, e.g. for log loss, uses a 2. order approximation and is therefore, sometimes, called \"hybrid gradient-Newton boosting\"). The trees are learned on the gradients. A tree searches for the best split among all (veeeery many) split candidates for all features. After a single tree is fit, the terminal node values are re-computed which corresponds to a line search step. The new HGBT uses a 2. order approximation of the loss, i.e. gradients and hessians (XGBoost paper, therefore sometimes called Newton boosting). In addition, it bins\/discretizes the...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27873"
  },
  {
    "number":29169,
    "text":"RFC make response \/ inverse link \/ activation function official\n\nWith questions like #29163 and with the private loss functions #15123 (almost everywhere) in place, I would like to discuss to make the inverse link function public. Models like LogisticRegression or HistGradientBoostingRegressor(loss=\"poisson\") have predictions like [CODE] where [CODE] is the prediction in \"link space\", e.g. linear predictor (\"eta\") for linear models. In line with the most recent nomenclature of HistGradientBoosting\\*, I propose the following public API for regressors and classifiers: - [CODE] - [CODE] or [CODE] - [CODE] Alternatives: 1. [CODE] is a link object which has 2 methods named like above. 2. 1-to-1 with the actual implementation: [CODE] and then as alternative 1. This would also expose the loss function (object), see also #28169. Further considerations - This would also make easier\/solve #18309 - Does this necessitate a SLEP? @scikit-learn\/communication-team @scikit-learn\/contributor-experience-team @scikit-learn\/core-devs @scikit-learn\/documentation-team ping",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29169"
  },
  {
    "number":26802,
    "text":"\u26a0\ufe0f CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jul 10, 2024) \u26a0\ufe0f\n\nCI is still failing on Linux_Runs.pylatest_conda_forge_mkl - test_fastica_eigh_low_rank_warning[31]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26802"
  },
  {
    "number":28151,
    "text":"RFC: Towards reproducible builds for our PyPI release wheels\n\nGiven the popularity of our project, our release automation might be considered an interesting target to conduct supply chain attacks to make our binaries ship spyware or ransomware to some of our users. One way to detect such attacks would be to: - make sure we produce reproducible builds environment variables. HHowever,this would not be enough. To get this fully work as expected, we would also need to guarantee that: - we use recent enough versions of pip\/setuptools\/wheel\/auditwheel\/delocate that honor [CODE]; - a full description of the build environment (e.g. versions and sha256 digests of the compilers and other build dependencies) is archived in our source repo for a given tag of scikit-learn. Ideally, all those build dependencies should themselves be byte-for-byte reproducible from their own public source code repo. Currently some build dependencies such as NumPy and Cython come from the [CODE] file which only specifies a minimum version. This means that we may end up with a newer versions of these dependencies than the one used to build the wheels for a given tag. [CODE] itself is not pinned, hence neither the dependencies it installs in its managed venvs (pip, setuptools, wheel, auditwheel, delocate). Furthermore, we do not archive or pin the versions and sha256 digests of the compilers...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28151"
  },
  {
    "number":26945,
    "text":"[PLSRegression] The standard scikit interface should have the argument in lowercase \"y\" not uppercase \"Y\"\n\n### Describe the bug The standard scikit interface should have the argument in lowercase \"y\" not uppercase \"Y\". Would you please modify it? [[[URL] ### Steps\/Code to Reproduce Please see the above link. ### Expected Results Change the lowercase \"y\" instead of uppercase \"Y\". ### Actual Results lowercase \"y\" ### Versions [CODE_BLOCK]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26945"
  },
  {
    "number":25647,
    "text":"sklearn.inspection.PartialDependenceDisplay fails with nulls\n\n### Describe the bug sklearn.inspection.PartialDependenceDisplay fails with nulls in the dimension of the partial dependence ### Steps\/Code to Reproduce ```python #This is a slimmed down version of the example in the sklear documentation #[URL] #I only need to change one line from sklearn.datasets import fetch_openml from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder from time import time from sklearn.pipeline import make_pipeline from sklearn.ensemble import HistGradientBoostingRegressor import numpy as np bikes = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True, parser=\"pandas\") # Make an explicit copy to avoid \"SettingWithCopyWarning\" from pandas X, y = bikes.data.copy(), bikes.target #####LOOK HERE####### #This line is changed to put the value to null X[\"weather\"].replace(to_replace=\"heavy_rain\", value=np.nan, inplace=True) mask_training = X[\"year\"] == 0.0 X = X.drop(columns=[\"year\"]) X_train, y_train = X[mask_training], y[mask_training] X_test, y_test = X[~mask_training], y[~mask_training] numerical_features = [\"temp\",\"feel_temp\",\"humidity\",\"windspeed\", ] categorical_features = X_train.columns.drop(numerical_features) hgbdt_preprocessor = ColumnTransformer( transformers=[ (\"cat\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan), categorical_features), (\"num\", \"passthrough\", numerical_features), ], sparse_threshold=1, verbose_featur...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25647"
  },
  {
    "number":30625,
    "text":"scikit-learn 1.6: Elliptic Envelope Fails with More Features than Samples\n\n### Describe the bug When using the EllipticEnvelope class in scikit-learn 1.6, the model raises an error when the number of features exceeds the number of samples in the input dataset. This issue occurs even when the data is preprocessed (e.g., scaled with StandardScaler) and is independent of the contamination or support fraction settings. This behavior differs from previous versions of scikit-learn, where the EllipticEnvelope was able to handle cases with more features than samples without raising errors. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Results with scikit-learn version < 1.6 [CODE_BLOCK] ### Actual Results Results with scikit-learn version >= 1.6 [CODE_BLOCK] ### Versions ```shell Versions System: python: 3.10.11 (tags\/v3.10.11:7d4cc5a, Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] executable: <redacted> machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.6.0 pip: 24.3.1 setuptools: 75.6.0 numpy: 2.0.2 scipy: 1.13.1 Cython: None pandas: 2.2.3 matplotlib: 3.9.2 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True thre...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30625"
  },
  {
    "number":26696,
    "text":"mutual_info_regression misbehaves when X is integer-typed\n\n### Describe the bug Mathematically, the mutual information between two continuous variables P and Q is symmetric, i.e. swapping the places of the two variables doesn't change the result. But I've found that when one of the variables is integer-typed, the result from [CODE] will change very much when you swap them over. It seems that when the [CODE] argument to [CODE] is integer-typed, something goes badly wrong. See demo code below. I'm definitely no expert, but here's my hunch about the cause. [CODE] calls [CODE], which contains this code: [CODE_BLOCK] As far as I can tell, when [CODE] is integer-typed this assignment rounds the results of the [CODE] operation to be integers. In the problematic case in the demo code below, this causes nearly all the values to be rounded to zero, destroying any information between the two variables. The [CODE] code _next_ does [CODE_BLOCK] but by then it is too late. ### Steps\/Code to Reproduce ```py import numpy as np from sklearn.feature_selection import mutual_info_regression ns = list(range(100)) P_int = np.array(ns + [1_000], dtype=np.int64) P_float = np.array(ns + [1_000], dtype=np.float64) Q = np.array(ns + [100], dtype=np.float64) # When both arguments are float64, the result looks plausible and swapping the places # of P and Q doesn't materially change the result. print( mutual_info_regression( np.expand_dims(P_float, axis=1), Q, discrete_features=False, random_state=1 ).ite...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26696"
  },
  {
    "number":31834,
    "text":"Resource cleanup issues in dataset loaders: files opened but not closed.\n\n### Describe the bug Two dataset loader functions in [CODE] have resource cleanup issues where files are opened but not properly closed using context managers, potentially leading to resource leaks. The first one is more important: ### in _lfw.py: Line 172: [CODE] - an image is opened each iteration of the loop. The file handle is never explicitly closed. PIL does not always immediately close the file. This can exhaust file descriptors. This one is less severe: ### In _kddcup99.py: Lines 390 - 394: The file is opened and manually closed using [CODE], but not inside a [CODE]\/[CODE] or [CODE] block. file_.close() appears after a loop without exceptions. This means that if an error occurs in the loop, the file remains open. ### Steps\/Code to Reproduce Code snippet shouldn't be necessary - ### Primary Issue in _lfw.py Opening many images without closing can exhaust system file descriptors Unclosed file handles can prevent garbage collection Applications or notebooks that repeatedly fetch the dataset could accumulate thousands of unclosed files ### Secondary Issue in _kddcup99.py If line.decode() fails (encoding issues), file remains open. If Xy.append() fails (memory constraints), file remains open. Keyboard interruption during process, file remains open. ### Expected Results All files should be opened using context managers, or [CODE_BLOCK] ensuring proper closure even if exceptions are raised. This ensure...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31834"
  },
  {
    "number":26224,
    "text":"SequentialFeatureSelector may not be working correctly with transformers that transform with respect to each sample\n\n### Describe the bug This is (probably) an extended issue of #25711, in which when [CODE] is used with [CODE], an [CODE] will be raised. After reading the code, I found that this is because [CODE] treats each feature (column) separately, as in [URL] This would cause [CODE] to raise an [CODE], since its specified columns to transform is regarding the original [CODE] but not each column of [CODE]. Then I was thinking that [CODE] may not be the only one that does not work correctly with [CODE]. For instance, [CODE] normalizes each sample (row), so when it is used with [CODE], it will be applied on each row of each column of [CODE], thus causing every single feature to become 1 or -1. Though no error will be raised, this is a serious error. ### Steps\/Code to Reproduce To test if my guess is true, I changed a few lines of code in the class [CODE] to print information: [URL] [CODE_BLOCK] Then I used the following code, respectively using a [CODE] and a [CODE]: ```python import pandas as pd import numpy as np from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, Normalizer from sklearn.ensemble import GradientBoostingClassifier from sklea...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26224"
  },
  {
    "number":30447,
    "text":"[CODE] raises an exception when metadata routing is enabled\n\n### Describe the bug In the latest release (v1.6.0), [CODE] raises an exception when using it with metadata routing enabled. This is because [CODE] dict gets unpacked even if [CODE], which is the default value. See this line: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No exception being raised. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.12 (main, Nov 6 2024, 20:22:13) [GCC 11.4.0] executable: <redacted>\/bin\/python machine: Linux-6.5.13netflix-g77293087f291-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.6.0 pip: None setuptools: 75.6.0 numpy: 1.26.4 scipy: 1.14.1 Cython: None pandas: 2.2.3 matplotlib: 3.9.3 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 4 prefix: libopenblas filepath: \/root\/pycharm_projects\/evaluations\/.venv\/lib\/python3.10\/site-packages\/...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30447"
  },
  {
    "number":25582,
    "text":"Add option to scale Matthews correlation coefficient (MCC) output to the [0, 1] range\n\n### Describe the workflow you want to enable Matthew's correlation coefficient is known to have a different range of possible values from most other classification performance metrics. While it's usual for metrics to lie in the 0, 1] range, MCC lies in [-1, 1] for the binary case. Even worse, this isn't exactly the case for the multiclass case, with the range being [-1\/(n_classes-1), 1] (an informal demonstration is present in the Additional context section). This makes it hard to compare or explain MCC in relation to other more common metrics. It would be of use to have an option to automatically scale the output of [CODE] metric to either the [0, 1] or [-1, 1] ranges. ### Describe your proposed solution A solution would be to add a parameter with 3 possible values, such as > scale: {\u2018ratio\u2019, \u2018corrcoef'} or None, default=None That way, if the scale parameter equals [CODE] (ranging from 0 to 1) the output would simply need to be scaled to > [CODE]. Similarly, in the case where scale equals [CODE] (ranging from -1 to 1) we scale instead to > [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context ### Demonstration that MCC lies in the range [-1\/(n_classes-1), 1] for an arbitrary number [CODE] of possible classes. Following from [definition \/ (sqrt(s^2 - p.p) sqrt(s^2 - t.t)) where c is the total number of samples correctly predicted, s is the tot...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25582"
  },
  {
    "number":30332,
    "text":"NuSVC argument [CODE] is not used\n\n### Describe the bug Like [CODE], the class [CODE] takes argument [CODE]. However, it looks like this argument is not used. After a quick look at the libsvm C code within sklearn as well as [libsvm's original documentation]([URL] this seems to be expected: \"[CODE] set the parameter C of class i to weight*C, for C-SVC\". I suggest that this argument should be removed from [CODE]'s constructor and from the documentation. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results As in the case of no [CODE], [CODE] should give the same [CODE] as an [CODE] with the same [CODE]. Also [CODE] should give the \"empty\" result. ### Actual Results In all cases above [CODE] with class weight behaves exactly as when no weights are given. ### Versions ```shell System: python: 3.9.16 |...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30332"
  },
  {
    "number":31859,
    "text":"Intercepts of Newton-Cholesky logistic regression get corrupted when warm starting\n\n### Describe the bug When using multinomial logistic regression with warm starts from a previous iteration, the final coefficients in the model are correct, but the intercepts somehow get filled with incorrect numbers somewhere. As a result, predictions from a warm-started model differ from those of a cold-start model that has more iterations on the same data. The issue appears to have been introduced recently as it works fine with version 1.5, but not with 1.6 or 1.7. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Intercepts should be the same, up to shifting by a constant if needed. ### Actual Results Intercepts are different, as are predicted probabilities ### Versions ```shell System: python: 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0] executable: \/home\/david\/miniforge3\/bin\/python machine: Linux-6.12.33+deb12-amd64-x86_64-with-glibc2.36 Python dependencies: sklearn: 1.7.1 pip: 24.2 setuptools: 74.1.2 numpy: 2.0.1 scipy: 1.14.1 Cython: 3.1.0 pandas: 2.2.3 matplotlib: 3.9.2 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 20 prefix: libscipy_openblas filepath: \/home\/david\/.local\/lib\/python3.12\/site-packages\/numpy.libs\/libscipy_openblas64_-99b71e71.so version: 0.3.27 threading_layer: pthreads architecture: Has...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31859"
  },
  {
    "number":25096,
    "text":"drop correlated features as a pipeline step\n\n### Describe the workflow you want to enable Doing FeatureSelection droping correlated features is standard ml proc that sklearn covers. But, as i interpret the documentation, sklearn treats the featureSelection based on correlations as a previous step, outside the pipeline. An in the pipeline, you can only do univarte or iterative featureSelections [docs]([URL] I would like to do FeatureSelection based on correlations, as a pipeline step ### Describe your proposed solution something like [CODE_BLOCK] Where MinCovDet_ ~ MinCovDet but returns the ingested df, so the classification step can do the fit ### Describe alternatives you've considered, if relevant Maybe a [CODE] wrapper that you can run on top of all the [CODE] correlators would be better ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25096"
  },
  {
    "number":30190,
    "text":"Towncrier categories overlap\n\n### Describe the issue linked to the documentation I had first commented. Major concerns: 1. [CODE] will always be affected with [CODE] and [CODE] and maybe [CODE]. It is ambiguous for us where to put these and possibly confusing to users. 2. There is not really a good place to put maintenance PRs and people would probably put them into [CODE] (which it is not) and [CODE] (which it is not). ### Suggest a potential alternative\/fix Discuss which (non-overlapping) categories are needed\/wanted.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30190"
  },
  {
    "number":28841,
    "text":"Version 1.0 breaks cross-validation with string targets\n\n### Describe the bug I just tried to upgrade the package from version 0.24.2 to the latest release. Doing so, my integration tests would start to fail, claiming that there would not be enough samples for at least one class. This only occurs if I use string-based targets instead of integers. As far as I have seen, there is no API change documented inside the changelog. Doing some testing, it seems like version 1.0 introduced the breaking change. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Both pipelines (once with integer targets, once with string targets) can be trained without issues. ### Actual Results ``` Traceback (most recent call last): File \"\/home\/stefan\/aaa\/run.py\", line 25, in <module> pipeline.fit( File \"\/home\/stefan\/aaa\/venv\/lib64\/python3.9\/site-packages\/sklearn\/base.py\", line 1474, in wrapper return fit_method(estimator, args, kwargs) File \"\/home\/stefan\/aaa\/venv\/lib64\/python3.9\/site-packages\/sklearn\/pipeline.py\", line 475, in fit self._final_estimator.fit(Xt, y, last_step_params[\"fit\"]) File \"\/home\/stefan\/aaa\/venv\/lib64\/python3.9\/site-packages\/sklearn\/base.py\", line 1474, in wrapper return fit_method(estimator, args, k...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28841"
  },
  {
    "number":28926,
    "text":"Performance Degradation in MeanShift When Data Has No Variance\n\n### Describe the bug When data provided to [CODE] consists of values with no variance (for example, two clusters of 0 and 1), the performance becomes extremely slow. I am unsure whether this is a bug or an unavoidable aspect of the algorithm's design. Any clarification would be appreciated. ### Steps\/Code to Reproduce [CODE_BLOCK] Link to Google Colab: [URL] ### Expected Results When data provided to [CODE] consists of values with no variance, the performance becomes as fast as when handling data with variance. ### Actual Results If [CODE] receives a 1D array with no variance, the computation is significantly slower. [CODE_BLOCK] Below is a control example, where the input has some variance: [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28926"
  },
  {
    "number":30983,
    "text":"Error in [CODE] when [CODE] is a pandas dataframe with [CODE] feature names\n\n### Describe the bug Hello, I've encountered an unexpected behavior when using [CODE] with input [CODE] being a pandas dataframe with column names having int dtype. I give an example below, and an example use case can be found in soda-inria\/tabicl#2. The problem comes from the fact that [CODE] interprets integers as column positions, while here the integers are the columns names. In my example below, [CODE] returns [CODE] which is interpreted as positions by [CODE] while the admissible positions for [CODE] are in [CODE]. A workaround for the user is to always convert names to positions prior to giving them to [CODE], like: [CODE_BLOCK] However I'm wondering if this is something that should be taken care of on the [CODE] side. Shouldn't [CODE] always interpret given column names as names, even when they are integers? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results ``` Traceback (most recent call last): File \"python3.13\/site-packages\/sklearn\/utils\/_indexing.py\", line 315, in _get_column_indices_for_bool_or_int idx = _safe_indexing(np.arange(n_columns), key) File \"python3.13\/site-packages\/sklearn\/utils\/_indexing.py\", line 270, in _safe_inde...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30983"
  },
  {
    "number":29079,
    "text":"Samples with nan distance are included in the computation of mean in [CODE] for uniform weights\n\n### Describe the bug The toy dataset and the distance computed by [CODE] are as follows. [CODE_BLOCK] When [CODE] is set to 'uniform', the second sample in [CODE] is _included_. See the code below. However, when [CODE] is set to 'distance', the second sample in [CODE] is _excluded_. This is because [CODE] where samples with nan distance are set to 0 when [CODE] is set to 'distance'. [URL] To takle this, we could also fill the nans with 0 when [CODE] is set to 'uniform'. [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:38:11) [Clang 14.0.6 ] executable: \/Users\/xxf\/miniconda3\/envs\/sklearn-env\/bin\/python machine: macOS-14.5-arm64-arm-64bit Python dependencies: sklearn: 1.6.dev0 pip: 23.2.1 setuptools: 68.0.0 numpy: 1.26.4 scipy: 1.13.0 Cython: 3.0.8 pandas: 2.1.0 matplotlib: 3.7.2 joblib: 1.3.0 threadpoolctl: 3.5.0 Built with OpenMP: True threa...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29079"
  },
  {
    "number":29085,
    "text":"[CODE] allow processing nan values\n\n### Describe the workflow you want to enable In some cases (for example memory-based collaborative filtering: ''' Modified correlation function that handles nan values. If it's impossible to to calculate the distance, it returns 2 - the maximum possible value. ''' cond = ~(np.isnan(a) | np.isnan(b)) # in case if there are only two # observations it's impossible # to compute coorrelation coeficient # it's invalid case - so we return # the biggest possible distance if sum(cond) <=1: return 2 a_std = a[cond].std() b_std = b[cond].std() # Pearson coefficient uses standard # deviations in the denominator, so # if any of them is equal to zero, # we have to return the biggest # possible distance. if a_std==0 or b_std==0: return 2 return orig_correlation(a[cond],b[cond]) example_array = np.array( [[ 5., 2., 4., 6., 5., 4., 6., 6., 7., 6.], [ 6., np.NaN, 4., 7., 5., 4., 8., 5., 7., 6.], [ 7., 10., 1., np.NaN, 9., 7., np.NaN, 3., np.NaN, 8.], [np.NaN, 2., 4., np.NaN, 4., 4., 7., 7., np.NaN, 6.], [ 8., 1., np.NaN, 7., 6., 2., 2., 8., 2., 1.], [ 8., np.NaN, np.NaN, np.NaN, 8., 7., 7., 4., 10., 9.], [ 8., 1., 6., 8., 5., 2., 2., np.NaN, 3., 1.], [ 6., np.NaN, 0., 5., 9., 7., 7., 3., 9., 6.], [np.NaN, 1., 7., 8., 5., 2., np.NaN, np.NaN, np.NaN, 1.], [ 8., 1., 7., ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29085"
  },
  {
    "number":29366,
    "text":"received ImportError: cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation'\n\n### Describe the bug When I'm importing KMeans from sklearn.cluster, I received this import error saying that cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' Currently I'm using the latest version for sklearn and Python 3.10. ### Steps\/Code to Reproduce [CODE] ### Expected Results NO ERROR ### Actual Results ``[CODE]sklearn.cluster` module gathers popular unsupervised clustering 3 algorithms. 4 \\\"\\\"\\\" 6 from ._affinity_propagation import AffinityPropagation, affinity_propagation ----> 7 from ._agglomerative import ( 8 AgglomerativeClustering, 9 FeatureAgglomeration, 10 linkage_tree, 11 ward_tree, 12 ) 13 from ._bicluster import SpectralBiclustering, SpectralCoclustering 14 from ._birch import Birch File ~\/...\/lib\/python3.10\/site-packages\/sklearn\/cluster\/_agglomerative.py:42 40 # mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast' 41 from . import _hierarchical_fast as _hierarchical # type: ignore ---> 42 from ._feature_agglomeration import AgglomerationTransform 44 ############################################################################### 45 # For non fully-connected graphs 48 def _fix_connectivity...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29366"
  },
  {
    "number":27375,
    "text":"Adding documentation about related library: Concrete ML, for privacy preserving ML\n\n### Describe the issue linked to the documentation We would like to add Concrete ML in the documentation. Concrete ML is an open-source package, providing privacy-preserving ML, thanks to so-called fully homomorphic encryption. Concrete ML contains so-called built-in models which are very very close to scikit-learn, XGB and skorch. Eg, see [URL] We've started to discuss with some of you, who advised me to go for an issue. I would propose: - [x] adding a category Privacy Preserving Machine Learning in .\/doc\/related_projects.rst, and adding Concrete ML inside --- done, this was [URL] - [x] adding a category Privacy Preserving Machine Learning in examples, and copy some of the examples of [URL] or make new ones (what you prefer!) [not to be done] - [ ] maybe propose a blog post, if you guest us. Will be done soon, [URL] and we are actually open to lot of suggestions. What do you think? ### Suggest a potential alternative\/fix Adding some content as proposed in the description.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27375"
  },
  {
    "number":24515,
    "text":"BUG log_loss renormalizes the predictions\n\n### Describe the bug [CODE] renormalizes [CODE] internally such that it sums to 1. This way, a really bad model, the predictions of which do not sum to 1, gets a better loss then it actually has. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] Result: [CODE] ### Actual Results Result: [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24515"
  },
  {
    "number":27447,
    "text":"Accept pathlib.Path for data_home in fetch_openml\n\n### Describe the workflow you want to enable When using [CODE] it would be nice if [CODE] objects were supported. Currently, there is a type check for [CODE], so I have to convert my path objects first. ### Describe your proposed solution Change the accepted type to [CODE] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27447"
  },
  {
    "number":25846,
    "text":"cross validation with Sklearn pipeline using custom data preprocessing\n\n### Describe the bug I am using this custom sklearn pipeline: ```python import numpy as np import pandas as pd from sklearn.model_selection import cross_validate from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import KFold, StratifiedKFold class MisCare(BaseEstimator, TransformerMixin): def __init__(self, missing_threshold): self.missing_threshold = missing_threshold def fit(self, X, y=None): self.columns_with_gap_ = X.columns[X.isna().sum() > int(self.missing_threshold*X.shape[0])] self.mode_ = X.mode().loc[0, :] return self def transform(self, X, y=None): tmp = X.copy() tmp.loc[:, self.columns_with_gap_] = tmp.loc[:, self.columns_with_gap_].fillna('GAP') tmp.fillna(self.mode_, inplace=True) return tmp def get_columns_with_gap(self): return self.columns_with_gap_ def get_columns_mode(self): return self.mode_ class ConstantCare(BaseEstimator, TransformerMixin): def fit(self, X, y=None): # Store the columns with non-constant values during the fitting stage self.columns_to_keep_ = X.loc[:, X.nunique() != 1].columns return self def transform(self, X, y=None): # Keep only the non-constant columns in the input data return X.loc[:, self.columns_to_keep_] def get_columns_to_keep(self): return self.columns_to_keep_ class CustomOneHotEncoder(OneHotEncoder): def __init__(self, kwargs): super().__in...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25846"
  },
  {
    "number":29989,
    "text":"GaussianMixture log-probabilities are numerically inaccurate\n\n### Describe the bug While building an extension to (already fitted) GaussianMixture models ([URL] I was using [URL] for testing and came across numerical inaccuracies in the log-probabilities computed by scikit-learn's GaussianMixture. These already occur with single components, where one can take scipy.stats.multivariate_normal.logpdf as the ground truth. The inaccuracies appear in the function [CODE]. The log-probabilities can be off by 0.2 (see the very last example), which really is not small if those probabilities are used for likelihoods. I uploaded the full script at [URL] but the important excerpts are below. A unrelated issue I have is that I want to build sklearn.mixture.GaussianMixture from scratch without fitting it, but .score and .predict_proba always give 0 and 1, respectively. Is there a initialization step I am missing to be allowed to use these functions? EDIT (by @ogrisel): here is a copy of the reproducer (to make this report self-contained): <details> ```python import numpy as np from numpy import array from scipy.stats import multivariate_normal from scipy.special import logsumexp from numpy.testing import assert_allclose from hypothesis import given, strategies as st, example, settings from hypothesis.extra.numpy import arrays import sklearn.mixture from sklearn.mixture._gaussian_mixture import _estimate_log_gaussian_prob def valid_QR(vectors): q, r = np.linalg.qr(vectors) return q.shape == ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29989"
  },
  {
    "number":24792,
    "text":"ValueError: The loss log_loss is not supported.\n\n### Describe the bug I have this error, and I can't find a solution when using SGDClassifier with the loss=\"log_loss\" parameter to take advantage of online learning as LR ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results train with log_loss ### Actual Results [CODE_BLOCK] ### Versions ```shell C:\\Users\\PC>python -m pip install --upgrade sklearn Requirement already satisfied: sklearn in c:\\users\\pc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0) Requirement already satisfied: scikit-l...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24792"
  },
  {
    "number":28525,
    "text":"Problem of get_params attribute in skleran0.20.3\n\n### Describe the bug Greetings I'm using Windows 7 and Orange 3.20.1 with sklearn 0.20.3 . I have generated some optimised ann models without knowing that this version of sklearn seems not to support get_params attribute and I have faced difficulties in extracting the weights and biases of the trained models. After exploring the internet I found that the latest sklearn 1.4 covers this property. Now due to the fact that generated models are not backward compatible considering this versions. I would be pleased if someone can guide me through determining the weights and the biases of the models trained in sklearn 0.20.3 without getting into trouble of training thousands of models again in sklearn 1.4. Great thanks ### Steps\/Code to Reproduce import pickle import sklearn # Load the model from a file model=pickle.load(open(\"d:\\\\1.pkcls\",\"rb\")) #params = model.get_params() params = model.get_params() ### Expected Results weights and biases of the model ### Actual Results Traceback (most recent call last): File \"<console>\", line 1, in <module> File \"<string>\", line 9, in <module> AttributeError: 'SklModelClassification' object has no attribute 'get_params' ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28525"
  },
  {
    "number":26548,
    "text":"Using [CODE] with [CODE] and floats raises an error\n\n### Describe the bug Using [CODE] with [CODE] raises an error if the array [CODE] contains floats. It does not seem to raise errors if [CODE] consists of integers. This was originally discussed in [URL] For example, this is fine: [CODE_BLOCK] I would expect this behavior whether [CODE] consists of floats or integers. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ```python --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[40], line 6 4 neigh = NearestNeighbors(algorithm='brute',metric_params={'p':0.5}) 5 neigh.fit(X) ----> 6 neigh.radius_neighbors(X[0].reshape(1,-1), radius=4, return_distance=False) File ~\/miniconda3\/envs\/dimmer_env\/lib\/python3.9\/site-packages\/sklearn\/neighbors\/_base.py:1161, in RadiusNeighborsMixin.radius_neighbors(self, X, radius, return_distance, sort_results) 1153 use_pairwise_distances_reductions = ( 1154 self._fit_method == \"brute\" 1155 and RadiusNeighbors.is_usable_for( 1156 X if X is not None else self._fit_X, self._fit_X, self.effective_metric_ 1157 ) 1158 ) 1160 if use_pairwise_distances_reductions: -> 1161 results = RadiusNeighbors.compute( 1162 X=X, 1163 Y=self._fit_X, 11...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26548"
  },
  {
    "number":24752,
    "text":"Fix scaling of LogisticRegression objective for LBFGS\n\n## Description The objective function of [CODE] is [CODE]. For LBFGS, the reformulation (having the same argmin) is much more favorable: [CODE]. Note that the division by 1\/C is already done for all solvers but [CODE]. ## Proposed Action Similar to [CODE]_logistic_regression_path[CODE]``python import warnings from pathlib import Path import numpy as np from sklearn.metrics import log_loss from sklearn.compose import ColumnTransformer from sklearn.datasets import fetch_openml from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import FunctionTransformer, OneHotEncoder from sklearn.preprocessing import StandardScaler, KBinsDiscretizer from sklearn.exceptions import ConvergenceWarning from sklearn.model_selection import train_test_split from sklearn._loss import HalfBinomialLoss from sklearn.linear_model._linear_loss import LinearModelLoss from sklearn.linear_model._glm.glm import _GeneralizedLinearRegressor from time import perf_counter import pandas as pd import joblib class BinomialRegressor(_GeneralizedLinearRegressor): def _get_loss(self): return HalfBinomialLoss() @joblib.Memory(location=\".\").cache def prepare_data(): df = fetch_openml(data_id=41214).frame df[\"Frequency\"] = df[\"ClaimNb\"] \/ df[...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24752"
  },
  {
    "number":30321,
    "text":"Error in impute\/_base.py _most_frequent when array contains None\n\n### Describe the bug TypeError: '<' not supported between instances of 'NoneType' and 'str' when calculating min in impute\/_base.py most_frequent_value = min( value for value, count in counter.items() if count == most_frequent_count ) when array has None value as the most frequent one. ### Steps\/Code to Reproduce array = numpy.array(['a','b',None,None]) ### Expected Results most frequent: 'a' imputed array: ['a','b','a','a'] ### Actual Results TypeError: '<' not supported between instances of 'NoneType' and 'str' COMMENT: fixed by changing two lines: #if array.size > 0: if np.array([a for a in array if a is not None]).size > 0: #counter = Counter(array) counter = Counter([a for a in array if a is not None]) ### Versions ```shell System: python: 3.12.3 (main, Nov 6 2024, 18:32:19) [GCC 13.2.0] executable: \/home\/padiadev\/venv\/bin\/python machine: Linux-6.8.0-45-generic-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.5.2 pip: 24.0 setuptools: None numpy: 1.26.4 scipy: 1.14.1 Cython: None pandas: 2.2.3 matplotlib: 3.9.2 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 2 prefix: libopenblas filepath: \/home\/padiadev\/venv\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: Haswell user_api: blas internal_api: openblas num_threads: 2 prefix: libsci...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30321"
  },
  {
    "number":28645,
    "text":"Support pyenv to manage development python version\n\n### Describe the workflow you want to enable The documentation conda; 2) or the system's python version (for linux users). My proposal would be to allow developers to also use the popular tool pyenv Mention in the documentation, or use a third-party tool like [CODE] to manage you python version. (...)` ### Describe alternatives you've considered, if relevant N\/A ### Additional context N\/A _Note:_ I'll be happy to contribute a PR for this is you believe that it could be useful.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28645"
  },
  {
    "number":25179,
    "text":"Improve error handling in _mutual_info.py\n\n### Describe the workflow you want to enable Hi all, I am currently trying to run the following code: [CODE_BLOCK] Which fails with: [CODE_BLOCK] The code does fail because in the file [CODE] all instances with a unique label are removed before the KDTree is called: ```python n_samples = c.shape[0] c = c.reshape((-1, 1)) radius = np.empty(n_samples) label_counts = np.empty(n_samples) k_all = np.empty(n_samples) nn = NearestNeighbors() for label in np.unique(d): mask = d == label count = np.sum(mask) if count > 1: k = min(n_neighbors, count - 1) nn.set_params(n_neighbors=k) nn.fit(c[mask]) r = nn.kneighbors()[0] radius[mask] = np.nextafter(r[:, -1],...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25179"
  },
  {
    "number":27777,
    "text":"HuberRegressor failed with ABNORMAL_TERMINATION_IN_LNSRCH on simple dataset\n\n### Describe the bug sklearn linear_model HuberRegressor fails with a very simple dataset. The code below fails with ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results no ValueError thrown ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.9.18 (main, Sep 11 2023, 13:30:38) [MS...",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27777"
  },
  {
    "number":31989,
    "text":"Implementing Divisive Analysis\n\n### Describe the workflow you want to enable I want to add Divisive Analysis Clustering to base scikit-learn in order to provide more options to developers. \"Divisive methods start when all objects are together (that is, at step 0 there is one cluster) and in each following step a cluster is split up, until there are _n_ of them.\" (Kaufman and Rousseeuw 1990). ### Describe your proposed solution Implement a class that performs divisive clustering extending BaseEstimatior and ClusterMixin. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context It has been implemented in R (cluster package). It's commonly used for marketing purposes and document and topic classification. Kaufman, L., & Rousseeuw, P. J. (1990). Finding Groups in Data. En Wiley series in probability and statistics. [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31989"
  },
  {
    "number":25213,
    "text":"Segmentation fault occurs when KernelPCA is loaded after torchvision\n\n### Describe the bug If KernelPCA is loaded after torchvision, then I get a segmentation fault. If I load them in the opposite order then I get no segmentaton fault. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No segmentation fault ### Actual Results Segmentation fault (core dumped) ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25213"
  },
  {
    "number":25637,
    "text":"Support nullable pandas dtypes in LabelBinarizer\n\n### Describe the workflow you want to enable I would like to be able to pass the nullable pandas dtypes (\"Int64\", \"Float64\", \"boolean\") into sklearn's LabelBinarizer. Because the dtypes become object dtype when converted to numpy arrays we get [CODE]: Repro with sklearn 1.2.1: [CODE_BLOCK] ### Describe your proposed solution We should get the same behavior as when int64, float64, and bool dtypes are used, which is no error: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Our current workaround is to convert the data to numpy arrays with the corresponding dtype that works prior to passing it into LabelBinarizer ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25637"
  },
  {
    "number":30279,
    "text":"Bad plotly figures rendering in the examples gallery\n\n### Describe the issue linked to the documentation Currently, I found 2 examples in the examples gallery of the scikit-learn documentation that use plotly instead of matplotlib for plots, for interactivity purposes: - URL] - [URL] [The PyData Sphinx Theme: [URL] This plotly rendering issue appears also in the official [Sphinx doc]([URL] ### Suggest a potential alternative\/fix Issues have been opened at - [URL] - [URL] Related: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30279"
  },
  {
    "number":26543,
    "text":"What happend to the idea of adding a 'handle_missing' parameter to the OneHotEncoder?\n\n### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by woodly0 June 7, 2023<\/sup> Hello, I'm having trouble understanding what finally happened to the idea of introducing a [CODE] parameter for the [CODE]. My current project could still benefit from such an implementation. There are many existing issues regarding this topic, however, I cannot deduct what was finally decided\/implemented and what wasn't. - #11996 - #12025 - #17317 - #23436 Considering the following features: [CODE_BLOCK] when using the encoder: [CODE_BLOCK] I get the output: [CODE_BLOCK] but what I'm actually looking for is to remove the [CODE], i.e. not create a new feature but set all the others to zero: [CODE_BLOCK] Is there a way to achieve this without using another transformer object?<\/div>",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26543"
  },
  {
    "number":24972,
    "text":"Dirichlet Multinomial Mixture Model\n\n### Describe the workflow you want to enable Is there an intention to implement the Dirichlet Multinomial Mixture Model with EM algorithm? Dirichlet Multinomial Mixture Model is a popular clustering model in NLP, Information Retrieval and Bioinformatics. ### Describe your proposed solution I have implemented a usable Dirichlet Multinomial Mixture Model with the Base Mixture class in scikit-learn. If it is needed, I can refine the doc and details and pull it to scikit-learn. [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24972"
  },
  {
    "number":25169,
    "text":"DOC: type annotation for model returned from fit methods is \"object\"\n\n### Describe the bug I noticed that the type annotation for model returned from fit methods is \"object\". This makes IDE like pycharm unable to perform type hints: ![image]([URL] ![image]([URL] ### Steps\/Code to Reproduce . ### Expected Results The type of returned model should be the class itself. ### Actual Results . ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25169"
  },
  {
    "number":29616,
    "text":"Student-t Mixture Model\n\n### Describe the workflow you want to enable Gaussian mixtures are extremely useful, but many datasets are noisy enough that a GMM fit can be challenging. In these cases, adding a degree of freedom by using a t distribution instead of a normal distribution can make fitting significantly simpler and easier. ### Describe your proposed solution In 2000, Peel & McLachlan published [Robust mixture modelling using the t distribution]([URL] This paper is the basis of [an unmaintained GitHub repo]([URL] containing an implementation that utilizes the existing scikit-learn [CODE] class infrastructure but is somewhat out of date. With some minor changes, that code in this repo has been extremely helpful with fitting some particularly noisy datasets that I need to routinely handle. It seems like this the sort of thing that would also be useful to others, especially since it fits wells into the current scikit-learn design. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I'd be happy to develop a PR to integrate the [CODE] code into the scikit-learn codebase, but I thought it would be a good idea to get feedback before starting that work.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29616"
  },
  {
    "number":28381,
    "text":"Missing import at documentation preprocessing.rst\n\n### Describe the issue linked to the documentation Missing import to sklearn.preprocessing [CODE_BLOCK][CODE]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28381"
  },
  {
    "number":29353,
    "text":"kernel_approximation.Nystroem with precomputed kernel\n\n### Describe the bug I am trying to get a Nystroem approximation of a pre computed kernel but it throws an error if I use n_components anything less than the number of datapoints. Unless my understanding is wrong, does this not defeat the point of the approximation? Please advise, code below: I have come from this resolved issue [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect this to work. ### Actual Results Instead it gives error: [CODE_BLOCK] ### Versions ```shell System: python: 3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0] executable: \/opt\/conda\/miniconda3\/envs\/python3.8\/bin\/python machine: Linux-6.1.0-21-cloud-amd64-x86_64-with-glibc2.17 Python dependencies: pip: 24.0 setuptools: 63.1.0 sklearn: 0.24.2 numpy: 1.21.6 scipy: 1....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29353"
  },
  {
    "number":27692,
    "text":"Typo error in readme file\n\n### Describe the issue linked to the documentation Found a small typo error under the readme file. The text mentions \"If you already have a working installation of numpy and scipy,\" but it should be \"If you already have a working installation of NumPy and SciPy,\" with \"NumPy\" and \"SciPy\" capitalized correctly. Should look like: \"If you already have a working installation of NumPy and SciPy, the easiest way to install scikit-learn is using pip.\" ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27692"
  },
  {
    "number":27767,
    "text":"EFF Reduce the size of shared objects of the C-extensions generated by Cython\n\n### Context scikit-learn uses C-extensions in critical part of its implementations via Cython. Each C-entension is build from one or several Cython translation unit (a [CODE] file with a potential [CODE] companion file). In scikit-learn, each C-extension build consists of a single Cython translation which is transpilled to a C or C++ translation unit, which is then compiled to a shared object file. The resulting C or C++ translation unit contains the code translation from Cython to C and large preambule and epylogue of macros, functions, structs, global variables such as virtual tables, Python module definition, etc. For instance, while the code of [[CODE]]([URL] only consists of less than 100 lines for a single function, the resulting [CODE] file consists of more than 3500 lines, most of being the preambule's and the epilogue's injected by Cython: <details> <summary> Content of the generated <code>sklearn\/utils\/heap.c<\/code> <\/summary> ``` \u25be macros -CYTHON_ABI -CYTHON_ASSUME_SAFE_MACROS -CYTHON_ASSUME_SAFE_MACROS -CYTHON_ASSUME_SAFE_MACROS -CYTHON_ASSUME_SAFE_MACROS -CYTHON_AVOID_BORROWED_REFS -CYTHON_AVOID_BORROWED_REFS -CYTHON_AVOID_BORROWED_REFS -CYTHON_AVOID_BORROWED_REFS -CYTHON_COMPILING_IN_CPYTHON -CYTHON_COMPILING_IN_CPYTHON -CYTHON_COMPILING_IN_CPYTHON -CYTHON_COMPILING_IN_CPYTHON -CYTHON_COMPILING_IN_NOGIL -CYTHON_COMPILING_IN_NOGIL -CYTHON_COMPILING_IN_NOGIL -CYTHON_COMPILING_IN_NOGIL -...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27767"
  },
  {
    "number":25095,
    "text":"\u26a0\ufe0f CI failed on Linux_nogil.pylatest_pip_nogil \u26a0\ufe0f\n\nCI failed on Linux_nogil.pylatest_pip_nogil - test_balance_property[74-False-LogisticRegressionCV]",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25095"
  },
  {
    "number":27993,
    "text":"[RFC] Allow handling of NaNs in multi-task Random Forests\n\n### Describe the workflow you want to enable Currently the RFR implementation is only capable of handling dense multi-task problems is there any scope to change the underlying algorithm to handle NaNs as a special case or would this break the API given that it might extend to allowing NaNs in the single class case? The hypothesis is that I may benefit from knowing the information about the available labels in the multi-task model even though I am missing labels for some tasks. ### Describe your proposed solution boolean flag in the constructor that allows NaNs to be ignored when calculating the impurity. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context ```python >>> import numpy as np >>> from sklearn.ensemble import RandomForestRegressor >>> rfr=RandomForestRegressor() >>> X = np.random.randn(5,10) >>> y = np.random.randn(5,2) >>> y[4,1]=np.nan >>> rfr.fit(X, y) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"\/Users\/chemix-rhys\/opt\/miniconda3\/envs\/chemix-py310\/lib\/python3.10\/site-packages\/sklearn\/base.py\", line 1151, in wrapper return fit_method(estimator, *args, kwargs) File \"\/Users\/chemix-rhys\/opt\/miniconda3\/envs\/chemix-py310\/lib\/python3.10\/site-packages\/sklearn\/ensemble\/_forest.py\", line 348, in fit X, y = self._validate_data( File \"\/Users\/chemix-rhys\/opt\/miniconda3\/envs\/chemix-py310\/lib\/python3.10\/site-packages\/sklearn\/base.py\", line 621, i...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27993"
  },
  {
    "number":29117,
    "text":"Request to update \"Choosing the Right Estimator\" Graphic (scikit-learn algorithm cheat sheet)\n\n### Describe the issue linked to the documentation As in the documented map here - [URL] The path after [CODE] (When \"YES\") goes to both classification & Regression. Also, Path after answering [CODE] goes to only one direction (Missing path to regression). ![Issue]([URL] ### Suggest a potential alternative\/fix #### It seems to be a [CODE] mistake, please change or update it accordingly",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29117"
  },
  {
    "number":30893,
    "text":"The [CODE] parameter for lasso regression can only be a [CODE]\n\n### Describe the issue linked to the documentation [URL] The line \"If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.\" is found under the \"Notes\" section. However, in the parameters section and the source, alpha is listed as float, default=1.0 ### Suggest a potential alternative\/fix The line should be deleted. Better yet, allow [CODE] to be an array, like for [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30893"
  },
  {
    "number":29504,
    "text":"Get error \"ValueError: Input contains NaN\" when MLP regression model is exploding numerically and when early_stopping=True\n\n### Describe the bug Hello, I was doing dummy tests with a very basic MLP regressor, and I got an error I was not expecting: \"ValueError Input contains NaN\". Full traceback: ``` File ~\/Documents\/o2_ml_2\/.venv\/lib\/python3.10\/site-packages\/sklearn\/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, kwargs) 1466 1468 1472: -> 1473 File ~\/Documents\/o2_ml_2\/.venv\/lib\/python3.10\/site-packages\/sklearn\/neural_network\/_multilayer_perceptron.py:752, in BaseMultilayerPerceptron.fit(self, X, y) [734]([URL]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29504"
  },
  {
    "number":31884,
    "text":"pairwise_distances_argmin_min \/ ArgKMin64 is not thread-safe\n\n### Describe the bug Problem found while test investigating failures found in #30041. I crafted a minimal reproducer below. It might be caused by a race condition (corruption) of shared intermediate buffers used in OpenMP threads. Some remarks: - the problem happens with either strategy (\"parallel_on_X\" vs \"parallel_on_Y\"); - running the reproducer with [CODE] hides the problem; - running the reproducer with a lower than default value for [CODE] makes the problem less likely to happen; - using [CODE] does not hide the problem for some reason... ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error. ### Actual Results ```python-traceback --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) Cell In[20], line 27 25 if shared_kwargs[\"return_di...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31884"
  },
  {
    "number":28310,
    "text":"[CODE] variance prediction fails on [CODE]\n\n### Describe the bug CODE] fails if [CODE] and [CODE] is [CODE]. The failure occurs at the line [CODE]. The problem occurred while writing an adapter in [CODE] and testing API contracts, see here: [URL] It seems surprising that the combination of [CODE] and [CODE] input is not strictly tested in [CODE]? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] does not fail and produces interface conformant predictions (a duple) ### Actual Results ``` --------------------------------------------------------------------------- TypeError Traceback (most recent call last) File [~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3802 3801 try: -> 3802 return self._engine.get_loc(casted_key) 3803 except KeyError as err: File index.pyx:153, in pandas._libs.index.IndexEngine.get_loc() File index.pyx:159, in pandas._libs.index.IndexEngine.get_loc() TypeError: '(slice(None, None, None), array([ True, True, True, True, True, True, True, True, True, True]))' is an invalid key During handling of th...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28310"
  },
  {
    "number":25963,
    "text":"RFC 0% tolerance for Codecov patch\n\nWe currently have a 1% tolerance on Codcov for the project and for each patch. Although having a tolearance on the project seems legit, we usually target a 100% coverage on PRs. The results is that sometimes all checks are green and you don't realize that you have uncovered lines unless you explicitly open the codcov tab. With @glemaitre we propose to set a 0% tolerance for the PRs. When there are lines that we know we can't cover, because it can only be triggered by a specific architecture or by a specific version of a dependency for instance, then we explicitly add a `[CODE]`.",
    "labels":[
      "RFC",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25963"
  },
  {
    "number":26182,
    "text":"CircleCI token in README broken\n\n### Describe the issue linked to the documentation <img src=\"[URL] width=\"80%\"> As is shown above, it seems that the CircleCI token is not generating correctly. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26182"
  },
  {
    "number":29523,
    "text":"KNNImputer - output shape not equal input shape\n\n### Describe the bug The output of the fit_tranform is not equal to the input shape, when the NaN's are all in one column ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] ### Actual Results [CODE] ### Versions ```shell System: python: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] executable: \/scratch\/miniconda3\/envs\/tls2image\/bin\/python machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.5.1 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.26.4 scipy: 1.13.0 Cython: None pandas: 2.2.1 matplotlib: 3.8.4 joblib: 1.4.0 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: \/scratch\/miniconda3\/envs\/tls2image\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: Zen user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: \/scratch\/miniconda3\/envs\/tls2image\/lib\/python3.11\/site-packages\/scipy.libs\/libopenblasp-r0-24bff013.3.26.dev.so version: 0.3.26.dev threading_layer: pthreads architecture: Zen user_api: openmp internal_api: openmp num_threads: 16 prefix: libgomp filepath: \/scratch\/miniconda3\/envs\/tls2image\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0 version: None user_api: blas internal_api: openblas num_threads: 1 prefix: libopenblas...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29523"
  },
  {
    "number":31039,
    "text":"RFC Move SLEPs to the main scikit-learn website\n\n## Background The website for scikit-learn enhancement proposals (SLEP) at [URL] is very hard to find if you don't know what you are looking for. A second difficulty is to know which SLEP is (fully) implemented in which scikit-learn release, see, e.g., #31037. ## Proposition Move SLEP website to the main scikit-learn website at [URL] @scikit-learn\/core-devs @scikit-learn\/communication-team @scikit-learn\/documentation-team ping",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31039"
  },
  {
    "number":27087,
    "text":"Add precompute in MultiTask Linear Models\n\n### Describe the workflow you want to enable Adding a 'precompute' parameter into constructor of MultiTaskElasticNet, MultiTaskLasso, MultiTaskElasticNetCV, and MultiTaskLassoCV in linear_model\/_coordinate_descent.py This parameter is only present in SingleTask models(ElasticNet, Lasso, LinearModelCV, LassoCV, ElasticNetCV), and has shown significant improvements in training speed. ### Describe your proposed solution In linear_model\/_coordinate_descent.py, a function cd_fast.enet_coordinate_descent_multi_task() in enet_path() will be executed when running these MultiTask models. However, this function does not have a precompute version in the cd_fast.pyx. So the solution would be implementing a new function within linear_model\/_cd_fast.pyx. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Is this because it's complicated or not feasible in mathematics?",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27087"
  },
  {
    "number":32109,
    "text":"Add inner max_iter or a smart automatic setting to Lasso inside graphical lasso\n\n[CODE] and [CODE] expose [CODE]. They should also expose [CODE]. Currently, the [CODE] of the outer iteration is also used for this inner iteration. This is unfortunate, e.g., if you set a small number of outer iterations. Popped up in [URL]",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32109"
  },
  {
    "number":26324,
    "text":"OPTICS not detecting last data as outlier\n\nThis needs investigation. Thanks for reporting it @yagao7411 ### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by yagao7411 April 30, 2023<\/sup> Hi, I am trying to use sklearn.cluster.OPTICS, but found an issue: I use 2 examples with exactly the same data but different orders. They give different results: 1) 1st example [CODE_BLOCK] 2) 2nd example [CODE_BLOCK] We can see X has the same data but different orders. The 2nd output is supposed to be correct as [100] should be an outlier. But oddly, if we change the order of the data, the model gave wrong results. Can anyone help? thanks Ya<\/div>",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26324"
  },
  {
    "number":25165,
    "text":"ValueError: buffer source array is read-only In DictionaryLearning using coordinate decent, numworkers = 15\n\n### Describe the bug I get \"source array is read only\" error in Dictionary Learning when I try to fit it to the Cifar10 Dataset. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The dictionary ### Actual Results ```pytb File \"sklearn\\linear_model\\_cd_fast.pyx\", line 568, in sklearn.linear_model._cd_fast.enet_coordinate_descent_gram File \"stringsource\", line 660, in View.MemoryView.memoryview_cwrapper File \"stringsource\", line 350, in View.MemoryView.memoryview.__cinit__ ValueError: buffer source array is read-only \"\"\" The above exception was the direct cause of the following exception: ValueError Traceback (most recent call last) Cell In [16], line 13 4 #out = dict_learning_online(x_train[np.random.choice(a=50000, size=10000)].reshape(10000, 3072), n_components=3072) 5 dict_learner = DictionaryLearning( 6 n_components= 56, 7 random_state=0, (...) 11 max_iter= 50 12 ) ---> 13 s_train = dict_learner.fit_transform(x_train.reshape(50000, 3072)) File c:\\Users\\shafi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142, in _wrap_method_output.<locals>.wrapped(self, X, args, kwargs) 140 @wraps(f) 141 def wrapped(self, X, args, kwargs): --> 142 data_to_wrap = f(self, X, args, *kwargs) 143 if isinstance(data_to_wrap, tuple): 144 # only wrap the first output for cross decomposition 145 return ( 146 _wrap_data_with_container(method...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25165"
  },
  {
    "number":29604,
    "text":"mRMR feature selector\n\n### Describe the workflow you want to enable Consider adding minimum redundancy maximum relevance (mRMR) feature selector to sklearn.feature_selection. ### Describe your proposed solution mRMR as another feature selector. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29604"
  },
  {
    "number":28976,
    "text":"[CODE] in HDSCAN\n\n### Describe the issue linked to the documentation I find the description of the [CODE] argument in sklearn.cluster.HDBSCAN confusing. It says \"The number of samples in a neighborhood for a point to be considered as a core point. This includes the point itself.\" But if I understand everything correctly [CODE] corresponds to the $k$ used to compute the core distance $\\text{core}_k\\left(x\\right)$ for every sample $x$ where the $k$'th core distance for some sample $x$ is defined as the distance to the $k$'th nearest-neighbor of $x$ (counting itself). (-> which exactly what is happening in the code here: [URL] where it is called [CODE]) I don't understand how both of these descriptions are equivalent. I would assume that other people might find that confusing as well. Link in Code: [URL] Link in Documentation: [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28976"
  },
  {
    "number":30094,
    "text":"Implement [CODE] as a distinct variant of matrix decomposition useful for binary data\n\n### Describe the workflow you want to enable Currently, there is no included implementation of a PCA algorithm made for handling binary data in the scikit-learn library. However, the algorithm for \"logistic PCA\" is well founded, although different methods for its estimation exist, and it meets the inclusion criteria of being at least 3 years since first publication, having over 200 citations between known papers, and has a wide use and usefulness. The proposed implementation follows that of Lee et al. (2010), which allows for the use of an optional L1 regularization term. The intent is to create a [CODE] implementation that mimics the existing [CODE] implementation to the greatest degree possible acknowledging their important differences, and which utilizes the adopted best practices to fit within the existing matrix decomposition algorithms and the current API of scikit-learn. References Tipping, Michael E. \"Probabilistic visualisation of high-dimensional binary data.\" Advances in Neural Information Processing Systems (1999): 592-598. Lee, Seokho, Jianhua Z. Huang, and Jianhua Hu. \"Sparse logistic principal components analysis for binary data.\" The Annals of Applied Statistics, 4.3 (2010): 1579-1601. Landgraf, Andrew J. and Yoonkyung Lee. \"Dimensionality reduction for binary data through the projection of natural parameters.\" Journal of Multivariate Analysis, v.180 (2020): 104668. ### Desc...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30094"
  },
  {
    "number":24864,
    "text":"LogisticRegression coefficients should be stored in column major order to improve sparse inference performance.\n\n### Describe the workflow you want to enable I want to enable the workflow of training a [CODE] model on sparse data, with substantial numbers of features and classes, with acceptable performance. Currently, inference for a [CODE] model slows rather substantially when using sparse features since the transpose of the coefficient matrix must be stored in row-major order to perform the sparse matrix multiply. Changing the ordering of the coefficient array improves the performance dramatically. This is illustrated in the below code snippet: [CODE_BLOCK] Additional information can be found [here]([URL] ### Describe your proposed solution My proposed solution is to ensure that the coeff...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24864"
  },
  {
    "number":29823,
    "text":"Misleading variable name for the example of AUC calculation\n\n### Describe the issue linked to the documentation In the [example of AUC calculation]([URL] it was given that: [CODE_BLOCK] Readers will assume that [CODE] is the prediction value. In fact, it should be the prediction probabilities, as required by [CODE]. ### Suggest a potential alternative\/fix Instead of using [CODE] and [CODE], giving the same name as required by [[CODE]]([URL] would be helpful. [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29823"
  },
  {
    "number":30832,
    "text":"Numpy Array Error when Training MultioutputClassifer with LogisticRegressionCV with classes underrepresented\n\n### Describe the bug When I train the MultioutputClassifer with LogisticRegressionCV with classes underrepresented, I get the following numpy error. I think this is connected to the issue #28178 and #26401. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results 1.6.1 ### Actual Results 1.6.1 ```pytb .venv\/lib\/python3.12\/site-packages\/sklearn\/model_selection\/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5. warnings.warn( Traceback (most recent call last): File \"error_skitlearn.py\", line 14, in <module> model.fit(X, y) File \".venv\/lib\/python3.12\/site-packages\/sklearn\/multioutput.py\", line 543, in fit super().fit(X, Y, sample_weight=sample_weight, fit_params) File \".venv\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1389, in wrapper return fit_method(estimator, *args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \".venv\/lib\/python3.12\/site-packages\/sklearn\/multioutput.py\", line 274, in fit self.estimators_ = Parallel(n_jobs=self.n_jobs)( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \".venv\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py\", line 77, in __call__ return super().__call__(iterable_with_config) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \".venv\/lib\/python3.12\/site-packages\/joblib\/parallel.py\", line 1918, in __call__ return output if self.return_generator else list(output) ^^^^^^^^^^^^ F...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30832"
  },
  {
    "number":31218,
    "text":"Add P4 classification metric\n\n### Describe the workflow you want to enable Hi, while working on a classification problem I found out there is no dedicated function to compute the P4 metric implemented in sklearn. As a reminder, P4 metrics is a binary classification metric that is commonly seen as an extension of the f_beta metrics because it takes into account all four True Positive, False Positive, True Negative and False Negative values, and because is it symmetrical unlike the f_beta metrics. P4 is defined as follows : P4 = 4 \/ ( 1\/precision + 1\/recall + 1\/specificity + 1\/NPV ) Wikipedia page right here. The function would return the tuple (p4_value, support) A second function [CODE] which would be the one actually used by users would return only the first element of the previously described [CODE] function. ### Describe alternatives you've considered, if relevant Extras : Since specificity and NVP are computed anyway, the [CODE] function could return the tuple (specificity, NVP, p4_sco...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31218"
  },
  {
    "number":27881,
    "text":"[RFC] Leaf Level Variance in Multi Output Decision Trees\n\n### Describe the workflow you want to enable For single output RFR trained with the squared error criterion the impurity of the leaves can be used as a crude but useful estimate of the aleatoric uncertainty. In the multi output case the impurity is the sum over outputs hence this is no longer possible to estimate. ### Describe your proposed solution Currently we can access the node values from [CODE] that stores the leaf means, I propose adding another data store or [CODE] that would store the square means of the leaves. Using these it is possible to work out the variance within the leaf that can then be used as a crude but useful estimate of the aleatoric uncertainty. The added benefit of doing this would be that this could be calculated for any choice of criterion rather than just the squared error. The principal issue is that it would double the amount of data stored. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context This measure of the aleatoric uncertainty is widely used in the work of frank hutter in more recent versions of SMAC - [URL]",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27881"
  },
  {
    "number":28169,
    "text":"RFC Expose a plublic method to compute the objective function\n\nI think that it would be valuable that all estimators that optimize some objective function expose a public method to compute it. My main motivation is for the callbacks, for early stopping or monitoring, but I'm sure it would be useful in other contexts. To really be practical, the signature should be the same across all estimators. What I have in mind is something like: [CODE_BLOCK] If we want to compute the objective function of the training set during fitting, we could allow to provide the current variables that are required to compute it at a given iteration, encapsulated in a single argument (a dict): [CODE_BLOCK] where the content of fit_state would be estimator specific, and detailed in the docstring of [CODE] for each estimator.",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28169"
  },
  {
    "number":26428,
    "text":"[CODE] should shuffle features when exploring for new splits\n\n### Summary Our implementation of [CODE] does not shuffle the feature at each node to find the best split. Note that our [CODE], [CODE], and [CODE] use a Fisher-Yates permutation. Not permuting features introduces a bias that can have some impact. For instance, it will impact the inspection and could lead to misinterpretation if one does not know about this implementation detail. Below, I show an example where correlated features will have close importance in the case of a [CODE]. For [CODE], a single feature (in an arbitrary manner) will be used and thus only a single feature is reported as important. ### Reproducer [CODE_BLOCK] ![image]([URL]",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26428"
  },
  {
    "number":26799,
    "text":"C++ back-end library for scikit-learn ?\n\n### Describe the workflow you want to enable Hi, This question is related to the design of a clear separation of back-end computational code of algorithms and the front-end code of plotting\/data access\/doc\/tests\/benchmarks. The two environments are clearly not related. The back-end needs a lot of technicality\/math\/scientific knowledge while the front-end needs a smart understanding of user activities (Culture, GUIs, UX, doc, support) etc. Scikit-learn is, IMHO, one of the best implementations of machine learning software. It relies internally of best state of the art algorithms and includes some of their best implementations in C++ (libsvm etc) and has one the most fluent while quite simple APIs (fit\/predict). My question is about the existence of plans to implement a well defined C++ library implementing the back-end including the algorithms and the common API in a Domain Specific Language (DSL). This library can directly be used in all front-end activities, easily imported in other languages (think a version of scikit-learn for R, a blasphem ?), or other languages like Java, Rust, Perl, SQL, etc. For python users, this separation makes it also possible to be sure, for example, that no GIL is to be cared of in computational codes. I know this is a wishful thinking, a dream. It can be technically complex, take months for a team to put in place, but is also a common practice for software professionals where usually the back-end and fron...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26799"
  },
  {
    "number":30761,
    "text":"Intermittent HTTP 403 on fetch_california_housing and other Figshare hosted data on Azure CI\n\nAlready noticed in URL] This seems to happen from time to time in doctests ([build log UNEXPECTED EXCEPTION: <HTTPError 403: 'Forbidden'> Traceback (most recent call last): File \"\/usr\/share\/miniconda\/envs\/testvenv\/lib\/python3.13\/doctest.py\", line 1395, in __run exec(compile(example.source, filename, \"single\", ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ compileflags, True), test.globs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"<doctest getting_started.rst[33]>\", line 1, in <module> File \"\/usr\/share\/miniconda\/envs\/testvenv\/lib\/python3.13\/site-packages\/sklearn\/utils\/_param_validation.py\", line 218, in wrapper return func(args, *kwargs) File \"\/usr\/share\/miniconda\/envs\/testvenv\/lib\/python3.13\/site-packages\/sklearn\/datasets\/_california_housing.py\", line 177, in fetch_california_housing archive_path = _fet...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30761"
  },
  {
    "number":30354,
    "text":"Enhance \"Choosing the Right Estimator\" Graphic (scikit-learn algorithm cheat sheet)\n\n### Describe the issue linked to the documentation In its user guide, scikit-learn offers a Choosing the right estimator, - if it can be paralleled, - etc (full proper list to be determined). EDIT: - The scikit-learn graph \/ map is great, but not sufficient IMHO because I would like to have, for each estimator, if I need to normalize the data or not, etc -> guidelines for each estimator - I would like a table that is separate from the map, this is also a cheat sheet but not to appear on the map, maybe at the bottom of the map on the same user guide page When discussing this with @jeromedockes and @Vincent-Maladiere, they told me about scikit-learn's [estimator tags]([URL] such as [[CODE]]([URL] It seems that that knowledge is already partially in the tags. ### Suggest a potential alternative\/fix - Maybe scikit-learn could have a table in the user guide with guidelines for each estimator? - Maybe scikit-learn could hold more tags? And the table could be built from those tags?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30354"
  },
  {
    "number":29062,
    "text":"Don't refit in FixedThresholdClassifier when original model is already trained.\n\n### Describe the workflow you want to enable I wrote some code for a demo that looks like this: [CODE_BLOCK] The goal here is to log some statistics but I was suprised to see that this took over 2 minutes to run. Granted, I am not doing anything in parallel, but it's only 10000 datapoints that need to be predicted\/thredholded. So it felt like something was up. I figured I'd rewrite the code a bit and was able to confirm that, probably, the [CODE] is refitting the internal classifier internally. ```python n_steps = 200 metrics = [] for i in trange(1, n_steps): # classifier_other_threshold = FixedThresholdClassifier( # classifier, threshold=i\/n_steps, response_method=\"predict_proba\" # ).fit(X_train, y_train) y_pred = classifier.predict_proba(X_train)[:, 1] > (i \/ n_steps) metrics.append({ 'threshold': i\/n_steps, 'f1': f1_scor...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29062"
  },
  {
    "number":28837,
    "text":"Meson does not fully build the project in one go and need to be run twice ?\n\nTo reproduce (I use Meson commands directly below to show that is is not related to meson-python): [CODE_BLOCK] cc @eli-schwartz in case you have any suggestions on this. I don't quite understand why the files need to be rebuilt, [CODE] does not shed too much light on it. Here is the [CODE] output on the second build, it says [CODE] needs to be rebuilt although the first build did not think it needed to be rebuilt for some reason ... <details> <summary>\"ninja -d explain\" output for the second build<\/summary> ``[CODE]build\/cp312' ninja explain: output meson-test-prereq of phony edge with no inputs doesn't exist ninja explain: meson-test-prereq is dirty ninja explain: output meson-benchmark-prereq of phony edge with no inputs doesn't exist ninja explain: meson-benchmark-prereq is dirty ninja explain: restat of output sklearn\/metrics\/_dist_metrics.cpython-312-x86_64-linux-gnu.so.p\/sklearn\/metrics\/_dist_metrics.pyx.c older than most recent input \/home\/lesteve\/dev\/scikit-learn\/build\/cp312\/sklearn\/utils\/_typedefs.pxd (1713170316072356503 vs 1713170351662119783) ninja explain: sklearn\/metrics\/_dist_metrics.cpython-312-x86_64-linux-gnu.so.p\/sklearn\/metrics\/_dist_metrics.pyx.c is dirty ninja explain: sklearn\/metrics\/_dist_metrics.cpython-312-x86_64-linux-gnu.so.p\/sklearn\/metrics\/_dist_metrics.pyx.c is dirty ninja explain: sklearn\/metrics\/_dist_metrics.cpython-312-x86_64-linux-gnu.so.p\/meson-generated_sklearn_...",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28837"
  },
  {
    "number":27528,
    "text":"Extra plots in partial dependence plots\n\n### Describe the workflow you want to enable As discussed in #19410, there has been interest in including additional visualizations along with the partial dependence visualizations. Extra plots would aid in the interpretation of partial dependence plots. It would be low overhead for the user to specify \"hist\" as an argument and have the feature distribution plotted in the same figure as the partial dependence plot. This issue only addresses the suggestion to improve partial dependence plots, not ICE plots. ### Describe your proposed solution #27388 introduces three new parameters to the [CODE] method in the [CODE] class:  [CODE]: A string or list of strings specifying what type of extra plot to include in the partial dependence display.  [CODE]: A dictionary where the keys should match the plot type specified in [CODE] and the values are kwarg dictionaries for each plot type. * [CODE]: Only used in one-way partial dependence plots when the extra plot is a scatter plot. Their defaults are all [CODE], which does not affect the current PDP display behavior. The following image can be generated by simply adding: [CODE] to the [CODE] call. ![image]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27528"
  },
  {
    "number":24797,
    "text":"[CODE] in examples\n\n### Describe the issue linked to the documentation Some [CODE]s are still present in the dev documentation and need to be fixed. Here a list: - [x] [classification\/plot_lda_qda.html]([URL] [URL] - [x] [cluster\/plot_adjusted_for_chance_measures.html]([URL] [URL] - [x] [datasets\/plot_iris_dataset.html]([URL] [URL] - [x] [decomposition\/plot_pca_3d.html]([URL] [URL] - [x] [decomposition\/plot_pca_iris.html]([URL] [URL] - [x] [linear_model\/plot_lasso_and_elasticnet.html]([URL] [URL] - [x] [linear_model\/plot_ols_3d.html]([URL] [URL] - [x] [linear_model\/plot_omp.html]([URL] [URL] - [x] [linear_model\/plot_sgd_early_stopping.html]([URL] [URL] - [x] [mixture\/plot_concentration_prior.html]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24797"
  },
  {
    "number":25052,
    "text":"IterativeImputer has no parameter \"fill_value\"\n\n### Describe the workflow you want to enable In the first imputation round of [CODE], an initial value needs to be set for the missing values. From its [docs]([URL] > initial_strategy {\u2018mean\u2019, \u2018median\u2019, \u2018most_frequent\u2019, \u2018constant\u2019}, default=\u2019mean\u2019 > Which strategy to use to initialize the missing values. Same as the strategy parameter in SimpleImputer. I have set the initial strategy to [CODE]. However, I want to define this constant myself. So, as I look at the parameters for [CODE] I find [CODE]: >When strategy == \u201cconstant\u201d, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and \u201cmissing_value\u201d for strings or object data types. Based on this information, one would assume that [CODE] also has the parameter [CODE], but it does not. ### Describe your proposed solution The parameter [CODE] needs to be added to [CODE] for when [CODE] is set to [CODE]. If this parameter is added, please also allow [CODE] as [CODE], for optimal compatibility with decision tree-based estimators. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25052"
  },
  {
    "number":29514,
    "text":"Allow missing values in multioutput metrics\n\n### Describe the workflow you want to enable In many multioutput problems, for example in chemoinformatics, there are missing values in target values, because only some properties are actually measured. Currently, scikit-learn requires all values to be present, leading to a lot of duplicated code across molecular property prediction projects, see e.g. [Open Graph Benchmark evaluation code]([URL] Such evaluators are peppered with lines like: [CODE_BLOCK] Another example from my recent paper [is available here]([URL] [CODE_BLOCK] This is irrirating, since basically people are forced to manually reimplement the multioutput case, filtering out NaN values ### Describe your proposed solution I see two possible solutions: 1. Always use NaN-aware functions instead, e.g. [CODE], and possibly print a warning if NaN values are detected. This would be a behavior similar to transformers, e.g. [CODE] ([list from the docs]([URL] 2. Add a parameter whether to ignore NaN values, e.g. [CODE] with possible values \"raise\" and \"ignore\". The default value of \"raise\" would exactly follow the existing behavior, while \"ignore\" would simply omit NaN values ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29514"
  },
  {
    "number":26535,
    "text":"HDBSCAN with cython development version yields TypeError: 'float' object cannot be interpreted as an integer\n\nPlenty of these errors in scipy-dev, seen recently in URL] e.g. this [build def test_outlier_data(outlier_type): \"\"\" Tests if np.inf and np.nan data are each treated as special outliers. \"\"\" outlier = { \"infinite\": np.inf, \"missing\": np.nan, }[outlier_type] prob_check = { \"infinite\": lambda x, y: x == y, \"missing\": lambda x, y: np.isnan(x), }[outlier_type] label = _OUTLIER_ENCODING[outlier_type][\"label\"] prob = _OUTLIER_ENCODING[outlier_type][\"prob\"] X_outlier = X.copy() X_outlier[0] = [outlier, 1] X_outlier[5] = [outlier, outlier] > model = HDBSCAN().fit(X_outlier) sklearn\/cluster\/tests\/test_hdbscan.py:59: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ sklearn\/cluster\/_hdbscan\/hdbscan.py:822: in fit self.labels_, self.probabilities_ = tree_to_labels( sklearn\/cluster\/_hdbscan\/_tree.pyx:56: in sklearn.cluster._hdbscan._tree.tree_to_labels cpdef tuple tree_to_labels( sklearn\/cluster\/_hdbscan\/_tree.pyx:70: in sklearn.cluster._hdbscan._tree.tree_to_labels labels, probabilities = _get_clusters( _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ > is_cluste...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26535"
  },
  {
    "number":27968,
    "text":"DOC doc build sphinx version link out-dated again\n\n### Describe the issue linked to the documentation The link to the sphinx versions for doc build at the end of [Building the documentation]([URL] is again out-dated, with sphinx version unpinned in #27656. ![image]([URL] ![image]([URL] ### Suggest a potential alternative\/fix Maybe use this link instead? [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27968"
  },
  {
    "number":24355,
    "text":"[CODE] returns [CODE] for valid arrays of dtype [CODE]\n\n### Describe the bug [CODE] returns unknown for arrays of integers if they have a dtype of [CODE], when it should instead return a valid type. I would be happy to contribute a fix, but I'm not entirely sure how the failing condition should be corrected: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] Output is [CODE] whereas once converted to a list, the output is [CODE] as expected. ### Expected Results [CODE] returns a valid type ([CODE]). ### Actual Results This issue resulted in enigmatic errors when using dependent functions such as [CODE], e.g.: [CODE_BLOCK] ### Versions ```shell System: python: 3.8.0 (default, Oct 6 2020, 12:20:13) [GCC 6.5.0 20181026] executable: ~\/.pyenv\/versions\/wat\/bin\/python3 machine: Linux-4.15.0-132-generic-x86_64-with-glibc2.27 Python dependencies: sklearn: 1.1.2 pip: 22.0.4 setuptools: 58.1.0 numpy: 1.23.2 scipy: 1.6.0 Cython: 0.29.21 pandas: 1.1.3 matplotlib: 3.3.3 joblib: 1.0.0 threadpoolctl: 2.1.0 Bu...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24355"
  },
  {
    "number":27533,
    "text":"Better inference of the columns remainder dtype in [CODE] from [CODE]\n\nA typical use case is to fit a [CODE] on a pandas dataframe such as: [CODE_BLOCK] While investigating the [CODE], the columns exposed for [CODE] is a bit weird: [CODE_BLOCK] [CODE_BLOCK] In terms of UX, I assume that one would expect to get the feature names in the remainder instead of the feature indices. However, our API to pass the information of the columns in scikit-learn is pretty flexible: we can have missed type of slice, array of int, etc. I would suggest that when the columns passed by the user are of a single type, then we make sure that the column type of the [CODE] is of the same type: if only indices are passed, we show indices, if names are passed, we show names, and if boolean are passed, we show boolean. With mixed types, we cannot really decide and we can keep the current behaviour. On a UX perspective, I assume this is better (together with the change in #27204). Any thoughts @lorentzenchr @adrinjalali @ogrisel @betatim @jeremiedbb",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27533"
  },
  {
    "number":31052,
    "text":"[CODE] with [CODE] assigns F1-score as [CODE] instead of [CODE]\n\n### Describe the bug According to docs: [CODE_BLOCK] However, when using the [CODE] function with the parameter zero_division set to [CODE], the expected behavior would be that undefined precision or recall values result in an F1-score of [CODE]. However, the function currently assigns an F1-score of [CODE] in these cases.\u200b ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.8 (main, Jan 14 2025, 22:49:36) [MSC v.1942 64 bit (AMD64)] executable: C:\\workspace\\learning\\jbcs2025\\.venv\\Scripts\\python.exe machine: Windows-11-10.0.22621-SP0 Pyth...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31052"
  },
  {
    "number":28711,
    "text":"RFC New parameters for penalties in LogisticRegression\n\nBased on the comment [URL] Currently, [CODE] uses [CODE] as inverse penalization strength, [CODE] to select the type of penalty and [CODE] to control the ration between l1 and l2 penalties. I propose the following: 1. Add [CODE] (as in [CODE], [CODE], [CODE] ...) instead of [CODE]. Fail if both are given at the same time. 2. Deprecate [CODE]. 3. Deprecate [CODE] which is redundant. [CODE] and [CODE] are enough.",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28711"
  },
  {
    "number":30834,
    "text":"Bug: AttributeError in [CODE] when handling [CODE] in [CODE] in [CODE]\n\n### Describe the bug When exporting a decision tree using [CODE] (or other related functions), an AttributeError occurs if a [CODE] value is passed to [CODE] instead of a string. [CODE_BLOCK] Causes: The function [CODE] expects a string but receives a [CODE] value. [CODE] does not have a .replace() method, causing an AttributeError. ## Possible Fix: Convert feature to a string before passing it to [CODE] in [CODE]. Modify line 581 in [CODE]: Before (causing error): [CODE_BLOCK] ## After (fixing error): [CODE_BLOCK] This ensures that [CODE] is always a string before calling [CODE]. ### Steps\/Code to Reproduce This piece of code triggers the error: [CODE_BLOCK] ### Expected Results A graph in PNG format. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.35 Python dependencie...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30834"
  },
  {
    "number":24243,
    "text":"TimeSeriesSplit add skip parameter\n\n### Describe the workflow you want to enable Dear sklearn community, I want to make an hour ahead forecast on a timeseries of at least a year with an interval of 5 minutes. To do CV, I can use TimeSeriesSplit with test_size = 12. I want to do this for many different times of the day, so I could use n_splits = 24 and have a split for each hour of the day. Then I want this for multiple days, so I could do n_splits = 24  nr_days. My problem is that I want to randomly test a few hours a day over many days of the year, to see how my forecast does on different periods of the year. To then make n_splits = 24  365 is too many splits. ### Describe your proposed solution I propose a [CODE] parameter, that allows skipping a number of samples before making the next split. This can be used to reduce the number of splits, while still allowing to cover a large time-period with splits. A few examples: Data: | index | foo | |------------|-----| | 2022-01-01 | 1 | | 2022-01-02 | 2 | | 2022-01-03 | 3 | | 2022-01-04 | 4 | | 2022-01-05 | 5 | | 2022-01-06 | 6 | | 2022-01-07 | 7 | | 2022-01-08 | 8 | | 2022-01-09 | 9 | | 2022-01-10 | 10 | test_size=1, max_train_size=1, n_splits=3, gap=1, skip=1 | train | test | |-------|-----| | 8 | 10 | | 6 | 8 | | 4 | 6 | test_size=1, max_train_size=1, n_splits=3, gap=1, skip=2 | train | test | |-------|-----| | 8 | 10 | | 5 | 7 | | 2 | 4 | test_size=2, max_train_size=1, n_splits=3, gap=1, skip=2 not enough data test_size=2, max...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24243"
  },
  {
    "number":28507,
    "text":"Allow [CODE] and [CODE] to have a higher max_samples than 1.0 when [CODE]\n\n### Describe the workflow you want to enable Currently, random\/extra forests can bootstrap sample the data such that [CODE]. This enables an out-of-bag sample estimate in forests. However, this only allows you to sample in principle up to at most 63% unique samples and then 37% of unique samples are for out-of-bag estimation. However, you should be able to control this parameter to a proportion greater. For instance, perhaps I want to leverage 80% of my data to fit each tree, and 20% to estimate oob performance. This requires one to set [CODE]. Beyond that, no paper suggests that 63% is required cutoff for bootstrapping the samples in Random\/Extra forest. I am happy to submit a PR if the core-dev team thinks the propose solution is simple and reasonable. See [URL] for a good reference and explanation. ### Describe your proposed solution The proposed solution is actually backwards-compatable and adds minimal complexity to the codebase. 1. We change [URL] to the following LOC: 2. ``[CODE][CODE]`. This is equivalent to the expected number of out-of-bag samples being at least 1. Parameters ---------- n_samples : int Number of samples in the dataset. max_samples : int or float The maximum number of samples to draw from the total available: - if float, this indicates a fraction of the total; - if int, this indicates the exact number of samples; - if None, this indicates the total number of samples. Returns -...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28507"
  },
  {
    "number":25412,
    "text":"InteractionTransformer\n\n### Describe the workflow you want to enable The latest 1.2 release is full of great features, e.g., full column name support. This brings me to one of my most desired features regarding building strong and realistic linear models: Interactions! It is currently very hard to add interaction terms between two feature groups, especially if they involve 1 to m transforms like OHE or SplineTransformers. ### Describe your proposed solution An idea of @lorentzenchr that I try to summarize: Create the transforms like a ColumnTransformer, but also adding interaction terms between columns generated by each transformer. The resulting columns could be glued to an other ColumnTransformer using [CODE]. ## Sketch of the API [CODE_BLOCK] ## Example 1: Interactions between two 1-m transforms Here, we would let each dummy variable of \"f1\" interact with each spline basis of \"f2\": [CODE_BLOCK] ## Example 2: Interaction between OHE and other features Each column generated by OHE \"f1\" would interact with numeric feature \"f2\" and also with numeric feature \"f3\". [CODE_BLOCK] ## Example 3: Interaction between two OHE, further features linear ``` interactor = InteractionTransformer( transformers=[ (\"f1_ohe\", OneHotEncoder(drop=...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25412"
  },
  {
    "number":30554,
    "text":"scikit-learn 1.6 changed behavior of growing trees\n\n### Describe the bug While porting scikit-survival to support scikit-learn 1.6, I noticed that one test failed due to trees in a random forest having a different structure (see this GitHub Actions log def _make_whas500(with_mean=True, with_std=True, to_numeric=False): x, y = load_whas500() if with_mean: x = standardize(x, with_std=with_std) if to_numeric: x = categorical_to_numeric(x) names = [\"(Intercept)\"] + x.columns.tolist() return DataSetWithNames(x=x.values, y=y, names=names, x_data_frame=x) whas500 = _make_whas500(to_numeric=True) rng = np.random.RandomState(42) mask = rng.binomial(n=1, p=0.15, size=whas500.x.shape) mask = mask.astype(bool) X = whas500.x.copy() X[mask] = np.nan X_train = X[:400] y_train = whas500.y[:400] weights = np.a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30554"
  },
  {
    "number":26848,
    "text":"Ridge and RidgeCV has different result even if alpha is the same\n\n### Describe the bug (From discussion in [URL] In one of my dataset, I try to specify only one value in alphas in RidgeCV, and expects RidgeCV should give me the same result as using Ridge with same alpha value. But it would not (see the code and output below). And the RidgeCV result seems abnormal. I found that the problem is related to some large values we used in sample_weight. I can reproduce it with make_regression. I guess the large value in sample_weight may caused some overflow in RidgeCV intermediate calculation? Given that RidgeCV and Ridge are using different solvers under the hood, I think it is not surprise that they may behave differently in this case? I just scaled down the weight and the problem is gone. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The output of RidgeCV and Ridge is the same. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26848"
  },
  {
    "number":31907,
    "text":"HDBSCAN modifies input precomputed distance matrix\n\n### Describe the bug When using [CODE] with [CODE], the input distance matrix is modified after calling [CODE]. The original [CODE] package (v0.8.40) works correctly. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Input matrix should remain unchanged (as in original hdbscan). ### Actual Results Input matrix is changed ### Versions ```shell System: python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0] executable: \/home\/username\/project\/bin\/python3 machine: Linux-6.14.0-27-generic-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.7.0 pip: 24.0 setuptools: 80.9.0 numpy: 2.2.6 scipy: 1.16.0 Cython: None pandas: 2.3.0 matplotlib: 3.10.3 joblib: 1.5.1 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 18 prefix: libscipy_openblas filepath: \/home\/username\/project\/lib\/python3.12\/site-packages\/numpy.libs\/libscipy_openblas64_-56d6093b.so version: 0.3.29 threading_layer: pthreads architecture: Haswell user_api: blas internal_api: openblas num_threads: 18 prefix: libscipy_openblas filepath: \/home\/username\/project\/lib\/python3.12\/site-packages\/scipy.libs\/libscipy_openblas-68440149.so version: 0.3.28 threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 18 prefix: libgomp filepath: \/home\/username\/project\/lib\/python3.12\/site-packages\/scikit_learn.libs\/lib...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31907"
  },
  {
    "number":30596,
    "text":"Improve user experience in the user guide - make it clear to users that images are clickable\n\n### Describe the issue linked to the documentation In the user guide, it's not very noticeable that it's possible to click on images which then leads users to the example in which the respective image is used and explained in detail. For instance see [here]([URL] ### Suggest a potential alternative\/fix It would be good to find find a solution that makes it clear to users that there's a hyperlink attached to the images which leads to the respective examples. The discussion came up in PR #30127 which contributes to issue #26927. @scikit-learn\/contributor-experience-team @scikit-learn\/documentation-team",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30596"
  },
  {
    "number":26328,
    "text":"Cross validation error of a Gaussian process with noisy target\n\n### Describe the bug Hi, I'm trying to use RandomizedSearchCV with GP with a vector of alpha's. It seems that with the cross validation the alpha are not being split into train\/test sets because I get the following error: ValueError: alpha must be a scalar or an array with same number of entries as y. (100 != 80) Is there any workaround, or am I missing something? Thanks! ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown and estimation is based on the values and target uncertainty ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26328"
  },
  {
    "number":30325,
    "text":"Classification report, digits variable\n\n### Describe the workflow you want to enable Hi, as of right now the digits variable which limits how many numbers are shown after the decimal point does not apply to the support column for the classification report. Support normally does not have decimals, but that happens when we apply sample weights to the report. ### Describe your proposed solution apply decimal variable to support column as well as the metric columns ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30325"
  },
  {
    "number":31653,
    "text":"DOC About Us page - clarify team descriptions\n\n### Describe the issue linked to the documentation References #31430 [@thomasjpfan note]([URL] >With the current governance, all the named roles are considered \"core\". Specifically, the contributor experience, communication, documentation, and maintainer teams are all \"core contributors\". >Before this PR, only the maintainer team can approve PRs. With this PR, any of the other teams can approve PRs. Although, in practice, I think we normally considered the other approvals as valid. [@ArturoAmorQ note]([URL] >Honest question, shall we modify the terminology across the documentation e.g. in about.rst? Such that it's clear who are those referred here. Team names and descriptions are not consistent. 1. GitHub Teams: [URL] - Communication Team - Contributor Experience Team - Core-devs - Documentation Team 2. About Us page: [URL] Active Core Contributors - Maintainers Team - Documentation Team - Contributor Experience Team - Communication Team ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31653"
  },
  {
    "number":30541,
    "text":"Glossary: See-also for [CODE] attribute references itself\n\n### Describe the issue linked to the documentation At <URL] the [CODE] entry references itself: > See also [components_ instead of itself ([CODE]) again? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30541"
  },
  {
    "number":27294,
    "text":"Incremental F-regression\n\n### Describe the workflow you want to enable For situations with many variables and low memory, for example lags taken from a high frequency time series and corresponding exogenous variables, it's possible to go though each column one by one or in batches, in an ordered manner, select the most important variables incrementally store them in a buffer with a predefined size and when the buffer is full replace variables with higher score than minimum score of buffer. The objective is to implement this behavior with Sklearn's F-regression. ### Describe your proposed solution Two variables: temperature and voltage Limit is 4 columns iter 1: Temperature N, Voltage N (keep both: 2 vars) iter 2: Temperature N-1, Voltage N-1 (keep both: 4 vars) iter 3: Temperature N-2, Voltage N-2 (replace existing column if score is higher than minimum of buffer) iter 4: ... ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27294"
  },
  {
    "number":27653,
    "text":"scikit-learn-1.3.2.tar.gz archive contains version 1.4.dev0\n\n### Describe the bug The package downloaded from [[URL] contains version 1.4.dev0: The file [CODE] defines: [CODE_BLOCK] ### Steps\/Code to Reproduce Download archive file. Check __init__.py ### Expected Results Version is 1.3.2 . ### Actual Results Version is 1.4.dev0 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27653"
  },
  {
    "number":29443,
    "text":"KernelDensity(bandwidth='silverman') doesn't throw proper error for 1d X\n\nEssentially the bandwidth estimation codepath is not covered in the common tests, but it should be :)",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29443"
  },
  {
    "number":29358,
    "text":"Sprints page\n\n### Describe the issue linked to the documentation The following sprints are listed: [URL] But, that is a small subset, given the list here: [URL] Are the sprints posted on the \"About Us\" page of a certain criteria, such as Dev sprints only? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29358"
  },
  {
    "number":32145,
    "text":"New min dependencies broke the doc build\n\nWe didn't run a dock build before merging [URL] which bumps min dependencies, and it broke the CI, see [URL] cc\/ @GaetandeCast @ogrisel",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32145"
  },
  {
    "number":30639,
    "text":"UnboundTransform implementing log and logit transforms\n\n### Describe the workflow you want to enable Most classifiers and regressors expected unbounded input. Bounded input typically comes in the forms (a, infty) and (a, b) with the important special cases (0, infty) for radii, counts, and other things that are always non-negative, and (0, 1) for probabilities and fractions. One needs a way to transform bounded data to the unbounded domain that works with all classifiers and regressors. ### Describe your proposed solution A new transform should be implemented to convert bounded domains into the unbounded domain, tentatively called [CODE]. The transform can optionally be chained by a PowerTransform to make the output more gaussian. The [CODE] will apply scale and shift options and the log transform or the logit transform to convert half-bounded and fully bounded data, respectively. Because bounds are difficult to estimate from a distribution, the user should pass the bounds on construction. ### Describe alternatives you've considered, if relevant  The Yeo-Johnson power transform does not make a bounded variable unbounded.  The QuantileTransform overfits on small samples and its tail behavior is unpredictable. It is also computationally expensive. * The FunctionTransform is a generic solution and able to perform this task, but the user has to provide the transforms. Since making data unbounded is such a common thing, having a specialized transform for this case is justified, to...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30639"
  },
  {
    "number":29017,
    "text":"Using decision boundary display to plot the relationship between any 2 features if model is fitted to more than 2 features\n\n### Describe the workflow you want to enable Currently, it seems like it is not possible to pass in a model that has been fitted to more than 2 features to the DecisionBoundaryDisplay.from_estimator method. Is it possible to allow that while only passing in the 2 features you are interested in looking at the relationship for? ### Describe your proposed solution Allowing users to use [decision boundary display ]([URL] to plot the relationship between any 2 features in a model that is fitted for more than 2 features. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29017"
  },
  {
    "number":25662,
    "text":"LinearDiscriminantAnalysis: svd solver gives different results than eigen and lsqr.\n\n### Describe the bug LinearDiscriminantAnalysis provides three methods for determining the discriminants: [CODE] (the default), [CODE] and [CODE]. The coefficients produced by the [CODE] and [CODE] methods agree with each other and disagree with that produced by [CODE]. This disagreement is particularly large when the classes have different scalings. Looking at the source code, the error could be partially due to improper scaling of the centered data. At line 509 of [CODE], the comments indicate that the features should be scaled by their within-class standard deviations, but are actually being scaled by the overall standard deviations. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Computing the norm of the difference between the coefficients computed by each method, we should see that the [CODE] and [CODE] methods agree with each other and with [CODE]. [CODE_BLOCK] ### Actual Results Computing the norm of the difference between the coefficients computed by each method, we actually see...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25662"
  },
  {
    "number":25592,
    "text":"set_config(transform_output=\"pandas\") does not act on inverse_transform\n\n### Describe the bug The new [CODE] functionality Cell In[25], line 16 13 assert isinstance(Xt, pd.DataFrame) 15 X = scaler.inverse_transform(Xt) ---> 16 assert isinstance(X, pd.DataFrame) AssertionError: ### Versions ```shell System: python: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] executable: \/usr\/bin\/python3.10 machine: Linux-5.14.0-1038-oem-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.1 pip: 23.0 setuptools: 65.3.0 numpy: 1.22.4 scipy: 1.9.0 Cython: 0.29.30 pandas: 1.5.1 matplotlib: 3.5.3 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: \/home\/afr\/.local\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-2f7c42d4.3.18.so version: 0...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25592"
  },
  {
    "number":27305,
    "text":"Monotonicity constraints for GradientBoostingClassifier and GradientBoostingRegressor\n\n### Describe the workflow you want to enable As a follow-up of #13649, I'd like to use [CODE_BLOCK] same as in [CODE] and int [CODE]. ### Describe your proposed solution Add [CODE] to [CODE] and [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27305"
  },
  {
    "number":31189,
    "text":"scikit-learn not included in conda env creation step for bleeding-edge install\n\n### Describe the issue linked to the documentation On the Contributing, the scikit-learn package itself is not mentioned or included. ![Image]([URL] but Step 6 asks to Check that the installed scikit-learn has a version number ending with .dev0 which raises errors if scikit-learn is not installed in the virtual environment ![Image]([URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31189"
  },
  {
    "number":26639,
    "text":"Scipy Warning Issued Only After LinearRegression calls\n\n### Describe the bug I am working on a project for work and the data is sensitive and I share those files. I've reduced the issue to the attached code and longer description. I get scipy MatReadWarnings when I called sklearn linear regression module. Further, the mat read warning I get is a deprecated method. This warning shows up if and only if I call the linear regression function that uses the sklearn.linear_model.LinearRegression class, just importing it did not cause any issues. No warning messages appeared. My environment is using scipy 1.10.1 and scikit-learn 1.2.2. Any thoughts? ### Steps\/Code to Reproduce I am working on a project for work and the data is sensitive and I share those files. I've reduced the issue to the following code. [CODE_BLOCK] If I run this code- I get this output > Loop 0 > Mat Loaded > sklearn Called > C:\\...\\anaconda3\\envs\\HCAcal\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:227: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new > Consider mio5.varmats_from_mat to split file into single variable files > matfile_dict = MR.get_variables(variable_names) If I comment out the line [CODE] no error message appears. If I comment out the line time.sleep(1) then there is a bit more randomness to the output. In the sense it will do it in groups of 3-5: > Loop 0 > Mat Loaded > sklearn...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26639"
  },
  {
    "number":29730,
    "text":"Add a LogTransformer and a LogWithShiftTransformer\n\n### Describe the workflow you want to enable I suggest adding new transformers to scikit-learn named [CODE] and [CODE], which would add the functionality of applying a logarithmic transformation and a logarithmic transformation capable of handling negative values in time series data, respectively. These transformers would be particularly useful for preprocessing time series data that may contain zero or negative values ### Describe your proposed solution Add two custom scikit-learn transformers: [CODE] and [CODE], designed for preprocessing time series with their corresponding inverse transformers. Currently one can able to do that with the [CODE] which is not efficient or checked. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29730"
  },
  {
    "number":27360,
    "text":"Documentation improvement for make_sparse_spd_matrix\n\n### Describe the issue linked to the documentation I have noticed that the make_sparse_spd_matrix function in scikit-learn's documentation states that it should return a sparse matrix. However, the function currently returns a dense numpy.ndarray. This seems to be a discrepancy between the documented behavior and the actual behavior of the function. ### Steps to Reproduce: Import the make_sparse_spd_matrix function from scikit-learn. Call the function with an appropriate input. Observe that the function returns a dense numpy.ndarray instead of a sparse matrix. Expected Results: The documentation should accurately reflect the behavior of the make_sparse_spd_matrix function, specifying that it returns a dense numpy.ndarray. ### Actual Results: The documentation states that the function should return a sparse matrix, but it returns a dense [CODE]. Versions: [CODE_BLOCK] Then: To address the documentation discrepancy regarding the make_sparse_spd_matrix function in scikit-learn, you have a few potential alternatives\/fixes: - Option 1: Update the Documentation You can propose an update to the documentation to accurately reflect the behavior of the make_sparse_spd_matrix function. In this case, the documentation should specify that the function returns a dense numpy.ndarray, not a sparse matrix. Here's an example of how the documentation might be updated: ``[CODE]n_dim[CODE]alpha[CODE]smallest_coef[CODE]smallest_coef * largest_e...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27360"
  },
  {
    "number":31077,
    "text":"Partial dependence broken when categorical_features has an empty list\n\n### Describe the bug When we pass an empty list to categorical_features, partial_dependence will raise an error ValueError: Expected categorical_features to be an array-like of boolean, integer, or string. Got float64 instead. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results . ### Actual Results ```pytb ValueError Traceback (most recent call last) Cell In[12], line 28 24 model = make_pipeline(preprocessor, LinearRegression()) 26 model.fit(iris, iris.sepal_length) ---> 28 pd = partial_dependence(estimator=model, X= iris, features= [\"sepal_length\"], categorical_features= []) File ~.venv\/lib\/python3.10\/site-packages\/sklearn\/utils\/_param_validation.py:213, in validate_params..decorator..wrapper(args, *kwargs) 207 try: 208 with config_context( 209 skip_parameter_validation=( 210 prefer_skip_nested_validation or global_ski...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31077"
  },
  {
    "number":28530,
    "text":"BUILD gcc14 cannot compile scikit-learn\n\n### Describe the bug When building 1.3.2+ with Fedora Rawhide at OBS, it is now failed with below error message (see [URL] ``` running build_ext building 'sklearn.metrics._dist_metrics' extension gcc -fno-strict-overflow -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -fcf-protection -fexceptions -fcf-protection -fexceptions -fcf-protection -fexceptions -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -I\/usr\/local\/lib64\/python3.12\/site-packages\/numpy\/core\/include -I\/usr\/include\/python3.12 -c sklearn\/metrics\/_dist_metrics.c -o build\/temp.linux-x86_64-cpython-312\/sklearn\/metrics\/_dist_metrics.o -g0 -O2 -fopenmp sklearn\/metrics\/_dist_metrics.c: In function \u2018__pyx_pf_7sklearn_7metrics_13_dist_metrics_16DistanceMetric64_22_pairwise_sparse_dense\u2019: sklearn\/metrics\/_dist_metrics.c:29086:29: warning: assignment discards \u2018const\u2019 qualifier from pointer target type [-Wdiscarded-qualifiers] 29086 | __pyx_v_x2_data = ((&(((__pyx_t_7sklearn_5utils_9_typedefs_float64_t const ) ( \/ dim=1 \/ ((char ) (((__pyx_t_7sklearn_5utils_9_typedefs_float64_t const ) ( \/ dim=0 \/ (__pyx_v_Y_data.data + __pyx_t_18  __pyx_v_Y_data.strides[0]) )) + __pyx_t_19)) )))) + (__pyx_v_i2  __pyx_v_n_features)); | ^ sklearn\/metrics\/_dist_metrics.c: In function \u2018__pyx_pf_7sklearn_7metrics_13_dist_metrics_16DistanceMetric64_24_pairwise_dense_sparse\u2019: sklearn\/metrics\/_dist_metrics.c:29871:27: warning: assignment discards \u2018const\u2019 qualifier from pointer target type [-Wdiscard...",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28530"
  },
  {
    "number":30450,
    "text":"Scikit-learn v1.6.0 breaks SelectFromModel when using a non-sklearn model\n\n### Describe the bug There seem to be a bug introduced by v1.6.0 where the SelectFromModel must use a model for which the parent class also has a [CODE] method. This works with sklearn models but not with 3rd party models using a sklearn type API. It looks like folks working on xgboost are busy making some changes on their side as well. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results This was working fine up until v1.6.0. ### Actual Results ``[CODE]transform_input[CODE]sklearn.set_config(...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30450"
  },
  {
    "number":28083,
    "text":"Reduce verbosity of error \"Input X contains NaN\" of PLSRegression\n\n### Describe the workflow you want to enable Currently if PLSRegression (and other algorithms, judging by google) gets any NaNs as input, it raises this long multiline exception: [CODE_BLOCK] (source is here: [URL] I am running the algorithm regularly at fixed intervals and unattended, so I am getting very verbose logs from time to time. It would be nice to have an option to reduce verbosity of this error. ### Describe your proposed solution Add an option to suppress long exception description and only emit \"Input X contains NaN\" message. Or only emit verbose error once, and then switch to short message. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28083"
  },
  {
    "number":29055,
    "text":"[CODE]s in the documentation\n\n### Describe the issue linked to the documentation Some [CODE] are present in the [CODE] documentation and need to be fixed. Here is a list: - [x] [gaussian_process\/plot_gpr_prior_posterior.html]([URL] #29380 - [x] [model_selection\/plot_cv_indices.html]([URL] #29072 - [x] [linear_model\/plot_sgd_iris.html]([URL] #29121 - [x] [linear_model\/plot_logistic_multinomial.html]([URL] #29120 - [x] [tree\/plot_iris_dtc.html]([URL] #29109 - [x] [svm\/plot_svm_margin.html]([URL] #29187 - [x] [ensemble\/plot_adaboost_twoclass.html]([URL] #29188 Contributors willing to address this issue, please fix one example per pull request. It is ok to fix other warnings or errors in a given example. Thanks for your help! ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29055"
  },
  {
    "number":26010,
    "text":"Introduce SIMD intrinsics for [CODE]\n\n# Context Pairwise distance computation is an essential part of many estimators in scikit-learn, and can take up a significant portion of run time in certain workflows. I believe that we may achieve significant performance gains in several (perhaps most) distance metric implementations by leveraging SIMD intrinsics. # Proof of Concept I built a quick proof of concept just to see what kinds of performance gains we could observe with a potentially-naive implementation of SIMD intrinsics. I chose to optimize the [CODE] function. This implementation uses intrinsics found in [CODE]. To ensure that the instructions are supported, it checks for the presence of the [CODE] instruction set ([CODE] implies [CODE]) and provides the optimized implementation if so. Otherwise it provides a dummy implementation just to appease Cython, and the main function falls back to the current implementation on [CODE]. Note that on most modern hardware, support for [CODE] is a reasonable expectation (indeed numpy assumes it is always present when optimization is enabled). For the specific implementation referred to here, please take a look at this PR: [URL] Note that the full benefit of the intrinsics are gained when compiling with [CODE], however the benefit is still significant when compiling with [CODE], as is often default (e.g when following the scikit-learn development instructions on linux). # Benchmarks The following benchmarks were produced by this gist: [U...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26010"
  },
  {
    "number":24845,
    "text":"Add user friendly string options for interaction constraints in HistGradientBoosting*\n\n### Describe the workflow you want to enable [CODE_BLOCK] instead of [CODE_BLOCK] ### Describe your proposed solution - [x] [CODE] is straight forward. - [x] [CODE] expands to a list that is quadratic in number of features. It might be more memory efficient to use generators internally. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context This was proposed as follow-up in [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24845"
  },
  {
    "number":25024,
    "text":"Fix broken links in the documentation\n\nA follow-up of [URL] If you want to work on this, please: - do one Pull Request per link - add a comment in this issue saying which link you want to tackle so that different people can work on this issue in parallel - mention this issue ([CODE]) in your Pull Request description so that progress on this issue can more easily be tracked Possible solutions for a broken link include: - find a replacement for the broken link. In case of links to articles, being able to link to a resource where the article is openly accessible (rather than behind a paywall) would be nice. - The link can be added to the [CODE] variable: [URL] This is the only thing to do for example when: + the link is broken with no replacement (for example in testimonials some companies were acquired and their website does not exist) + the link works fine in a browser but is flagged as broken by [CODE] tool. This may happen because some websites are trying to prevent bots to scrape the content of their website Something that may be useful in the complicated cases is to search on the [Internet Archive]([URL] for the broken link. You may be able to look at the old content and it may help you to find an appropriate link replacement. List of broken links from a [CODE] local run: - [x] [CODE] governance.rst [CODE_BLOCK] - [x] [CODE] related_projects.rst [CODE_BLOCK] - [x] [CODE] modules\/linear_model.rst ``` 404 Client Error: Not Found for url: [URL]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25024"
  },
  {
    "number":25763,
    "text":"BUG IsotonicRegression errors with set_config(transform_output=\"pandas\")\n\n### Describe the bug [CODE] is broken by [CODE]. Mabye similar to #25365. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] Or at least no error! ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25763"
  },
  {
    "number":25079,
    "text":"RFC refactoring of PartialDependenceDisplay\n\nRelated to [URL] I would like to discuss the possibility to refactor [CODE] to reduce its usability. The idea behind this refactoring is: - make it consistent with other displays: calling [CODE] is equivalent to a call to [CODE]. - it will simplify greatly the code base: the codebase was already refactored but this is still difficult to understand logic where axes are interacting with each other. - keep the figure close to standard [CODE]: it allows a user that knows [CODE] to quickly modify and tune the figure without knowing some implementation details enforced by [CODE]. ### Sharing y-axis One main reason for having [CODE] doing several plots is mainly for setting a common y-axis. However, [CODE] allows such a feature when creating a figure: [CODE_BLOCK] ![image]([URL] Here, the call could be done with [CODE] instead of the call to [CODE] + plotting. In this case, a user can still manipulate the difference axes using [CODE] and make any usual matplotlib. ### Surprising API With the current [CODE], we will have some parameters specific to our settings: [CODE] to set the number of columns and fitted attribute [CODE] and [CODE]. The latest attributes ...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25079"
  },
  {
    "number":29929,
    "text":"Custom estimator's fit() method throws \"RuntimeWarning: invalid value encountered in cast\" in Linux Python 3.11\/3.12\n\n### Describe the bug We have a custom estimator class that inherits from CODE] and [CODE]. We run automated unit tests in Azure DevOps pipelines on both Windows Server 2022 and Ubuntu 22.04.1. All the tests pass on Windows. On Python 3.12.6 in Linux the test with the stacktrace shown below fails with: [CODE] This causes the test and hence build to fail because we set [CODE] before running the tests. On Python 3.11.10 in Linux this test actually passes; but a different test using the same custom estimator fails with an identical stacktrace. And yet this latter test passes on Python 3.12 in Linux! Note this change in numpy 1.24.0: [URL] especially this bit: > The precise behavior is subject to the C99 standard and its implementation in both software and hardware. I can probably work around this error in our tests by using a [numpy.errstate: ``` Traceback (most recent call last): File \"\/home\/vsts\/work\/1\/tests\/<our_test_module>\", line 76, in test_gen_data grid_search.fit(data[features].values) File \"\/opt\/hostedtoolcache\/Python\/3.12.6\/x64\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1473, in wrapper return fit_method(estimator, args, *kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/opt\/hostedtoolcache...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29929"
  },
  {
    "number":29102,
    "text":"Allow users to override [CODE] of the BaseSearchCV\n\n### Describe the workflow you want to enable Currently, the [CODE] has some level of customization enabled with the [CODE] method. I would like to enable more customization. In particular the [CODE] method calls [CODE] function for all of the CV splits in parallel. This makes it difficult to modify the fitting and scoring behavior for particular models. Allowing users to create custom [CODE] methods would enable many use cases that are currently hard - such as working with other modeling and data ecosystems not fully supported by sklearn. ### Describe your proposed solution 1. I would propose making [CODE] a method on the [CODE] class. As far as I can tell. this function is only used in this class so it could be moved fully (with some deprecation period). Or, if you wanted to keep [CODE] as a function you could have a small method alias in [CODE] that calls the function. 2. Change the delayed call in [CODE] [URL] [CODE_BLOCK] to using the method: [CODE_BLOCK] For most users the function will continue as before. For users with more complex use-cases they are free to create a subclass with their own implementation of [CODE]. ### Describe alternatives you've considered, if relevant 1. Currently we would need to also implement the [CODE] function to modify the lines above and achieve this. By allowing just the [CODE] to be modified we will make it easier for users. 2. One can try to monkeypatch or otherwise modify the [CODE] fun...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29102"
  },
  {
    "number":28437,
    "text":"Allow boosting of estimators in scikit-learn pipelines using residuals\n\n### Describe the workflow you want to enable I want to be able to use multiple estimators in one pipeline. E.g. [CODE_BLOCK] This currently doesn't work: [CODE_BLOCK] This is for good reason: naively passing predict from the linear model to the fit for the gbm will lead to overfitting! ### Describe your proposed solution Allow boosting estimators in scikit learn pipeline by subtracting the predict output from y at [CODE] time and then adding the predict outputs at transform time. This enhancement would allow multiple regression estimators to be used consecutively, where each subsequent estimator fits on the residuals of the previous ones. In my example above, at fit time the linear step proceeds as normal. Then the gbm step detects 2 chained estimators, and subtracts the prediction from the linear step from y, and then fits to the residual. At transform time, the predictions from the linear model would be added to the predictions from the gbm model. To simplify things, I think you still wouldn't be allowed to connect an estimator to a transformer. ### Describe alternatives you've considered, if relevant Vecstack is a good alternative, but its slow. You have to cross-validate each estimator, which gets expensive if you chain more than 2 in a row. ```python from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression, Ridge from sklearn.ensemble impo...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28437"
  },
  {
    "number":27613,
    "text":"ValueError when calling [CODE]\n\n### Describe the bug On a specific machine, calling [CODE] on any estimator (example with [CODE] below) raises a [CODE]. I could not reproduce it on a different machine. may be relevant: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error ### Actual Results [CODE_BLOCK] ### Versions [CODE] ```shell System: python: 3.11.5 (main, Aug 25 2023, 13:19:50) [GCC 11.4.0] executable: \/home\/jerome\/.virtualenvs\/3.11\/bin\/python machine: Linux-6.2.0-33-generic...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27613"
  },
  {
    "number":32003,
    "text":"[CODE] transformed validation dataset still contains null \/ missing values\n\n### Describe the bug I use [CODE] to [CODE] a training dataset which is properly cleaned up. However, using the same encoder to [CODE] validation \/ test dataset still contains null \/ missing values. ### Steps\/Code to Reproduce [CODE_BLOCK] The following pass: [CODE_BLOCK] The following fails!: [CODE_BLOCK] ### Expected Results [CODE] on validation dataset should clean up the values, leaving no missing and\/or null values. ### Actual Results The assertion code on validation dataset fails! ### Versions ```shell System: python: 3.13.3 (main, Aug 14 2025, 11:53:40) [GCC 14.2.0] executable: \/home\/khteh\/.local\/share\/virtualenvs\/JupyterNotebooks-uVG1pv5y\/bin\/python machine: Linux-6.14.0-28-generic-x86_64-with-glibc2.41 Python dependencies: sklearn: 1.7.1 pip: 25.0 setuptools: 80.9.0 numpy: 2.3.2 scipy: 1.16.1 Cython: None pandas: 2.3.1 matplotlib: 3.10.5 joblib: 1.5.1 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32003"
  },
  {
    "number":31223,
    "text":"Support orthogonal polynomial features (via QR decomposition) in [CODE]\n\n### Describe the workflow you want to enable I want to introduce support for orthogonal polynomial features via QR decomposition in [CODE], closely mirroring the behavior of R's [CODE] function. In regression modeling, using orthogonal polynomials can often lead to improved numerical stability and reduced multi-collinearity among polynomial terms As an example of what the difference looks like in R, <pre> #fits raw polynomial data without an orthogonal basis model_raw <- lm(y ~ I(x) + I(x^2) + I(x^3), data = data) #model_raw <- lm(y ~poly(x,3,raw=TRUE), data = data) #fits the same degree-3 polynomial using an orthogonal basis model_poly <- lm(y ~ poly(x, 3), data = data) <\/pre> This behavior cannot currently be replicated with [CODE]'s [CODE], which only produces the raw monomial terms. As a result transitioning from R to Python often leads to discrepancies in model behavior and performance. ### Describe your proposed solution I propose extending [CODE] with a new parameter: <pre> PolynomialFeatures(..., method=\"raw\") <\/pre> Accepted values: - [CODE] (default): retains existing behavior, returning standard raw terms - [CODE]: applies QR decomposition to each feature to generate orthogonal polynomial features. Because R's [CODE] only operates on 1D input vectors, my thought was to apply QR decomposition feature by feature when the input is multi-dimensional. Each column is processed independently, mirrori...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31223"
  },
  {
    "number":27696,
    "text":"DecisionTreeClassifier does not support 'auto' as an option for max_features\n\n### Describe the bug I was using scikit-learn version 1.3.2, trying to fit a DecisionTreeClassifier to my data, and I got an error that the option 'auto' was invalid. The documentation Cell 2 line 5 [1] X, y = make_classification() [3] X_train, X_test, y_train, y_test = train_test_split(X, y) ----> [5] DecisionTreeClassifier(max_features='auto').fit(X_train, y_train) File c:\\Users\\\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1145, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, kwargs) 1140 partial_fit_and_fitted = ( 1141 fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator) 1142 ) 1144 if not global_skip_validation and not partial_fit_and_fitted: -> 1145 estimator._validate_params() 1147 with config_context( 1148 skip_parameter_validation=( 1149 prefer_skip_nested_validation or global_skip_validation 1150 ) 1151 ): 1152 return fit_method(estimator, args, kwargs) File c:\\Users\\\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:638, in BaseEstimator._validate_params(self) 630 def _val...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27696"
  },
  {
    "number":32150,
    "text":"Latex not correctly rendered for ridge\n\n### Describe the issue linked to the documentation Description In the online API reference, formula is displayed as: [CODE] it should instead look like <img width=\"299\" height=\"77\" alt=\"Image\" src=\"[URL] \/> Steps to Reproduce the Issue: Please see [[URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32150"
  },
  {
    "number":26248,
    "text":"Add sample_weight parameter to OneHotEncoder.fit(...)\n\n### Describe the workflow you want to enable I have a dataset with huge number of duplicates - so in order to speed-up learning process I prefer to remove these duplicates and pass [CODE] to the [CODE] method of estimator. I override the [CODE] method of estimator and remove duplicates in this wrapper. But If I use [CODE] as pre-processing step, it produces sparse output - so the input to [CODE] is sparse. My dataset is large, so I strongly want to use sparse output of [CODE]. And effective duplicate removal in sparse input is not a trivial task - it must use sorting or hashing. So I want to perform duplicate removal and [CODE] calculation before one hot encoding. But I use the [CODE] parameter of [CODE] - and want level frequencies to be calculated with the use of [CODE]. ### Describe your proposed solution Add [CODE] parameter to [CODE] function and calculate level frequencies using [CODE] parameter. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26248"
  },
  {
    "number":27436,
    "text":"Misleading error message for HDBSCAN exception\n\n### Describe the bug Before computing the minimum spanning tree, HDBSCAN checks if the number of connected components in the mutual-reachability graph is greater than 1. Here is the snippet - URL] However, the connected components could be greater than 1 even if there are more than [CODE] neighbours for all samples in the distance matrix. A similar [issue in itself is disconnected, which does not work for HDBSCAN. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Exception with error message indicating the possibility of disconnected knn graph. ### Actual Results ```python Trace...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27436"
  },
  {
    "number":30774,
    "text":"Deprecation message of check_estimator does not point to the right replacement\n\nSee here [URL] I believe it should point to [CODE] as suggested in the doc string. Also not sure you want to keep the sphinx directive in the warning message.",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30774"
  },
  {
    "number":29962,
    "text":"DOC merging the examples related to OPTICS, DBSCAN, and HDBSCAN\n\nAs stated in [URL] it would be great to reduce the number of examples in the gallery. Right now, we have three examples for: - OPTICS: [URL] - DBSCAN: [URL] - HDBSCAN: [URL] Those clustering methods are really close to each others; some being an improvement from another one. Therefore, we could rework a single example that is not only a demo but rather show the pros & cons from each approach.",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29962"
  },
  {
    "number":30275,
    "text":"ColumnTransformer does not validate sparse formats for X\n\n### Describe the bug If the underlying transformers all accept sparse input data, [CODE] should also be able to accept sparse input data. That's indeed the case for the [CODE], [CODE], [CODE] and [CODE] formats but it raises errors for the [CODE], [CODE], [CODE] formats because those are not \"subscriptable\". As a possible fix, we could validate sparse input data by using [CODE] which will then convert to a \"subscriptable\" sparse format. Currently it is not done as [CODE] relies on its own [CODE] which often entirely bypasses the validation, maybe for performance reasons ? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.5 | packaged by conda-forge | (main, Aug 8 2024, 18:32:50) [Clang 16.0.6 ] executable: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/bin\/python machine: macOS-14.5-arm64-arm-64bit Python dependencies: sklearn: 1.6.dev0 pip: 24.2 setuptools: 73.0.1 numpy: 2.1.0 scipy: 1.14.1 Cython: 3.0.11 pandas: 2.2.2 matplotlib: 3.9.2 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/lib\/libopenblas.0.dylib version: 0.3.27 threading_layer: openmp architecture: VORTEX user_api: openmp internal_api: openmp num_threads: 8 prefix: libomp filepath: \/Users...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30275"
  },
  {
    "number":28921,
    "text":"Undocumented change in tree_.value example for DecisionTreeClassifier between versions 1.3.2 and 1.4.2\n\n### Describe the issue linked to the documentation In the the 1.4.2 docs the [Understanding the decision tree structure page]([URL] provides code and output in order to inspect [CODE], but the tree diagram and output from the code snippet are inconsistent. The diagram shows integer values that represent the number of records in that class at each node. The new output from the code appears to be the percentage? of the total number of records that are in the respective class. The [1.3.2 docs]([URL] were consistent on this page between the code output and the diagram lower describing the values array, so I expect that something changed between the versions but wasn't documented, at least here in this example. I can't find where this change to [CODE] is documented and it appears to be causing confusion (see for example on [stack overflow]([URL] ### Suggest a potential alternative\/fix I would suggest updating the visual and documenting more clearly what to expect from [CODE] for [CODE] in 1.4.2 since it is evidently different compared to 1.3.2. I am working with some code that inspects the trees and would appreciate insight to make sure that I make the necessary adjustments to get the same values that I did with 1.3.2.",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28921"
  },
  {
    "number":25116,
    "text":"Error when attempting to passthrough transformer step if tuning transformer\n\n### Describe the bug I am using an imblearn pipeline to perform dimensionality reduction before model training. I would like try either a PCA or skipping the dimensionality reduction step completely (setting step to None). I am also tuning the number of components used for PCA. My code throws an error during grid search when the dimensionality_reduction step is set to None but the dimensionality_reduction__n_components is set to a floating number. Is it possible to allow passthrough of a transformer and ignore any setting of parameters associated with that step? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. dimensionality_reduction__n_components parameter is ignored because dimensionality_reduction is not set. ### Actual Results ```console Traceback (most recent call last): File \"\/usr\/local\/lib\/python3.7\/dist-packages\/joblib\/externals\/loky\/process_executor.py\", line 428, in _process_worker r = call_item() File \"\/usr\/local\/lib\/python3.7\/dist-packages\/joblib\/externals\/loky\/process_executor.py\", line 275, in __call__ return self.fn(self.args, *self.kwargs) File \"\/usr\/local\/lib\/python3.7\/dist-packages\/jobl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25116"
  },
  {
    "number":30742,
    "text":"[CODE], and [CODE] parameters to[CODE] are optional\n\n### Describe the bug [CODE] has the signature [CODE] indicating that both [CODE], and [CODE] may not be specified when calling [CODE]. However, omitting only [CODE] results in [CODE]. Also, when omitting both [CODE] and [CODE], or only [CODE] the result is [CODE] This indicates, contrary to the signature, that [CODE] and [CODE]StratifiedKFold[CODE]y[CODE]split[CODE]StratifiedKFold[CODE]StratifiedGroupKFold[CODE]_BaseKFold[CODE].split[CODE]StratifiedKFold[CODE]split[CODE]_BaseKFold[CODE]StratifiedGroupKFold[CODE]y[CODE]groups[CODE]None[CODE]`` --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[2], line 2 1 sgkf = StratifiedGroupKFold(n_splits=5) ----> 2 next(sgkf.split(X=X, y=y, groups=None)) # TypeError File \/<PATH>\/lib\/python3.12\/site-packages\/sklearn\/model_selection\/_split.py:411...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30742"
  },
  {
    "number":31554,
    "text":"Allow batch based metrics calculation of sklearn.metrics\n\n### Describe the workflow you want to enable I have a lot of data and need to calculate metrics such as accuracy_score, jaccard_score, f1_score, recall, precision etc. ### Describe your proposed solution When I try to calculate these it can literally take days, so i created a small solution which can batch and avg in the end, or for the weighted metrics it can do a weighted avg of each, this accelerated the calculation to just a couple of minutes, because I have a 32 core CPU. I'm willing to contribute with the proper guidance as I'm unfamiliar with the codebase, but I think many people can benefit from this. I'm unsure if there is already a work around of this present in the codebase, but if there is one do let me know, thanks a lot. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31554"
  },
  {
    "number":30364,
    "text":"Expose [CODE] in [CODE]\n\n### Describe the workflow you want to enable [CODE_BLOCK] ### Describe your proposed solution Add a keyword arg like in [CODE] ### Describe alternatives you've considered, if relevant Explicitly defining with [CODE] ### Additional context Would be a convenience",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30364"
  },
  {
    "number":25198,
    "text":"Sparse data representations results in worse models than dense data for some classifiers\n\n### Describe the bug Using scipy sparse matrices with sklearn LogisticRegression greatly improves speed and therefore is desirable in many scenarios. However, it appears that sparse versus dense data representations yield different (worse) results for some sklearn classifiers. My perhaps naive assumption is that sparse versus dense is just a method of representing the data and operations performed on the sparse or dense data (including model training) should yield identical or nearly identical results. A notebook gist looking at sparse versus dense results for nine solvers can be found here: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Dense AUC: 1.0 Sparse AUC: 1.0 ### Actual Results Dense AUC: 1.0 Sparse AUC: 0.584 ### Versions ```shell Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fc557a3f310> Traceback (most recent call last): File \"\/databricks\/python...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25198"
  },
  {
    "number":28994,
    "text":"StratifiedShuffleSplit requires three copies of a lower class, rather than 2\n\n### Describe the bug When we want to use [CODE] to train test split across classes, we would expect we need 2 samples of the lowest represented class: 1 for test, one for train. We don't get this: we need 3 samples of the lowest class sklearn version 1.2.1 ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results We expect to get a test set and a train set that both contain 1 example of each class when we have 2 representatives. [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ] executable: bin\/python machine: macOS-14.2.1-arm64-arm-64bit Python dependencies: sklearn: 1.2.1 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.26.4 scipy: 1.10.0 Cython: 3.0.0 pandas: 2.2.2 matplotlib: 3.7.0 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP:...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28994"
  },
  {
    "number":30166,
    "text":"The best model and final model in RANSAC are not same.\n\n### Describe the bug The best model and final model in RANSAC are not same. Therefore, the final model inliers may not be same as the best model inliers. In [CODE], the following code snippet computes the final model using all inliers so the final model is not same as the best model computed using the selected samples before. [CODE_BLOCK] ### Steps\/Code to Reproduce Please debug the code using a custom loss function. Probably, you would observe the difference for default loss functions as well. ### Expected Results Different [CODE] and [CODE] for the best and final estimators. Accordingly, [CODE] are not same for the best estimator and the final estimator. However, the code uses the best estimator's [CODE] for the final estimator. ### Actual Results The best estimator: [CODE_BLOCK] The final estimator: [CODE_BLOCK] ### Versions ```shell System: python: 3.10.9 | packaged by conda-forge | (main, Feb 2 2023, 20:20:04) [GCC 11.3.0] executable: \/opt\/conda\/bin\/python machine: Linux Python dependencies: sklearn: 1.5.2 pip: 24.2 setuptools: 75.1.0 numpy: 1.26.4 scipy: 1.13.0 Cython: None pandas: 2.1.1 matplotlib: 3.5.3 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 8 prefix: libgomp filepath: \/opt\/conda\/lib\/libgomp.so.1.0.0 version: None user_api: blas internal_api: mkl num_threads: 4 prefix: libmkl_rt filepath: \/opt\/conda\/lib\/libmkl_rt.so.2 v...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30166"
  },
  {
    "number":30910,
    "text":"Wrong result in log_loss when labels and corresponding y_pred columns are not ordered\n\n### Describe the bug Log loss is not computed correctly when labels (and their corresponding columns in [CODE]) are not in ascending (for numbers) \/ lexicographic (for strings) order. ### Steps\/Code to Reproduce [CODE_BLOCK] Labels beings strings is not the issue, swiping \"dog\" with 1 and \"cat\" with 0 reproduces the bug. ### Expected Results Both log losses should be zero since in both cases [CODE] predicts [CODE] with 100% probability ### Actual Results [CODE_BLOCK] First loss with ordered labels is zero. Second loss is 36.043 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30910"
  },
  {
    "number":27879,
    "text":"Pandas Copy-on-Write mode should be enabled in all tests\n\n### Describe the bug Pandas COW will be enabled by default in version 3.0. For example, today I just found that [CODE] doesn't work properly with it enabled. There are probably many other examples that could be uncovered by testing. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error. ### Actual Results ``[CODE]TargetEncoder` to X and y. 189 190 Parameters (...) 201 Fitted encoder. 202 \"\"\" --> 203 self._fit_encodings_all(X, y) 204 return self File ~\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/sklearn\/preprocessing\/_target_encoder.py:332, in TargetEncoder._fit_encodings_all(self, X, y) 330 if self.smooth == \"auto\": 331 y_variance = np.var(y) --> 332 self.encodings...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27879"
  },
  {
    "number":27256,
    "text":"Lasso incompatible with scipy=1.11 with sparse X\n\n### Describe the bug The Lasso() regressor seems to be incompatible with the newest release of scipy=1.11.0 with sparse X input. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results With fit_intercept=False, it fails with a strange TypeError which looks like it's not intended. With fit_intercept=True, it fails with a TypeError saying it wants a csc array, which is provided though. This seems to be an inconsistency with the latest scipy version and should be considered in the conda installation if it turns out to be an unintended bug. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27256"
  },
  {
    "number":25572,
    "text":"RFC Guideline for usage of Cython types\n\n## Goal Have a documented consensus on which types to use in Cython code. ### Types We should distinguish between floating point numbers and integers. We may also split the use cases of integers: As data value and as index for pointers and memoryviews. ## Linked issues #24153, [URL] [URL] [URL] Also note the Cython bug with [CODE] types: [URL]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25572"
  },
  {
    "number":24764,
    "text":"MLPEstimator do not report the proper [CODE] after successive call of [CODE]\n\n### Describe the bug It seems that [CODE] and [CODE] does not take into account [CODE] with [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] You can also try [CODE] which leads to over iterating. ### Expected Results [CODE] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24764"
  },
  {
    "number":30907,
    "text":"DOC Update wikipedia article for scikit-learn\n\n### Describe the issue linked to the documentation The wikipedia article on scikit-learn, but it is documentation nevertheless. ### Suggest a potential alternative\/fix We can potentially get inspired by other projects wikipedia articles, such as [XGBoost]([URL] but some axis that can be added to the article are: - [x] The [wiki article for TensorFlow]([URL] has an [CODE] section. We can do the same using our [Testimonials]([URL] as inspiration. - [ ] Update the [CODE] section and add more recent developments and features as per the [release highlights]([URL] - [x] Add an [CODE] section to mention e.g. [this Open Source Software Award]([URL] or [this price for innovation]([URL] - [ ] Mention additional metrics, e.g. the [2021]([URL] and [2022]([URL] kaggle surveys question regarding which machine learning frameworks are used by data scientists on a regular basis? The priority should be to update the English version, but not limited to it. In any case, this issue will be considered as solved once all the above points are addressed for said language. Because of that, if you have contributed to the article, please keep the community posted by commenting on this issue.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30907"
  },
  {
    "number":27026,
    "text":"Cannot install scikit-learn==1.0.2 with poetry toml file\n\n### Describe the bug I am using a poetry.toml file to install the dependencies of my package. But when trying to install scikit-learn==1.0.2, it fails. Below is my poetry.toml file: [CODE_BLOCK] ### Steps\/Code to Reproduce So I have created a conda environement for python-3.11 env and then installed the dependencies, the following are the steps: [CODE_BLOCK] ### Expected Results No error is thrown and the dependencies are installed ### Actual Results ``` n_class = get_nr(model) n_class = n_class  (n_class - 1) \/\/ 2 dec_values = np.empty((T_indptr.shape[0] - 1, n_class), dtype=np.float64) cdef BlasFunctions blas_functions blas_functions.dot = _dot[double] ^ ------------------------------------------------------------ sklearn\/svm\/_libsvm_sparse.pyx:412:29: Cannot assign type 'double (int, double , int, double , int) except  nogil' to 'dot_func' Traceback (most recent call last): File \"\/tmp\/tmprjb2urui\/.venv\/lib\/python3.11\/site-packages\/Cython\/Build\/Dependencies.py\", line 1325, in cythonize_one_helper return cythonize_one(*m) ^^^^^^^^^^^^^^^^^ File \"\/tmp\/tmprjb2urui\/.venv\/lib\/python3.11\/site-packages\/Cython\/Build\/Dependencies.py\", line 1301, in cythonize_one raise CompileError(None, pyx_file) Cython.Compiler.Errors.CompileError: sklearn\/svm\/_libsvm_sparse.pyx warning: sklearn\/tree\/_criterion.pxd:57:45: The keyword 'nogil' should appear at the end of the function signature line. Placing it before 'except' or 'noexcept' wil...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27026"
  },
  {
    "number":29416,
    "text":"LogisticRegressionCV does not handle sample weights as expected when using liblinear solver\n\nNote: this is a special case of a the wider problem described in: - [URL] ### Describe the bug [CODE] used within [CODE] with [CODE] solver not returning the same coefficients when weighting samples using [CODE] versus when repeating samples based on weights. NOTE: L801 in [CODE] does not pass [CODE] into scorer when scorer is not specified, needs fixing. ### Steps\/Code to Reproduce ```python import numpy as np from sklearn.datasets import make_classification from sklearn.metrics import get_scorer from sklearn.linear_model import LogisticRegression, LogisticRegressionCV from sklearn.model_selection import LeaveOneGroupOut import sklearn sklearn.set_config(enable_metadata_routing=True) rng = np.random.RandomState(0) X, y = make_classification( n_samples=300000, n_features=8, random_state=10, n_informative=4, n_classes=2, ) n_samples = X.shape[0] \/\/ 3 sw = np.ones_like(y) # We weight the first fold n times more. sw[:n_samples] = rng.randint(0, 5, size=n_samples) groups_sw = np.r_[ np.full(n_samples, 0), np.full(n_samples, 1), np.full(n_samples, 2) ] splits_weighted = list(LeaveOneGroupOut().split(X, groups=groups_sw)) # We repeat the first fold n times and provide splits ourselves and overwrite ## initial resampled data X_resampled_by_weights = np.repeat(X, sw.astype(int), axis=0) ##Need to know number of repitions made in total n_reps = X_resampled_by_weights.shape[0] - X.shape[0] y_re...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29416"
  },
  {
    "number":30772,
    "text":"Wrong Mutual Information Calculation\n\n### Describe the bug #### Issue I encountered a bug unexpectedly while reviewing some metrics in a project. When calculating mutual information using the CODE], I noticed values higher than entropy, which is [impossible. ##### Implication Any algorithm sorting features based on [CODE] or any metric based on this function may be affected. Thanks a lot for putting time on this. P.S. In the minimal example, the feature is fixed (all one). However, I encountered the same issue in other scenarios as well. The example is just more simplified. The problem persists on both Linux and Mac. I attached personal computer session info. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ] executable: \/Users\/\/miniconda3\/envs\/\/bin\/python machine: macOS-15.1.1-arm64-arm-64bit Python dependencies: sklearn: 1.6.1 pip: 23.3.1 setuptools: 68...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30772"
  },
  {
    "number":30770,
    "text":"Issue with binary classifiers in _check_sample_weight_equivalence?\n\n### Describe the bug Hello, I tried to make my custom binary classifier pass the estimator checks with scikit-learn 1.6. The sample weight equivalence properties worked on <1.5 and not 1.6. I think the issue is related to how the binary tag is enforced on the generated targets for the _check_sample_weight_equivalence : [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect both to pass. ### Actual Results From the following snippet, copy-pasted mostly from : [URL] ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils import shuffle from sklearn.utils.estimator_checks import _enforce_estimator_tags_y class BinaryClassifier(ClassifierMixin, BaseEstimator): def __sklearn_tags__(self): tags = super().__sklearn_tags__() tags.classifier_tags.multi_class = False return tags rng = np.random.RandomState(42) n_samples = 15 X = rng.rand(n_samples, n_samples * 2) y = rng.randint(0, 3, size=n_samples) # Use random integers (including zero) as weights. sw = rng.randint(0, 5, size=n_samples) X_weighted = X y_weighted = y # repeat samples according to weights X_repeated = X_weighted.repeat(repeats=sw, axis=0) y_...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30770"
  },
  {
    "number":29163,
    "text":"Explain how predict_proba is computed from (Hist)GradientBoostingClassifier\n\n### Describe the issue linked to the documentation I think users would find it useful to see how class probabilities are computed for gradient boosting classifiers \ud83d\ude0a Tried to look into the source but could not understand it. Would be up to taking care of this, but I'd need a reference. Thank you! ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29163"
  },
  {
    "number":25596,
    "text":"Train-test-split for multilabel datasets\n\n### Describe the workflow you want to enable Train-test splits for 2-dimensional targets (i.e. multi-label datasets). I saw couple of issues related to multi-label classification, but as far as I can tell, train-test-split has not been addressed there. Related forum questions: - [URL] - [URL] ### Describe your proposed solution See description. ### Describe alternatives you've considered, if relevant Use of one of the following packages: - [URL] - [URL] with iterative_stratification method: [URL] ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25596"
  },
  {
    "number":31722,
    "text":"[CODE] for [CODE] may fail randomly with sparse vs dense data\n\n### Describe the bug The [<code>test_unsorted_indices<\/code>]([URL] function occasionally fails on CI when comparing the coefficients of [CODE] trained on dense vs sparse data. I suspect this is due to additional randomness introduced by the internal cross-validation and Platt scaling when [CODE] is set. See the [SVC documentation]([URL] for reference. ### Steps\/Code to Reproduce Unfortunately, I haven't been able to reproduce the failure reliably. I've only seen it fail three times when creating or reviewing PRs, but the error disappears after re-running CI. I've also tried looping through various [CODE] values without triggering a failure locally. For now, I'm labelling this with \"Hard\" and \"Needs Reproducible Code.\" ### Expected Results [CODE_BLOCK] should consistently pass. ### Actual Results In rare cases, the assertion fails: ```console AssertionError: Not equal to tolerance rtol=1e-07, atol=0 Mismatched elements: 2 \/ 2880 (0.0694%...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31722"
  },
  {
    "number":25612,
    "text":"Simplify decision tree removing redundant decisions\n\n### Describe the workflow you want to enable Description: Add a new method simplify() to the decision tree Class that returns a simplified version of the decision tree by pruning redundant leaves that do not add new decision paths. This simplification method will create a new instance of the tree with fewer nodes, allowing users to obtain a more concise and interpretable tree. Motivation: Decision trees are known for their ability to provide easily interpretable models, but sometimes they can grow to a size that makes interpretation challenging. A simplified version of the decision tree will make it easier for users to understand the model, especially in situations where the tree has a large number of redundant leaves. Example: !image method to the decision tree module that returns a simplified version of the tree. The method will prune redundant leaves that do not add new decision paths. The new tree will have fewer nodes and be easier to interpret. Example Usage: [CODE_BLOCK] ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25612"
  },
  {
    "number":25534,
    "text":"[CODE] returns error for [CODE] with int64 arrays\n\n### Describe the bug When CODE] is called with two numpy int64 arrays as y_true and y_pred, an error is thrown in the [CODE] function in [sklearn <ipython-input-34-c0c8e8c6d0fb> 7 ----> 8 precision_score(y_true, y_pred, labels=class_names, average='macro', zero_division=0) 5 frames \/usr\/local\/lib\/python3.8\/dist-packages\/sklearn\/metrics\/_classification.py 1755 array(0.5, 1. , 1. ]) 1756 \"\"\" -> 1757 p, _, _, _ = precision_recall_fscore_support( 1758 y_true, 1759 y_pred, [\/usr\/local\/lib\/python3.8\/dist-packages\/sklearn\/metrics\/_classification.py 1546 # Calculate tp_sum, pred_sum, true_sum ### 1547 samplewise = average == \"samples\" -> 1548 MCM = multilabel_confusion_matrix( 1549 y_true, 1550 y_pred, [\/usr\/local\/lib\/python3.8\/dist-packages\/sklearn\/metrics\/_classification.py]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25534"
  },
  {
    "number":29830,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Sep 13, 2024) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29830"
  },
  {
    "number":26270,
    "text":"Unhelpful error message when running a classifier on a regression outcome\n\n### Describe the bug When running a classifier on a regression outcome, we get a really unhelpful error message: ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results An error message that puts the user on the right track. ### Actual Results We could add the following sentence to the error message \"Maybe you are trying to fit a classifier (discrete classes) on a regression target (continuous values)\". ### Versions ```shell Present on main. In [6]: import sklearn; sklearn.show_versions() System: python: 3.10.7 (main, Mar 10 2023, 10:47:39) [GCC 12.2.0] executable: \/usr\/bin\/python3 machine: Linux-5.19.0-38-generic-x86_64-with-glibc2.36 Python dependencies: sklearn: 1.3.dev0 pip: 22.2 setuptools: 59.6.0 numpy: 1.21.5 scipy: 1.8.1 Cython: 0.29.34 pandas: 1.3.5 matplotlib: 3.5.2 joblib: 1.1.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26270"
  },
  {
    "number":31290,
    "text":"[CODE] triggers [CODE] when used with [CODE]\n\n### Describe the bug Here's something I noticed while looking into [URL] The test [CODE_BLOCK] checks that a copy is produced, and that no [CODE] is produced Indeed, no copy is raised, but why is using [CODE] with a slice allowed to not make a copy? Is this intentional? Based on responses, I can suggest what to do instead in [URL] (I am a little surprised that this always makes copies, given that a lot of the discussion in [URL] centered around wanting to avoid copies) ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No [CODE] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.11 (main, Dec 4 2024, 08:55:07) [GCC 11.4.0] executable: \/home\/marcogorelli\/scikit-learn-dev\/.venv\/bin\/python machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.7.dev0 pip: 24.2 setuptools: None numpy: 2.1.0 scipy: 1.14.0 Cython: 3.0.11 pandas: 2.2.2 matplotlib: None joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libscipy_openblas ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31290"
  },
  {
    "number":25550,
    "text":"OneHotEncoder [CODE] attribute description in presence of infrequent categories\n\n### Describe the issue linked to the documentation ### Issue summary In the OneHotEncoder documentation both for v1.2 >>> pd.DataFrame(Xt, columns = enc.get_feature_names_out()) ent_categories_ x0_c x0_d x0_e x0_infrequent_sklearn 0 0.0 0.0 0.0 1.0 1 0.0 0.0 0.0 1.0 2 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 5 0.0 0.0 0.0 0...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25550"
  },
  {
    "number":30278,
    "text":"Unifying references style in docstrings in _pca.py\n\n### Describe the issue linked to the documentation A very minor suggested change to write references section of function docstrings in identical style in _pca.py This implements the method of [CODE]_ POST-EDIT This implements the method from: [CODE]_ ORIGINAL (line 324 to 347) For n_components == 'mle', this class uses the method from: [CODE]_ Implements the probabilistic PCA model from: `Tipping, M. E.,...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30278"
  },
  {
    "number":31750,
    "text":"Full Python\/sklearn Adaptation of py-earth\n\n### Describe the workflow you want to enable A full Python (not c or cython) port of py-earth, an archived sklearn project. ### Describe your proposed solution - MARS regression is a great and really practical technique. - py-earth implemented this, based in the R earth library. - The archived state of py-earth means it's only possible to get working with old dependencies which limits the ability to use it with newer tools and in more current workflows.. ### Describe alternatives you've considered, if relevant - I tried to modernise py-earth, but got tripped up on lots of issues such as Python 2 to 3 conversion, the old scipy dependencies etc. - py-earth was mostly consistent with sklearn, but not completely. - I've created a full Python port (repo still private, as the repo is still a bit messy), as a secondary output of my PhD. - I would like to try introduce it as a 'spiritual' successor to py-earth and collaborate with the sklearn community. - Keen to get some guidance on approaching this, as I'm relatively new to contributing. ### Additional context - For policy and decision contexts, the stepwise linear approach and combination of a visualisable model and change points, means MARS regression has advantages over other modelling methods. - For changepoint analysis involving gradients, MARS is easier and nicer to work with than PELT-based changepoints (ruptures). - What this means is that in sklearn workflows, it's potentially a ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31750"
  },
  {
    "number":25187,
    "text":"Early Stopping for GridSearchCV, RandomizedSearchCV\n\n### Describe the workflow you want to enable - I have a custom model implementing the BaseEstimator, for which I am using scikit-learn's hyperparameter searches. - I am running an exhaustive grid search, all possible parameters for my model. - If one parameter setting gives a sufficiently high accuracy (e.g. F1 = 1.0), I wish to stop the grid search and free the compute for other runs. ### Describe your proposed solution GridSearchCV, RandomizedSearchCV, and others should have an early stopping criteria. I should be able to specify a threshold accuracy, such that when the value returned by the scoring function passes this threshold, other jobs are stopped. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context  As parameter search happens in parallel, it may require some rewrite of the parallelization code so that results can be checked as they return: [URL] See, for instance, [URL]  I am opening this feature request to start a discussion; I am happy to do the development and submit a pull request if we agree on what should be built.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25187"
  },
  {
    "number":28429,
    "text":"preprocesor gives a Value Error when using Simple Imputer + StandardScaler in Pipeline (reopened issue)\n\n### Describe the bug I get a ValueError when using classifier.predict with a variable that has been preprocesse diwht a ColumnTransofrmer that uses SimpleIMputer+StandardScaler with numerical var and OneHotEncoder with categorical var. [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown, and X2 and y ready to use a LogisticRegression() as classifier ### Actual Results ``` base.py:457: UserWarning: X has feature names, but LogisticRegression was fitted without feature names warnings.warn( anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in ?(self, dtype) 1996 def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray: 1997 values = self._values ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28429"
  },
  {
    "number":30008,
    "text":"DOC update MAPE description\n\n### Describe the issue linked to the documentation References #29775 ### Issue The text in the MAPE formula is incorrect. [CODE_BLOCK] ### Discussion for resolution 1. update the upper range 2. provide examples - >MAPE is not in the range [0, 1], it can be arbitrarily large, right? I have to say I don't really know how to word this to make it clearer that a percentage is not returned despite the name. - >Indeed you can make an error of 200%. We need to reformulate and drop the notion of upper bound. - >Maybe give an example where the error is 1 (or 100%) in the user guide to clarify that the returned value is 1 and not 100? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30008"
  },
  {
    "number":24651,
    "text":"Default value for solving an SVM in primal and dual should be determined automatically\n\n### Describe the workflow you want to enable Currently the default value for LinearSVC and LinearSVR with regards to solving the problem in dual is True. This means that for users that are unfamiliar with support-vector machines and do not know the importance of this parameter they could be using te framework inefficiently. I would propose for the default value to be None and a simple check to occur. If n_samples > n_features it's set to dual and otherwise it is set to primal. ### Describe your proposed solution This is a quick and easy fix and would only require a few lines of code in the fit method: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24651"
  },
  {
    "number":32076,
    "text":"[CODE_BLOCK] should take [CODE_BLOCK] as an argument\n\n### Describe the workflow you want to enable The current implementation of TargetEncoder uses [CODE_BLOCK]-cross-validation to avoid data leakage. In cases of longitudinal or clustered data, it is desirable to ensure that rows belonging to the same group or cluster belong to the same train-folds to avoid data-leakage. ### Describe your proposed solution This could be achieved by introducing an optional[CODE_BLOCK] parameter and the use of [CODE_BLOCK]-cross-validation if the [CODE_BLOCK] is not [CODE_BLOCK]. ### Describe alternatives you've considered, if relevant The alternative is to continue ignoring group structure. ### Additional context _No response_",
    "labels":[
      "help wanted",
      "Enhancement"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32076"
  },
  {
    "number":26308,
    "text":"Use scipy.stats.yeojohnson PowerTransformer\n\nInside [CODE], we should use [[CODE]]([URL] instead of our own implementation. [CODE] was release with scipy 1.2.0. With PR #24665, we now have even 1.5.0 as minimum. Edit: Note that [URL] to be release in scipy 1.12 will also resolve #23319.",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26308"
  },
  {
    "number":29016,
    "text":"MultiOutputClassifier does not rely on estimator to provide pairwise tag\n\n### Describe the bug I use the [CODE] function to make [CODE] multilabel. Then, if I use the linear or rbf kernel the cross_validation function works perfectly fine. However, when I use [CODE] with precomputed kernel is having an [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results An weighted f1-score. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)] executable: C:\\Users\\bscuser\\anaconda3\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.2.2 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.26.4 scipy: 1.11.4 Cython: None pandas: 2.1.4 matplotlib: 3.8.0 joblib: 1.2.0 threadpoolctl: 2.2.0 Built with OpenMP: ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29016"
  },
  {
    "number":28898,
    "text":"HistGradientBoostingClassifier raise error with monotonic constraints and categorical features\n\n### Describe the bug Creating an HistGradientBoostingClassifier with _monotonic_cst_ and _categorical_features_ is not possible because it throws an error. The _monotonic_cst_ is a numeric feature that is not included in the categorical features. ### Steps\/Code to Reproduce [CODE_BLOCK] > ValueError: Categorical features cannot have monotonic constraints. [CODE_BLOCK] > ValueError: Categorical features cannot have monotonic constraints. [CODE_BLOCK]python { \"name\": \"ValueError\", \"message\": \"Categorical features cannot have monotonic constraints.\", \"stack\": \"--------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[13], line 10 6 print(X_adult.dtypes) 7 hist = HistGradientBoostingClassifier( 8 monotonic_cst={\\\"age\\\": 1}, categorical_features=\\\"from_dtype\\\" 9 ) ---> 10 hist.fit(X_adult, y_adult) 12 hist = HistGradientBoostingClassifier( 13 monotonic_cst={\\\"age\\\": 1}, categorical_features=[\\\"workclass\\\", \\\"education\\\"] 14 ) 15 hist.fit(X_adult, y_adult) File ~\/Projects\/your_project\/.venv\/lib\/python3.10\/site-packages\/sklearn\/base.py:1474, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, *kwargs) ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28898"
  },
  {
    "number":28003,
    "text":"NearestCentroid FutureWarning with cosine metric\n\n### Describe the bug NearestCentroid class throw a FutureWarning when using [CODE]. Actually, the metric is use for two things: - inside [CODE] for the computation of the centroids: - with euclidean: centroids are computed using the mean of features - with manhattan: centroids are computed using the median [CODE_BLOCK] - inside [CODE]: the metric is used for the pairwise distance [CODE_BLOCK] But, what if I want to use centroids computed with the mean of features and using cosine (or other) metric to compute the pairwise distance ? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results ``` ..\/sklearn\/neighbors\/_nearest_centroid.py:150: FutureWarning: Support for distance metrics other than eu...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28003"
  },
  {
    "number":25844,
    "text":"X does not have valid feature names, but IsolationForest was fitted with feature names\n\n### Describe the bug If you fit an [CODE] using a [CODE] it generates a warning [CODE_BLOCK] This only seems to occur if you supply a non-default value (i.e. not \"auto\") for the [CODE] parameter. This warning is unexpected as a) X does have valid feature names and b) it is being raised by the [CODE] method but in general is supposed to indicate that predict has been called with ie. an ndarray but the model was fitted using a dataframe. The reason is most likely when you pass contamination != \"auto\" the estimator essentially calls predict on the training data in order to determine the [CODE] parameters: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Does not raise \"X does not have valid feature names, but IsolationForest was fitted with feature names\" ### Actual Results raises \"X does not have valid feature names, but IsolationForest was fitted with feature names\" ### Versions ```shell System: python: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] executable: \/home\/david\/dev\/warpspeed-timeseries\/.venv\/bin\/python machine: Linux-5.15.0-67-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.1 pip: 23.0.1 setuptools: 67.1.0 numpy: 1.23.5 scipy: 1.10.0 Cython: 0.29.33 pandas: 1.5.3 matplotlib: 3.7.1 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas prefix: libopenblas filepath: \/home\/david\/dev\/...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25844"
  },
  {
    "number":26211,
    "text":"Possible inconsistency between documentation and code for user guide of PLSCanonical\n\n### Describe the issue linked to the documentation Recently I've been learning Partial Least Squares (PLS), and learned that there are some variations of PLS, namely PLS-Canonical, PLS-SVD, PLS2 and PLS1 (mainly from the User Guide of cross decomposition methods, with emphasis on the two-block case, by JA Wegelin]([URL] Nevertheless, I still have questions about the details of predicting $Y$ based on new samples. In the user guide, it states that to compute $Y$, we need > The idea is to try to predict the transformed targets $\\Omega$ as a function of the transformed samples $\\Xi$, by computing $\\alpha \\in \\mathbb{R}$ such that $\\Omega = \\alpha \\Xi$. > > ... > > ... and as a result the coefficient matrix $\\beta = \\alpha P \\Delta^T$. However in code, [CODE] is computed by [CODE_BLOCK] This code is located at [L355]([URL] of [CODE]. If I am correct, here [CODE] is a transformation matrix such that [CODE] would be $\\Xi$. Therefore [CODE] would be $P$. And [CODE] is $\\Delta$. As a result, the code is computing $\\beta = P \\Delta^T $, which I think is inconsistent with the documentation described above. ### Suggest a potential alternative\/fix In my honest opinion, maybe we should add the computation of $\\alpha$ into the code. Moreover, I think $\\alpha$ shouldn't be a scalar; instead, $\\alpha$ should be a vector of length $R$, where $R$ is the number of columns of $\\Xi$, i.e. the num...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26211"
  },
  {
    "number":25492,
    "text":"Enable feature selectors to pass pandas DataFrame to estimator\n\n### Describe the workflow you want to enable When running SequentialFeatureSelector (or, presumably, other feature selection methods) with a pandas DataFrame input, the reduced-feature input is passed to the estimator as a numpy array. This seems inconsistent with the other transformers that successfully maintain the pandas indices. My actual use case is the following: Select the best k features that complement a set of fixed features. I was going to implement it like this: (happy to take suggestions for alternatives). [CODE_BLOCK] ((10, 3), (10, 5)) [CODE_BLOCK] ValueError: All the 5 fits failed. ..... File \"\/tmp\/ipykernel_275\/897617596.py\", line 2, in <lambda> appender= FunctionTransformer(lambda X:pd.concat([X,fixed_features.reindex(X.index)],axis=1)) AttributeError: 'numpy.ndarray' object has no attribute 'index' ### Describe your proposed solution I believe that it should be possible to pass the reduced matrix to the estimator as a pandas DataFrame, and this would be consistent with the way other Transformers work currently. ### Describe alternatives you've considered, if relevant An alternative that would suffice for my particular ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25492"
  },
  {
    "number":30638,
    "text":"Documenting return array types\n\nSince we are introducing Array API compatibility we are discussing that some functions (especially in the metrics section) would not return the input array type, but a numpy array. How would we document that, so that users know what they get as a return type? We have started to discuss this here, but this discussion a bit scattered and in this issue I am trying to bring this together. I would think we need to find a standard way of how to talk about return types in the docstrings. - use the terms [CODE] and [CODE] (or something more eye-catching) for the input array type - from the docstring, link to a dedicated section in the glossary, explaining the differences between [CODE] and [CODE] and which are the implications for the users What are the general feelings about that? @ogrisel @OmarManzoor @adrinjalali (I don't want to bother you by tagging, but it would be interesting to hear the takes of betatim, thomasjpfan, lesteve and jeremiedbb as well if they are interested :sweat_smile:)",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30638"
  },
  {
    "number":29200,
    "text":"Add Sparse DataFrame support\n\n### Describe the workflow you want to enable Hello ! It seems that we can configure ColumnTransformer to return a DataFrame via the set_output function. On the other hand, this requires deactivating sparsity on each underlying transformer which has a negative impact on memory consumption (as explained among others here URL] However, we can in principle use a Sparse DataFrame to store sparse data, right? ![image. ```python import pandas as pd from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder from sklearn.compose import ColumnTransformer class PandasColumnTransformer(ColumnTransformer): \"\"\" > Auto-handle Pandas DataFrame : take DataFrame as input and return Dataframe as output after transform\/fit_transform > Handle Sparse DataFrame \"\"\" def fit(self, X, y=None): if type(X) is not pd.DataFrame: raise Exception('PandasColumnTransformer is designed for Pandas DataFrame only') return super().fit(X, y) def transform(self, X, params): if type(X) is not pd.DataFrame: raise Exception('PandasColumnTransformer is designed for Pandas DataFrame only') transformed=super().transform(X, params) #### After applying transform, we ensure that we return a DataFrame \/ sparse DataFrame return self.to_pandas(transformed, X.index) # .fit_transform doesnt use .transform, we need to override it too def fit_transform(self, X, y=None, params): if type(X) is not pd.DataFrame: raise Exception('PandasColumnTransformer is designed for Pandas DataFrame ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29200"
  },
  {
    "number":27563,
    "text":"sklearn.utils._param_validation.InvalidParameterError: The 'zero_division' parameter of precision_score must be a float among {0.0, 1.0, nan} or a str among {'warn'}. Got nan instead\n\n### Describe the bug I'm trying to use [CODE] with [CODE] for the [CODE]. It's not working with [CODE] but working when I do manual cross-validation with the same pairs. ### Steps\/Code to Reproduce Here's the data files to reproduce: [sklearn_data.pkl.zip]([URL] [CODE_BLOCK]python # If I use cross_val_score: cross_val_score(estimator=estimator, X=X, y=y, cv=cv, scoring=scoring, n_jobs=n_jobs [CODE_BLOCK]pytb \/Users\/jespinoz\/anaconda3\/envs\/soothsayer_py3.9_env2\/lib\/python3.9\/site-packages\/sklearn\/model_selection\/_validation.py:839: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27563"
  },
  {
    "number":24992,
    "text":"Sklearn pip installation broke because of issues with setuptools\n\n### Describe the bug Apparently because of this other issue [URL] Installation of sklearn breaks with pip install right now. [CODE_BLOCK] I've read some other teams sticking to setuptools 65.5.1 for the moment, scikit learn seems to be aiming at 60+ ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error should be thrown ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24992"
  },
  {
    "number":30542,
    "text":"AttributeError: 'super' object has no attribute '__sklearn_tags__'\n\n### Describe the bug [CODE_BLOCK] ### Steps\/Code to Reproduce . ### Expected Results Working XGBClassifier model ### Actual Results None ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30542"
  },
  {
    "number":31628,
    "text":"DOC: Glossary contains several FIXME tags\n\n### Describe the issue linked to the documentation The Glossary for scikit-learn contains several FIXME tags. [URL] ### Suggest a potential alternative\/fix FIXME tags can be used for future improvement, but I think they should belong in code comments instead of the Glossary page.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31628"
  },
  {
    "number":32049,
    "text":"The dcg_score and ndcg_score documentation are hard to understand\n\n### Describe the issue linked to the documentation The documentation for the [CODE] and [CODE] leave much to be desired. I believe this is also a by-product of competing definitions of the discount cumulative gains (DCG) and normalised DCG (nDCG) in literature. Namely $$\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)}$$ and $$\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)}.$$ The [CODE] uses the former definition, I do not think this is very clear. The description for the DCG score ([CODE]) says \"Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount\". While this is technically correct to what [CODE] does, like many maths equations, it is hard to understand without using maths notation. If a user wants to clarify the exact equation of the DCG used past the description they might go to the references, however, - the first reference is the Wikipedia which offers both definitions; - the third reference, \"Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)\", defines the discount function an a much more general way. While interesting does not give the user any insight into how the [CODE] is actually implemented. My criticism of the [CODE] is the same. ### Suggest a potential a...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32049"
  },
  {
    "number":24967,
    "text":"Transformer for nominal categories, with the goal of improving category support in decision trees\n\nI'd like to find out how keen people would be for adding a transformer like this. ### Describe the workflow you want to enable Improved support for nominal categories in tree based models. Nominal categories are ones with no order to them, for example colour or country (c.f. ordinal categories that are ordered). ### Describe your proposed solution In #12866 @amueller linked to Splitting on categorical predictors in random forests by cleverly computed numerical values (blue -> 0.643, green ->0.123, ...) that allow you to achieve similar\/equal performance by using [CODE] as splitting decision in a tree node compared to performing an exhaustive search of all possible categorical splits. I would implement this as a transformer that you use together with a random forest or other tree based model in a pipeline. ### Describe alternatives you've considered, if relevant There have been several PRs attempting to add native category support to decision trees. #12866 is the latest one. My impression is that it would be cool to have this in trees but that it is not easy to do, several people have tried but no PR has landed yet. The \"Breimann trick\" that is used is also limited in the number of categorical values it supports, where this new idea seems to support unlimited categorical values and multi class classification. ### Additional context An earlier paper that is cited in \"Splitting cat...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24967"
  },
  {
    "number":30195,
    "text":"issue in building from source with Windows64 Python 3.12.7\n\n### Describe the bug I am currently following the guide on building from source did not run successfully. \u2502 exit code: 1 \u2570\u2500> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. full command: 'C:\\Users\\Eden_\\Desktop\\Coder\\MachineLearning\\scikit-learn\\.venv\\Scripts\\python.exe' 'C:\\Users\\Eden_\\Desktop\\Coder\\MachineLearning\\scikit-learn\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py' prepare_metadata_for_build_editable 'C:\\Users\\Eden_\\AppData\\Local\\Temp\\tmpxaql9nw9' cwd: C:\\Users\\Eden_\\Desktop\\Coder\\MachineLearning\\scikit-learn Preparing editable metadata (pyproject.toml) ... error error: metadata-generation-failed \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. [notice] A new rele...",
    "labels":[
      "Documentation",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30195"
  },
  {
    "number":24501,
    "text":"plot_learning_curve.py should not sort the fit time axis before plotting\n\nDears, About 10 months ago, the [CODE] example was changed by Mr. @thomasjpfan to sort the [CODE] plot axis. In my humble opinion, that's wrong because a learning curve is train-size ascending regardless the time it spent training. For very small train sizes\/intervals one can se a slightly smaller train size taking a slightly longer time to fit. Furthermore, a computer performance may oscillates in any learning curve step which should not affect the actual order of the learning curve steps in the plot. [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24501"
  },
  {
    "number":25655,
    "text":"Implement class to reverse item encodings\n\n### Describe the workflow you want to enable Hi! As a psychologist working with questionnaire data, you will find yourselves in situations where you have to reverse the values in certain variables (i.e. questionnaire items), because they have reversed logic (see this link. ### Describe your proposed solution Implement a class that reverses variable values: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant Most certainly users would have to use this class as input to [CODE] because you only want to reverse certain variables and leave the rest as it is. ### Additional context Not sure though, if this might out of scope of scikit-learn. If not, maybe there's a contrib project that could use this?",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25655"
  },
  {
    "number":29790,
    "text":"Include T-Processes Subclass of Gaussian-Processes\n\nThis is a feature request to implement T-process. Moving the discussion into the issue tracker to get more visibility. ### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by conradstevens May 3, 2024<\/sup> I am implementing T-Processes (TP)s In addition to Gaussian Processes (GP)s. Due to the great similarities in structure and functionality I have made a new class _TProcessRegressor_ a subclass of _GaussianProcessRegressor_. However, this will result in duplicate code. If it where up to me, there would be a class __stochasticProcessRegressor_ that GPs and TPs would both inherit from. This would minimize duplicate code and make integrating other stochastic processes (eg Gamma Processes) more straight forward. However, this architecture change would complicate an eventual pull request. I am interested to hear peoples thoughts on implementing TPs and the pros and cons of the two architectures. T-Process Background: [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29790"
  },
  {
    "number":29168,
    "text":"Unable to use TunedThresholdClassifierCV' in kaggle\n\n### Describe the bug Unable to use TunedThresholdClassifierCV' in Kaggle ### Steps\/Code to Reproduce from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report from sklearn.model_selection import TunedThresholdClassifierCV, train_test_split X, y = make_classification( n_samples=1_000, weights=[0.9, 0.1], class_sep=0.8, random_state=42 ) X_train, X_test, y_train, y_test = train_test_split( X, y, stratify=y, random_state=42 ) classifier = RandomForestClassifier(random_state=0).fit(X_train, y_train) print(classification_report(y_test, classifier.predict(X_test))) ### Expected Results no error ### Actual Results from sklearn.model_selection import TunedThresholdClassifierCV ImportError: cannot import name 'TunedThresholdClassifierCV' from 'sklearn.model_selection' (\/opt\/conda\/lib\/python3.10\/site-packages\/sklearn\/model_selection\/__init__.py) ### Versions ```shell System: python: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] executable: \/opt\/conda\/bin\/python3.10 machine: Linux-5.15.133+-x86_64-with-glibc2.31 Python dependencies: sklearn: 1.2.2 pip: 23.3.2 setuptools: 69.0.3 numpy: 1.26.4 scipy: 1.11.4 Cython: 3.0.8 pandas: 2.2.2 matplotlib: 3.7.5 joblib: 1.4.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 4 prefix: libopenblas filepath: \/...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29168"
  },
  {
    "number":25261,
    "text":"'DataFrame' object has no attribute 'dtype'\n\n### Describe the bug I believe my attached program is correct, but it errors out prematurely. with the error: DataFrame' object has no attribute 'dtype'. I've spent many hours trying to debug this. I know the key trace line is [CODE_BLOCK] The problem is X is a Pandas DataFrame, which does not have a dtype attribute. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expecting the round trip thru transform() and inverse_transform() to yield the identity function. ### Actual Results ```pytb Traceback (most recent call last): File \"\/Users\/gerard\/........\/src\/test\/python\/bug_function_xformer.py\", line 21, in <module> pipe.fit(df) File \"\/Users\/gerard\/........\/env\/lib\/python3.9\/site-packages\/sklearn\/pipeline.py\", line 406, in fit self._final_estimator.fit(Xt, y, fit_params_last_step) File \"\/Users\/gerard\/........\/env\/lib\/python3.9\/site-packages\/sklearn\/compose\/_column_transformer.py\", line 693, in fit self.fit_transform(X, y=y) File \"\/Users\/gerard\/........\/env\/lib\/python3.9\/site-packages\/sklearn\/utils\/_set_output.py\", line 142, in wrapped data_to_wrap = f(self, X, *args, kwargs) File \"\/Users\/gerard\/........\/env\/lib\/python3....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25261"
  },
  {
    "number":29383,
    "text":"API Change default argument for [CODE] to be [CODE] in [CODE]\n\n### Summary #27639 changed the default behavior of [CODE] to be proportions rather than absolute weighted sample counts. This then caused some discrepancies in how the tree is to be interested and plotted in the examples. #29331 introduced a fix in the documentation to align the explanation of [CODE] and what was shown in the examples about the tree structure. It is a bit weird that [CODE] by default shows different values compared to if you inspect [CODE] itself. ### Proposal 1. Rather than go through a tedious deprecation cycle, I propose to change the default behavior of [CODE] to have [CODE] to match what is shown in [CODE]. 2. Ambitiously, I would also say that [CODE] is ambiguous and poorly named and we should also change it to [CODE], which is more clear what that array is. Seeing as how [CODE] lives in Cython, and we still don't have public support of that API, I would even say that does not need a deprecation cycle, as the ppl who would use it are power users. ### Extra Comments @adam2392 if you feel adventurous you may want to open an issue about aggressively switching the default to [CODE] in [CODE] in order to match [CODE] and get other maintainers feeling about it. Not sure if other visualization functions are affected by this to be 100% honest \u2026 _Originally posted by @lesteve in [URL]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29383"
  },
  {
    "number":27271,
    "text":"feature_names returned by load_breast_cancer() is np.array, not list.\n\n### Describe the bug According to the online documentation(URL] [CODE] and [CODE] should be a list, but the value returned by the [CODE] function is a numpy array. This isn't a big deal, but it can cause errors in some functions. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results ![tree \/var\/folders\/gb\/pxd5k4k97dnc9s5kxnt9nq2w0000gn\/T\/ipykernel_27513\/3107534418.py in <module> 10 tree = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train) 11 ---> 12 plot_tree(tree, class_names=cancer.target_names, 13 feature_names=cancer.feature_names) 14 plt.show() ~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/utils\/_param_validation.py in wrapper(args, *kwargs) 199 params = {k: v for k, v in params.arguments.items() if k not in to_ignore} 200 --> 201 validate_parameter_constraints( 202 parameter_constraints, params, caller_name=func.__qualname__ 203 ) ~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/sklearn\/utils\/_param_validation.py in validate_parameter_constraints(parameter_cons...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27271"
  },
  {
    "number":25216,
    "text":"Mention that MLPRegressor can function as an autoencoder\n\n### Describe the issue linked to the documentation The MLPRegressor can function as an autoencoder by passing X as input and target (i.e. X == y). [CODE_BLOCK] I use PCA for dimensionality reduction a lot, but kept going to torch for autoencoders for comparison purposes. I thought adding an autoencoder to scikit-learn would be a good idea, but then it hit me that this is equivalent to an autoencoder. Yet this is not mentioned in the docs anywhere. Relevant articles: Docs: [URL] Guide: [URL] ### Suggest a potential alternative\/fix I think mentioning this in the docs, or in the dimensionality reduction user guide could be a good idea. I'm happy to make a PR, but I'd like to get your input first.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25216"
  },
  {
    "number":30675,
    "text":"Possible bug in sklearn 1.6.1 PartialDependenceDisplay.from_estimator when target and feature are both binary\n\n### Describe the bug PartialDependenceDisplay.from_estimator does not seem able to handle dummy variables when the response variable is binary. See example below. The example works fine in 1.5.2 but returns CODE] in 1.6.1 ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results PDP plots for age and smoker. ### Actual Results ```tb --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[1], [line 19 16 rf_model = RandomForestClassifier(n_estimators=100, random_state=42) 17 rf_model.fit(X, y) ---> 19 pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1]) File ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:707, in PartialDependenceDisplay.from_estimator(cls, estimator, X, features, sample_weight, categorical_features, feature_names, target, response_method, n_cols, grid_resolution, percentiles, method, n_jobs, ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30675"
  },
  {
    "number":26296,
    "text":"Add support for [CODE] arg in [CODE]\n\n### Describe the workflow you want to enable Similar to other transformers (e.g., [CODE]), support an arg [CODE] in [CODE]. ### Describe your proposed solution Add an arg [CODE] in [CODE]. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26296"
  },
  {
    "number":28891,
    "text":"Easily retrieve mapping from OrdinalEncoder\n\n### Describe the workflow you want to enable It would be nice to be able to easily retrieve mapping in the form of a dictionary [CODE_BLOCK] Currently .categories_ attribute only retrieves list of seen categories, without mapping. This becomes especially important with options to handle missing or infrequent values, which are leading to questions \"What value does infrequent categories map to?\" and so on. ### Describe your proposed solution Add .categories_map_ attribute to OrdinalEncoder ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28891"
  },
  {
    "number":29378,
    "text":"DOC Improve maintainers page\n\nI've found the maintainers page",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29378"
  },
  {
    "number":29799,
    "text":"Importing sklearn takes too much time compared to other imports except spaCy\n\n### Describe the bug Can you open a separate issue please with more details about your problem? In particular, please include the following information in your new issue: - is this a regression in scikit-learn 1.5, i.e. did your code work scikit-learn 1.4? + I tested other versions still same but it's slower than compared to older versions probably due to new features etc. But 1.5.1 less problematic than 1.5 - ideally a stand-alone snippet that reproduces the behaviour on your machine and that others can try to run. Without a stand-alone snippet to reproduce, there is very little we can do to help ... [CODE] Takes 155 seconds to load. [CODE] 384 seconds in my machine at python 3.12 and scikit-learn version 1.5.1 Windows 11 Latest version of Windows SDK and VM with 8gb ram. Other modules loaded less than 1 seconds. Full code of script: [URL] _Originally posted by @lesteve in [URL] ### Steps\/Code to Reproduce [CODE] ### Expected Results Excepted: Loaded in 2 minutes ### Actual Results Results: [CODE] takes 155 seconds, [CODE] takes 384 seconds ### Versions ```shell Main machine: python: 3.12.5 (tags\/v3.12.5:ff3bc82, Aug 6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)] executable: c:\\Users\\victim\\AppData\\Local\\Programs\\Python\\Python312\\python.exe machine: Windows-11-10.0.22631-SP0 Python dependencies: sklearn: 1.5.1 pip: 24.2 setuptools: 73.0.1 numpy: 1.26.4 scipy: 1.14.1 Cython: 3.0.11 pandas: None matpl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29799"
  },
  {
    "number":28144,
    "text":"DOC Need Gini coefficient implementation\n\n### Describe the workflow you want to enable Hello there! For many reasons for classical binary classifications in different areas of business the Gini Score is used. The Gini score is calculated as follows: Gini=2\u00d7ROC AUC\u22121 The recent implementation of sklearn.metrics.roc_auc_score [URL] requires the formula above to be implemented. The current functionality doesn't allow this. This simple fuction will allow many data scientists to clean up their code. Thanks in advance! ### Describe your proposed solution My suggestion is to add this simple function gini_score in sklearn.metrics described as gini_score = 2*roc_auc_score - 1. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28144"
  },
  {
    "number":29271,
    "text":"COLAB fetch_20newsgroups gets 403 from [URL] but OK with browser\n\n### Describe the bug COLAB [CODE_BLOCK] It might be AWS are blocking Google Colab addresses. [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results 403 should not be presented ### Actual Results <html> <head><title>403 Forbidden<\/title><\/head> <body> <center><h1>403 Forbidden<\/h1><\/center> <\/body> <\/html> ### Versions ```shell System: python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-6.1.85+-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 23.1.2 setuptools: 67.7.2 numpy: 1.25.2 scipy: 1.11.4 Cython: 3.0.10 pandas: 2.0.3 matplotlib: 3.7.1 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 2 prefix: libopenblas filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libo...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29271"
  },
  {
    "number":28793,
    "text":"Unexpected behavior of sklearn.feature_selection.mutual_info_regression if copy=False\n\n### Describe the bug The parameter [CODE] of the function [CODE] is described as follows [URL] I read it as both [CODE] and [CODE] should be modified if [CODE] and [CODE] has continuous features. However, [CODE] is always copied. I think the lines [URL] should be [CODE_BLOCK] Similarly to the the treatment of [CODE] [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The result should be [CODE_BLOCK] since both [CODE] and [CODE] should be modified in place by the function [CODE]. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.7 (main, Dec 5 2023, 19:13:35) [GCC 10.2.1 20210110] executable: \/usr\/local\/bin\/python machine: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31 Python dependencies: sklearn: 1.3.0 pip: 23.3.1 setuptools: 69.0.2 numpy: 1.23.2 scipy: 1.12.0 Cython: 3.0.9 pandas: 2.1.4 matplotlib: 3.8.2 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 12 ...",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28793"
  },
  {
    "number":25623,
    "text":"KernelDensity incorrect handling of bandwidth\n\n### Describe the bug I was using kernel density estimator [URL] using 'silverman' or 'scott' as the bandwidth argument. Then I found that the bandwidth automatically adjusted by the algorithm is independent of the actual scale of the dataset. In fact, I was shocked to find that the calculation of a bandwidth in [URL] for 'silverman' and 'scott' does not check the scales of data at all. Suppose I fit the model [CODE] to some 2D data [CODE] and get the bandwidth as [CODE]. Next, I fit the model [CODE] to the same 2D data [CODE] but with all elements multiplied by, say, 20 and get the bandwidth as [CODE]. I found that these two values of [CODE] are equal (it is calculated from the shape of [CODE], see the source code). But obviously they should differ by a factor of 20 if the bandwidth is really computed in a truly adaptive manner. For your reference, I want to mention that scipy's KDE [URL] calculates the covariance of data to extract the scale of data. I think this is the right thing to do. Note that if the bandwidth is incorrect, everything else is incorrect too, including probablities of samples, etc. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Different bandwidths for data sets with different scales. ### Actual Results 0.31622776601683794 0.31622776601683794 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25623"
  },
  {
    "number":27109,
    "text":"Add baseline estimator to HGBT\n\n### Describe the workflow you want to enable I would like to specify a baseline estimator like in [CODE]. ### Describe your proposed solution Add a parameter [CODE] (or if has to be the same, then [CODE]) to [CODE] and [CODE]. ### Describe alternatives you've considered, if relevant The [CODE] does not do the same thing. ### Additional context In particular a linear model as baseline could 1. Improve fit time 2. Allow for non-constant extrapolation on important features, quite in contrast to trees.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27109"
  },
  {
    "number":27644,
    "text":"installing scikit-learn in alpine\n\n### Describe the bug i am trying to install scikit-learn in an alpine image, python:3.9-alpine, but it is failing This is my dockerfile [CODE_BLOCK] And this is the error [CODE] what is the right way to install scikit-learn in an alpine image? Thanks ### Steps\/Code to Reproduce create a dockerfile like this [CODE_BLOCK] ### Expected Results scikit-learn is installed correctly ### Actual Results `#8 237.6 FAILED: scipy\/stats\/stats_pythran.cpython-39-aarch64-linux-gnu.so.p\/meson-generated.._stats_pythran.cpp.o #8 237.6 c++ -Iscipy\/stats\/stats_pythran.cpython-39-aarch64-linux-gnu.so.p -Iscipy\/stats -I..\/sc...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27644"
  },
  {
    "number":30461,
    "text":"from sklearn.datasets import make_regression FileNotFoundError\n\n### Describe the bug When running examples\/application\/plot_prediction_latency.py a FileNotFoundError occurs as there is no file named make_regression in datasets dir. I have cloned the scikit-learn repo and installed it using [CODE_BLOCK] Completely unable to [CODE_BLOCK] or [CODE_BLOCK] albeit it showing up when [CODE_BLOCK] ### Steps\/Code to Reproduce from sklearn.datasets import make_regression ### Expected Results No error is thrown ### Actual Results Exception has occurred: FileNotFoundError [Errno 2] No such file or directory: '\/private\/var\/folders\/0q\/80gytspx42v3rtlkkq_h59jw0000gn\/T\/pip-build-env-53amsfeb\/normal\/bin\/ninja' ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30461"
  },
  {
    "number":24611,
    "text":"A note about documentation on the logistic regression.\n\n### Describe the issue linked to the documentation Dear Authors. First of all, let me express my respect for your hard work and success, acclaimed and used by thousands of ML specialists. I would like to report a small issue in the documentation, providing a justification for my concern and proposal. The documentation says. Every single regression model predicts some statistic (typically expected value, but can be also quantile, trimmed mean) of the conditional distribution of the response, which is always a numerical outcome. This applies to the linear, logistic, probit, ordinal, multinomial, Poisson (BTW, this explains why it returns fractions rather than integers), negative binomial, fractional logistic regression, beta, gamma, Cox, and many, many more regression models. The logistic regression is no different - the conditional distribution here is Bernoulli (or binomial with k=1), and the interpretation of the E(Y|X=x) is by def. the probability of the event (if success\/occurrence\/presence is coded as 1 and the failure\/absence as 0). In terms of the Generalized Linear Model, it's simply a GLM with binomial\/Bernoulli conditional distribution and logit link. Or simpler - binomial regression with logit link. There are lots of monographs devoted exclusively to the regression aspects of the LR. There are also materials written by typically ML-oriented specialists like Prof. Andrew Ng (Stanford), placing it in frames of th...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24611"
  },
  {
    "number":31382,
    "text":"ENH assert statement using AssertionError for [CODE] file\n\n### Describe the workflow you want to enable According to the Bandit Developers document. This caused various protections to be removed. Consider raising a semantically meaningful error or AssertionError instead. As [CODE] has the assert keyword, I would like to update the assert statement. ### Describe your proposed solution My proposed solution is to use AssertionError instead of assert. Current (Line 616): [CODE] Proposal: [CODE] [CODE] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context If you accept my offer, I will make a PR.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31382"
  },
  {
    "number":24508,
    "text":"Sparse random projection description is incorrect in docs\n\n### Describe the bug See: [URL] The docs say that if s = 1 \/ density, then the weights for drawing the values of the sparse matrix should be 1\/(2s), 1 - 1\/s, and 1\/(2s). However, the reference gives weights 1\/(2\\density), etc. The sklearn docs should give weights of 1\/2\\s, 1-s, and 1\/2\\s. (It looks like someone just read 1\/2s as 1\/(2s) instead of 1\/2\\s.) Hopefully this is implemented correctly in the actual code! ### Steps\/Code to Reproduce N\/a ### Expected Results N\/a ### Actual Results N\/a ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24508"
  },
  {
    "number":28098,
    "text":"Adding (Mini-Batch) Spherical K-Means Clustering\n\n### Describe the workflow you want to enable We need to use the spherical k-means algorithm to cluster text embeddings on a hypersphere. Currently there seem to be no reliable implementations available. Since we have a lot of input-data, we would like to implement the mini-batch version of the algorithm. ### Describe your proposed solution Spherical k-means is a trivial modifications of normal k-means clustering where the centroids are projected onto the hypersphere in each step. Thus, we could add a boolean option [CODE] to the constructor of [CODE] that toggles between standard k-means and spherical k-means. It is my understanding that we would then only have to l2-normalize the input data, l2-normalize the initial centroids here: [URL] and l2-normalize the updated centroids after this call: [URL] ### Describe alternatives you've considered, if relevant There is the [spherecluster]([URL] module, which has been broken since the release of scikit-learn 1.0.0 and is no longer maintained, therefore no viable alternative for users. ### Additional context We are happy to prepare a PR for this ourselves!",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28098"
  },
  {
    "number":30281,
    "text":"scipy.optimize._optimize.BracketError in some cases of power transformer\n\n### Describe the bug Similar to #27499, in very few cases the power transformation fails. Edit: Actually, it starts with a [CODE] because at this point the lambda is 292.8\u2026 And thus the out becomes [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error. ### Actual Results ```python \/tmp\/test_venv\/lib\/python3.12\/site-packages\/sklearn\/preprocessing\/_data.py:3438: RuntimeWarning: overflow encountered in power out[pos] = (np.power(x[pos] + 1, lmbda) - 1) \/ lmbda --------------------------------------------------------------------------- BracketError Traceback (most recent call last) Cell In[1], line 3 1 from sklearn.preprocessing import PowerTransformer 2 transformer = PowerTransformer() ----> 3 transformer.fit([[23.80762687], [23.97982808], [23.97586205]]) File \/tmp\/test_venv\/lib\/python3.12\/site-packages\/sklearn\/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, kwargs) 1466 estimator._validate_params() 1468 with config_context( 1469 skip_parameter_validation=( 1470 prefer_skip_nested_validation or global_skip_validation 1471 ) 1472 ): -> 1473 return fit_method(estimator, args, kwargs) File \/tmp\/test_venv\/lib\/python3.12\/site-packages\/sklearn\/preprocessing\/_data.py:3251, in PowerTransformer.fit(self, X, y) 3231 @_fit_context(prefer_skip_nested_validation=True) 3232 def fit(self, X, y=None): 3233 \"\"\"Estimate the optimal parameter lambda for each feature...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30281"
  },
  {
    "number":25642,
    "text":"SimpleImputer with the rule strategies median-1\/median+1\n\n### Describe the workflow you want to enable I have run into an issue with [CODE]. Given a feature of, say, integer type, it is completely reasonable to impute the median to missing values. However, when the overall number of records is even, there is a decent chance, that the median falls between two integers according to the well-known rule (Sorted[N\/2-1] + Sorted[N\/2])\/2. The issue is, that technically, this kind of imputation breaks the domain of the feature, it used to be integer, but now there are spectacular .5 numbers, which can act weirdly in further processing. Long story, short, when a sequence like 4, 3, ?, 2, 4, 5, 1 is imputed by 3.5, it is not an integer sequence anymore. ### Describe your proposed solution My recommendation is to introduce something like an \"adjusted median\", which would ensure that the imputed value is a value of the domain of the feature. My recommendation is to pick Sorted[N\/2-1] or Sorted[N\/2], whichever has the highest number of occurances in the data. If equal, take the smallest. Basically the \"most_frequent\" strategy applied to Sorted[N\/2-1] and Sorted[N\/2] only. ### Describe alternatives you've considered, if relevant Alternative solutions and strategy names could work as well. In the problem described above, the issue is that median calculation is limited to its mathematical definition. [CODE], just like percentile functions in [CODE] offer more flexibility, as what happens in ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25642"
  },
  {
    "number":26430,
    "text":"Conformal inference\n\n### Describe the workflow you want to enable Conformal prediction (CP) is a statistical technique for producing prediction sets without assumptions on the predictive algorithm (often a machine learning system) and only assuming exchangeability of the data. Given this method is assumption lean and thereby very defendable for causal inference, it would be nice if there was out-of-the-box support. ### Describe your proposed solution Some example code can be found here]([URL] ### Describe alternatives you've considered, if relevant The current options for using conformal prediction with an sklearn model are: 1. Write your own code 2. Import from [MAPIE]([URL] or another library ### Additional context I'm happy to implement these changes, but before filing a PR I wanted to get feedback if this aligns with sklearn's structure.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26430"
  },
  {
    "number":28634,
    "text":"KMeans clustering with [0, 1]-entries feature vectors outputs centroids with entries outside [0, 1]\n\n### Describe the bug I have a dataset for clustering based on purchase months, available [here][1]. All feature vectors have entries which sum to 1. I did not expect to have centroids with the strict constraint of the unitary sum. However, they should (almost must) have entries between 0 and 1. According to the code I provide below, am I doing something weird or unexpected? [1]: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A vector of centroids with entries between 0 and 1. ### Actual Results O...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28634"
  },
  {
    "number":26109,
    "text":"HalvingRandomSearchCV and HalvingGridSearchCV do not support multimetric scoring.\n\nAccording to [URL] _HalvingRandomSearchCV and HalvingGridSearchCV do not support multimetric scoring._ When will this be implemented? When trying to use multimeric scoring today, I get: [CODE]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26109"
  },
  {
    "number":25405,
    "text":"Feature request: for GridSearchCV and RandomizedSearchCV, add a kwarg for a preprocessing step after CV splits have been made\n\n### Describe the workflow you want to enable See solution ### Describe your proposed solution Add a new kwarg to cross-validators such as GridSearchCV and RandomizedSearchCV that allows you to use preprocessing steps (such as scalers) on the train & test sets created during each cross validation fold. For example, if I wanted to use StandardScaler()'s [CODE] method on the training set and the [CODE] method on the test set each time a new cross validation split is made, I'd imagine a new kwarg (I'll call it \"post_split\" here) looking like: [CODE_BLOCK] Here, the \"training_set\" key signifies the training set created by that cross-validation split, and the \"test_set\" key would signify the test set created by that cross-validation split. The StandardScaler's [CODE] method only gets called on the cross validation training data, and the [CODE] method only gets called on the cross validation test data. Once that is done, that cross-validation ### Describe alternatives you've considered, if relevant _No response_ ### Additional context The problem: When the current implementation of GridSearchCV and RandomizedSearchCV perform their train-test splits on scaled data, they have to do it on data that has been scaled together with the [CODE] method of a scaler. This creates a data leak between the training and test sets created for cross-validation folds because t...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25405"
  },
  {
    "number":32036,
    "text":"Classification metrics don't seem to support sparse?\n\nWhile working on #31829, I noticed that although most metrics in [CODE] say they support sparse in the docstring (and include \"sparse matrix\" in [CODE]), when you actually try, you get an error. Essentially in [CODE], we do: [URL] [CODE] then calls [CODE] with [CODE] set to the default [CODE]. [CODE_BLOCK] Gives the following error: <details open> <summary>Error<\/summary> ``` TypeError Traceback (most recent call last) Cell In[11], line 1 ----> 1 accuracy_score(sparse_col, sparse_col) File ~\/Documents\/dev\/scikit-learn\/sklearn\/utils\/_param_validation.py:218, in validate_params.<locals>.decorator.<locals>.wrapper(args, kwargs) 212 try: 213 with config_context( 214 skip_parameter_validation=( 215 prefer_skip_nested_validation or global_skip_validation 216 ) 217 ): --> 218 return func(args, kwargs) 219 except InvalidParameterError as e: 220 # When the function is just a wrapper around an estimator, we allow 221 # the function to delegate validation to the estimator, but we replace 222 # the name of the estimator by the name of the function in the error 223 # message to avoid confusion. 224 msg = re.sub( 225 r\"parameter of \\w+ must be\", 226 f\"parameter of {func.__qualname__} must be\", 227 str(e), 228 ) File ~\/Documents\/dev\/scikit-learn\/sklearn\/metrics\/_classification.py:373, in accuracy_score(y_true, y_pred, normalize, sample_weight) 371 # Compute accuracy for each possible representati...",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32036"
  },
  {
    "number":31599,
    "text":"[CODE] deals with [CODE] inconsistently\n\n### Describe the bug When one of the scorers in CODE] is not a [CODE] object, it is handled incorrectly. See line [here. This (possibly) bug is present only in 1.7.0 since before this, the [CODE] kwarg was being passed to all functions without a check of accepting sample weights. Possible fix: Use [CODE]) or [CODE] _before_ checking for the [CODE] attribute. ### Steps\/Code to Reproduce ```python import numpy as np from sklearn.base import BaseEstimator, RegressorMixin from sklearn.metrics._scorer import _BaseScorer, _MultimetricScorer # Step 1: Define a simple estimator class SimpleEstimator(BaseEstimator, RegressorMixin): def fit(self, X, y): self.mean_ = np.mean(y) return self def predict(self, X): return np.full(X.shape[0], self.mean_) # Step 2: Define a custom scorer inheriting from _BaseScorer and a function which estimates score class SimpleScorer(_BaseScorer): def _score(self, method_caller, estimator, X, y_true, sample_weight=None): y_pred = method_caller(estimator, \"predict\", X) return self._score_func(y_true, y_pred, self._kwargs) def default_score(estimator, X, y, sample_weight=None, kws): return estimator.score(X, y, sample_weight=sample_weight) def mse(y, y_pred): return np.mean((y - y_pred)2) # Step 3: Create a _MultimetricScorer with multiple scorers scorers = { \"mse\": SimpleScorer(mse, sign=1, kwargs={}), \"default\": default_score } multi_scorer = _MultimetricScorer(scorers=scorers) # Step 4: Generate sample data X...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31599"
  },
  {
    "number":26656,
    "text":"Pipeline doc references inexistent example\n\n[URL] > parameter name separated by a [CODE], as in the example below There is no example below that shows that technique.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26656"
  },
  {
    "number":29849,
    "text":"Adding scikit-learn to the pydata-sphinx-theme gallery of sites\n\nAs described in the title, I wonder if we want to add scikit-learn to the list of pydata-sphinx-theme gallery of sites: [URL] If we do I can ask pydata-sphinx-theme about it.",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29849"
  },
  {
    "number":31525,
    "text":"Issue with the [CODE] diagram representation with non-default alphas\n\nIt seems that we introduced a regression in the HTML representation. The following code is failing: [CODE_BLOCK] leads to the following error: ``[CODE]_repr_html_[CODE]hasattr(estimator, \"_repr_html_\") return [CODE] or [CODE] depending 143 on [CODE]. 144 \"\"\" --> 145 return self._html_repr() File ~\/Documents\/teaching\/demo_data_science_agent\/.pixi\/envs\/default\/lib\/python3.13\/site-packages\/sklearn\/utils\/_repr_html\/estimator.py:480, in estimator_html_repr(estimator) 469 html_template = ( 470 f\"<style>{style_with_id}<\/style>\" 471 f\"<body>\" (...) 476 '<div class=\"sk-container\" hidden>' 477 ) 479 out.write(html_template) --> 480 _write_estimator_html( 481 out, 482 estimator, 483 estimator.__class__.__name__, 484 estimator_str, 485 first_call=True, 486 is_fitted_css_class=is_fitted_css_class, 487 is_fitted_icon=is_fitted_icon, 488 ) 489 with open(str(Path(__file__).parent \/ \"estimator.js\"), \"r\") as f: 490 script = f.read() File ~\/Documents\/teaching\/demo_data_science_a...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31525"
  },
  {
    "number":25957,
    "text":"Unable to pass splits to SequentialFeatureSelector\n\n### Describe the bug This runs fine with e.g. [CODE], but according to the documentation, it should also be able to take an iterable of splits. However, passing splits from the cross validator fails Im fairly certain I have done similar things in the past to other classes in scikit-learn requiring a [CODE] parameter. If somebody could confirm wether this is a bug, or I'm doing something wrong, that would great. Sorry if this is unneeded noise in the feed. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expected to run without errors ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25957"
  },
  {
    "number":26598,
    "text":"DOC Link in warning about doc build with Sphinx versions outdated\n\n### Describe the issue linked to the documentation The link in the warning about sphinx versions at the end of 'Building the documentation' and even when the qualifier syntax is updated, that search no longer shows the circle CI sphinx version. ### Suggest a potential alternative\/fix Could this link just be to the file [CODE] ? Or link to github search for 'sphinx' within this file?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26598"
  },
  {
    "number":27981,
    "text":"Nested Cross Validation using cross_validate does not show correct fitted model.\n\n### Describe the bug Hi all, I am trying to do nested cross validation using for example [CODE] or [CODE] together with [CODE]. When using the cross_validate function together with the parameter setting: [CODE] , the results show the incorrect fitted estimator for each split. Without the correct fitted model shown for each split, this functionality is rather useless. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Since the [CODE] parameter of [CODE] is set to [CODE] I would have expected that it would show the refitted model from [CODE] instead of the initial defined pipeline ([CODE]). ### Actual Results ```python {'fit_time': array([0.05277681, 0.04694057, 0.03554106]), 'score_time': array([0.00047064, 0.00036621, 0.00040603]), 'estimator': [ GridSearchCV(estimator=Pipeline(steps=[('estimator', DummyClassifier())]), param_grid=[{'estimator': [SVC()], 'estimator__C': [1, 10], 'estimator__kernel': ('linear', 'rbf')}, {'estimator': [DecisionTreeClassifier()]}]), GridSearchCV(estimator=Pipeline(steps=[('estimator', DummyClassifier())]), param_grid=[{'estimator': [SVC()], '...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27981"
  },
  {
    "number":24786,
    "text":"Gaussian process [CODE] [CODE] parameter: pass as [CODE]?\n\n### Discussed in [URL] I opened this as a discussion, but it is probably more suited for an issue, as it may lead to a documentation fix. System Info [CODE_BLOCK] <div type='discussions-op-text'> <sup>Originally posted by elcorto October 26, 2022<\/sup> ## Preliminaries I'm a bit confused by [CODE]'s internal hyperparameter representation and the nature of the [CODE] argument of the [CODE] method. The doc strings of these methods say [CODE_BLOCK] However, the GP's internal hyper optimizer code path seems to work with [CODE]. [CODE_BLOCK] The value of the [CODE] attribute is actually `...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24786"
  },
  {
    "number":28254,
    "text":"DecisionTree does not handle properly missing values in criterion partitioning\n\n### Describe the bug I tried using CODE] with [CODE] in version 1.4.0 on data containing NaNs and got the following error: [CODE_BLOCK] This is my first time opening an issue to an open-source project before, so I apologize if this is ill-formatted or lacking of details. Please let me know if I can provide more information. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect no error since [CODE] supports NaNs and according to the documentation for [CODE], ![image Cell In32], [line 14 11 clf = RandomForestClassifier() 12 selector = RFECV(clf, cv=3) ---> 14 selector.fit(X_train, y_train) File [c:\\Use...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28254"
  },
  {
    "number":24469,
    "text":"DOC Mention pandas dataframe support in [CODE] in FAQ\n\n### Describe the issue linked to the documentation FAQ question: Why does Scikit-learn not directly work with, for example, pandas.DataFrame?. ### Suggest a potential alternative\/fix As above",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24469"
  },
  {
    "number":31604,
    "text":"[CODE] fails with pyarrow==16.0.0\n\n### Describe the bug [CODE] fails with pyarrow==16.0.0 because [CODE] expects a pyarrow boolean type and cannot handle getting a numpy boolean array or a list passed. I found apache\/arrow#42013 addressing this and it was fixed for version 17.0.0. Upgrading my pyarrow version has resolved the issue for me. We accept pyarrow==12.0.0 as a minimum (optional) dependency. In the CI, we test in [CODE] with pyarrow==20.0.0 (only). ### Steps\/Code to Reproduce Run [CODE]. ### Expected Results no errors ### Actual Results Traceback: ```pytb array_type = 'pyarrow_array', indices_type = 'series' @pytest.mark.parametrize( \"array_type\", [\"list\", \"array\", \"series\", \"polars_series\", \"pyarrow_array\"] ) @pytest.mark.parametrize(\"indices_type\", [\"list\", \"tuple\", \"array\", \"series\"]) def test_safe_indexing_1d_container_mask(array_type, indices_type): indices = [False] + [True]  2 + [False]  6 array = _convert_container([1, 2, 3, 4, 5, 6, 7, 8, 9], array_type) indices = _convert_container(indices, indices_type) > subset = _safe_indexing(array, indices, axis=0) sklearn\/utils\/tests\/test_indexing.py:229: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ sklearn\/utils\/_indexing.py:323: in _safe_indexing return _pyarrow_indexing(X, indices, indices_dtype, axis=axis) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31604"
  },
  {
    "number":28430,
    "text":"Name of a class impacts the value of its __metadata_request__* variables\n\n### Describe the bug The metadata request of an object are dependent on its name. Basically identical class can have different behaviours when calling [CODE] if they have different name. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28430"
  },
  {
    "number":25293,
    "text":"_SetOutputMixin changes default order of inheritance\n\n### Describe the bug Inheriting from [CODE] now implicitly adds the wrapped [CODE] method of superclass to subclasses and as a consequence can change the order in which multiple inheritance is resolved. This is caused by the [CODE]. I don't know if this is documented somewhere but it seems to me like a very tricky behavior that can cause a lot of headache to debug. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results code will print [CODE] ### Actual Results code prints [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25293"
  },
  {
    "number":28298,
    "text":"Trees are doing too many split with missing values\n\nIn the following example: [CODE_BLOCK] we get the following tree:: ![image]([URL] The path #12\/#14 is weird. Indeed, we should have some missing values in #14 but we are still able to split based on [CODE] that is not possible. So there is something fishy to investigate there.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28298"
  },
  {
    "number":30323,
    "text":"DOC Example on model selection for Gaussian Mixture Models\n\n### Describe the issue linked to the documentation We have an example that illustrates how to use the BIC score to tune the number of components and the type of covariance matrix parametrization here: [URL] However, the BIC score is not meant to be computed in a CV loop, but instead directly on the training set. So we should not use it with a [CODE] call. Indeed, the BIC score already penalizes the number of parameters depending on the number of data-points in the training set. Instead, we should call the [CODE] on the default [CODE] method of the GMM estimator, which computes the log-likelihood and is a perfectly fine metric to select the best model on held out data in a CV loop. Note that we can keep computing the BIC score for all the hparam combinations but we should either do it in a single for loop (without train-test split), e.g.: [CODE_BLOCK] So in summary I would recommend to: - update the existing [CODE] code to use the [CODE] default that would use the built-in log-likelihood based model evaluation (averaged on the test sets of the CV loop); -...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30323"
  },
  {
    "number":26585,
    "text":"Array API support for k-means\n\nThis is an early issue to publicly discuss the possibility (or not) to use the Array API (see #22352) for k-means and make it run on GPUs using PyTorch in particular. @fcharras has already started to run some promising experiments using the raw PyTorch API. Maybe you could link to a gist with your code? Unfortunately, the current state of the Array API is likely too limiting because AFAIK it does not yet expose the equivalent of CODE], [CODE] and [CODE]. The purpose of this issue is to precisely identify what is blocking us with the current state of Array API and discuss potential solutions: - use this use case to report to the Array API standardization committee what are our needs to make the spec evolve and benefit everybody; - alternatively, explore the use of multi-dispatch system such as [uarray.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26585"
  },
  {
    "number":30986,
    "text":"enh: support aggregation\/bagging functions other than mean\n\n### Describe the workflow you want to enable Currently KNNRegressor, BaggingRegressor and ForestRegressor only support mean [URL] [URL] [URL] ### Describe your proposed solution BaggingRegressor and ForestRegressor could also support median and custom aggregation function that user specifies, but will be mean by default to ensure no breaking change. For eg: [CODE_BLOCK] Proposed implementation [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Why? Robustness to inaccuracies of individual estimators. Consider individual predictions are 101, 102, 103, 104, 150. - With mean aggregation, the output will be 112 (current implementation) - With median aggregation, the output will be 103",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30986"
  },
  {
    "number":27947,
    "text":"Allowing to group infrequent categories in [CODE]\n\n### Describe the workflow you want to enable [CODE] and [CODE] have built-in support for categorical features and use an [CODE] to encode them. Each feature must have less than [CODE] (255) categories and if there are more we get a [CODE]. Would it be useful to add a parameter allowing to group together the least frequent categories when there are too many (and thus avoid the error)? This amounts to setting [CODE] in the internal [CODE]. The workflow I would like to enable is fitting a HGB estimator when a categorical feature has a few too many categories, without needing to encode them myself beforehand. ### Describe your proposed solution the HGB estimators would have a parameter (maybe something like [CODE]) to control whether they should display the current behavior (raise an error) or group together rare categories (with the OrdinalEncoder's [CODE]) when there are more than [CODE] categories ### Describe alternatives you've considered, if relevant It is relatively easy to apply an OrdinalEncoder before but (i) it is more verbose, and we have to use a pipeline and probably a ColumnTransformer, (ii) we lose the dtypes of pandas dataframe columns so we have to specify the categorical columns manually instead of using [CODE], (iii) we end up doing the categorical feature encoding twice, once before and once inside the estimator ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27947"
  },
  {
    "number":25922,
    "text":"Make complex numbers check optional in [CODE]\n\n### Describe the workflow you want to enable I recently extended my library PySR Cell In [9], line 1 ----> 1 model.fit(X, y) File ~\/Documents\/PySR\/pysr\/sr.py:1737, in PySRRegressor.fit(self, X, y, Xresampled, weights, variable_names) 1733 self._setup_equation_file() 1735 mutated_params = self._validate_and_set_init_params() -> 1737 X, y, Xresampled, weights, variable_names = self._validate_and_set_fit_params( 1738 X, y, Xresampled, weights, variable_names 1739 ) 1741 if X.shape[0] > 10000 and not self.batching: 1742 warnings.warn( 1743 \"Note: you are running with more than 10,000 datapoints. \" 1744 \"You should consider turning on batching ([URL] \" (...) 1749 \"More datapoints will lower the search speed.\" 175...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25922"
  },
  {
    "number":31059,
    "text":"\"The Python kernel is unresponsive\" when fitting a reasonable sized sparse matrix into NearestNeighbors\n\n### Describe the bug Hi all, I have a python code that has been running every day for the past years, which uses NearestNeighbors to find best matches. All of a sudden, in both our TEST and PRD environments, our code has been crashing on the NearestNeighbors function with the following message: \"The Python kernel is unresponsive\". This started last Friday 21st of March 2025. What puzzles me is that we haven't made any modifications to our code, the data hasn't changed (at least in our TEST environment) and we didn't change the version of scikit-learn. The exact command that throws the error is: [CODE_BLOCK] where X is a sparse matrix compressed to sparse rows that contains 38506x53709 elements. We run the code on Databricks (runtime 15.4LTS, where scikit-learn is on 1.3.0). I also tried with scikit-learn 1.4.2 (preinstalled in Databricks runtime 16.2) but had the same issue. The error suggests a memory issue, but I'm struggling to understand why this would happen now while the context is exactly the same as what it was before. Furthermore, we use the same code with the same Databricks cluster for another data set which is at least 6x bigger and that one runs successfully in just a few seconds. I'm not a data scientist and therefore quite confused as to why this would no longer run. Since our environment didn't change, I was wondering if anything would have changed in respe...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31059"
  },
  {
    "number":26865,
    "text":"Scikit learn wheel failing to compile on Debain\n\n### Describe the bug Wheel is failing to compile Previously it was working with same versions <img width=\"1364\" alt=\"image\" src=\"[URL] <img width=\"1369\" alt=\"image\" src=\"[URL] ### Steps\/Code to Reproduce pip version 22.0.4, 23.1.2 Python 3.9.5 gcc is already the newest version (4:8.3.0-1). ### Expected Results scikit-learn should compile and install properly ### Actual Results scikit learn is failing to compile ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26865"
  },
  {
    "number":31283,
    "text":"\u26a0\ufe0f CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 05, 2025) \u26a0\ufe0f\n\nCI is still failing on Linux_free_threaded.pylatest_free_threaded - Test Collection Failure",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31283"
  },
  {
    "number":29361,
    "text":"TransformedTargetRegressor warns about set_output set to pandas\n\n### Describe the bug If [CODE] is set to [CODE], [CODE] warns unnecessarily. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No warning. ### Actual Results 3 times the same warning: [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29361"
  },
  {
    "number":28232,
    "text":"Regression in [CODE] due to internal [CODE]\n\nIn [URL] we make sure that the output of [CODE] and the [CODE] are consistent. However, it seems that we have a side effect when the [CODE] is created inside a [CODE] in some case. The example below will provide a dataframe and the identity function will return as-is and the column name will not be consistent with the [CODE]. [CODE_BLOCK] [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28232"
  },
  {
    "number":30736,
    "text":"[CODE] incorrect for complex valued matrices\n\n### Describe the bug The [CODE] utility function accepts complex valued inputs without error, but the result is inconsistent with [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expected the singular values to be numerically close. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.4 (main, Jul 5 2023, 08:40:20) [Clang 14.0.6 ] executable: \/Users\/clane\/miniconda3\/bin\/python machine: macOS-13.7-arm64-arm-64bit Python dependencies: sklearn: 1.7.dev0 pip: 25.0 setuptools: 65.5.0 numpy: 2.2.2 scipy: 1.15.1 Cython: 3.0.11 pandas: 2.2.3 matplotlib: 3.10.0 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libscipy_openblas filepath: \/Users\/clane\/Projects\/misc\/scikit-learn\/.venv\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libscipy_openblas64_.dylib version: 0.3.28 threading_layer: pthreads architecture: neoversen1 user_api: blas internal_api: openblas num_threads: 8 prefix: libscipy_openblas filepath: \/Users\/clane\/Projects\/misc\/scikit-learn\/.venv\/lib\/python3.11\/site-packages\/scipy\/.dylibs\/libscipy_openblas.dylib version: 0.3.28 threading_layer: pthreads architecture: neoversen1 user_api: openmp internal_api: openmp num_threads: 8 prefix: libomp filepath: \/opt\/homebrew\/Cellar\/libomp\/19.1.3\/lib\/libomp.dylib version: N...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30736"
  },
  {
    "number":25292,
    "text":"get_feature_names_out not working on periodic SplineTransformers\n\n### Describe the bug When using a SplineTransformer with argument [CODE] there seems to be a disagreement with the number of columns of the transformed features and the names returned by [CODE] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Expected that [CODE] ### Actual Results [CODE] is 4 [CODE]is 7 ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25292"
  },
  {
    "number":30422,
    "text":"Code Smells and Linting Errors in check-meson-openmp-dependencies.py\n\n### Describe the workflow you want to enable Using the Python Linter set to PEP 8 and Test Driven Development using the Sci-Kit Lean testing suite. ### Describe your proposed solution I propose to reduce redundant code with helper functions, specifically with the has_openmp_flags function that iterates through the compiler and linker lists, repeating a few lines of code. By adding a helper that takes the lists as parameters, the code can be reduced. Change a few vague names (like file or message) to be more specific (like message_file and error_message), shorten long lines of code, and write missing docstrings. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context These issues were found as part of a code review for a school project.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30422"
  },
  {
    "number":27555,
    "text":"Louvain community detection fails to recognize sparse matrix instance\n\n### Describe the bug TypeError being thrown by sknetwork\/utils\/check.py:130, in check_format(input_matrix, allow_empty) I don't think this should be happening. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27555"
  },
  {
    "number":27435,
    "text":"Enable [CODE] in OneHotEncoder\n\n### Describe the workflow you want to enable Currently the [CODE] parameter in [CODE] objects support [CODE] as potential choices, with a strong encouragement to use [CODE] to allow proper use of regularized linear modeling downstream. However, if there exists features that are constant for a data matrix then it would be really great if [CODE] took care of dropping that feature entirely during coding. If all columns in the input matrix were constant perhaps an error could be raised during [CODE] when this option was used for object initialization. I would like to do the following. [CODE_BLOCK] So that this were possible. [CODE_BLOCK] Please note that this is distinct from using [CODE] which would definitely remove the constant columns but also the first column for each feature group in the data matrix columns, which would be pretty crazy. Perhaps, instead of [CODE] this could become the default choice as well, as this encoding perfectly preserves all the information in the input data matrix. ### Describe your proposed solution It would be great to have [CODE] as an option. ### Describe alternatives you've considered, if relevant I have considered writing my own [CODE], but it feels redudant and error prone to do so. I have also tried to specify the constant features per column via the [CODE] method, but there are columns for which I set values to [CODE] (corresponding to the non-constant columns) but the object didn't like that. ### Additional ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27435"
  },
  {
    "number":30027,
    "text":"SGDOneClassSVM model does not converge with default stopping criteria(stops prematurely)\n\n### Describe the bug SGDOneClassSVM does not converge with default early stopping criteria, because the used loss is not actual loss, but only error, which can be easily 0.0 and then increase as the model converges to adequate solution. That is, the used for stopping and reported with verbose \"loss\" value doesn't accout for the full model formula\/regularization. Also, pay attention to bias term to gauge convergence. [URL] The optimization almost always stops after 6 epochs, the initial epoch, plus the 5 for stopping tolerance (can't change the number of epochs for the stopping tolerance btw). The problem does not manifest with toy data(small dimensiaonal), becasue 6 epochs is likely enough for convergence to satisfactory solution. In the reproduction code, mind the console output and comments. Possible workaround at the end of reproduction code, is to use tol=None with manual epoch limit(max_iter), but that slows the optimization down by a lot, since forbids the use of learning_rate=\"adaptive\". ### Steps\/Code to Reproduce ```python import numpy as np from matplotlib import pyplot as plt import pandas as pd #from sklearn.linear_model import Ridge from sklearn.datasets import make_regression from timeit import timeit from sklearn.linear_model import SGDOneClassSVM from sklearn.svm import OneClassSVM from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30027"
  },
  {
    "number":25584,
    "text":"ValueError: buffer source array is read-only when derializing a Tree from a readonly buffer.\n\nAs observed on our Circle CI and reproduced locally: [CODE_BLOCK] [URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25584"
  },
  {
    "number":27754,
    "text":"[CODE]s [CODE] fails on Prescott architecture\n\n### Describe the bug On machines with the Prescott architecture, tests using [CODE] unexpectedly fail due to #23994, which forces [CODE]. To my mind, this makes little sense as an estimator cannot be aligned which only makes sense for simple arrays. I have no idea what the intent of #23994, is. Maybe replacing [CODE_BLOCK] by [CODE_BLOCK] is sufficient to fix this? But as I said, I don't really know what the point of that PR was. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Passes ### Actual Results On the Prescott architecture, this results in the following error: ``` ValueError Traceback (most recent call last) Cell In[2], line 13 9 if \"check_estimators_pickle\" not in check.func.__name__: 11 continue ---> 13 check(estimator) File ~\/micromamba\/envs\/RandomForestClassifier\/lib\/python3.10\/site-packages\/sklearn\/utils\/_testing.py:156, in _IgnoreWarnings.__call__.<locals>.wrapper(args, kwargs) 154 with warnings.catch_warnings(): 155 warnings.simplefilter(\"ignore\", self.category) --> 156 return fn(args, kwargs) File ~\/micromamba\/envs\/RandomForestClassifier\/lib\/python3.10\/site-packages\/sklearn\/utils\/estimator_checks.py:2007, in check_estimators_pickle(name, estimator_orig, readonly_memmap) 2004 estimator.fit(X, y) 2006 if readonly_memmap: -> 2007 unpickled_estimator = create_memmap_backed_data(estimator) 2008 else: 2009 # pickle and unpickle! 2010 pickled_estimator = pickle.dumps(estimator) F...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27754"
  },
  {
    "number":28732,
    "text":"Docs say parameter sample_weight of LinearRegression.fit must be array but number is also valid\n\n### Describe the issue linked to the documentation The documentation page for the CODE] method of the [CODE] class mentions that the [CODE] parameter must be of type [CODE] or [CODE] ([docs. ### Suggest a potential alternative\/fix I see two possible fixes: - Change the documentation to address the fact that numbers are valid values for [CODE] however they have no effect since there is no difference in the relative weight of the samples. - Change the code so that an error or warning is raised if the [CODE] parameter is a [CODE] or an [CODE].",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28732"
  },
  {
    "number":31548,
    "text":"DOC About Us page: multi-column list for emeritus contributors\n\nReferences #31519 References #30826 --- It would be good to keep the file. The new proposed layout looks like this, and it save 28 lines of whitespace. So users can get to the important section faster, how to support scikit-learn. ### Before <img width=\"1145\" alt=\"Screenshot 2025-06-12 at 6 53 17 AM\" src=\"[URL] \/> ### After <img width=\"970\" alt=\"Screenshot 2025-06-12 at 6 52 40 AM\" src=\"[URL] \/> _Originally posted by @reshamas in [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31548"
  },
  {
    "number":30394,
    "text":"Is there any interest to provide SymmetricNMF\n\n### Describe the workflow you want to enable Hi! I have a [prototype implementation of Symmetric NMF]([URL] that I [ported from matlab]([URL] There are 2 main papers on it, the oldest one from 2012 has ~500 citations: [URL] Symmetric NMF is useful for clustering graphs and allows for soft clustering where a node may belong to multiple clusters. Example, here node 2 has strong membership to cluster 1 and partial membership to cluster 2. It can be seen as an alternative to Gaussian Mixture Models for soft clustering problems, however SymmNMF works directly with an affinity matrix whereas soft clustering with a GMM would typically require projecting the affinity matrix to some low dimensional space. ![Figure_1]([URL] ### Describe your proposed solution Merge this implementation into sklearn once I've tested that it's robust and reliable: [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30394"
  },
  {
    "number":31344,
    "text":"Add MultiHorizonTimeSeriesSplit for Multi-Horizon Time Series Cross-Validation\n\n### Describe the workflow you want to enable The current [CODE] in scikit-learn supports cross-validation for time series data with a single prediction horizon per split, which limits its use for scenarios requiring forecasts over multiple future steps (e.g., predicting 1, 3, and 5 days ahead). I propose adding a new class, [CODE], to enable cross-validation with multiple prediction horizons in a single split. This would allow users to: - Specify a list of horizons (e.g., [CODE]) to generate train-test splits where the test set includes indices for multiple future steps. - Evaluate time series models for short, medium, and long-term forecasts simultaneously. - Simplify workflows for applications like demand forecasting, financial modeling, or weather prediction, avoiding manual splitting. Example usage with daily temperatures: [CODE_BLOCK] Expected output: [CODE_BLOCK] ### Describe your proposed solution I propose implementing a new class, [CODE], inheriting from [CODE]. The class will: - Add a [CODE] parameter (list of integers) to specify prediction steps. - Modify the [CODE] method to generate test indices for each horizon while preserving temporal order. - Include input validation to ensure valid horizons and splits. To ensure the correctness of MultiHorizonTimeSeriesSplit, we will develop unit tests covering various configurations and edge cases. For benchmarking, we will assess the computati...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31344"
  },
  {
    "number":28731,
    "text":"Update index handling in [CODE]\n\n### Describe the workflow you want to enable As noted in #27037, handling the index of an input container can be hairy. The solution implemented in #27044 works, but it excludes [CODE] input types. I'd like to modify the logic in the [:method:[CODE]]([URL] so that it checks if the [CODE] is a [CODE] _or_ [CODE]. This would allow transformers that accept 1-dimensional inputs and output 2-dimensional dataframes to persist their indices. ### Describe your proposed solution I'd like to change [line 124]([URL] from this: [CODE_BLOCK] To this: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant User sets the index on their own: [CODE_BLOCK] ### Additional context I recognize _most_ transformers in [CODE] expect 2-dimensional inputs. But some packages that depend on [CODE] (like [[CODE]]([URL] have transformers that transform 1-dimensional input into 2-dimensional output. I believe this would greatly benefit them. See the [newly updated [CODE]]([URL] for an example. I'm willing to submit a PR if this is an acceptable enhancement.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28731"
  },
  {
    "number":30988,
    "text":"Make the halving searches scoring parameter also accept single value containers\n\nCurrently, the HalvingRandomSearchCV and the HalvingGridSearchCV support only a single scoring metric. Because of that, only a single string or callable is accepted as scoring parameter. However, this causes it to not accept a single metric if it's wrapped in a container (meaning a list or tuple with only one element or a dict with only one key-value pair). While in the long run, the halving search variants should also accept and use multiple scoring metrics, it would still be a good improvement for consistency and for frameworks that use the different searches interchangeably if the halving search variants could also accept those containers as long as they contain just one element.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30988"
  },
  {
    "number":26395,
    "text":"GridSearchCV support callback for MLFlow\n\n### Describe the workflow you want to enable I would like to save off the results of all runs in GridSearchCV to MLFlow. MLFlow CODE_BLOCK] See [URL] for more details: I would like to use [CODE] to do the above because it comes with many other features (e.g. [CODE], multi-threading, etc ...)[ ### Describe your proposed solution A callback parameter to [CODE]. Perhaps [CODE_BLOCK] ### Describe alternatives you've considered, if relevant To hack the scorer for this purpose: [URL] This is suboptimal because: 1. If you want to return multiple metrics, you cannot save multiple scores using the provided API. This is because we have to pass multiple scorers, not a function that generates multiple scores. 2. Enabling [CODE] will call the scorer callback too many times and it is not easy to distinguish between the training and testing scoring. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26395"
  },
  {
    "number":31183,
    "text":"Upper bound the build dependencies in [CODE] for release branches\n\n### Describe the workflow you want to enable Upper bound the build dependencies on release branches makes it easier to build the wheel in the future. This has two benefits: - The wheels become easier to build when using the newest build dependency does not work. (Historically, I've seen issues with Cython) - If we wanted to backport a fix the wheel building is more stable. ### Describe your proposed solution On release branches, provide a upper bound to the build dependencies in [CODE]. SciPy does this already: [URL] ### Describe alternatives you've considered, if relevant Leave the build dependencies to be unbounded.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31183"
  },
  {
    "number":30492,
    "text":"Version 1.6 docs inconsistency related to isolation forest.\n\n### Describe the issue linked to the documentation The current [isolation forest docs]([URL] say this: ![image]([URL] And this: ![image]([URL] After trying myself locally I can also confirm that you need to context manager for the actual speedup. ### Suggest a potential alternative\/fix The [CODE] did not cause a speedup locally but the context manager did so there is probably a situation with a docstring that needs updating. We should probably just change the docstring for the input of the estimator? But it could also make sense to mention the context manager more boldly. @glemaitre had some ideas on this and knows more about the internals here.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30492"
  },
  {
    "number":29202,
    "text":"scikit-learn algorithm cheat sheet (Diagram)\n\n### Describe the bug It seems there is some wrong path, described for choosing the estimator in the main diagram as compared to the previous versions. [URL] So, the moment you hit the above link you will get a diagram to choose the right estimator, but it is not taking me in the right direction. I think so (issue mentioned in the screenshot) Wrong diagram screenshot ![image]([URL] correct diagram screenshot ![image]([URL] ### Steps\/Code to Reproduce Getting wrong direction for estimator ### Expected Results correct diagram screenshot ![image]([URL] ### Actual Results Wrong diagram screenshot ![image]([URL] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29202"
  },
  {
    "number":25072,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly_PyPy.pypy3 \u26a0\ufe0f\n\nCI failed on Linux_Nightly_PyPy.pypy3 Unable to find junit file. Please see link for details.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25072"
  },
  {
    "number":24658,
    "text":"Update min joblib to 1.2.0\n\n### Describe the workflow you want to enable As of 3 days ago, Joblib 1.2.0 has [significant improvements]([URL] over the Joblib 1.1.1 that sklearn currently supports I suggest that sklearn should increase joblib version in required deps ### Describe your proposed solution Modify [CODE] in [CODE] to [CODE] [URL] ### Describe alternatives you've considered, if relevant N\/A. No backwards compatibility issue in this upgrade. ### Additional context I'm happy to make the PR with change.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24658"
  },
  {
    "number":25484,
    "text":"MAINT Improve scikit-learn reliability by replacing [CODE]s with typed memoryviews\n\n### Context Using CODE] is not encouragged and using [typed memoryviews. On a side-note, better uniform canonical support of readonly buffers has notable value: some frameworks like Ray or libraries like joblib make use of memory mapping which change the writability of buffers. Yet it might crash in some context if scikit-learn's implementations have no clear support for readonly data and this is fatal to users' workflows. In overall, we have a poor overview for such support but we must. Efforts in this meta-issue will improve the overview for this support. --- As an example, the following pattern: [CODE_BLOCK] must be changed to: [CODE_BLOCK] where [CODE] is a concrete type or a fused type. See [occurences of [CODE] in the codebase]([URL] :bulb: Do note, as mentioned by @thomasjpfan in [URL] that not all occurences of [CODE] must be removed. :bulb: People interested starting working with Cython might be interested in reading [Cython Best Practices, Conventions and Knowledge section of scikit-learn documentation]([URL] ### Proposed solution: treat one file per PR - Only perform changes necessary change to use memoryviews - If the file is big, you can have several PR to treat it - Create PRs for each warning with the following title: ...",
    "labels":[
      "help wanted",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25484"
  },
  {
    "number":30430,
    "text":"Example of binning of continous variables for chi2\n\n### Describe the issue linked to the documentation The [chi2]([URL] doesn't work on continuous variables. This issue has numerous discussions, e.g. [here]([URL] The Matlab counterpart command, [fscchi2]([URL] solves this issue by automatically binning data. I believe that the example of chi2 feature selection with pre-binning may be beneficial. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30430"
  },
  {
    "number":26116,
    "text":"Changing classification_report to also output the number of predictions along with the support.\n\n### Describe the workflow you want to enable The current [CODE]. * Changing the return statement of `precision_reca...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26116"
  },
  {
    "number":31098,
    "text":"Failing CI for check_sample_weight_equivalence_on_dense_data with LinearRegerssion on debian_32bit\n\nHere is the last scheduled run (from 1 day ago) that passed: [URL] and here is a more recent run that failed (all CI is failing today): [URL] [CODE_BLOCK] Full failure log: <details> ``` 2025-03-28T06:36:32.3433619Z =================================== FAILURES =================================== 2025-03-28T06:36:32.3434358Z \u001b[31m\u001b[1m_ test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] _\u001b[0m 2025-03-28T06:36:32.3434613Z 2025-03-28T06:36:32.3434838Z estimator = LinearRegression(positive=True) 2025-03-28T06:36:32.3435117Z check = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression') 2025-03-28T06:36:32.3435705Z request = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>> 2025-03-28T06:36:32.3435878Z 2025-03-28T06:36:32.3436047Z @parametrize_with_checks( 2025-03-28T06:36:32.3436274Z list(_tested_estimators()), expected_failed_checks=_get_expected_failed_checks 2025-03-28T06:36:32.3436498Z ) 2025-03-28T06:36:32.3436684Z def test_estimators(estimator, check, request): 2025-03-28T06:36:32.3436909Z # Common tests for estimator instances 2025-03-28T06:36:32.3437101Z with ignore_warnings( 2025-03-28T06:36:32.3437316Z ...",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31098"
  },
  {
    "number":30160,
    "text":"Change forcing sequence in newton-cg solver of LogisticRegression\n\n### Describe the workflow you want to enable I'd like to have faster convergence of the CODE] solver of [CODE] based on scientific publications with empirical studies as done in [A Study on Truncated Newton Methods for Linear Classification (2022). ### Describe your proposed solution It is about the inner stopping criterion in a truncated Newton solver, i.e. when should the inner solver for \"hessian @ coefficients = -gradient\" stop. $eta = \\eta$ is the forcing sequence. #### Current stopping criterion $residual ratio = \\frac{\\rVert res\\lVert_1}{\\rVert grad \\lVert_1} \\leq \\eta$ with $res = residual = grad - hess @ coef$ and $\\eta = \\min([0.5, \\sqrt{\\rVert grad \\lVert_1]})$ (this eta is called adaptive forcing sequence. #### Proposed stopping criterion As recommended by Chapter VII. - Replace residual ratio with the quadratic approximation ratio $j\\frac{Q_j - Q_{j-1}}{Q_j}$ and $Q_j = grad @ coef_j + \\frac{1}{2} coef_j^T @ hessian @ coef_j$ and $j$ is the inner iteration number. - Optionally replace L1-norm by L2-norm. For the quadratic ratio, this does not matter much. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30160"
  },
  {
    "number":24464,
    "text":"DOC See Also descriptions do not match for multiple functions\/classes\n\n### Describe the issue linked to the documentation While working on a docstring-related pull request (#24259) I noticed that, sometimes, the See Also description for the same function\/class does not match. For instance, the [CODE] description was different depending on the class I looked at: [URL] [URL] I decided to [investigate it further and see if it was occuring elsewhere (click here to check the gist where I regexed the raw files)]([URL] Today, at commit [2f8b8e7f1]([URL] we have 56 functions\/classes that have some sort of different descriptions at some See Also. From looking at them at first glance I have sorted the differences I get into 4 categorys: 1. Those where the description is related to the current class\/function; - For instance, inside the [CODE] class, [CODE] is described as the \"equivalent\" version. That is a different description from the one we see inside [CODE], but inside the context of the [CODE] class, it makes sense. [URL] 2. Those related to extra \"\\n\" on the text; [URL] [URL] 3. Those that are different per se; - For instance, the [CODE] descriptions we mentioned earlier have no obvious reason to be different. 4. Those that ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24464"
  },
  {
    "number":29783,
    "text":"Running RFECV.fit inside joblib.Parallel causes ValueError or AttributeError\n\n[CODE_BLOCK] You can get two types of errors: [CODE_BLOCK] or [CODE_BLOCK] I don't quite understand what is happening yet but it seems like there is a side-effect somewhere I would have thought that the inner parallelism would do copy but apparently not. Using [CODE] in [URL] seems to fix it: ```diff diff --git a\/sklearn\/feature_selection\/_rfe.py b\/sklearn\/feature_selection\/_rfe.py index 8ccbffce9b..99aa8e2b4f 100644 --- a\/sklearn\/feature_selection\/_rfe.py +++ b\/sklearn\/feature_selection\/_rfe.py @@ -886,7 +886,7 @@ class RFECV(RFE): func = delayed(_rfe_single_fit) scores_features = parallel( - func(rfe, self.estimator, X, y, train, test, scorer, routed_params) + func(clone(rfe), self.estimator, X, y, train, test, scorer, routed_params) ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29783"
  },
  {
    "number":24255,
    "text":"Random Forest Neighbors\n\n### Describe the workflow you want to enable Random Regression forests allow for an interpretation where the values at a new point [CODE] is a weighted linear combination of the values observed in the dataset <img width=\"188\" alt=\"image\" src=\"[URL] I think it would be helpful if one could obtain these weights for using the sklearn random forest for defining (for example) local neighbourhoods. ### Describe your proposed solution I think there should be a method which would allow one to access these neighbourhoods. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context If there is any interest I could look deeper into this issue and work to get a PR on it.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24255"
  },
  {
    "number":26963,
    "text":"Path to adopt 32-bit implementations for [CODE]\n\n## Motivation Having and using 32-bit implementations of [CODE] allows for better preservation of dtype, lower memory footprint, and more consistent Cython code. ## Strategy Work has already been started in #25914 to add the code for the new 32-bit implementations. These will not be directly used yet, and the PR instead focuses on the actual creation of the new classes. Consequently, [CODE] are bound to [CODE] for consistency and backwards compatibility. Following this PR, we will need to begin an API deprecation to move users away from constructing trees directly, and instead using a factory method (similar to [CODE]). Then, later, we can separate [CODE] from [CODE] so that we have a singular dispatcher [CODE] and two type-specialized implementations. The deprecation process involves: 1. Introducing [CODE] to [CODE] with the same signature of [CODE], which is in charge of constructing the type-specialized trees directly. 2. Adding a [CODE] to [CODE] to begin deprecation, and suppressing the warning using a context manager in [CODE] to enforce it as the \"correct\" way to construct the trees. 3. Complete the deprecation by introducing a separate [CODE] into the hierarchy which comes with the same [CODE] method and fully remove the [CODE] from [CODE] since they should no longer be directly constructed, as well as getting rid of the [CODE] method since it should only exist in [CODE]. At the end, the expected API is: [CODE_BLOCK] cc...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26963"
  },
  {
    "number":29735,
    "text":"Improve documentation to specify the interface of metric as a callable in KNNImputer\n\n## Repurpose issue to solve In CODE], there is no mention regarding the expected interface of the parameter [CODE] apart from the signature when passing a callable. We could reuse the documentation of [CODE] that provides more details that are necessary to implement the function. <details> ## Original issue ### Describe the bug I am trying to run KNNImputer with a custom [CODE] function from pull request [#23286 If all the coordinates are missing or if there are no common present coordinates then NaN is retur...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29735"
  },
  {
    "number":32104,
    "text":"FeatureUnion with polars output can error due to duplicate column names\n\n### Describe the bug FeatureUnion concatenates outputs of its transformers _before_ the [CODE] wrapper renames columns based on [CODE] (adding the transformer name prefix). This works with pandas but not polars which does not allow creating a dataframe with duplicate feature names in addition to the reproducer below, [CODE] fails if we replace \"pandas\" with \"polars\" in the test ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results dataframe with column names [CODE] ### Actual Results ``` Traceback (most recent call last): File \"...\/feature_union.py\", line 26, in <module> fu.fit_transform(df) # ERROR during hstack step as both transformers have output column names ['a', 'b'] ~~~~~~~~~~~~~~~~^^^^ File \"...\/scikit-learn\/sklearn\/utils\/_set_output.py\", line 316, in wrapped data_to_wrap = f(self, X, args, *kwargs) File \"...\/scikit-learn\/sklearn\/pipeline.py\", line 1970, in fit_transform return self._hstack(Xs) ~~~~~~~~~~~~^^^^ File \"...\/scikit-learn\/s...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32104"
  },
  {
    "number":27012,
    "text":"inverse_transform of SimpleImputer with empty features changes order of columns\n\n### Describe the bug When one uses a SimpleImputer with [CODE] and [CODE], then if a column has only missing values during fit, but has values during transform, then the inverse transform will shift all columns to the right of that column one to the left. (Note that I found this while experimenting and is not something that we rely on being fixed) ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I think the best way to go is to fill the missing columns with the value from [CODE]. [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27012"
  },
  {
    "number":25038,
    "text":"Instability in fastica for float32\n\nOriginally seen in [URL] for more details on a single random seed and only for Atlas but I have managed to reproduce with OpenBLAS see below. For now we have a work-around for the CI in #24198 but it is still an issue and in an ideal world, someone would investigate and try to see whether it is fixable. Other possibly related issues and PRs are mentioned in [URL] [URL] and [URL] Here is a snippet that reproduces the same problem on OpenBLAS: [CODE_BLOCK] You get a warning with a division by zero: [CODE_BLOCK] And then the following traceback because NaNs are passed into a BLAS routine eventually: ``` ----------------------------------------...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25038"
  },
  {
    "number":29498,
    "text":"I want to import kerasregressor I have tried this from scikeras.wrappers import KerasClassifier , but I am getting the error cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py)\n\n### Describe the bug I want to import kerasregressor I have tried this from scikeras.wrappers import KerasClassifier , but I am getting the error cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py) ### Steps\/Code to Reproduce I want to import kerasregressor I have tried this from scikeras.wrappers import KerasClassifier , but I am getting the error cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py) ### Expected Results I want to import kerasregressor I have tried this from scikeras.wrappers import KerasClassifier , but I am getting the error cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py) ### Actual Results I want to import kerasregressor I have tried this from scikeras.wrappers import KerasClassifier , but I am getting the error cannot import name '_depr...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29498"
  },
  {
    "number":27655,
    "text":"[CODE]: allow [CODE] linkage and [CODE] metric.\n\nHi, I'm trying to run [CODE] with precomputed (Euclidean) distance matrices. However, I can't get it to work with [CODE] and [CODE] due to this [CODE]: [URL] Would you consider a PR making this into a warning when [CODE]? This would allow passing precomputed distance matrices to the Ward method. Thank you, Vini",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27655"
  },
  {
    "number":26892,
    "text":"Balanced Accuracy Score is NOT equal to Recall Score\n\n### Describe the bug By definition balanced accuracy should be equal to recall averaged over all the classes. Current implementation gives different answers. Please see the example below. [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26892"
  },
  {
    "number":31871,
    "text":"Proposal to Contribute Uncertainty Quantification via Aleatoric\/Epistemic Decomposition to scikit-learn\n\n### Describe the workflow you want to enable Hi, While ensemble methods like RandomForestRegressor are widely used, scikit-learn currently lacks native support for estimating and exposing predictive uncertainty\u2014an increasingly essential feature in many applied domains such as healthcare, scientific modeling, and decision support systems. ### Describe your proposed solution I propose adding functionality to expose both: Aleatoric uncertainty (data-driven), Epistemic uncertainty (model-driven). Importantly, this is not just a concept\u2014I have already implemented this wrapper as part of my ongoing PhD research. The approach is detailed in a preprint available here: [URL] . The implementation is functional, tested, and used in geophysical mapping described in the paper. This contribution builds on established research by Mohammad Hossein Shaker and Eyke H\u00fcllermeier in uncertainty estimation for Random Forest Classification, and I have extended those principles to Random Forest Regression. The approach is detailed in this article available here: [URL] Thanks ### Describe alternatives you've considered, if relevant ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31871"
  },
  {
    "number":30762,
    "text":"DOC JupyterLite link _query_package() got multiple values for argument 'index_urls'\n\nClicking on the Jupyterlite button of this example PythonError: Traceback (most recent call last): File \"\/lib\/python312.zip\/_pyodide\/_base.py\", line 574, in eval_code_async await CodeRunner( File \"\/lib\/python312.zip\/_pyodide\/_base.py\", line 396, in run_async await coroutine File \"<exec>\", line 3, in <module> File \"\/lib\/python3.12\/site-packages\/piplite\/piplite.py\", line 121, in _install return await micropip.install( ^^^^^^^^^^^^^^^^^^^^^^^ File \"\/lib\/python3.12\/site-packages\/micropip\/_commands\/install.py\", line 142, in install await transaction.gather_requirements(requirements) File \"\/lib\/python3.12\/site-packages\/micropip\/transaction.py\", line 55, in gather_requirements await asyncio.gather(*requirement_promises) File \"\/lib\/python3.12\/site-packages\/micropip\/transaction.py\", line 62, in add_requirement return await self.add_requirement_inner(Requirement(req)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/lib\/python3.12\/site-packages\/micropip\/transaction.py\", line 151, in add_requirement_inner await self._add_requirement_from_package_index(req) File \"\/lib\/python3.12\/site-packages\/micropip\/transaction.py\", line 186, in _add_requirement_from_package_index metadata = await package_index.query_package( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: _query_package() got multiple values for argument 'index_urls' O [URL] new_error [URL] _PyEM_TrampolineCall_JS [URL]",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30762"
  },
  {
    "number":25607,
    "text":"Ordinal encoder not encoding missing values as np.nan\n\n### Describe the bug The documentation for OrdinalEncoder states that the default encoded_missing_value value is np.nan but when I run the encoder, it replace missing values with -9223372036854775808. The same behaviour is seen even if I manually specify the argument to be np.nan. The same behaviour is also seen for the unknown_value argument when handle_unknown is set to \"use_encoded_value\". This again is resulting in -9223372036854775808, even when the value np.nan is specified. ### Steps\/Code to Reproduce trainX = np.array(['test', 'check', 'another_test', '']) testX = np.array(['check', 'yet_another_test', 'test', '']) trainX = pd.Series(trainX) trainX = trainX.replace([\"^\\s$\"], np.NaN, regex=True) testX = pd.Series(testX) testX = testX.replace([\"^\\s$\"], np.NaN, regex=True) ordinal_encoder = OrdinalEncoder( handle_unknown=\"use_encoded_value\", unknown_value=np.nan ) ordinal_encoder.fit(trainX.values.reshape(-1, 1)) trainX = ordinal_encoder.transform( trainX.values.reshape(-1, 1) ).astype(\"int\") testX = ordinal_encoder.transform( testX.values.reshape(-1, 1) ).astype(\"int\") print(testX) print(trainX) ### Expected Results Expected results are that the second and last value in testX is encoded as np.nan and the last value in trainX is encoded as np.nan. ### Actual Results Unable to upload screenshot. Output shows the second and last value in testX and the last value in trainX are encoded as -9223372036854775808. trainX: ar...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25607"
  },
  {
    "number":26832,
    "text":"Documentation on how DTs deal with nans\n\n### Describe the issue linked to the documentation This webpage suggests that DTs can handle nans in a way that is not just imputation: [URL] However, I cannot find any documentation on how this is accomplished. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26832"
  },
  {
    "number":29221,
    "text":"Maintenance: GPU CI follow up work\n\nThis is a follow up to #24491. As we are starting touse the GPU CI workflow we are noticing things we don't like and things which are missing. Let's collect them in this issue as a way to keep track of them. Things to add:   ] comment (or other notification) in a PR to show the workflow's status  use [URL] description in [discord",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29221"
  },
  {
    "number":29678,
    "text":"root_mean_squared_log_error & mean_squared_log_error: ValueError should be raised only if y_true or y_pred contain a value below -1, not below 0\n\n### Describe the bug For the [CODE] & [CODE] evaluation metrics, if any of the values in [CODE] or [CODE] are below 0, the following [CODE] exception is raised: [CODE_BLOCK] However, the actual calculations behind these errors are valid for values of [CODE] & [CODE] larger than -1, so any values in [CODE] or [CODE] that are in the range [0, -1[ should be valid when calculating these errors. The equations are shown below, note that the log() of any value larger than 0 is valid: $$RMSLE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log{(y_{pred}+1)}-\\log{(y_{true}+1)})^2}$$ $$MSLE=\\frac{1}{n} \\sum_{i=1}^n (\\log{(y_{pred}+1)}-\\log{(y_{true}+1)})^2$$ The thresholds that trigger the [CODE] exception should be adjusted as shown below: [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. Errors should only be throw...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29678"
  },
  {
    "number":26550,
    "text":"AttributeError: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32'\n\n### Describe the bug I am using the pyldavis library which depends on scikit learn, I have been working for months without any problem. Today at night a new error appeared when I tried the library [CODE] AttributeError: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32'. If I try to import the function directly [CODE] it returns the same error ### Steps\/Code to Reproduce [CODE] ### Expected Results No error is thrown ### Actual Results File sklearn\/metrics\/_pairwise_distances_reduction\/_base.pyx:1, in init sklearn.metrics._pairwise_distances_reduction._base() AttributeError: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32' ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26550"
  },
  {
    "number":29048,
    "text":"Make [CODE] parameter consistent in the different metric\n\nThis is an issue to report the step to actually take over the work of @marctorsoc in [URL] and split the PR into smaller one to facilitate the review process. The intend is to make the [CODE] parameter consistent across different metrics in scikit-learn. In this regards, we have the following TODO list: - [x] Introduce the [CODE] parameter to the [CODE] function when [CODE] and [CODE] are empty. - [URL] - [x] Introduce the [CODE] parameter to the [CODE] and remove [CODE]. - [URL] - [x] Introduce the [CODE] parameter to the [CODE] function - [URL] - [x] Introduce the [CODE] parameter to the [CODE] function - #23183 - #28509 - [ ] <del>Open a PR to make sure the empty input lead to [CODE] in [CODE] function.<\/del> [CODE] should raise an error instead: see [URL] All those items have been addressed in #23183 and can be extracted in individual PRs. The changelog presenting the changes should acknowledge @marctorsoc. In addition, we should investigate #27047 and check if we should add the [CODE] parameter to the [CODE] and [CODE] as well. This might add two additional items to the list above.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29048"
  },
  {
    "number":25641,
    "text":"Importing RandomForestRegressor produces AttributeError: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32'\n\n### Describe the bug Importing RandomForestRegressor produces AttributeError: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32' ### Steps\/Code to Reproduce from sklearn.ensemble import RandomForestRegressor ### Expected Results No error is thrown. ### Actual Results Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"C:\\Python3\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\", line 5, in <module> from ._base import BaseEnsemble File \"C:\\Python3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 18, in <module> from ..tree import ( File \"C:\\Python3\\lib\\site-packages\\sklearn\\tree\\__init__.py\", line 6, in <module> from ._classes import BaseDecisionTree File \"C:\\Python3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 42, in <module> from ._criterion import Criterion File \"sklearn\\tree\\_criterion.pyx\", line 1, in init sklearn.tree._criterion File \"sklearn\\tree\\_splitter.pyx\", line 1, in init sklearn.tree._splitter File \"sklearn\\tree\\_tree.pyx\", line 1, in init sklearn.tree._tree File \"C:\\Python3\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\", line 6, in <module> from ._ball_tree import BallTree File \"sklearn\\neighbors\\_ball_tree.pyx\", line 1, in init sklearn.neighbors._ball_tree File \"C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\__init__.py\", line 41, in <module> from . import cluster File \"C:\\Pyth...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25641"
  },
  {
    "number":28977,
    "text":"Consider bumping C standard in meson.build from C99 to C17\n\n### Describe the bug Currently, trying to build scikit-learn with the python 3.13 free-threaded build leads to a compilation error related to usage of [CODE] in CPython internals. This leaks into public code via cython's adding [CODE] to module init code. See [URL] where scipy made a similar change for similar reasons. C17 is well-supported by downstream compilers, including MSVC. CPython itself is built with C11, which is a superset of C17. Opening this as an issue instead of just making a pull request to see if there are good reasons besides inertia why [CODE] specifies C99. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results successful build ### Actual Results ``` FAILED: sklearn\/_loss\/_loss.cpython-313t-darwin.so.p\/meson-generated_sklearn__loss__loss.pyx.c.o ccache cc -Isklearn\/_loss\/_loss.cpython-313t-darwin.so.p -Isklearn\/_loss -I..\/sklearn\/_loss -I\/Users\/goldbaum\/.pyenv\/versions\/3.13-dev-nogil\/include\/python3.13t -fvisibility=hidden -fdiagnostics-color=always -Wall -Winvalid-pch -std=c99 -O0 -g -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -MD -MQ sklearn\/_loss\/_loss.cpython-313t-darwin.so.p\/meson-generated_sklearn__loss__loss.pyx.c.o -MF sklearn\/_loss\/_loss.cpython-313t-darwin.so.p\/meson-generated_sklearn__loss__loss.pyx.c.o.d -o sklearn\/_loss\/_loss.cpython-313t-darwin.so.p\/meson-generated_sklearn__loss__loss.pyx.c.o -c sklearn\/_loss\/_loss.cpython-313t...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28977"
  },
  {
    "number":28644,
    "text":"\u26a0\ufe0f CI failed on Wheel builder \u26a0\ufe0f\n\nCI is still failing on Wheel builder [CODE_BLOCK]",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28644"
  },
  {
    "number":30183,
    "text":"The Affinity Matrix Is NON-BINARY with[CODE]\n\n### Describe the issue linked to the documentation ## Issue Source: [URL] ## Issue Description The Affinity Matrix Is _non-binary_ with[CODE]=[CODE]. I.e., when a precomputed distance matrix is given as [CODE], the affinity matrix from SpectralClustering.fit().affinity_matrix_ is NOT binary (as described in the document). It has 3 values: 0.0, 1.0, and 0.5. ## Reproducible Code Snippet Generate a random distance ,a [CODE_BLOCK] ## Machine & Version Info [CODE_BLOCK] ### Suggest a potential alternative\/fix Since the affinity matrix is calculated as [CODE] [source_code]([URL] and that the [CODE] is calculated by [CODE] [source_co...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30183"
  },
  {
    "number":28877,
    "text":"Feature request to use intermediate column transformer outputs\n\n### Describe the workflow you want to enable I am trying to do the following: ```python import pandas from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.pipeline import make_pipeline, Pipeline from sklearn.base import TransformerMixin, BaseEstimator # Input Data df = pandas.DataFrame([[\"car\",0.1,0.0],[\"car\",0.2,0.0],[\"suv\",0.0,0.2]],columns=['vehicleType','features_car','features_suv']) # Custom Transformer class GetScore(BaseEstimator, TransformerMixin): # type: ignore \"\"\"Apply binarize transform for matching values to filter_value.\"\"\" def __init__(self): \"\"\"Initialize transformer with expected columns.\"\"\" pass def dot_product(self, x) -> float: \"\"\"Return 1.0 if input == filter_value, else 0.\"\"\" return x[0]x[2] + x[1]  x[3] def fit(self, X, y=None): # type: ignore \"\"\"Fit the transformer.\"\"\" \"\"\"Transform the given data.\"\"\" if type(X) == pandas.DataFrame: x = X.apply(lambda x: self.dot_product(x), axis=1) return x.values.reshape((-1, 1)) def transform(self, X: pandas.DataFrame): \"\"\"Transform the given data.\"\"\" if type(X) == pandas.DataFrame: x = X.apply(lambda x: self.dot_product(x), axis=1) return x.values.reshape((-1, 1)) # elif type(X) == numpy.ndarray: # vector_func = numpy.vectorize(self.dot_product) # x = vector_func(X) # return x.reshape((-1, 1)) def get_feature_names_out(self) -> None: \"\"\"Return feature names. Required for onnx conversion.\"\"\" pass one...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28877"
  },
  {
    "number":31091,
    "text":"RFC set up Codespaces to ease contributor experience especially during sprints?\n\nIMO this could be useful as fall-back during sprints, in particular for pesky company Windows laptops, where I (and others for example @adrinjalali and @glemaitre) have been guilty to debug the Windows situation rather than focussing on more important stuff \ud83d\ude05. Try it on my fork URL] ![Image, maybe due to my addons not sure. !Image: - build time from scratch: ~7 minutes - run full test suite: ~13 minutes (with or without [CODE] has similar timings) - doc [CODE] (i.e. no example) ~9 minutes first time, ~1 minute second time Pricing: 120 core hours + 15GB storage free per month. With the default 2-core machine, which is probably enough for sprints. See [doc]([URL] for more details. I guess we may want to add light documentation about it somewhere. - numpy mentions codespaces without much detailed instructions: [URL] - scipy does something similar: [URL] Previous related conversations: - devcontainer: [URL] - gitpod: [URL]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31091"
  },
  {
    "number":27953,
    "text":"CalibratedClassifierCV gives a NotFittedError when accessing the underlying XGBoostClassifier feature_importances property\n\n### Describe the bug I am using CalibratedClassifierCV and XGBoost in a Pipeline and was able to train the model and use it to make predictions, etc. But I cannot access the underlying property of the XGBoost model. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Feature importance should be returned ### Actual Results ``[CODE]importance_type` 1266 parameter. When model trained with multi-class\/multi-label\/multi-target dataset, 1267 the feature importance is \"averaged\" over all targets. The \"average\" is defined (...) 1276 1277 \"\"\" -> 1278 b: Booster = self.get_booster() 1280 def dft...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27953"
  },
  {
    "number":26798,
    "text":"Backward compatibility issue for loading models with joblib in 1.3.0\n\n### Describe the bug Version 1.3.0 has broken backward compatibility for loading older models with [CODE]. ### Steps\/Code to Reproduce ## Step 1 : Save a model in 1.2.X [CODE_BLOCK] ## Step 2 : Upgrade to 1.3.0 and try loading older model [CODE_BLOCK] ### Expected Results The model loads correctly. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26798"
  },
  {
    "number":26812,
    "text":"Clarify the differences between [CODE] and [CODE]\n\n### Describe the issue linked to the documentation Currently the documentation for [CODE] says that is \"Similar to SVC with parameter [CODE], but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples\". This is confusing, because by default they are not equivalent. [CODE] has a different default loss function and regularizes the intercept, as mentioned in these StackOverflow posts: - [URL] - [URL] - [URL] As a result, the current documentation can mislead and give result to research that does the wrong thing, as it almost happened to me. ### Suggest a potential alternative\/fix A big red box with a warning should be at the beginning of the documentation for [CODE], describing the ways in which differs from [CODE] and that it is not a \"true\" linear SVM (because of the intercept regularization). In the future, it would be desirable in my opinion to change the default values\/implementation of [CODE] so that both [CODE] and [CODE] with parameter [CODE] solve the same optimization problem by default. Otherwise the risk of it leading to wrong research results is very big.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26812"
  },
  {
    "number":29858,
    "text":"Sklearn train_test_split gives incorrect array outputs.\n\n### Describe the bug I suspect this is because I give the function more than one array to split, but according to the documentation train_test_split should be able to take any number of arrays? Code to reproduce: [CODE_BLOCK] output is: [CODE_BLOCK] My dataset is split into three arrays. I expect train_test_split to split the dataset along the first axis with 2509 elements. Outputs are garbled and are inconsistent in both their first and second axis. I would expect the output to be f.ex (1756,9), (1756,21), (1756,2), and 753,... for the validation. ### Steps\/Code to Reproduce test_numerical = np.random.rand(2509, 9) test_categorical = np.random.rand(2509, 21) test_targets = np.random.rand(2509, 2) tr_num, tr_cat, tr_targ, vl_num, vl_cat, vl_targ = train_test_split(test_numerical, test_categorical, test_targets, test_size=0.3) print(tr_num.shape, tr_cat.shape, tr_targ.shape) print(vl_num.shape, vl_cat.shape, vl_targ.shape) ### Expected Results I expected the output to look like: [CODE_BLOCK] ### Actual Results (752, 9) (1757, 9) (752, 21) (1757, 21) (752, 2) (1757, 2) ### Versions ```shell System: python: 3.12.3 (main, Jul 31 2024, 17:43:48) [GCC 13.2.0] executable: \/home\/anders\/std_env\/bin\/python3.12 machine: Linux-6.8.0-44-generic-x86_64-with-glibc2.39 Python dependencies: sklearn: 1.5.1 pip: 24.2 setuptools: 74.0.0 numpy: 1.26.4 scipy: 1.14.0 Cython: None pandas: 2.2.2 matplotlib: 3.9.1 joblib: 1.4.2 threadpoolctl: 3....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29858"
  },
  {
    "number":30594,
    "text":"DOC: Example of [CODE] with [CODE] DataFrames\n\n### Describe the issue linked to the documentation Currently, the example here >>> X, y = iris['data'], iris['target'] >>> X.head() sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 >>> y.head() 0 0 1 0 2 0 3 0 4 0 >>> X_train, X_test, y_train, y_test = train_test_split( ... X, y, test_size=0.33, random_state=42) # rows will be shuffled >>> X_train.head() sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 96 5.7 2.9 4.2 1.3 105 7.6 3.0 6.6 2.1 66 5.6 3.0 4.5 1.5 0 5.1 3.5 1.4 0.2 122 7.7 ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30594"
  },
  {
    "number":25604,
    "text":"MLPR with solver='lbfgs', nonzero alpha doesn't make the same result from multiple run.\n\n### Describe the bug I have tested the simple regression modeling with MLPRegressor(solver='lbfgs', alpha=0.01, tol=0.0001, random_state=42) with sample data with three input parameters and three targets. I tested multiple run in Linux OS. But, sometimes I got different regression.score(X_test, y_test). But, if I change the solver to 'adam' or set alpha to zero, it makes the same value all the time. So, I am suspicious that there is randomness with respect to solver, alpha, and tol. But, it's hard to find the cause from the source code. I wonder if anybody has the same experience. ### Steps\/Code to Reproduce ```py import warnings warnings.simplefilter('ignore') for k in range(100): import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPRegressor data = np.array( [ [1.3436424, 8.4743374, 5.0, 67.34851990996208, 130.0, 126.0], [1.3436424, 8.4743374, 1.0, 63.34851990996208, 130.0, 126.0], [7.6377462, 2.5506903, 5.0, 277.4131267778526, 130.0, 126.0], [7.6377462, 2.5506903, 1.0, 273.4131267778526, 130.0, 126.0], [4.9543509, 4.4949106, 5.0, 189.98360040364014, 130.0, 126.0], [4.9543509, 4.4949106, 1.0, 85.98360040364014, 130.0, 126.0], [6.5159297, 7.8872335, 5.0, 389.335314633451, 130.0, 126.0], [6.5159297, 7.8872335, 1.0, 385.335314633451, 130.0, 126.0], [0.93859587, 0.28...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25604"
  },
  {
    "number":27982,
    "text":"Ensure that we have an example in the docstring of each public function or class\n\nWe should make sure that we have a small example for all public functions or classes. Most of the missing examples are linked to functions. I could list the following classes and functions for which [CODE] did not find any example: - [x] sklearn.base.BaseEstimator - [x] sklearn.base.BiclusterMixin - [x] sklearn.base.ClassNamePrefixFeaturesOutMixin - [x] sklearn.base.ClassifierMixin - [x] sklearn.base.ClusterMixin - [x] sklearn.base.DensityMixin - [x] sklearn.base.MetaEstimatorMixin - [x] sklearn.base.OneToOneFeatureMixin - [x] sklearn.base.OutlierMixin - [x] sklearn.base.RegressorMixin - [x] sklearn.base.TransformerMixin - [x] sklearn.base.clone - [x] sklearn.base.is_classifier - [x] sklearn.base.is_regressor - [x] sklearn.cluster.affinity_propagation - [x] sklearn.cluster.cluster_optics_dbscan - [x] sklearn.cluster.cluster_optics_xi - [x] sklearn.cluster.compute_optics_graph - [x] sklearn.cluster.estimate_bandwidth - [x] sklearn.cluster.k_means - [x] sklearn.cluster.mean_shift - [x] sklearn.cluster.spectral_clustering - [x] sklearn.cluster.ward_tree - [x] sklearn.covariance.graphical_lasso - [x] sklearn.covariance.ledoit_wolf - [x] sklearn.covariance.ledoit_wolf_shrinkage - [x] sklearn.covariance.shrunk_covariance - [x] sklearn.datasets.clear_data_home - [x] sklearn.datasets.dump_svmlight_file - [x] sklearn.datasets.fetch_20newsgroups - [x] sklearn.datasets.fetch_20newsgroups_vectorized - [x] s...",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27982"
  },
  {
    "number":26711,
    "text":"AttributeError: This 'LabelEncoder' has no attribute 'set_output'\n\n### Describe the bug I tried to call 'set_output' from LabelEncoder object and got the AttributeError. The document]([URL] [CODE_BLOCK] <\/br> Then estimator should have 'get_feature_names_out' to make '_auto_wrap_is_configured' returns True. [(utils._set_output.py)]([URL]",
    "labels":[
      "Bug",
      "help wanted",
      "Documentation"
    ],
    "label_count":3,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26711"
  },
  {
    "number":30714,
    "text":"Version 1.0.2 requires numpy<2\n\n### Describe the bug Installing scikit-learn version 1.0.2 leads to the following error: [CODE_BLOCK] This seems to indicate a mismatch between this version of scikit-learn and numpy versions greater than 2.0 (Specifically 2.2.2 was being installed, following the only restriction of [CODE]). This can be solved by indicating to use a numpy version older than 2.0 by modifying step 1 to: [CODE_BLOCK] ## Additional references [URL] [URL] ### Steps\/Code to Reproduce 1. Install scikit-learn through pip [CODE_BLOCK] 2. Use scikit-learn [CODE_BLOCK][CODE]``shell OS: Ubuntu 24.10 (latest) Python version 3.10 Scikit-learn version: 1.0.2 pip version: 24.3.1 setuptools version: 65.5....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30714"
  },
  {
    "number":31450,
    "text":"Spherical K-means support (unit norm centroids and input)\n\n### Describe the workflow you want to enable Hi, I was wondering if there is\u2014or has been\u2014any initiative to support cosine similarity in the KMeans implementation (i.e., spherical KMeans). I find the algorithm quite useful and would be happy to propose an implementation. The addition should be relatively straightforward. ### Describe your proposed solution Enable the use of cosine similarity with KMeans or implement a separate SphericalKMeans class. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31450"
  },
  {
    "number":31608,
    "text":"(Perhaps) safer version of halving search\n\n### Describe the workflow you want to enable I find the current experimental implementation of HalvingGridSearchCV problematic. At the first rounds it tends to select candidates with hyperparameters adapted to small sample sizes that are bad in hindsight, when it's too late. Think of regularization, tree depth, number of leaves, etc. This is a problem with CV in general, but 4\/5 or 9\/10 are a far cry from #samples \/ #candidates. ### Describe your proposed solution I've the following suggestion, although TBH I haven't thoroughly thought about it: take a splitter as usual and in each iteration of the splitter reduce the candidates, say by 2 or 3. So, for example, you start with cv=5 and 100 candidates, fit them on folds 2-5, compute scores on fold 1, discard half the candidates, proceed to the next split with test fold = 2 and 50 remaining candidates, etc. It obviously requires more resources than the current implementation, but early selected candidates would be better adapted to the last rounds. ### Describe alternatives you've considered, if relevant Implementing the above on top of GridSearchCV. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31608"
  },
  {
    "number":26614,
    "text":"Column-wise parallelism for Column Transformer\n\n### Describe the workflow you want to enable cc @Vincent-Maladiere Currently, when parallelizing the [CODE], it creates 1 job per transformer. This is inefficient as it may create stragglers. For instance, in the case when there are 20 columns with a slow transformer (e.g. 1min per col) and 20 others with a very fast transformer (e.g. 1 second per col), parallelizing the operation transformer-wise may take 20mins to run. On the other hand, if we can parallelize column wise, the operation can go down to 1 min. ### Describe your proposed solution Parallelize the [CODE] on each column, instead of each transformer. According to @GaelVaroquaux, we could [special case the univariate transformers that can be column-wise parallelized]([URL] (for instance, [Standard Scaler]([URL] Multivariate would stay transformer wise parallelized (e.g. [PCA]([URL] This could be done by introducing transformer tags. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Discussed with the [issue of parallelizing skrub's TableVectorizer]([URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26614"
  },
  {
    "number":29734,
    "text":"Default argument pos_label=1 is not ignored in f1_score metric for multiclass classification\n\n### Describe the bug I get a [CODE] for [CODE] default argument value to [CODE] metric with argument [CODE] for the iris flower classification problem: [CODE_BLOCK] According to the documentation, the [CODE] argument should be ignored for the multiclass problem: [URL] _The class to report if [CODE] and the data is binary, otherwise this parameter is ignored._ Setting [CODE] explicitly to None solves the problem and produces the expected output, see below. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ```pytb Cross-validated F1 Scores (micro average): [nan nan nan nan nan] Mean F1 Score: nan [C:\\Users\\r...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29734"
  },
  {
    "number":30615,
    "text":"average_precision_score produces unexpected output when scoring a single sample\n\n### Describe the bug When using [CODE] and scoring a single sample, the metric ignores [CODE] and will always produce a score of 1.0 if [CODE] and otherwise will return a score of 0. I would have expected that it would instead raise an exception. Potentially related to #30147, however I'm focusing on the minimal example with just a single sample. ### Steps\/Code to Reproduce [CODE_BLOCK] Additionally, you can see that the average_precision_score returns a score opposite of what precision and recall return: [CODE_BLOCK] ### Expected Results I would have expected the metric to raise an exception, similar to what happens when ROC_AUC is called with a single sample: [CODE_BLOCK] [CODE_BLOCK] ### Actual Results Refer to code sni...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30615"
  },
  {
    "number":31257,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Apr 28, 2025) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31257"
  },
  {
    "number":26821,
    "text":"Add [CODE] to [CODE] CV splitters\n\nRight now [CODE] classes take [CODE] as the strata, while that's not always the case. In [CODE] we allow a [CODE] arg (which I'm wondering if it should be called [CODE]), which defines the groups samples belong to. And inside, we basically do this: [CODE_BLOCK] As you can see, we're passing [CODE] as [CODE] to the splitter. I think it would make sense to add a [CODE] arg to the splitters, and if [CODE], we'd take values in [CODE] instead, as it is now. Note that now that we have SLEP6, we won't need a separate class for them, and we'd simply request [CODE] for the splitter: [CODE_BLOCK] cc @marenwestermann",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26821"
  },
  {
    "number":25522,
    "text":"Behaviour of [CODE] and [CODE] (and [CODE])\n\nThis issue is an RFC to clarify the expected behavior [CODE] and [CODE] (or [CODE] and [CODE] equivalently) when used with [CODE]. ### Estimators to be considered The estimators to be considered can be found in the following manner: [CODE_BLOCK] which give [CODE_BLOCK] ### Review the different behaviors We will evaluate the behaviour by doing the following experiment: - set [CODE] (or [CODE]) and [CODE] - [CODE] the estimator and check [CODE] (or [CODE]) - set [CODE] (or [CODE]) - [CODE] the estimator and check [CODE] (or [CODE]) The idea is to check if we report the total number of iterations or just the number of iterations of the latest [CODE] call. #### GLM estimators ```python from sklearn.linear_model import GammaRegressor, PoissonRegressor, TweedieRegressor Estimators = [GammaRegressor, PoissonRegressor, TweedieRegressor] for klass in Estimators: print(klass.__name__) estimator = klass(warm_start=True, max_iter=2, verbose=True).fit(X_reg, y_reg) print(f\"{estimator.n_iter_=}\") pri...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25522"
  },
  {
    "number":28825,
    "text":"tree.export_graphviz numpy error\n\n### Describe the bug When using tree.export_graphviz in 1.4.2 the following error appears: [CODE_BLOCK] The same code works normally in 1.2.2. ### Steps\/Code to Reproduce ```py from sklearn import tree from sklearn import metrics import matplotlib.pyplot as plt data_X = [[0.21, 327], # 1 [0.39, 497], # 1 [0.50, 1122], # 2 [0.76, 907], # 1 [0.87, 2757], # 1 [0.98, 2865], # 1 [1.13, 3045], # 2 [1.34, 3914], # 2 [1.67, 4849], # 2 [1.81, 5688]] # 2 data_Y = ['1', '1', '2', '1', '1', '1', '2', '2', '2', '2'] test_X = [[0.26, 689], [...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28825"
  },
  {
    "number":29922,
    "text":"Random forest regression fails when calling data: probably a numerical error\n\n### Describe the bug It is known that random forrest regression (as well as many decision tree-based methods) are not affected by the scale of the data and don't require any scaling in the feature matrix or response vector. This includes all types of scaling, like standard normalization (remove the mean, divide by the standard deviation) as well as simple scale scaling (constant multiplication or general linear transformations). However, here there is an example where the absolute scale drastically affects the performance of random forest. Just by multiplying the response by a small number, the performance drastically falls. I am pretty sure this is associated to numerical errors, but notice that the scale factor is not close to machine epsilon. Note:  I actually found this example by first noticing that RF was drastically failing with my scientific data, and fixing it by rescaling the response vector to more reasonable values. This is of course a very simple solution, but I can imagine many users having similar problems and not being able to find this fix given that this should not be required. I am more than happy to help fixing this bug, but I wanted to documented it first and check with the developers first in case there is something I am missing. ### Steps\/Code to Reproduce ```py import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegr...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29922"
  },
  {
    "number":26843,
    "text":"Where is CharNGramAnalyzer?\n\n### Describe the issue linked to the documentation In the example exercises for working with text data, the text says to use CharNGramAnalyzer. Unless I am missing something, like that this is part of some other class, I cannot find this anywhere in the docs, nor in the code. Documentation: [URL] ### Suggest a potential alternative\/fix Based on what the answer is, whether this class does not exist anymore or refers to something else, I think the text exercise can be adapted. I can do a PR for this, once I know the answer. I also tried googling for an answer, so it might also be that I m missing something obvious that will explain it.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26843"
  },
  {
    "number":27338,
    "text":"Feature Request: Out-of-Vocabulary (OOV) Token Handling in CountVectorizer\n\n### Describe the workflow you want to enable Users of CountVectorizer frequently encounter situations where, during the transformation of test data or new unseen data, there are words that the vectorizer hasn't seen during the training (fit) phase. In such situations, the default behavior is to ignore these words. However, for many NLP applications (like sentiment analysis, named entity recognition, etc.), having a representation for these OOV words is crucial. Hence, an inbuilt mechanism to handle OOV words by representing them with a special token would be beneficial. ### Describe your proposed solution 1. Additional Parameter: Introduce a new optional parameter in CountVectorizer called oov_token which defaults to None. When oov_token is None, the vectorizer behaves as it currently does. When oov_token is provided (e.g., <OOV>), any word encountered during the transform phase that is not in the learned vocabulary gets represented by this token. 2. Vocabulary Inclusion: The oov_token, if provided, should be part of the vocabulary, and its position\/index in the vocabulary should be consistent. 3. OOV Token Counting: The count of the OOV token should reflect the total count of all OOV words in a document. ### Describe alternatives you've considered, if relevant Custom Vectorizer: Users currently have to extend the CountVectorizer class to handle OOV, which adds extra overhead. An inbuilt feature would...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27338"
  },
  {
    "number":28386,
    "text":"Proper sparse support in [CODE]\n\n### Describe the workflow you want to enable As of version 1.4 the implementation for PCA supports sparse inputs. That's grand, but the PCA implementation doesn't offer a [CODE] method. The [CODE] does, but the way it handles sparse input is by casting it to dense first. This \"works\", but it may be more elegant\/preformant to apply the same sparse trick to the [CODE] implementation. ### Describe your proposed solution As the [release notes]([URL] suggest it may just be a matter of investigating if the [CODE] can be used here as well. But I'd need to dive in and check the implementation to understand if this is possible. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28386"
  },
  {
    "number":29101,
    "text":"When a Pipeline step is changed via set_params, the set_output state is cleared\n\n### Describe the bug When a Pipeline step is set via [CODE], the subsequent output of [CODE] is a numpy ndarray even if previously the pipeline's output was set to be of type pandas.DataFrame via a call to [CODE]. This only happens when the entire step is set: [CODE], not when only a step's parameters are set: [CODE]. The issue doesn't occur if, instead of calling [CODE] on the Pipeline object, the option is set globally with [CODE]. The same problem may or may not occur with ColumnTransformer, I haven't checked. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A pandas DataFrame ### Actual Results A numpy ndarray ### Versions ```shell System: python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] executable: \/usr\/bin\/python3 machine: Linux-6.1.85+-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.2.2 pip: 23.1.2 setuptools: 67.7.2 numpy: 1.25.2 scipy: 1.11.4 Cython: 3.0.10 pandas: 2.0.3 matplotlib: 3.7.1 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 2 prefix: libopenblas filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-5007b62f.3.23.dev.so version: 0.3.23.dev threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 2 prefix: libgomp filepath: \/usr\/local\/lib\/python3.10\/dist-package...",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29101"
  },
  {
    "number":28881,
    "text":"[CODE] should respect [CODE]\n\n### Describe the workflow you want to enable The current implementation of [CODE] seems to calculate (shrinked) averages of [CODE]. In cases with [CODE], it would be more natural to work with (shrinked) weighted averages. ### Describe your proposed solution In case of [CODE], shrinked averages should be replaced by corresponding shrinked weighted averages. However, I am not 100% sure if [CODE] are accessable by a transformer. ### Describe alternatives you've considered, if relevant The alternative is to continue ignoring sample weights. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28881"
  },
  {
    "number":30984,
    "text":"The estimators_ attribute can no longer be accessed for the AdaBoostClassifier class\n\n### Describe the bug The documentation of the AdaBoostClassifier [GCC 13.3.0] executable: \/home\/kkladny\/psi4conda\/envs\/adaboost\/bin\/python machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.6.1 pip: 25.0.1 setuptools: 75.8.2 numpy: 2.2.3 scipy: 1.15.2 Cython: None pandas: 2.2.3 matplotlib: 3.10.1 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 20 prefix: libscipy_openblas filepath: \/home\/kkladny\/psi4conda\/envs\/adaboost\/lib\/python3.11\/site-packages\/numpy.libs\/libscipy_openblas64_-6bb31eeb.so version: 0...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30984"
  },
  {
    "number":28913,
    "text":"mypy errors when depending on sklearn\n\n### Describe the workflow you want to enable less errors when analyzing python code relying on sklearn using mypy ### Describe your proposed solution Better code? Typing annotations in the right places? ### Describe alternatives you've considered, if relevant N\/A ### Additional context error: Skipping analyzing \"scipy\": module is installed, but missing library stubs or py.typed marker [import-untyped] error: Skipping analyzing \"sklearn\": module is installed, but missing library stubs or py.typed marker [import-untyped] error: Skipping analyzing \"joblib\": module is installed, but missing library stubs or py.typed marker [import-untyped] error: Skipping analyzing \"sklearn.ensemble\": module is installed, but missing library stubs or py.typed marker [import-untyped] error: Skipping analyzing \"sklearn.metrics\": module is installed, but missing library stubs or py.typed marker [import-untyped]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28913"
  },
  {
    "number":31350,
    "text":"SimpleImputer casts [CODE] into [CODE] when using \"most_frequent\" strategy\n\n### Describe the bug The column [CODE] changes from [CODE] to [CODE] when I transform it using [CODE]. Here is a list of related Issues and PRs that I found while trying to solve this problem: #29381 #18860 #17625 #17526 #17525 If this is truly a bug, I would like to work on a fix. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results This is the output I expected to see on the terminal [CODE_BLOCK] I expected [CODE] to keep the same [CODE] as the original [CODE]. ### Actual Results The actual results for when [CODE] is called is: [CODE_BLOCK] Observe that the [CODE] for [CODE] is now object instead of category. ### Versions ```shell System: python: 3.12.3 | packaged by Anaconda, Inc. | (main, May 6 2024, 19:46:43) [GCC 11.2.0] executable: \/home\/user\/miniconda3\/envs\/prod\/bin\/python mach...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31350"
  },
  {
    "number":26794,
    "text":"Handle np.nan \/ missing values in KBinsDiscretizer\n\nEspecially when the output is one-hot encoded, it would be quite natural to encode missing values as zeros or add an extra missingness indicator feature. Related to: - [URL]",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26794"
  },
  {
    "number":30408,
    "text":"[CODE] for [CODE]\n\n### Describe the workflow you want to enable I would like to be able to use [CODE] with the [CODE] preprocessing for streaming cases or when my data doesn't fit in memory. As I understand from this paper [URL] it would probably only be possible to compute the robustly estimated variance and mean up to some precision epsilon, that could probably become an attribute of the class. Not sure whether this should be considered a new algorithm or not, let me know what you think. ### Describe your proposed solution I haven't looked much into it but I think there were 2 approaches: - use one of the solutions proposed in [URL] - @amueller was suggesting in [URL] that binning could be an option. Maybe this is actually mentioned in the paper above ### Describe alternatives you've considered, if relevant An alternative proposed in this [SO comment]([URL] is to load the data column by column if it reduces the memory load. However, that would be super impractical in my setting where I just cannot load all data into memory at once. ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30408"
  },
  {
    "number":24287,
    "text":"Enhance and support the common-pitfalls page with referred published references.\n\n### Describe the issue linked to the documentation The documentation comes with a guide called Common pitfalls and recommended practices. [doi]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24287"
  },
  {
    "number":31373,
    "text":"SimpleImputer converts [CODE] extension array to [CODE], subsequently crashing with numpy [CODE] values\n\n### Describe the bug When using the [CODE] with a pyarrow-backed pandas DataFrame, any float\/integer data is converted to [CODE]\/[CODE] instead. This causes the imputer to be fitted to [CODE], crashing on a dtype assertion when passing it a numpy-backed [CODE] DataFrame after fitting. The flow is the following: 1. The imputer calls [CODE]: [URL] 2. This calls [CODE]: [URL] 3. This calls [CODE]: [URL] 4. Our input is a pandas dataframe: [URL] 5. This now checks if the dtypes need to be converted: [URL] 6. Our input is backed by an extension array _and_ [CODE] is an integer datatype, so we return [CODE] here: [URL] 7. Finally we pass the \"needs conversion\" check and convert the dataframe to [CODE] (which is [CODE] here, which apparently means [CODE]): [URL] ### Steps\/Code to Reproduce ```py import polars as pl from sklearn.impute import SimpleImputer import numpy as np print( SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0) .fit(pl.DataFrame({\"a\": [10]}, schema={\"a\": pl.Int32}).to_pandas(use_pyarrow_extension_array=False)) ._f...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31373"
  },
  {
    "number":27362,
    "text":"Redundant DataConversionWarning in BaseForest\n\n### Describe the bug A column vector is expected in [CODE] even though the [CODE] is reshaped to be 2D: [CODE_BLOCK] ref: [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The warning seems redundant considering the array is transformed into a 2D array right afterwards anyways. I'm wondering if there is any reason to keep this warning here? ### Actual Results [CODE_BLOCK] ### Versions ```shell >>> import sklearn; sklearn.show_versions() System: python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) [Clang 14.0.6 ] executable: \/Users\/adam2392\/miniforge3\/envs\/sktree\/bin\/python machine: macOS-13.5.2-arm64-arm-64bit Python dependencies: sklearn: 1.3.0 pip: 23.2.1 setuptools: 65.7.0 numpy: 1.25.2 scipy: 1.10.1 Cython: 0.29.36 pandas: 2.0.3 matplotlib: 3.7.2 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27362"
  },
  {
    "number":27441,
    "text":"partial_dependence() computes conditional partial dependence\n\n### Describe the bug For the case of correlated predictors (clearly highly common) the CODE] function gives different answers for [CODE] = \"recursion\" and [CODE] = \"brute\", see my [post the two methods are not equivalent for tree based algorithms, and (ii) that [CODE] = \"recursion\" actually computes the conditional $E[f(x_S,X_C)|X_S=x_s]$ instead of the (desired) interventional $E[f(x_S,X_C)| \\mathbf{do}(X_S=x_s)]$ ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results We would like the two methods to yield the same pdp values, so the last line should yie...",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27441"
  },
  {
    "number":28299,
    "text":"[API] A public API for creating and using multiple scorers in the sklearn-ecosystem\n\n### Describe the workflow you want to enable I would like a public stable interface for multiple scorers that can be developed against for the sklearn eco-system. Without this, it makes it difficult for libraries to provide any consistent API for dealing with evaluation with multiple scorers unless they: 1. Rely exclusively on [CODE] for evaluation as its the only place user input from multiple metrics can be funneled directly through to sklearn for evaluation. 2. Implement custom wrapper types. 3. Refuse to support multiple metrics. Why developers may prefer an externally sklearn supported multi-metric API: 1. Custom evaluation protocols can be developed that evaluate multiple objectives and benefit from sklearn's correctness (_i.e. caching, metadata and response values_). 2. Custom multi-scoring wrappers do not have to version against the verison of sklearn installed. (_See alternatives considered_) 3. Users can rely more on the same interface in sklearn-ecosystem of compliant libraries. --- Context for suggestion: In re-developing Auto-Sklearn, we perform Hyperparameter Optimization, which can include evaluating many metrics. We require custom evaluation protocols not trivially satisfied by [CODE] or the related family of provided sklearn functions. Previously, AutoSklearn would implement it's own metrics, however we'd like to extend this to any sklearn compliant scorer. Using a [CODE] is ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28299"
  },
  {
    "number":29107,
    "text":"Incorrect invalid device error introduced in #25956\n\n### Describe the bug #25956 introduced a new [CODE] function to test whether a tensor is on CPU. However, the implementation of the test, which is [CODE], is incorrect -- the device will actually not be a string, but [CODE]. Therefore, you should attempt to get the [CODE] attr, and use that if available. ### Steps\/Code to Reproduce You can view a sample error here: [URL] ### Expected Results [CODE] should not be thrown. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK] ```",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29107"
  },
  {
    "number":28609,
    "text":"Print warning if user passed only one class into StratifiedKFold\n\n### Describe the workflow you want to enable StratifiedKFold and other stratified splitters were designed to balance cross validation based on target or some features. Currently, if you pass a column with only one class (which majority of times is a user mistake) there is no warning, and it works as the usual KFold validation. I encountered a situation when I mistakenly passed wrong target format with only one class, and StratifiedKFold didn't warn me, so this quite problematic mistake in validation went unnoticed for entire project. ### Describe your proposed solution Print a warning if stratification column has only one unique value. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28609"
  },
  {
    "number":30818,
    "text":"UnsetMetadataPassedError can point towards the wrong method\n\n### Describe the bug When [CODE], for a missing [CODE], [CODE] message states that a [CODE] is missing. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect an error message pointing towards the missing [CODE], and perhaps a less verbose message when only one metadata is passed. Something like: 'sample_weight' are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call [CODE] on the estimator for using 'sample_weight' or [CODE] for not using it. See the Metadata Routing User guide... ### Actual Results ['sample_weight'] are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call [CODE] on the estimator for each metadata in ['sample_weight'] that you want to use and [CODE] for not using it. See the Metadata Routing User guide... ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30818"
  },
  {
    "number":29367,
    "text":"Build failure under Termux: can not execute sklearn\/_build_utils\/version.py\n\n### Describe the bug When attempting to install the newest (and older) versions of scikit-learn-1.6.dev0 using regular pip install, the installation fails due to a missing version.py file. This issue occurs on an Android system using Termux. Environment details: OS: Linux localhost 4.14.186+ (Android) Architecture: aarch64 Python: 3.11 The error occurs during the Meson build process, which tries to execute a non-existent file: \/data\/data\/com.termux\/files\/home\/downloads\/scikit-learn\/sklearn\/_build_utils\/version.py A workaround patch has been developed to address this issue: [CODE_BLOCK] This patch: - Extracts the version number from pyproject.toml - Removes the \"dev\" suffix - Creates the missing version.py file with a generic Python shebang - Makes the file executable After applying this patch, the Meson build process completes successfully, and the installation can proceed: ``` cpu family: aarch64 Host machine cpu: aarch64 Compiler for C supports arguments -Wno-unused-but-set-variable: YE Compiler for C supports arguments -Wno-unused-function: YES Compiler for C supports arguments -Wno-conversion: YES Compiler for C supports arguments -Wno-misleading-indentation: YES Library m found: YES Program python3 found: YES (\/data\/data\/com.termux\/files\/usr\/bin\/python3.11) Run-time dependency OpenMP for c found: YES 5.1 Found pkg-config: YES (\/data\/data\/com.termux\/files\/usr\/bin\/pkg-...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29367"
  },
  {
    "number":27652,
    "text":"Add individual penalization to precision matrix in graphical_lasso.py\n\n### Describe the workflow you want to enable Friedman et al. (2008) describe the coordinate descent procedure used for the graphical lasso. In the paper, there is a REMARK 2.1, which states that the objective function to be optimized can be modified to allow for a matrix of penalty values, rather than a scalar value. ### Describe your proposed solution This has been implemented [here]([URL] However, linear_model._cd_fast.pyx still has to be updated to allow for vectorized alpha input in enet_coordinate_descent_gram ### Describe alternatives you've considered, if relevant _No response_ ### Additional context This change is motivated to allow for prior incorporation into the inference procedure. When strong priors for edges are available, this can affect the strength of the corresponding edges' penalization.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27652"
  },
  {
    "number":31219,
    "text":"Add Categorical Feature Support to [CODE]\n\n### Describe the workflow you want to enable I want to impute missing values in categorical columns using a similar approach to [CODE], which currently works only for continuous data. Specifically, I want to enable the following workflow: - Identify and handle categorical columns in the dataset - Use classifier models (e.g., RandomForestClassifier) to impute missing values in categorical columns based on other features - Integrate with existing pipelines seamlessly, without needing to separate and impute categorical columns manually ### Describe your proposed solution Extend the current [CODE] class (or create a new class, such as [CODE]) to handle categorical data: - Detect categorical columns automatically (e.g., using [CODE] or [CODE]) or accept them via a [CODE] parameter - Encode the categorical variables using an internal encoder (e.g., [CODE]) - Use a classifier model (e.g., [CODE]) instead of a regression model for those columns - Predict only the missing values, then inverse transform the predictions back to the original categories This would enable more robust and automatic preprocessing for datasets that have numeric and categorical features. ### Describe alternatives you've considered, if relevant - Manually encoding categorical variables, using a classifier-based imputation strategy outside of [CODE] - Using other libraries like [CODE] or [CODE], which support mixed-type imputation but lack full integration with Scikit-l...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31219"
  },
  {
    "number":24601,
    "text":"Add tolerance [CODE] to LinearRegression for sparse matrix solver lsqr\n\n===BEGIN EDIT=== ## Description [CODE] should have a parameter [CODE] that is passed to the LSQR routine for solving with sparse matrices. This way, it should be (more or less) equal to [CODE] ===END EDIT=== ### Description [CODE] performs different on sparse matrix than on numpy arrays. The built-in unit test [CODE] works well with two features, but not with other feature counts, e.g. [CODE]. Other combinations of [CODE] and [CODE] lead to even higher discrepancies. There was a similar issue ([URL] in 2019, that was fixed ([URL] and complemented by mentioned unit test. ### Steps\/Code to Reproduce Original code from [URL] Only modification: [CODE]. [CODE_BLOCK] ### Expected Results Coefficients from both regressions shoul...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24601"
  },
  {
    "number":24702,
    "text":"TfidfVectorizer binary parameter\n\n### Describe the issue linked to the documentation The [CODE] parameter [CODE] is described as: > binary bool, default=False > If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0\/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0\/1 outputs). Yet there's no parameter called [CODE] so it's not clear what parameters the last comment is referring to in order to get 0\/1 outputs. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24702"
  },
  {
    "number":29111,
    "text":"ColumnTransformer ignores certain column t\n\n### Describe the bug I have a series of fitted CountVectorizers each will preprocess a column of the dataframe. However when putting them in ColumnTransformer, a few CountVecrotizers were given the whole dataframe rather than just the column it is supposed to process. ### Steps\/Code to Reproduce [CODE_BLOCK] It will got an error: [CODE] The reason is that ct.transformers[148] was given a dataframe rather than a series. Any idea why only 148 is seeing this issue? Shouldn't ct treats all countvectorizers indifferently? If I sequentially run the countvectorizers: [CODE_BLOCK] It will work as I have explicitly enforced each input to the countvectorizer is a pd.Series. ### Expected Results ColumnTransformer sends only the required column as pd.Series to each countvectorizer. ### Actual Results Error, ct.transformers[148] is thrown in the whole dataframe rather than the column as a pd.Series ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29111"
  },
  {
    "number":28911,
    "text":"DOC Add Tidelift to sponsor list\n\n### Describe the issue linked to the documentation Add Tidelift to sponsor list [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28911"
  },
  {
    "number":29595,
    "text":"PowerTransformer's standardize sensitive to small differences in data, with MinMaxScaler unable to scale\n\n### Describe the bug Edit - when I wrote \"sensitive to small differences in data\", I meant the different outputs when a value is changed from 4.61 to 4.62 as shown below. Edit2 - I'm not sure this should be even flagged as a bug, rather it's an unexpected result that arises when you really shouldn't use PowerTransformer on certain data. However, users might apply the PowerTransformer on hundreds of features at once, and perhaps the situation described below could raise a warning? Hello, I'm trying to test how a Yeo-Johnson transformation might affect a model I'm working on (however, this issue is related to standardization rather than to the Yeo-Johnson transformation itself). For certain features, which aren't well suited for a Yeo-Johnson or Box-Cox transformation, the algorithm returns extremely similar values (when standardize=False), with differences only in the last few significant digits. This is consistent with scipy's [CODE] and behaving as intended. However, then the standardization (standardize=True) can either yield unexpected results, or not, depending on small differences in the original data. In one of such cases, the standardization (standardize=True) returns values that maintain the original trend, but are very small, in the order of 10^-17. This behavior of the standardization algorithm is very sensitive to minimal differences in the original data, as if...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29595"
  },
  {
    "number":29697,
    "text":"GaussianProcessRegressor: wrong std and cov results when n_features>1 and no y normalization\n\n### Describe the bug When [CODE] and [CODE] is [CODE], the [CODE] seems to return bad std and cov results, as it doesn't consider the scale of the different features (while it seems to be ok when [CODE] and [CODE] is [CODE]). By taking a look at the code, we can see that [CODE] uses the [CODE] attribute to compute the variance and covariance but this attribute is set to [CODE] when [CODE] is set to [CODE] (default value), giving equal scale to all features. To fix this bug, one should always compute [CODE] from the training data and use the boolean attribute [CODE] to undo the normalization of [CODE] if necessary. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Without output normalization, the variance of the second output should be 10 times larger than the first output. ### Actual Results Without output normalization, the variance of the second output is equal to the variance of the first output. ### Versions ```shell System: python: 3.9.12 (tags\/v3.9.12:b28265d, Mar 23 2022, 23:52...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29697"
  },
  {
    "number":29588,
    "text":"MAINT Replace [CODE] with [CODE] in codebase\n\nIn [URL] we defined typedefs for commonly used Cython types throughout the codebase. Running [CODE], I found the following files contain [CODE], which could be replaced with [CODE] from [URL] - [ ] .\/sklearn\/metrics\/_pairwise_distances_reduction\/_radius_neighbors_classmode.pyx.tp - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/common.pxd - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/_bitset.pyx - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/_predictor.pyx - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/histogram.pyx - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/_binning.pyx - [ ] .\/sklearn\/ensemble\/_hist_gradient_boosting\/_bitset.pxd - [ ] .\/sklearn\/linear_model\/_sgd_fast.pyx.tp ~Is there any reason to leave these as [CODE] vs replacing them with the [CODE] to consolidate our types?~ To address this issue, it would be best to open up a PR one-by-one and implement the said change and cimport statement, and then link to the issue here.",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29588"
  },
  {
    "number":29542,
    "text":"FEA Add missing-value support to sparse splitter in RandomForest and ExtraTrees\n\n### Summary While missing-value support for decision trees have been added recently, they only work when encoded in a dense array. Since [CODE] and [CODE] both support sparse [CODE], if a user encodes [CODE] inside sparse [CODE], it should still work. ### Solution Add missing-value logic in [CODE] in [CODE], [CODE] and [CODE] in [CODE]. The logic is the same as in the dense case, but just has to handle the fact that [CODE] is now sparse CSC array format. ### Misc. FYI [URL] will introduce native support for missing values in the [CODE] models (i.e. random splitter). One thing I noticed though as I went through the PR is that the current codebase still does not support missing values in the sparse splitter. I think this might be pretty easy to add, but should we re-open this issue technically? Xref: [URL] _Originally posted by @adam2392 in [URL]",
    "labels":[
      "help wanted"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29542"
  },
  {
    "number":24949,
    "text":"Regression in 1.2.dev: GenericUnivariateSelect with _parameter_constraints\n\n### Describe the bug When [CODE], previously [CODE] would be accepted, but that option is not provided in [CODE], and so an error is raised. It's perhaps not a common usecase, and my code that's breaking with this probably should just use [CODE] directly, but I thought I should bring it up. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)] executable: C:\\Users\\breiniger\\Miniconda3\\envs\\pnl_dev\\python.exe machine: Windows-10-10.0.19044-SP0 Python dependencies: sklearn: 1.2.dev0 pip: 21.1.3 setuptools: 52.0.0.post20210125 numpy: 1.20.3 scipy: 1.6.2 Cython: 0.29.21 pandas: 1.4.1 matplotlib: 3.3.4 joblib: 1.2.0 threadpoolctl: 2.2.0 Built with OpenMP: True threadpoolctl info: filepath: C:\\Users\\breiniger\\Miniconda3\\envs\\pnl_dev\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24949"
  },
  {
    "number":28368,
    "text":"Crash in T-SNE\n\n### Describe the bug I got a crash using the (external) hdbscan package is: [CODE_BLOCK] Note that it also happens when only a relative small proportion of points are equal (but only sometimes?), this is just the easiest way to show it. By default some warnings are displayed: > ...\\sklearn\\decomposition\\_pca.py:685: RuntimeWarning: invalid value encountered in divide > self.explained_variance_ratio_ = self.explained_variance_ \/ total_var > ...\\sklearn\\manifold\\_t_sne.py:1002: RuntimeWarning: invalid value encountered in divide > X_embedded = X_embedded \/ np.std(X_embedded[:, 0]) * 1e-4 In the end it appears to be a problem in [CODE], not (always?) being able to handle [CODE] values. For example, this reproduces the crash: [CODE_BLOCK] One layer deeper, the crash occurs inside [CODE], as follows: ```py import...",
    "labels":[
      "Bug",
      "help wanted"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28368"
  },
  {
    "number":26768,
    "text":"AttributeError \"Flags object has no attribute 'c_contiguous'\" when using KNeighborsClassifier predict\n\n### Describe the bug I tried to build a K-nearest neighbor model. I used the default `[CODE][CODE][CODE][CODE]`` AttributeError Traceback (most recent call last) Cell In[5], line 5 1 knn1 = KNeighborsClassifier(n_neighbors=2) 3 knn1.fit(X, y) ----> 5 knn1.predict(X) File ~\/.local\/lib\/python3.10\/site-packages\/sklearn\/neighbors\/_classification.py:246, in KNeighborsClassifier.predict(self, X) 244 check_is_fitted(self, \"_fit_method\") 245 if self.weights == \"uniform\": --> 246 if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for( 247 X, self._fit_X, self.metric 248 ): 249 probabilities = self.predict_proba(X) 250 if self.outputs_2d_: File ~\/.local\/lib\/python3.10\/site-packages\/sklearn\/metrics\/_pairwise_distances_reduction\/_dispatcher.py:471, in ArgKminClassMode.is_usable_for(cls, X, Y, metric) 448 @classmethod 449 def is_usable_for(cls, X, Y, metric) -> bool: 450 \"\"\"Return True if the dispatcher can be used for the given parameters. 451 452 Parameters (...) 468 True if the PairwiseDistancesReduction can be used, else False. 469 \"\"\" 470 return ( --> 471 ArgKmin.is_usable_for(X, Y, metric) 472 # TODO: Support CSR matrices. 473 and not issparse(X) 474 and not issparse(Y) 475 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26768"
  },
  {
    "number":27643,
    "text":"Sphinx version information in \"Building the documentation\" section needs reevaluation\n\n### Describe the issue linked to the documentation At the bottom of the [\"Building the documentation\" section]([URL] there is a warning about the best performing [CODE] version, which leads to [this file]([URL] supposedly mirroring the configuration on [CODE]. In the file, the suggested version is [CODE]. However, [CODE] is the minimum necessary to make the current package combination work. If the version is reverted back to [CODE], the following error will appear: [CODE]. ### Suggest a potential alternative\/fix Replace [CODE] with [CODE].",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27643"
  },
  {
    "number":28933,
    "text":"DOC D2_log_loss_score is in wrong section\n\n`[CODE]` was added in [URL] but the function is documented in regression metrics with other D2 scores, while this one is a classification metric. Ping @OmarManzoor for a follow-up PR maybe ?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28933"
  },
  {
    "number":27695,
    "text":"pipeline using FunctionTransformer with feature_names_out=... fails when applied to dataframe argument\n\n### Describe the bug (based on this stackoverflow question: [URL] I have a simple sklearn (1.3.1) pipeline where the first step is renaming its input features, so I implemented feature_names_out as below. If I fit the pipeline on a numpy array using [CODE], everything is fine and it reports output feature names as [CODE], [CODE]. However if I fit on the dataframe directly with [CODE], then [CODE] gives a stack trace ending with [CODE]. (from the answer) The problem is that FunctionTransformer by default applies func directly to the input without converting the input first; so [CODE] produces a dataframe with columns still [CODE], and [CODE] gets fitted on that frame, setting its [CODE] attribute also to [CODE], which contradicts what comes out of [CODE] (having been passed through your [CODE]). The suggested workaround is to set [CODE] in your FunctionTransformer: this will convert the input to a numpy array, so that the subsequent step won't be fitted on a dataframe, so won't have a [CODE] set. (Or make sure a dataframe argument has its columns renamed to make [CODE] as I ended up doing.) ### Steps\/Code to Reproduce ```python from typing import List import numpy as np import pandas as pd from sklearn.preprocessing import FunctionTransformer, StandardScaler from sklearn.pipeline import make_pipeline def with_suffix(_, names: List[str]): return [name + '__log' for name in na...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27695"
  },
  {
    "number":28717,
    "text":"Warning with DecisionBoundaryPlot and polars DataFrame\n\n### Describe the bug Consider passing a polars DataFrame into [CODE]: [CODE_BLOCK] This raises a warning: [CODE_BLOCK] This issue is analogous to #23311, which passed a pandas DataFrame. See also #25896. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No warning is raised. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.1 (main, Jan 11 2023, 20:36:56) [Clang 14.0.0 (clang-1400.0.29.201)] executable: \/Users\/patrick\/Documents\/Duke\/teaching\/BIOSTAT821\/sandbox\/.venv\/bin\/python machine: macOS-14.3.1-arm64-arm-64bit Python dependencies: sklearn: 1.4.1.post1 pip: 24.0 setuptools: 65.5.0 numpy: 1.26.4 scipy: 1.12.0 Cython: None pandas: 2.2.1 matplotlib: 3.8.3 joblib: 1.3.2 threadpoolctl: 3.3.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 8 prefix: libomp filepath: \/Users\/patrick\/Documents\/Duke\/teaching\/BIOSTAT821\/sandbox\/.venv\/lib\/python...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28717"
  },
  {
    "number":26390,
    "text":"SplineTransformer(extrapolate=\"periodic\") outputs nan values for constant features\n\nWhile reviewing #24145 I discovered the following bug: [CODE_BLOCK] Batman! This is caused by: - [URL] [CODE] is typically zero for constant features. While fixing it for constant features is probably easy (just use [CODE]), I wonder if other related numerical stability problems can be triggered for nearly constant features. But maybe we can solve this problem in two stages: first the nan problem caused by modulus by exact zero and then investigate behavior on nearly constant data.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26390"
  },
  {
    "number":30992,
    "text":"UID-based Stable Train-Test Split\n\n### Describe the workflow you want to enable During model development, it's common to perform train-test splits multiple times on a dataset. This may occur during development iterations, when the dataset evolves over time, or when working with different data subsets. However, the current [CODE] function has a subtle failure mode: even with a fixed random seed, it doesn't guarantee consistent splits in these scenarios. A simple modification like adding a single row or reordering the dataset can result in completely different splits, making debugging particularly challenging. This limitation is well-recognized in the data science community. This is documented in a blog post [1], and popular book like Aur\u00e9lien G\u00e9ron's \"Hands-On Machine Learning with Scikit-Learn and Tensorflow\" includes custom implementations to address this problem [2-4]. To resolve this, we propose implementing a stable splitting mechanism based on unique identifiers. This approach would ensure that specific entries consistently remain in the same split, regardless of dataset modifications. ```python from sklearn.model_selection import train_test_split df = pd.DataFrame({ 'id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110], 'feature': [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1], 'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] }) train, test= train_test_split( df, test_size=0.3, random_state=42 ) print(\"initial split:\\n\", test) df = pd.concat([df, pd.DataFrame( {'id': [1...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30992"
  },
  {
    "number":28970,
    "text":"User Should Have An Option To Assign Different criterions With Different Percentage Of Trees In Random Forest\n\n## Describe the workflow you want to enable ### Detailed Explanation Of Proposed Workflow User can mention how many percentage of trees in [CODE] & [CODE] will follow which [CODE] ### Advantages Of Implementing Above Functionality Better results can be achieved in certain domains and this feature will help reserchers ## Describe your proposed solution ### This Is How The Feature Will Look At User's End When Coding In Python3 [CODE_BLOCK] ### Explanation Of Above Code After implementation of this new feature, [CODE] parameter will also accept a [CODE] where percentage can be passed as value for a particular criterion as key If sum of all values is less than 1 then percentage of trees left will follow default [CODE] and if it's more than 1 then an error will be raised In above code, other than [CODE] and [CODE], there is a [CODE] criterion also where each tree falling under [CODE] criterion can have any random [CODE] ## Describe alternatives you've considered, if relevant ### Alternative Code Using [CODE] ```python import numpy as np from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sk...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28970"
  },
  {
    "number":24694,
    "text":"CI \"no OpenMP\" build environment actually has OpenMP\n\nThis avoided catching a regression where an unprotected [CODE] was introduced. As a side-comment: Pyodide build needs to be built without OpenMP. From [URL] there is OpenMP in the build environment: [CODE_BLOCK] From [URL] Looking at why we get OpenMP in the \"no OpenMP\" build: - libopenblas can be compiled without openmp i.e. with pthreads for Linux and Windows, e.g. see [this]([URL] - there is no libopenblas with pthreads on OSX [anaconda.org\/conda-forge\/libopenblas\/files?sort=basename&sort_order=desc]([URL] So it seems like if we want an \"no OpenMP\" build we need it to be Linux or Windows. Not sure whether there was a good reason to have it on OSX originally.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24694"
  },
  {
    "number":29000,
    "text":"KFold(n_samples=n) not equivalent to LeaveOneOut() cv in CalibratedClassifierCV()\n\n### Describe the bug Calling CODE] with [CODE] (where n is the number of samples) can give different results than using [CODE], but the docs for [CODE] say these should be equivalent. In particular, the [CODE] class has an [CODE] attribute, which means [this branch the error may be unnecessary) but, either way, the two different [CODE] values seem like they should behave equivalently. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] and [CODE] should function identically. Instead, [CODE] succeeds and [CODE] throws. ### Actual Results ``` Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29000"
  },
  {
    "number":30909,
    "text":"Improve [CODE] switching for metrics\n\nSupercedes #26758 Switching [CODE] for metrics, involves some manipulation for [CODE] (switch column you pass) and [CODE] (for binary, multiply by -1) as you must pass the values for the positive class. In discussions in #26758 we thought of two options:  Add an example demonstrating what you need to do when switching [CODE]  Expose the (currently private) functions [[CODE]]([URL] and [[CODE]]([URL] This is a RFC to discuss if we prefer one, or both options. cc @glemaitre and maybe @ogrisel ?",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30909"
  },
  {
    "number":28659,
    "text":"Dubious claim in accuracy_score doc about being equal to jaccard_score\n\n### Describe the issue linked to the documentation The documentation for [CODE]  Accuracy = (TP + TN) \/ (TP + TN + FP + FN) Accuracy includes true negatives, while the Jaccard index doesn't. In practice: [CODE_BLOCK] ### Suggest a potential alternative\/fix I think this claim can just be removed.",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28659"
  },
  {
    "number":26640,
    "text":"ArrayAPI: [CODE] improper comparison to set\n\n### Describe the bug The newly introduced [CODE] is comparing types with respect to defined sets. See here [URL] This is not working as NumPy's dtype object is \"tricky\". ### Steps\/Code to Reproduce [CODE_BLOCK] Because [CODE_BLOCK] ### Expected Results The check should be more robust. I suggest using directly [CODE] (or vendoring it). ### Actual Results N\/A ### Versions [CODE_BLOCK] cc @thomasjpfan",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26640"
  },
  {
    "number":26292,
    "text":"Add support for bools in [CODE]\n\n### Describe the workflow you want to enable Suppose you wanna impute a bool array. Because it has NaNs, it's gonna be of dtype float and work fine: [CODE_BLOCK] However, now suppose that the value you pass actually has no NaNs (e.g., because the current data happens to have no NaNs), so there's nothing to impute. In this case, the array is of type bool: [CODE_BLOCK] Which makes the imputation fail because [CODE] isn't supported: [CODE_BLOCK] This previous code generates an exception: [CODE_BLOCK] This is because [CODE] is \"b\", which is not in {\"i\", \"u\", \"f\", \"O\"} and so it fails. ### Describe your proposed solution My solution is to support [CODE]. I believe it shouldn't be a great effort. ### Describe alternatives you've considered, if relevant An alternative solution is for me to create a custom imputer class that copy-pastes the behavior I want from `SimpleImput...",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26292"
  },
  {
    "number":24876,
    "text":"[CODE]s in the documentation\n\n### Describe the issue linked to the documentation Some [CODE]s are present in the dev documentation and need to be fixed. Here is a list: - [x] [bicluster\/plot_bicluster_newsgroups.html]([URL] - [x] [cluster\/plot_bisect_kmeans.html]([URL] #24891 - [x] [decomposition\/plot_ica_blind_source_separation.html]([URL] #24908 - [x] [ensemble\/plot_gradient_boosting_categorical.html]([URL] #24902 - [x] [cluster\/plot_color_quantization.html]([URL] #24893 - [x] [ensemble\/plot_stack_predictors.html]([URL] #24918 - [x] [inspection\/plot_linear_model_coefficient_interpretation.html]([URL] #24920 - [x] [~~cross_decomposition\/plot_compare_cross_decomposition.html~~]([URL] - [x] [cluster\/plot_cluster_comparison.html]([URL] #24927 - [x] [model_selection\/plot_successive_halving_heatmap.html]([URL] #24968 - [x] [manifold\/plot_compare_methods.html]([URL] #24909 - [x] [cluster\/plot_kmeans_assumptions.html]([URL] #24928 - [x] [decomposition\/plot_faces_decomposition.html]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24876"
  },
  {
    "number":28617,
    "text":"Error compiling with GCC14 in i686\n\n### Describe the bug This is another error compiling with GCC14, different to the error reported in #28530 It happens when compiling in i386 in the Fedora build system. I get an \"incompatible pointer type\" [CODE]random_UINT32_t [CODE]typedefs_uint32_t [CODE]random_UINT32_t [CODE]typedefs_uint32_t [CODE]unsigned int [CODE]long unsigned int [CODE]python setup.py build[CODE]`` sklearn\/linear_model\/_cd_fast.c: In function \u2018__pyx_f_7sklearn_12linear_model_8_cd_fast_rand_int\u2019: sklearn\/linear_model\/_cd_fast.c:20417:59: error: passing argument 1 of \u2018__pyx_f_7sklearn_5utils_7_random_our_rand_r\u2019 from incompatible pointer type [-Wincompatible-pointer-types] 20417 | __pyx_t_1 = __pyx_f_7sklearn_5utils_7_random_our_rand_r(__pyx_v_random_state); if (unlikely(__pyx_t_1 == ((__pyx_t_7sklearn_5utils_7_random_UINT32_t)-1) && __Pyx_ErrOccurredWithGIL())) __PYX_ERR(0, 40, __pyx_L1_error) | ^~~~~~~~~~~~~~~~~~~~ | | | __pyx_t_7sklearn_5utils_9_typedefs_uint32_t  {aka unsigned int } sklearn\/linear_model\/_cd_fast.c:20308:151: note: expected \u2018__pyx_t_7sklearn_5utils_7_random_UINT32_t \u2019 {aka \u2018long unsigned int \u2019} but argument is of type \u2018__pyx_t_7sklearn_5utils_9_typedefs...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28617"
  },
  {
    "number":24411,
    "text":"MLPRegressor - Validation score wrongly defined\n\n### Describe the bug In MLPRegressor, if the option early_stopping is set as True, the model will monitor the loss calculated on the validation set in stead of the training set, using the same loss formulation which is the mean squared error. However, as implemented in the line 719 in the source code: [CODE_BLOCK] The function \"score\", which returns (to confirm) the coefficient of determination, is used. This is not correct. It should be something like: [CODE_BLOCK] ### Steps\/Code to Reproduce Sorry, I don't have time to write a simple code. But the error is quite clear. ### Expected Results The validation score must be mean squared error. ### Actual Results Coefficient of determination ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug",
      "Enhancement"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24411"
  },
  {
    "number":24573,
    "text":"NaNs and Infs when decomposing EDS\n\n### Describe the bug !image Input In [29], in <cell line: 4>() 1 scrop.change_dtype('float32') ----> 4 scrop.decomposition(True, algorithm='svd', output_dimension=15) 6 scrop.learning_results.save('LowSite4_pca_15_results') 8 scrop.plot_decomposition_results() File \/usr\/local\/anaconda3\/lib\/python3.9\/site-packages\/hyperspy\/learn\/mva.py:451, in MVA.decomposition(self, normalize_poissonian_noise, algorithm, output_dimension, centre, auto_transpose, navigation_mask, signal_mask, var_array, var_func, reproject, return_info, print_info, svd_solver, copy, kwargs) 448 mean = None 450 if algorithm == \"SVD\": --> 451 factors, loadings, explained_variance, mean = svd_pca( 452 data_, 453 svd_solver=svd_solver, 454 output_dimension=output_dimension, 455 centre=centre, 456 auto_transpose=auto_transpose, 457 kwargs, 458 ) 460 elif algorithm == \"MLPCA\": 461 if var_array is not None and var_func is not None: File \/usr\/local\/anaconda3\/lib\/python3.9\/site-packages\/hyperspy\/learn\/svd_pca.py:285, in svd_pca(data, output_dimension, svd_solver, centre, auto_transpose, svd_flip, kwargs) 282 else: 283 auto_transpose = False --> 285 U, S, V = svd_solve( 286 data, 287 output_dimension=output_dimension, 288 svd_solver=svd_solver, 289 svd_flip=svd_flip, 290 kwargs, 291 ) 293 explained_variance = S  2 \/ N 295 if aut...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24573"
  },
  {
    "number":31131,
    "text":"Duplicate\/incomplete dependency information\n\n### Describe the issue linked to the documentation Scikit-learn dependecies are described in two places:  [Installing scikit-learn]([URL] [URL]  [Installing the development version of scikit-learn]([URL] [URL] ### Suggest a potential alternative\/fix I suggest removing the incomplete [Dependencies]([URL] section from _[Installing the development version of scikit-learn]([URL] and referring to the extensive dependency list under _[Installing scikit-learn]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31131"
  },
  {
    "number":30512,
    "text":"Fail to pickle [CODE] with [CODE]\n\n### Describe the bug Spotted in scikit-lego, running [CODE] fails with [CODE] and [CODE]. cc: @koaning ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Not to raise ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.12 (main, Nov 6 2024, 20:22:13) [GCC 11.4.0] machine: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.6.0 pip: 24.1 setuptools: None numpy: 2.2.0 scipy: 1.15.0rc1 Cython: None ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30512"
  },
  {
    "number":30024,
    "text":"One-class SVM probabilistic output\n\n### Describe the workflow you want to enable LIBSVM introduced one-class probabilistic outputs last year in version 3.31. ### Describe your proposed solution Add a [CODE] argument to [OneClassSVM]([URL] similar to the [SVC]([URL] and [NuSCV]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context One-class probabilistic outputs are based on a density-based binning of decision values as described [here]([URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30024"
  },
  {
    "number":24353,
    "text":"BUG: MLLE implementation does not yield expected results\n\n### Describe the bug The implemented manifold learning method Modified Locally Linear Embedding (MLLE) in \/sklearn\/manifold\/_locally_linear.py line 403 ('method==modified') does not produce the expected results, see attached img. From the reference 3] it is clear that MLLE should reconstruct a lower dimesional manifold which preserves local geometric characteristics of the higher-dimensional input data. This means data points of the same color which are close in the 'Original S-curve' 3D data should also be close in reconstructed 2D manifold. Also points which are far away from each other along the local coordinate along the S-shape should be far away in the reconstruction. Meaning the yellow points and the blue points should be the furthest apart in the reconstructed data. As can be seen from the [example n_samples = 1500 S_points, S_color = datasets.make_s_curve(n_samples, random_state=rng) def plot_3d(points, points_color, title): x, y, z = points.T fig, ax = plt.subplots( figsize=(6, 6), facecolor=\"white\", tight_layout=True, ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24353"
  },
  {
    "number":30013,
    "text":"Return 3D array instead of list of arrays for multioutput [CODE]\n\n### Describe the workflow you want to enable Currently, using [CODE] for multioutput predictions returns a list of [CODE], consisting of 2D arrays of shape [CODE], with probabilities of class 0 and 1. This is quite surprising, since in all other cases pure [CODE] is returned. This also makes reshaping the output inconvenient, requiring a call to [CODE]. For example, to get positive class probabilities, e.g. to compute multioutput AUROC, I have to do (typing for clarity): [CODE_BLOCK] Only then the resulting [CODE] has shape [CODE], with predicted class 1 probability in columns. ### Describe your proposed solution Return [CODE] instead of list of arrays in the multioutput case. Just calling [CODE] internally would be enough. ### Describe alternatives you've considered, if relevant It could also be nice to include a utility function to extract positive class probabilities. [CODE] is quite non-obvious transformation, while also being necessary in practice to compute column-wise metrics based on probability. ### Additional context _No response_ EDIT: I also found another bug caused by the current implementation. In grid search CV, when using any multioutput prediction, this line will error: [URL] Error: ``` Traceback (most recent call last): File \"\/home\/jakub\/.cache\/pypoetry\/virtualenvs\/scikit-fingerprints-VjWItXgH-py3.9\/lib\/python3.9\/site-packages\/sklearn\/model_selection\/_validation.py\", line 971, in _score scores...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30013"
  },
  {
    "number":30247,
    "text":"Notes to update the release process\n\nThis issue is used to consolidate the point that needs to be updated in the release process, notably due to the adoption of towncrier. ### RC release process: - when requesting to bump the version number of [CODE] in [CODE], we need to request changing the root RST file targeted by towncrier in the [CODE] file. - add that we should generate the changelog with [CODE]: Generate the changelog with towncrier but keep the fragments: [CODE] (we should adapat the version number)",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30247"
  },
  {
    "number":26358,
    "text":"Mean Absolute Percentage Error\n\n### Describe the workflow you want to enable Mean Absolute Percentage Error (MAPE): MAPE is a new evaluation metric for a regression problem. It is calculated as the mean absolute percentage error between the predicted and actual values. MAPE is a more robust metric than other metrics such as root mean squared error (RMSE) because it is not affected by outliers. ### Describe your proposed solution The mean absolute percentage error (MAPE) is a measure of prediction accuracy of a forecasting method in statistics. It usually expresses the accuracy as a ratio defined by the formula: Code snippet MAPE = \u2211 |(At \u2212 Ft)\/At| Use code with caution. Learn more. First, the MAPE is not affected by outliers. Second, the MAPE is more intuitive to understand than the RMSE. Third, the MAPE is more sensitive to changes in the forecast values. However, the MAPE also has some disadvantages. First, the MAPE can be very sensitive to small changes in the actual values. Second, the MAPE is not always a good measure of accuracy for forecasting methods that are not linear. Here are some proposed solutions to the MAPE: Use a robust measure of accuracy: The MAPE is not a robust measure of accuracy, meaning that it is sensitive to outliers. If there are outliers in the data, the MAPE may not be a good measure of accuracy. In this case, it is better to use a robust measure of accuracy, such as the median absolute error (MAE). Use a linear forecasting method: The MAPE is not...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26358"
  },
  {
    "number":24638,
    "text":"TransformedTargetRegressor is incompatible with GaussianProcessRegressor when using return_std=True\n\n### Describe the bug [CODE] is incompatible with [CODE] when using [CODE] due to the fact that the model returns a tuple rather than an array. ### Steps\/Code to Reproduce ```python import numpy as np from sklearn.preprocessing import PowerTransformer from sklearn.compose import TransformedTargetRegressor from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.datasets import fetch_openml from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from sklearn.compose import make_column_transformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import OrdinalEncoder from sklearn.compose import make_column_selector def load_ames_housing(): df = fetch_openml(name=\"house_prices\", as_frame=True) X = df.data y = df.target features = [ \"YrSold\", \"HeatingQC\", \"Street\", \"YearRemodAdd\", \"Heating\", \"MasVnrType\", \"BsmtUnfSF\", \"Foundation\", \"MasVnrArea\", \"MSSubClass\", \"ExterQual\", \"Condition2\", \"GarageCars\", \"GarageType\", \"OverallQual\", \"TotalBsmtSF\", \"BsmtFinSF1\", \"HouseStyle\", \"MiscFeature\", \"MoSold\", ] X = X[features] X, y = shuffle(X, y, random_state=0) X = X[:600] y = y[:600] return X, np.log(y) X, y = load_ames_housing() cat_selector = make_column_selector(dtype_include=object) num_selector = make_column_selector(dtype_include=np.number) cat_tree_processor = OrdinalEncoder( handle_unknown=\"use_encoded_value\", unknown_...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24638"
  },
  {
    "number":29534,
    "text":"decomposition.PCA(svd_solver='covariance_eigh') is less stable with numpy==2.0\n\n### Describe the bug CODE] is less stable with numpy==2.0 I noticed this issue as some tests started failing at the downstream [dask-ml\/#997 System: python: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)] executable: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\envs\\numpy2\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.5.1 pip: 24.0 setuptools: 69.5.1 numpy: 2.0.0 scipy: 1.14.0 Cython: None pandas: 2.2.2 matplotlib: None joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP:...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29534"
  },
  {
    "number":26752,
    "text":"HistGradientBoostingRegressor is slower when torch not imported\n\n### Describe the bug This is perhaps not a bug but an opportunity for improvement. I've noticed that scikit-learn runs considerably faster if I happen to have CODE] before any [CODE] imports. This first block of code runs much slower: [CODE_BLOCK] Than this second block of code: [CODE_BLOCK] Here's the run times over 6 runs each on my actual code, the only difference being an import of [CODE] ![image. But even manually setting this to 10 doesn't bridge the gap. I don't know enough about [CODE] or [CODE] to be able to work out what else is going on, I'm guessing someone who's worked on [CODE]...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26752"
  },
  {
    "number":27085,
    "text":"MAINT validate_params for plot_tree (#25882)\n\n### Describe the bug The March 20, 2023 commit (MAINT validate_params for plot_tree (#25882) ) of the file: scikit-learn\/sklearn\/tree\/_export.py introduced the parameter validation of the plot_tree function that does not seem to agree with the documentation in the docstring or website. The parameter validation seems to omit the bool option described in the help. This option was previously permissible. Has it been removed as a valid option or is the parameter validation missing this option? [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results ``` Traceback (most recent call last): File ~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\spyder_kernels\\py3compat.py:356 in compat_exec exec(code, globals, locals) File c:\\temp\\decisiontree.py:26 plot_tree(dt, feature_names=features, class_names=True) File ~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\sklearn\\utils\\_param_valid...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27085"
  },
  {
    "number":28175,
    "text":"Inconsistency in [CODE] Threshold Behavior\n\n### Describe the bug I've encountered an unexpected behavior in [CODE] when using a decision stump (a tree with one root node and two leaf children). My assumption is based on the standard decision tree logic where a feature value [CODE] is classified to the left child if [CODE]. Therefore, I expect the following assertion to always be true: [CODE_BLOCK] This assertion is meant to test that a feature value equal to the root node's threshold is classified to the left child node, in accordance with the [CODE] rule. However, I have observed that in approximately 25% of the cases, the data point is unexpectedly classified to the right child node instead of the left. The issue persists regardless of whether the [CODE] is explicitly converted to [CODE] or not. Given scikit-learn's documentation stating that > All decision trees use [CODE] arrays internally. If training data is not in this format, a copy of the dataset will be made this behavior is puzzling. Precise thresholding is crucial for my application. Thank you for your time and effort in maintaining this important library and for any insights you can provide regarding this issue. ### Steps\/Code to Reproduce ```python import numpy as np from sklearn.tree import DecisionTreeClassifier correct, wrong = [], [] n_trials = 500 for seed in range(n_trials): rand = np.random.RandomState(seed) X = rand.normal(size=(100, 1)) y = rand.randint(0, 2, size=X.shape[0]) clf = DecisionTreeClassifie...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28175"
  },
  {
    "number":27952,
    "text":"HistGradientBoosting pickle portability between 64bit and 32bit arch\n\n### Describe the bug HistGradinetBoosting models use [CODE_BLOCK] to represent the [CODE_BLOCK] in TreePredictor nodes [URL] This seems to cause issues with using pickled HistGradientBoosting models which are trained on a 64 bit environment, in 32 bit environments ( like Pyodide which is where I encountered this issue). I know that for a while the other Tree models in sklearn had a similar problem but I am not 100% what the solution was. Would changing the type to be [CODE_BLOCK] be an acceptable solution here? ### Steps\/Code to Reproduce ## Steps to reproduce 1. Train a model in python on a 64 bit system 2. Pickle the output 3. Load that pickle on a 32 bit python environment like Pyodide 4. Attempt to run the prediction on the loaded model see this repo for a full example: [URL] ### Expected Results The pyodide code to run and give the expected output ### Actual Results ## Error message Running the above gives the following error message when trying to execute the Pyodide code ``` PythonError: Traceback (most recent call last): File \"\/lib\/python311.zip\/_pyodide\/_base.py\", line 571, in eval_code_async await CodeRunner( File \"\/lib\/python311.zip\/_pyodide\/_base.py\", line 394, in run_async coroutine = eval(self.code, globals, locals) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"<exec>\", line 61, in <module> File \"\/lib\/python3.11\/site-packages\/sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py\", l return se...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27952"
  },
  {
    "number":25301,
    "text":"Bisecting Kmeans predict and transform.argmin do not give same results\n\n### Describe the bug The bisecting KMeans algorithm does not give the same result when using [CODE] and [CODE]. I'm not sure what the desired output would be, and whether they should be equal. This, however, does go against how other clusterers (most notably the regular KMeans) work. I think the difference lies in the fact that the predict method works hierarchically through the original tree, while the transform method just calculates the euclidean distance between the terminal cluster centers. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results pred and pred_2 should be equal I think. ### Actual Results They are not equal, but pred_km and pred_km_2 are. ### Versions ```shell System: python: 3.9.15 (main, Oct 11 2022, 21:39:54) [Clang 14.0.0 (clang-1400.0.29.102)] executable: \/Users\/stephantulkens\/Documents\/code\/similarity\/.venv\/bin\/python machine: macOS-12.5-arm64-arm-64bit Python dependencies: sklearn: 1.2.0 pip: 22.3.1 setuptools: 65.6.3 numpy: 1.24.1 scipy: 1.9.3 Cython: None pandas: 1.5.2 matplotlib: 3.6.2 joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: libomp filepath: \/Users\/stephantulkens\/Documents\/code\/similarity\/.venv\/lib\/python3.9\/site-packages\/sklearn\/.dylibs\/libomp.dylib version: None num_threads: 10 user_api: blas internal_api: openblas prefix: libopenblas filepath: \/Users\/st...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25301"
  },
  {
    "number":30840,
    "text":"StandardScaler is [CODE]\n\n### Describe the bug The StandardScaler seems to be stateless in version 1.6.1. But fit changes the state of the StandardScaler if I got it correctly. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results False ### Actual Results True ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30840"
  },
  {
    "number":28218,
    "text":"StratifiedGroupKFold not ensuring Stratified splits\n\n### Describe the bug The existing implementation of the \"StratifiedGroupKFold\" class does not consistently achieve accurate stratified splits when dividing datasets into subsets, particularly when the dataset contains a relatively small number of samples. None of the resulting splits guarantee the presence of at least one sample from every class in both the training and testing sets. This issue can be better illustrated with an example. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results One of the expected results is the following possible result: [CODE_BLOCK] This result is obtained by changing the [CODE] in [CODE] to 5. The error is either: - the randomization used in [CODE] as presented in issue #24656 . It could be problematic when we want to implement an object close to the [CODE] as presented in issue #24247 . - the class repartition is not ensuring by the object, and the trainset might not contain all classes. This could be a real problem when using the splitter in an automated pipeline. ### Actual Results Res...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28218"
  },
  {
    "number":24604,
    "text":"Not able to install on python 3.10 with pip\n\n### Describe the bug [CODE_BLOCK] I'm not able to resolve dependencies issue while trying to install scikit-learn below the error output i get: ```bash (InstallK) C:\\Users\\XXXXXXX>pip install scikit-learn Collecting scikit-learn Using cached scikit-learn-1.1.2.tar.gz (7.0 MB) Installing build dependencies ... error error: subprocess-exited-with-error \u00d7 pip subprocess to install build dependencies did not run successfully. \u2502 exit code: 1 \u2570\u2500> [63 lines of output] Collecting setuptools<60.0 Using cached setuptools-59.8.0-py3-none-any.whl (952 kB) Collecting wheel Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB) Collecting Cython>=0.28.5 Using cached Cython-0.29.32-py2.py3-none-any.whl (986 kB) Collecting oldest-supported-numpy Using cached oldest_supported_numpy-2022.8.16-py3-none-any.whl (3.9 kB) Collecting scipy>=1.3.2 Using cached scipy-1.9.1.tar.gz (42.0 MB) Installing build dependencies: started Installing build dependencies: finished with status 'error' error: subprocess-exited-with-error pip subprocess to install build dependencies did not run successfully. exit code: 1 [34 lines of output] Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"aarch64\" and platform_python_implementation != \"PyPy\"' don't match your environment Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"arm64\" and platform_system == \"Darwin\"' don't match your environment Ignoring numpy: markers 'python_vers...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24604"
  },
  {
    "number":28725,
    "text":"RFE and RFECV allow features_to_select to be larger than available features\n\n### Describe the bug If the [CODE] or the [CODE] objects are initialized with a [CODE] or a [CODE] (respectively) attribute larger than the number of features present in the [CODE] variable that is passed to the [CODE] method, I would expect an error to be raised. However a result is returned where [CODE] is equal to the number of features in [CODE]. ### Steps\/Code to Reproduce For the [CODE] class: [CODE_BLOCK] For the [CODE] class: [CODE_BLOCK] ### Expected Results Expected that an exception is raised stating that the [CODE] or [CODE] variables cannot be greater than the number of available features. The downside is that if we raise an error we can break someone's code that was unintentionally passing a number o features larger than the available ones. So would it be best to raise a warning instead? ### Actual Results A result is computed where [CODE] is equal to the number of features in [CODE]. ### Versions ```shell System: python: 3.9.18 (main, Mar 17 2024, 15:49:30) [GCC...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28725"
  },
  {
    "number":24313,
    "text":"BayesianRidge prediction standard deviation affected by uniform sample weights\n\n### Describe the bug The standard deviation of predictions obtained by setting return_std=True on predict(), is clearly affected by uniform sample_weight vectors on fit(). A uniform sample_weight vector will act as a constant on the likelihood function, and I am therefore questioning the effect it has on the standard deviation of predictions. As shown below by the example, the scale (1 vs. 20) of the uniform sample_weight vector directly affects the scale of the standard deviations. The example is based on the Curve Fitting with Bayesian Ridge Regression example: return np.sin(2  np.pi  x) size = 25 rng = np.random.RandomState(1234) x_train = rng.uniform(0.0, 1.0, size) y_train = func(x_train) + rng.normal(scale=0.1, size=size) x_test = np.linspace(0.0, 1.0, 100) n_order = 3 X_train = np.vander(x_train, n_order + 1, increasing=True) X_test = np.vander(x_test, n_order + 1, increasing=True) reg = BayesianRidge(tol=1e-6, fit_intercept=False) init = [1.0, 1e-3] reg.set_params(alpha_init=init[0], lambda_init=init[1]) fig, axes = plt.subplots(1, 2, figsize=(8, 4)) for i, ax in enumerate(axes): if i == 0: reg.fit(X_train, y_train, sample_weight=np.ones(size)) ymean, ystd = reg.predict(X_test, return_std=True) ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\") ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\") ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\") ax.fi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24313"
  },
  {
    "number":26193,
    "text":"Thresholds can exceed 1 in [CODE] while providing probability estimate\n\nWhile working on [URL] I found out that something was odd with [CODE] that returns a threshold greater than 1. A non-regression test (that could be part of [CODE]) could be as follow: [CODE_BLOCK] The reason is due to the following: [URL] Basically, this is to add a point for [CODE] and [CODE]. However, the [CODE] rule does not make sense in the case [CODE] is a probability estimate. I am not sure what would be the best fix here. A potential workaround would be to check [CODE] in which case we should clip [CODE] to not be above 1.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26193"
  },
  {
    "number":27778,
    "text":"check_regressor_multioutput does not allow np.float32 predictions\n\n### Describe the bug When testing my scikit-learn interface for my NN regressor, [CODE] fails with an [CODE] This is because the check only checks for [CODE]. I can resolve this by casting the resulting array to [CODE], but then [CODE] fails instead because it applies a smaller tolerance for [CODE] arrays. Judging from the error message, I would guess that [CODE] should be allowed in [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown (at least not for this reason). ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] executable: \/home\/david\/prog\/venvs\/tab_bench_venv\/bin\/python3.10 machine: Linux-6.2.0-36-generic-x86_64-with-glibc2.35 Python dependencies: sklearn: 1.3.2 pip: 22....",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27778"
  },
  {
    "number":25227,
    "text":"Possible bug in the implementation of histogram in histogram based gradient boosting.\n\n### Describe the bug I was looking at the code in the file [CODE] and I believe there might be a bug. The note at the top of the file describes the following: [CODE_BLOCK] This is used in several places in the file for example here: ``` cpdef void _build_histogram( const int feature_idx, const unsigned int [::1] sample_indices, # IN const X_BINNED_DTYPE_C [::1] binned_feature, # IN const G_H_DTYPE_C [::1] ordered_gradients, # IN const G_H_DTYPE_C [::1] ordered_hessians, # IN hist_struct [:, ::1] out) nogil: # OUT \"\"\"Return histogram for a given feature.\"\"\" cdef: unsigned int i = 0 unsigned int n_node_samples = sample_indices.shape[0] unsigned int unrolled_upper = (n_node_samples \/\/ 4) * 4 unsigned int bin_0 unsigned int bin_1 unsigned int bin_2 unsigned int bin_3 unsigned int bin_idx for i in range(0, unrolled_upper, 4): bin_0 = binned_feature[sample_indices[i]] bin_1 = binned_feature[sample_indices[i + 1]] bin_2 = binned_feature[sample_indices[i + 2]] bin_3 = binned_feature[sample_indices[i + 3]] ou...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25227"
  },
  {
    "number":26288,
    "text":"kMeans stopped working with numpy 1.24.2\n\n### Describe the bug following this: File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"<ipython-input-1-e678a054b804>\", line 1, in <module> self.clusters_model.predict(word_vector) File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 1025, in predict labels, _ = _labels_inertia_threadpool_limit( File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\", line 784, in _labels_inertia_threadpool_limit with threadpool_limits(limits=1, user_api=\"blas\"): File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 151, in threadpool_limits return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api) File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\threadpoolctl.py\", line 171, in __init__ self._original_info = self._set_threadpool_limits() File \"C:\\Users\\837726756\\Anaconda3\\envs\\merlin-infra\\lib\\site-packages\\threadpoolctl.py\", line 268, in _set_threadpool_limits modules = _ThreadpoolInfo(prefixes=self._prefixes, File \"C:\\Users\\837726756\\A...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26288"
  },
  {
    "number":27907,
    "text":"Dummy estimators don't have the [CODE] nor [CODE] attributes\n\n### Describe the bug [CODE] and [CODE] estimators don't have the [CODE] nor [CODE] attributes. The reason is that they don't call [CODE] during [CODE] like other estimators do. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No errors. ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.11.2 (tags\/v3.11.2:878ead1, Feb 7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)] executable: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Scripts\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.3.2 pip: 23.3.1 setuptools: 68.2.2 numpy: 1.24.4 scipy: 1.11.3 Cython: 3.0.5 pandas: 2.1.2 matplotlib: 3.8.0 joblib: 1.3.2 threadpoolctl: 3.2.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll version: 0.3.21 threading_layer: pthreads architecture: Zen user_api: openmp internal_api: openmp num_threads: 16 prefix: vcomp filepath: C:\\Users\\Mavs\\Documents\\Python\\ATOM\\venv311\\Lib\\site-packages\\sklearn\\.libs...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27907"
  },
  {
    "number":28966,
    "text":"Is it intention to drop [CODE] in 1.5?\n\n### Describe the bug While running the 1.5.0 release candidate, my collaborator @FBruzzesi on scikit-lego ran our testing suite and noticed something breaking. This is mostly just a ping issue so folks are aware, feel free to close if this feels like a false alarm. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28966"
  },
  {
    "number":27189,
    "text":"F1 score not calculated properly\n\n### Describe the bug According to the definition [Clang 15.0.7 ] executable: \/Users\/Kjell\/mambaforge\/envs...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27189"
  },
  {
    "number":24499,
    "text":"Reference for sklearn.feature_selection.chi2\n\n### Describe the issue linked to the documentation Hi folks, I am somewhat in doubt that the [CODE] function is implemented correctly. At least, checking the source code, it is entirely unclear to me why that kind of scoring would make sense. Best, Felix ### Suggest a potential alternative\/fix Is there any reference that justifies the computation in the implemented way? Then it should be added to the documentation.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24499"
  },
  {
    "number":28520,
    "text":"Create estimators for inference only\n\n### Describe the workflow you want to enable Allow a trained estimator to be converted into a form suitable only for predict\/transform type operations and not fitting. In many cases, the estimator could be made more compact or performant as part of this transformation. For instance, feature selection steps in a pipeline may rely on complex models during training, but at inference they simply drop unused features. When deploying the model, conversion of the feature selection step to a simpler form could save memory and model load time. ### Describe your proposed solution Add a new method to BaseEstimator [CODE] which returns a model which retains all predict\/transform methods but does not necessarily support fitting. By default it would return [CODE]. Estimators and transformers could override this as necessary. Pipeline would convert each step. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28520"
  },
  {
    "number":24878,
    "text":"Building interactive demos for examples\n\n### Describe the issue linked to the documentation This is not an issue but rather a good-to-have feature for documentation and this issue is more of a discussion. It would be nice to have links to or embedding interactive demos made with Gradio inside documentation. I've built two apps that you might see how it looks like: - Anomaly detection: URL] - Classification: [URL] We can have below demo (but more official version \ud83d\ude05) embedded in your documentation. <img width=\"1155\" alt=\"Ekran Resmi 2022-11-10 15 11 41\" src=\"[URL] The current workflow for users is to download python code or run binder. This reduces amount of work in this workflow too! If you think embedded demos is a bit overkill what we could do is to create these demos and host them on [Spaces We will also host the Spaces in better hardware (8 vCPU 32 GiB RAM) to make sure it's always running and all good. \u2728 You can see how it's implemented in Kornia for rst docs [here]([URL] It looks like [this]([URL] inside Kornia docs. <img width=\"1353\" alt=\"Ekran Resmi 2022-11-10 15 15 08\" src=\"[URL] As for Keras, we just put a redirection link inside docs since their document generation is more layered. <img width=\"1309\" ...",
    "labels":[
      "RFC",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24878"
  },
  {
    "number":25519,
    "text":"empirical_covariance silently returns invalid results on inputs with a complex dtype\n\n### Describe the bug Considering complex inputs $X$, like in radar image processing: File \"dev_covariance_emc_bug.py\", line 16, in <module> assert_array_equal(C1, C2) File \"...\\Anaconda3\\envs\\env_ml\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 934, in ass...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25519"
  },
  {
    "number":24552,
    "text":"PolynomialFeatures: allow separate degrees for individual features\n\n### Describe the workflow you want to enable [PolynomialFeatures]([URL] allows a degree parameter, but the same parameter is used for all features. Let's say I have three features [x1, x2, x3] and I want to allow up to cubic terms in x1 and x2, but only quadratic terms in x3. This is not currently possible, but would be a useful feature in many situations. ### Describe your proposed solution Currently, degree can be an [CODE] or [CODE]. How about generalizing this so that [CODE] and [CODE] can be [CODE] or [CODE] of size=number of features?",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24552"
  },
  {
    "number":30139,
    "text":"The input_tags.sparse flag is often incorrect\n\n### Describe the bug If I understood correctly the developer API for tags, [CODE] tells us whether an estimator can accept sparse data or not. For many estimators it seems that [CODE] is False but should be True. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE] as [CODE] accepts sparse input data. ### Actual Results [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30139"
  },
  {
    "number":31128,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Apr 09, 2025) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31128"
  },
  {
    "number":26197,
    "text":"Overflow in ParameterGrid.__len__\n\n### Describe the bug Dear all, the __len__ function from the ParameterGrid gets an overflow when having a large HP parameter set. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Should not throw an error ### Actual Results The problem is that the [CODE] is smaller than [CODE] in the example (see [URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26197"
  },
  {
    "number":27065,
    "text":"Documents fixed to increase speed by removing for loop\n\n### Describe the issue linked to the documentation x = np.array(sorted([n for n in percentiles[cls_name].keys()])) ### Suggest a potential alternative\/fix x = np.array(sorted(list(percentiles[cls_name].keys())))",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27065"
  },
  {
    "number":24462,
    "text":"Implement p-value splitting criterion for Decision Trees\n\n### Describe the workflow you want to enable The current list of valid criterions for Decision Trees are: {\u201csquared_error\u201d, \u201cfriedman_mse\u201d, \u201cabsolute_error\u201d, \u201cpoisson\u201d} With regard to regression problems, I have run into numerous situations where I would very much like to make splits on the data based on a t-test statistics comparing the means between the two split subsamples. The primary issue with using MSE or Friedman MSE is that the quality of a split is dependent on both the marginal impact of splitting on a particularly variable, as well as the frequency in which that split is triggered. It's not uncommon where I want to identify splits that have highly different means, even though that splitting rule would come into effect rarely. See this post (I am not the author) for more information on why splitting on p-values could be valuable: [URL] ### Describe your proposed solution The solution would be to add a new splitting criterion called \"p-value\" or \"t-test\" or something similar. The quality of a split would be measured based on either the p-value or the magnitude of the t-statistic. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24462"
  },
  {
    "number":31304,
    "text":"DOC Link Visualization tools to their respective interpretation in the User Guide\n\n### Describe the issue linked to the documentation As of today, some of our [Display objects]([URL] point towards the [[CODE]]([URL] section of the User Guide, some of them point toward the respective plotted function, some of them do both. As sometimes users want to know how to interpret the plot and sometimes they want to understand the plot API, we've resorted to linking both, e.g. for the [RocCurveDisplay]([URL] we have: [CODE_BLOCK] Contributors willing to address this issue, please fix one of the following listed Display Objects per pull request. - [x] [inspection.PartialDependenceDisplay]([URL] points to [[CODE]]([URL] It should point [[CODE]]([URL] as well. #31313 - [x] [metrics.ConfusionMatrixDisplay]([URL] points only to point [[CODE]]([URL] It should point to [[CODE]]([URL] as well. #31306 - [x] [metrics.DetCurveDisplay]([URL] points to [[CODE]]([URL] It should point [[CODE]]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31304"
  },
  {
    "number":24916,
    "text":"Make error message uniform when calling [CODE] before [CODE]\n\nWhile working #24838, we found out that we are not consistent with the error type and message when calling [CODE] before [CODE]. From @jpangas: > Here is the updated list of the estimators that raise inconsistent errors when [CODE] is called before [CODE]. Currently, these estimators have been whitelisted. Before we submit any PR for a particular estimator, we can remove the estimator from the list and run the test in #25223 to check if the code we submitted has raised the correct [CODE]. > Remember to add your estimator and PR in the same change log entry in [CODE] introduced in #25294. Please link the PRs you'll make with this issue so that I can update this list for all of us to know which estimators have been worked on: - [x] AdditiveChi2Sampler() #25291 - [x] Binarizer() #25294 - [x] MaxAbsScaler() #25294 - [x] MinMaxScaler() #25294 - [x] Normalizer() #25294 - [x] OrdinalEncoder() #25294 - [x] PowerTransformer() #25294 - [x] QuantileTransformer() #25294 - [x] RobustScaler() #25294 - [x] StackingClassifier() #25324 - [x] StackingRegressor() #25324 - [x] StandardScaler() #25294 - [x] TfidfTransformer() #25294 - [x] VotingClassifier() #25324 - [x] VotingRegressor() #25324 - [x] GaussianRandomProjection() #25308 - [x] GenericUnivariateSelect() #25308 - [x] IterativeImputer() #25367 - [x] RFE() #25308 - [x] RFECV() #25308 - [x] SelectFdr() #25308 - [x] SelectFpr() #25308 - [x] SelectFromModel() #25308 - [x] SelectF...",
    "labels":[
      "help wanted",
      "Enhancement",
      "New Feature"
    ],
    "label_count":3,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24916"
  },
  {
    "number":30130,
    "text":"DOC Motivate preferably using conda-forge's distribution of scikit-learn\n\nA lot of people use scikit-learn's python wheels uploaded on PyPI. Wheels were not designed for scientific packages and this leads to a variety of problems for users who use them \u2014 for more information see [the limitations of PyPi]([URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30130"
  },
  {
    "number":27493,
    "text":"Survey: Open-Source Documentation for Newcomers\n\n### Describe the issue linked to the documentation Hello Scikit-learn Community! We are researchers from George Mason University in the United States, looking for open-source contributors to participate in our survey on open-source software (OSS) project documentation use when onboarding. If you are 18 or older and contributed to an OSS project in the last one year, you can help! The objective of our research is to better understand what type of documentation in OSS projects is more helpful for project newcomers. The survey will take approximately 10 minutes, and upon successful completion of the survey, you will be able to participate in a draw for a chance to win one of four Amazon.com gift cards worth $50 via email. The link to survey: [URL] We would be greatly appreciative if you would be willing to participate in this study and help us improve the effectiveness of the onboarding to software projects. If this message would best be directed or reposted somewhere, please let us know, and we will be happy to modify\/repost it. Thank you for considering participating in our research study! Research Team: Nursena Kurubas, Dr. Kevin Moran and Dr. Brittany Johnson IRBNet #: 2029296-1 ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27493"
  },
  {
    "number":28892,
    "text":"Automatically handle missing values in OrdinalEncoder\n\n### Describe the workflow you want to enable Currently, NaN values in OrdinalEncoder are either passed through as NaN, or encoded into user-specified value. It would be nice to have a third option: consider NaN values as another category and map them into [CODE] or some other value. ### Describe your proposed solution Add another [CODE] option [CODE], that encodes them into another category ### Describe alternatives you've considered, if relevant _No response_ ### Additional context There is also some confusion with user-specified value: if for example I set this value as [CODE], will it interfere with [CODE] category that was present during fit? Or all categories will be moved accordingly? Manually setting some other values like [CODE] or [CODE] is usually incompatible with common categorical features interfaces, e.g. nn.Embedding from Pytorch and so on",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28892"
  },
  {
    "number":26599,
    "text":"DOC Sphinx global var [CODE] removed in Sphinx 7.0.0\n\n### Describe the issue linked to the documentation Following the doc contribution guide is installed but upon build the following error occurs: <details> <summary>Error message<\/summary> ``` Traceback (most recent call last): File \"\/home\/<usr>\/miniconda3\/envs\/sklearn-env\/lib\/python3.9\/site-packages\/sphinx\/builders\/html\/__init__.py\", line 1093, in handle_page output = self.templates.render(templatename, ctx) File \"\/home\/<usr>\/miniconda3\/envs\/sklearn-env\/lib\/python3.9\/site-packages\/sphinx\/jinja2glue.py\", line 196, in render return self.environment.get_template(template).render(context) File \"\/home\/<usr>\/miniconda3\/envs\/sklearn-env\/lib\/python3.9\/site-packages\/jinja2\/environment.py\", line 1301, in render self.environment.handle_exception() File \"\/home\/<usr>\/miniconda3\/envs\/sklearn-env\/lib\/python3.9\/site-packages\/jinja2\/environment.py\", line 936, in handle_exception raise rewrite_traceback_stack(source=source) File \"\/home\/<usr>\/miniconda3\/envs\/sklearn-env\/lib\/python3.9\/site-packages\/sphinx\/themes\/basic\/page.html\", line 10, in top-level template code {%- extends \"layout.html\" %} File \"\/home\/<usr>\/Documents\/scikit-learn\/doc\/themes\/scikit-learn-modern\/layout.html\", line 36, in top-level template code <link rel=\"stylesheet\" href=\"{{ pathto('_static\/' + style, 1) }}\" type=\"text\/css\" \/> jinja2.exceptions.UndefinedError: 'style' is undefined The above exception was the direct cause of the following exception: Traceback (most recent ca...",
    "labels":[
      "Bug",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26599"
  },
  {
    "number":29729,
    "text":"Remove outdated brand file identity.pdf\n\n### Describe the issue linked to the documentation This document is outdated : doc\/logos\/identity.pdf ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29729"
  },
  {
    "number":27752,
    "text":"cross_validate error? when processing regression estiamtor.\n\n### Describe the bug The test_score return by cross_validate is not equal to the estimator.score() function. !image, but the results are too different. And I use KFold to split dataset, and get test scores, they are very different to the test_score return by cross_validate. !image [MSC v.1916 64 bit (AMD64)] executable: D:\\ProgramData\\Anaconda3\\envs\\sklearn-env\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.2.2 pip: 23.0.1 setuptools: 67.3.2 numpy: 1.26.0 scipy: 1.11.3 Cyt...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27752"
  },
  {
    "number":25730,
    "text":"FeatureUnion not working when aggregating data and pandas transform output selected\n\n### Describe the bug I would like to use [CODE] transform output and use a custom transformer in a feature union which aggregates data. When I'm using this combination I got an error. When I use default [CODE] output it works fine. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown when using [CODE] transform output. ### Actual Results ```python --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[5], line 25 23 # This does not work. 24 set_config(transform_output=\"pandas\") ---> 25 print(make_union(MyTransformer()).fit_transform(data)) File ~\/.local\/share\/virtualenvs\/3e_VBrf2\/lib\/python3.10\/site-packages\/sklearn\/utils\/_set_output.py:150, in _wrap_method_output.<locals>.wrapped(self, X, args, kwargs) 143 if isinstance(data_to_wrap, tuple): 144 # only wrap the first output for cross decomposition 145 return ( 146 _wrap_data_with_container(method, data_to_wrap[0], X, self), 147 data_to_wrap[1:], 148 ) --> 150 return _wr...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25730"
  },
  {
    "number":25787,
    "text":"Criteria for fixing connectivity in agglomerative clustering\n\n### Describe the workflow you want to enable Hi, Great work on maintaining sklearn! The current fix_connectivity clustering. cc: @GaelVaroquaux ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25787"
  },
  {
    "number":25075,
    "text":"Wrong description for the [CODE] in mean_shift docs\n\n### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by gittar November 28, 2022<\/sup> Should in these doc fragments the variable [CODE] be replaced by [CODE]? It appears nowhere else in the codefile. [URL] [URL] <\/div>",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25075"
  },
  {
    "number":29610,
    "text":"\u26a0\ufe0f CI failed on Wheel builder (last failure: Aug 08, 2024) \u26a0\ufe0f\n\nCI is still failing on Wheel builder",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29610"
  },
  {
    "number":30826,
    "text":"DOC Donating to the project\n\n### Describe the issue linked to the documentation For discussion. Updating this page: [URL] Include option(s) for various donation links (in addition to directly via NF), such as GitHub Sponsors and Benevity, OC. ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30826"
  },
  {
    "number":30525,
    "text":"OPTICS.fit leaks memory when called under VS Code's built-in debugger\n\n### Describe the bug Running clustering algorithm with n_jobs parameter set to more than 1 thread causes memory leak each time algorithm is run. This simple code causes additional memory leak at each loop cycle. The issue will not occur if i replace manifold reduction algorithm with precomputed features. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Program's memory usage nearly constant between loop cycles ### Actual Results Program's memory usage increases infinitely ### Versions ```shell System: python: 3.10.6 (tags\/v3.10.6:9c7b4bd, Aug 1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)] executable: .venv\\Scripts\\python.exe machine: Windows-10-10.0.26100-SP0 Python dependencies: sklearn: 1.6.0 pip: 24.3.1 setuptools: 63.2.0 numpy: 1.25.2 scipy: 1.14.1 Cython: None pandas: 2.2.3 matplotlib: 3.10.0 joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 16 prefix: vcomp filepath: .venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None user_api: blas internal_api: openblas num_threads: 16 prefix: libopenblas filepath: .venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll version: 0.3.23.dev threading_layer: pthreads architecture: Cooperlake user_api: blas internal_api:...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30525"
  },
  {
    "number":31894,
    "text":"TunedThreasholdClassiffierCV not understanding [CODE] as a valid [CODE]\n\nThis code [CODE_BLOCK] gives this: ```py Traceback (most recent call last): File \"\/tmp\/2.py\", line 17, in <module> est.fit(X, y, sample_weight=sample_weight) File \"\/path\/to\/scikit-learn\/sklearn\/base.py\", line 1366, in wrapper return fit_method(estimator, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/path\/to\/scikit-learn\/sklearn\/model_selection\/_classification_threshold.py\", line 129, in fit self._fit(X, y, params) File \"\/path\/to\/scikit-learn\/sklearn\/model_selection\/_classification_threshold.py\", line 742, in _fit routed_params = process_routing(self, \"fit\", *params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/path\/to\/scikit-learn\/sklearn\/utils\/_metadata_requests.py\", line 1636, in process_routing request_routing = get_routing_for_object(_obj) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/path\/to\/scikit-learn\/sklearn\/utils\/_metadata_requests.py\", line 1197, in get_routing_for_object return deepcopy(obj.get_metadata_routing()) ^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/path\/to\/scikit-learn\/sklearn\/model_selection\/_classification_threshold.py\", line 871, in get_metadata_routing scorer=self._get_curve_scorer(), ^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/path\/to\/scikit-learn\/sklearn\/model_selection\/_classification_threshold.py\", line 880, in _get_curve_scorer curve_scorer = _CurveScorer.fr...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31894"
  },
  {
    "number":30748,
    "text":"Unexpected behavior for subclassing [CODE]\n\n### Describe the issue linked to the documentation Hey, I don't know if I should call this a bug, but for me at least it was unexpected behavior. I tried to subclass from [CODE] to implement a customization, so having a simplified configuration, which is used to build a sequence of transformations. It generates an [CODE], due to not having an instance attribute with the same name as a positional argument (same is true for a kwarg) of the subclasses's init. Find a minimal example below. Is this expected behavior? It does not harm to set the instance attributes with the same name, but it is surprising it is demanded and is very implicit. Also, it does not pop up, when you instantiate the object, but only when you try to call a method on it. In case it is absolutely necessary, it may need some documentation. In addition, I tried to globally skip parameter validation and it did not help in this situation, which might be a real bug? Thanks for your help, and your good work:) A simple example: ```python import sklearn sklearn.set_config( skip_parameter_validation=True, # disable validation ) from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder from sklearn.base import BaseEstimator, TransformerMixin import pandas as pd class TakeColumn(BaseEstimator, TransformerMixin): def __init__(self, column: str): self.column = column def __str__(self): return self.__class__.__name__ + f\"[{self.column}]\" def fit(self, ...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30748"
  },
  {
    "number":30868,
    "text":"Calibration cannot handle different dtype for prediction and sample weight.\n\n### Describe the bug This is from the comment: [URL] . I did not find a corresponding issue. Please close if this is duplicated. Aligning the types here [URL] can help resolve the problem, but the casting is done for every grad calculation. Hopefully there's a better solution. Users can workaround the issue by using [CODE] for sample weights. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error. ### Actual Results Here, xgboost outputs [CODE], but [CODE] is [CODE]. These mismatched types lead to the following error: ```python-traceback File \"\/home\/jiamingy\/.anaconda\/envs\/xgboost_dev\/lib\/python3.12\/site-packages\/sklearn\/calibration.py\", line 673, in _fit_calibrator calibrator.fit(this_pred, Y[:, class_idx], sample_weight) File \"\/home\/jiamingy\/.anaconda\/envs\/xgboost_dev\/lib\/python3.12\/site-packages\/sklearn\/calibration.py\", line 908, in fit self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/home\/jiamingy\/.anaconda\/envs\/xgboost_dev\/lib\/python3.12\/site-packages\/sklearn\/calibration.py\", line 855, in _sigmoid_calibration opt_result = minimize( ^^^^^^^^^ File \"\/home\/jiamingy\/.anaconda\/envs\/xgboost_dev\/lib\/python3.12\/site-packages\/scipy\/optimize\/_minimize.py\", line 738, in minimize res = _minimize_lbfgsb(fun, x0...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30868"
  },
  {
    "number":32046,
    "text":"rendering of 'routing' note in the documentation\n\n### Describe the issue linked to the documentation the rendering of this section seems to be over-indented leading to some funky rendering in html: example: [URL] <img width=\"1174\" height=\"683\" alt=\"Image\" src=\"[URL] \/> source code: [URL] ### Suggest a potential alternative\/fix remove one level of indent source code: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32046"
  },
  {
    "number":27356,
    "text":"Building from source fails using conda instructions\n\n### Describe the bug I am trying to build the development version of scikit-learn in Windows Subsystem for Linux running Ubuntu 20.04.5 and conda 23.7.2. After creating the environment, building scikit-learn fails as cython is not found for some reason. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A succesful build. ### Actual Results [CODE] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27356"
  },
  {
    "number":30427,
    "text":"Fix incorrect short_summary for sklearn.kernel_approximation module\n\n### Describe the issue linked to the documentation The short_summary for the sklearn.kernel_approximation module is currently set to \"Isotonic regression,\" which is incorrect. ### Suggest a potential alternative\/fix Update the short_summary for sklearn.kernel_approximation.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30427"
  },
  {
    "number":29302,
    "text":"Remove uneeded SKLEARN_BUILD_PARALLEL environment variable\n\nThis is only actually used if we build with setuptools and most of our CI builds are with Meson, i.e. the only setuptools build has [CODE] Here are the usages of SKLEARN_BUILD_PARALLEL, most should be removed some should be kept (at least the one in [CODE] and maybe the [CODE] although it could be only in the case of [CODE]) [CODE_BLOCK]",
    "labels":[
      "help wanted",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29302"
  },
  {
    "number":24525,
    "text":"Should we continue to support compiler=intelem?\n\nI have an build refactor removing [CODE] and [CODE] and only uses [CODE] that successfully builds our wheels and passes tests. I think it is best to move to a pure [CODE] implementation first, because there are still some lingering issues [CODE]. For example, no editable wheels with [CODE]: [URL] [CODE] does not make it easy to support custom compilers: [URL] It is doable, but it requires more complexity. (I have not got the intelem compiler fully working on our CI yet, but it is working locally in Docker). Furthermore, I think our [Intel ICC job]([URL] is not checking the Intel build correctly. Specifically, the following compiles the code correctly with [CODE]: [URL] But a few lines later, we run: [URL] which overrides the icc compiled extensions with gcc compiled extensions. One can see that gcc is used in the [build logs]([URL] Also in the logs we see that [CODE] itself is going to be deprecated: [CODE_BLOCK] Should we support continue to support and build with the Intel compil...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24525"
  },
  {
    "number":25019,
    "text":"The shape of dual_coef_ of KernelRidge, SVR\n\n### Describe the bug Fear of potential bugs. The shape of dual_coef_ does not match in two models that use kernels (e.g. KernelRidge and SVR) KernelRidge has a shape of (n_samples,) SVR has a shape of (1, n_samples). Is there a compelling reason for this discrepancy? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Preferably in the form (n_samples,) ### Actual Results >print(kr.dual_coef_.shape) (10,) >print(svr.dual_coef_.shape) (1,10) ### Versions ```shell System: python: 3.10.8 | packaged by conda-forge | (main, Nov 4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)] executable: C:\\Users\\11665307\\Anaconda3\\envs\\test\\python.exe machine: Windows-10-10.0.19042-SP0 Python dependencies: sklearn: 1.1.3 pip: 22.2.2 setuptools: 65.5.0 numpy: 1.23.5 scipy: 1.9.3 Cython: None pandas: None matplotlib: None joblib: 1.2.0 threadpoolctl: 3.1.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp prefix: vcomp filepath: C:\\Users\\11665307\\Anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll version: None num_threads: 8 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\11665307\\Anaconda3\\envs\\test\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll version: 0.3.20 threading_layer: pthreads architecture: SkylakeX num_threads: 8 user_api: blas internal_api: openblas prefix: libopenblas filepath: C:\\Users\\11665307\\Anaconda3\\envs...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25019"
  },
  {
    "number":25603,
    "text":"Building from source fails on Linux systems with pre-installed Intel OpenMP\n\n### Describe the issue linked to the documentation On systems that have Intel compilers with OpenMP support (like [CODE] and [CODE]), building scikit-learn from source fails with the following error. [CODE] ### Suggest a potential alternative\/fix Building scikit-learn from source uses [CODE] to compile with OpenMP. However, Intel compilers require [CODE] option to compile OpenMP applications. See Intel OpenMP documentation [here]([URL] I was able to overcome the build failure by explicitly setting the [CODE] environment variable as below. [CODE] The documentation needs to direct users compiling with Intel compilers to set the environment variable before building from source.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25603"
  },
  {
    "number":24614,
    "text":"GLM doesn't have an offset option\n\n### Describe the workflow you want to enable As described at #24155, GLM should also support the offset option. ### Describe your proposed solution For example [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24614"
  },
  {
    "number":31901,
    "text":"QuantileTransformer is incredibly slow\n\n### Describe the workflow you want to enable This is a feature request to improve performance of the QuantileTransformer. It takes ~60 minutes to fit, uses a huge amount of memory when transforming large non-sparse dataframes with 30M+ rows and 500 columns. It also does not support sample_weight. Ideally it should be as fast as catboost's Pool quantize method, which does many of the same computations in a fraction of the time: [URL] ### Describe your proposed solution See source code for [URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31901"
  },
  {
    "number":25247,
    "text":"ValueError: buffer source array is read-only\n\n### Describe the bug Hi, When training RandomForestClassifier using multiple cores (n_jobs=-1) I get the following error (full traceback below): ValueError: buffer source array is read-only This doesn't happen when using just one core or when a small dataset is used for training (by subsampling a large one). It's not straightforward to provide reproducible code as this happens with a fairly large dataset (~100K training records). The code is running on a MacBook Pro (6-Core Intel Core i7, Monterey 12.5.1) under Python 3.9 (see version info below). Note: A similar error is mentioned in bug reports #15851 and #16331 - but it appears this issue has not been fully fixed. Thanks, Ron ### Steps\/Code to Reproduce Here are the relevant lines of code: [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results ```pytb Traceback (most recent call last): File \"\/Users\/ron.katriel\/PycharmProjects\/Classifier\/COST\/DRG\/sklearn_sgdclassifier_autocoder.py\", line 365, in <module> clf_drg_code = trainClassifier(df_train, target_column, num_estimators, model_filename) File \"\/Users\/ron.katriel\/PycharmProjects\/Classifier\/COST\/DRG\/sklearn_sgdclassifier_auto...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25247"
  },
  {
    "number":24558,
    "text":"SelfTrainingClassifier: use of warm_start in base_estimator\n\n### Describe the issue linked to the documentation Hi, I construct an instance of SelfTrainingClassifier with a base_constructor having warm_start set as True. When calling fit in SelfTrainingClassifier, is the classifier trained incrementally, or at every fit information of previous training is overwritten? I tried to figure out in documentation, but I didn't find whether re-use previous training info was allowed. I would like to pre-train a model on unsupervised data and later fine-tune it on labeled data. Thank you for your support, Best ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24558"
  },
  {
    "number":29417,
    "text":"Documentation section 3.4.1.1 has incorrect description that would be correct if the [CODE] metric were to be tweaked and renamed\n\n### Describe the issue linked to the documentation (Very similar to issue #13887 which was reported and fixed 5 years ago, so I have borrowed much of the text.) In the documentation, section 3.4.1.1. \"Common cases: predefined values\", the remark: > All scorer objects follow the convention that higher return values are better than lower return values. is not 100% correct, as the [CODE] metric used for regression is _not_ a \"greater is better\" metric, as far as I can tell. If I may, I would love to implement the PR myself, as it would be my first time contributing to a large, well-known library. ### Suggest a potential alternative\/fix 1. I suggest implementing a function named [CODE] which simply returns the negative of the value of max_error; this is a direct analogy to what is done in the case of \u2018neg_mean_absolute_error\u2019 and others. A better model has a lower value of mean absolute error, therefore a larger value of the mean absolute error implies a better model. The same is true for maximum error, where it is also the case that a better model is assigned a lower loss. 2. Remove references to [CODE] from section 3.4.1.1 and replace them with [CODE].",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29417"
  },
  {
    "number":25571,
    "text":"Bug in Calibration Curve Documentation\n\n### Describe the bug [URL] In the calibration curve page, a \"scores_df\" is generated to showcase supporting model evaluation metrics in addition to the calibration curves. I noticed that my ROC AUC score was unusually low and noticed that it was consuming Y_PRED instead of Y_PROB. This is incorrect and may confuse users in the future. ### Steps\/Code to Reproduce Not applicable. ### Expected Results Expected AUC in the 65-75 range for my application. ### Actual Results Observed AUC in the 50-55 range instead. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25571"
  },
  {
    "number":31302,
    "text":"Reference [CODE] from [CODE]'s docstring in a \"See also section\"\n\n### Describe the issue linked to the documentation The docstring of [CODE] should point to the [CODE] factory method as a complementary tool that both computes the curves points and display them using matplotlib. If you want to open a PR for this, please review some \"See also\" sections in other sections using [CODE] or the search feature of your IDE or github code search: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31302"
  },
  {
    "number":29640,
    "text":"BinMapper within HGBT does not handle sample weights\n\n### Describe the bug BinMapper under _hist_gradient_boosting does not accept sample weights as input leading to mismatch of bin thresholds outputted when calculating weighted versus repeated samples. Linked to Issue #27117 ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error thrown ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ] executable: \/Users\/shrutinath\/micromamba\/envs\/scikit-learn\/bin\/python machine: macOS-14.3-arm64-arm-64bit Python dependencies: sklearn: 1.6.dev0 pip: 24.0 setuptools: 70.1.1 numpy: 2.0.0 scipy: 1.14.0 ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29640"
  },
  {
    "number":30389,
    "text":"Make [CODE] and [CODE] public\n\nSince we are moving, [CODE] and [CODE] into a new module, I'm wondering if we should make them public as well. I can imagine some people that don't want to use [CODE] but still want to set [CODE] or [CODE].",
    "labels":[
      "help wanted",
      "Documentation"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30389"
  },
  {
    "number":24564,
    "text":"convert builtin module import to supported form\n\n### Describe the bug convert builtin module import to supported form ### Steps\/Code to Reproduce import builtins as builtins ### Expected Results No error is thrown ### Actual Results import builtins as builtins ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24564"
  },
  {
    "number":31019,
    "text":"Allow column names to pass through when fitting [CODE] dataframes\n\n### Describe the workflow you want to enable Currently when fitting with a [CODE] DataFrame, the feature names do not pass through because it does not implement a [CODE] method. Example: [CODE_BLOCK] Expected output [CODE_BLOCK] Actual output [CODE_BLOCK] All other attributes on [CODE] are what I'd expect. ### Describe your proposed solution This should be easy enough to implement by adding another check within [CODE]: 1. Add [CODE] method, borrowing logic from [[CODE]]([URL] [CODE_BLOCK] 2. Add an additional check to [[CODE]]([URL] [CODE_BLOCK] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context [URL]",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31019"
  },
  {
    "number":27268,
    "text":"macOS.pylatest_conda_forge_mkl sometimes fails pickling test\n\nThis test has failed in [URL] with [those logs]([URL] It also failed on the [nightly CI]([URL] but the logs were too old and are no preserved. [CODE_BLOCK]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27268"
  },
  {
    "number":31923,
    "text":"404 when fetching datasets with sklearn.datasets.fetch_openml\n\n### Describe the bug My Azure DevOps pipeline started failing to fetch data from OpenML with 404 as of 9 August. My original line in a Jupyter notebook uses CODE]; but I've not been able to download any other dataset either (e.g., iris, miceprotein). The SPECT dataset at OpenML [here : File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"<ipython-input-3-de4cc69a81bb>\", line 1, in <module> fetch_openml(name='SPECT') File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper return func(args, *kwargs) File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\datasets\\_openml.py\", line 1127, in fetch_openml bunch = _download_data_to_bunch( File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\datasets\\_openml.py\", line 681, in _download_data_to_bunch X, y, frame, categories = _retry_with_clean_cache( File \"C:\\...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31923"
  },
  {
    "number":27893,
    "text":"sklearn.cluster.HDBSCAN shape error when making medoids with precomputed metric\n\n### Describe the bug When fitting with HDBSCAN with metric=\"precomputed\" and store_centers='medoid', it would raise the ValueError [CODE] Claiming the shape of input distance matrix not square, but the input is actually square. It would only occur when points are clustered, i.e., if all points are noises, this would not occur. It seems the bug is in function \\_weighted_cluster_center, where the input matrix for pairwise_distances is the 'variable' defined as [CODE] with X as input matrix and mask as labels_, though I am not sure how to fix it. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No Error is thrown. Fit finished. ### Actual Results ```pytb ValueError Traceback (most recent call last) Cell In[65], line 1 ----> 1 clusterer.fit(d) File ~\/miniconda3\/envs\/cz_cadd\/lib\/python3.8\/site-packages\/sklearn\/cluster\/_hdbscan\/hdbscan.py:852, in HDBSCAN.fit(self, X, y) 849 self.probabilities_ = new_probabilities 851 if self.store_centers: --> 852 self._weighted_cluster_center(X) 853 return self File ~\/miniconda3\/envs\/cz_cadd\/lib\/python3.8\/site-packages\/sklearn\/cluster\/_hdbscan\/hdbscan.py:912, in HDBSCAN._weighted_cluster_center(self, X) 909 self.centroids_[idx] = np.average(data, weights=strength, axis=0) 910 if make_medoids: 911 # TODO: Implement weighted argmin PWD backend --> 912 dist_mat = pairwise_distances( 913 data, metric=self.metric, self._metric_params 914 ) 915 dist_mat = dist_...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27893"
  },
  {
    "number":31810,
    "text":"CI: Enable GitHub Actions App for ppc64le (Power architecture) support\n\nHi scikit-learn team, We\u2019re reaching out to propose enabling CI support for the ppc64le (IBM Power) architecture in your repository, as part of a broader effort to ensure cross-platform compatibility in the scientific Python ecosystem. We\u2019re using a GitHub Actions (GHA)-based runner service provided and maintained by IBM to run jobs for the ppc64le architecture. This setup has already been successfully integrated into projects like: \u2705 [cryptography]([URL] \ud83d\udccc [Tracking issue in NumPy]([URL] We\u2019d now like to propose enabling the GitHub Actions app in this repository to allow running CI jobs for ppc64le directly via GitHub Actions. This would support upstream compatibility and help ensure continued support for the Power architecture in scikit-learn. Key Benefits: \ud83d\udd12 Ephemeral and secure runners, isolated per job \ud83d\udee0\ufe0f Maintained by IBM, requires no setup effort from your side \ud83d\udd01 Integrates with existing GitHub Actions workflows \ud83d\udcda Technical documentation and usage details: [URL] We\u2019re happy to assist with the setup or provide any additional details the team may need. Thanks so much!",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31810"
  },
  {
    "number":32067,
    "text":"Enhance the warning message for metadata default value change\n\n### Describe the workflow you want to enable Currently the warning raised for [Deprecation \/ Default Value Change]([URL] is quite generic [CODE_BLOCK] Would it be possible to specify the class in question ? Something like [CODE_BLOCK] ### Describe your proposed solution I think we can get the class through the owner attribute of the MethodMetadataRequest which raises the warning. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32067"
  },
  {
    "number":26398,
    "text":"Custom Tie Breaking Criterion Voting Ensemble\n\n### Describe the bug We were wondering whether there is anything regarding customise the tie breaking criterion the Voting ensemble does ? ### Steps\/Code to Reproduce N\/A ### Expected Results N\/A ### Actual Results N\/A ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26398"
  },
  {
    "number":26793,
    "text":"Handle np.nan \/ missing values in SplineTransformer\n\nI think it would be quite natural to add an option to [CODE] to accept inputs with missing values as follows: - [CODE] the default (keep current behavior) - [CODE]\/\"constant\": encode missing values by setting all output features for that input column to 0 (or some other constant, see discussion below), - [CODE]: append an extra binary feature as missingness indicator and encode missing values as 0 on the remaining output features. Note that [CODE] would be different and statistically more meaningful than [CODE] with [CODE] and furthermore would make for leaner ML pipelines (better UX). I am not sure if we need to add the [CODE] option. It would break the property the sum of output values of a given [CODE] encoding always sum to 1 while [CODE] would preserve this property (in addition to make missingness more explicit to the downstream model in case missingness is informative one way or another). If we want to preserve the sum to 1 property while not adding an explicit missingness indicator feature, maybe we could instead provide [CODE] (not sure about the name) that would encode missing values as [CODE] to preserve the \"sum to 1\" property. Not entirely sure if this would result in a more interesting prior than the zero encoding.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26793"
  },
  {
    "number":31235,
    "text":"MLP Classifier \"Logistic\" activation function providing ~constant prediction probabilities for all inputs when predicting quadratic function\n\n### Describe the bug Repeatedly the sigmoid activation function produces very similar (multiple dp) outputs for the prediction probabilities, seemingly similar around the average of the predicted value, similar to a linear function. It works when predicting a linear function, but higher order tends to cause issues. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The prediction does not resemble target data ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.13.2 (tags\/v3.13.2:4f8bb39, Feb 4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)] executable: c:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\python.exe machine: Windows-10-10.0.19045-SP0 Python dependencies: sklearn: 1.6.1 pip: 25.0.1 setuptools: None numpy: 2.2.3 scipy: 1.15.2 Cython: None pandas: 2.2.3 matplotlib: 3.10.1 joblib: 1.4.2 threadpoolctl: 3.6.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libscipy_openblas filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-43e11ff0749b8cbe0a615c9cf6737e0e.dll version: 0.3.28 threading_layer: pthreads architecture: Haswell user_api: openmp internal_api: openmp num_threads: 8 prefix: vcomp filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\.libs\\vcom...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31235"
  },
  {
    "number":31728,
    "text":"Making the extension contract stable through version upgrades\n\n### Describe the workflow you want to enable Currently, every time [CODE] releases a new minor version - e.g., 1.5.0, 1.6.0, 1.7.0 - compliant extensions, e.g., custom transformers, classifiers, etc, break - specifically, referring to the API conformance as tested through [CODE] or [CODE]. These repeated breakages in the \"extender contract\" contrast the stability of the usage contract, which is stable and professionally managed. For a package like [CODE] which means to be a standard not just for ML algorithms but also an API standard that everyone uses, this is not a good state to be in - \"do not break user code\" is the maxim that gets broken for power users writing extensions. Of course maintaining downwards compatibility is not always possible, but nothing should break without a proper warning. ### Describe your proposed solution The main reason imo why this keeps happening is that [CODE] is not using a proper pattern that ensures stability of the extension contract - and also no secondary deprecation patterns in relation to it. A simple pattern that could improve a lot would be the \"template pattern\", in a specific form to separate likely changing parts such as the boilerplate (e.g., [CODE] vs [CODE] and such) from the extension locus. Reference: [URL] Examples of how this can be used to improve stability:  [CODE], for a different API, has a separation between [CODE] calling an internal [CODE], where change-pro...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31728"
  },
  {
    "number":30353,
    "text":"Hang when fitting [CODE] to a specific dataset\n\n### Describe the bug I am trying to fit an [[CODE]]([URL] to a specific dataset. The training process gets stuck, never finishing. scikit-learn uses a fork of LIBSVM [version 3.10.0]([URL] from [2011]([URL] The equivalent code using a newer version of LIBSVM succeeds, suggesting that there is an upstream bug fix that scikit-learn could merge in. ### Steps\/Code to Reproduce [libsvm_problematic_dataset.csv]([URL] [CODE_BLOCK] ### Expected Results ``` INFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit. ................................................................................................. WARNING: using -h 0 may be faster .............................. WARNING: using -h 0 may be faster ............. WARNING: using -h 0 may be faster .................................................................. WARNING: using -h 0 may be faster .........",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30353"
  },
  {
    "number":26927,
    "text":"Add links to examples from the docstrings and user guides\n\nWe have a rich set of examples covering a very broad range of issues, but they're not necessarily easily discoverable via the section on the bottom of the API pages. This meta-issue is to keep track of the examples for which we've already included links in the docs, and to fix the rest. An example of how to include links to examples in API docs and the user guide can be found here: [URL] . Please see how it can be done before claiming an example. For each example, please check if it's already included in the docs, either user guides or API docs, and if not, claim it by leaving a comment at the bottom of this issue and open a pull request for it. You can open one PR for more than one example if they're very related. Note that not all examples need to be included in this project. For instance, if an example is only a usage example, and putting a link to it doesn't add much for the users, then we can ignore it (please do leave a comment if this is your finding). Also, not all examples are in a good shape, and in your PR you can also improve the example itself. How to create a link to an example? To create a link to the example [CODE], you can write: [CODE_BLOCK] which would automatically create the link. Notice the pattern where slashes ([CODE]) are converted to an underscore ([CODE]) and the [CODE] prefix. __IMPORTANT__: Please limit your pull requests to a limited scope, single or two classes, or a single example so th...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26927"
  },
  {
    "number":25074,
    "text":"Output is missing in this particular codeblock in \"sklearn.compose.ColumnTransformer\"\n\n### Describe the issue linked to the documentation At the last line of the code that is X_trans = ct.fit_transform(X) the output(X_trans variable) is not specified which is confusing. This particular code block is present in [CODE] <img width=\"926\" alt=\"Screenshot 2022-11-30 at 10 37 12 AM\" src=\"[URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25074"
  },
  {
    "number":27820,
    "text":"Issue with MeanShift?\n\n### Describe the bug Since I updated to python 3.11, Meanshift through me an error.... ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results No error ### Actual Results ```pytb --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[9], [line 5 2 import numpy as np 3 X = np.array([1, 1], [2, 1], [1, 0], [4 4, 7], [3, 5], [3, 6]]) ----> [5 clustering = MeanShift(bandwidth=2).fit(X) 6 clustering.labels_ 7 clustering.predict([0, 0], [5, 5]]) File [~\/miniconda3\/lib\/python3.11\/site-packages\/sklearn\/base.py:1152 1145 estimator._validate_params() 1147 with config_context( 1148 skip_parameter_validation=( 1149 prefer_skip_nested_validation or global_skip_validation 1150 ) 1151 ): -> 1152 return fit_method(estimator, args, kwargs) File [~\/miniconda3\/lib\/python3.11\/site-packages\/sklearn\/cluster\/_mean_shift.py:516]([URL]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27820"
  },
  {
    "number":31408,
    "text":"estimator_checks_generator does not return (estimator, check) when hitting an expected failed check\n\n### Describe the bug Currently running sklearn_check_generator with mark=\"skip\" on our estimators. [URL] I would like to start running those checks with \"xfail\". But when I do so any test 'marked' via the [CODE] dictionary gives a [CODE] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results estimator_checks_generator to yield (estimator, check) tuples whether xfail or skip was requested ### Actual Results ``` functools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator') functools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator') functools.partial(<function check_estimator_tags_renamed at 0x7ec1883e62a0>, 'BaseEstimator') functools.partial(<function check_valid_tag_types at 0x7ec1883e6200>, 'BaseEstimator') functools.partial(<function check_estimator_repr at 0x7ec1883e51c0>, 'BaseEstimator') functools.partial(<function check_no_attributes_set_in_init at 0x7ec1883e4b80>, 'BaseEstimator') functools.partial(<function check_fit_score_takes_y at 0x7ec1883da0c0>, 'BaseEstimator') functools.partial(<function check_estimators_overwrite_params at 0x7ec1883e4a40>, 'BaseEstimator') functools.partial(<function chec...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31408"
  },
  {
    "number":28610,
    "text":"DOC: update FAQs to add permission using images\n\n### Describe the issue linked to the documentation We receive many inquiries on the mailing list if developers can have permission to use the images in scikit-learn for their work. Add an FAQ to answer this question: - code is under a BSD 3-clause licence, so the permission is granted - please cite us. link to the citation page. FAQs page: [URL] Cite us: [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28610"
  },
  {
    "number":24828,
    "text":"RFC CLI Tool Proposal\n\n## Introduction Other OSS projects such as [CODE] and [CODE] have made use of development CLI tools such as [CODE] and [CODE] to provide a collected and singular entry point into development tasks for contributors. I find these tools very helpful personally, and think they add a large amount of value, especially for onboarding new contributors. Recently Pandas has also begun discussion and implementation of their own development CLI tool. @noatamir has made a great writeup on some of the benefits and justifications for such a tool (see: [URL] I highly recommend taking the time to read their writeup. Some of the benefits of such a tool include: 1. Increased discoverability of tools, decreasing time spent jumping around documentation. This is especially helpful for those who may not read every single word of documentation (such is my bad habit \ud83d\ude05). 2. Consistency for CI and development. We can establish a 1-1 between CI checks and [CODE] commands, e.g. [CODE] which could then impose both [CODE] and [CODE] (cf. [URL] 3. Ease of installation. Users could install whatever dependencies are necessary depending on what workflow they want enabled. A call to [CODE] could check for the presence of the necessary dependencies and install if needed (or optionally prompt). 4. Reduced barrier to entry. Quite frankly, having to run various commands for tasks, anywhere from linting to building C sources, can be overwhelming, especially to new developers. Recently [CODE] e...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24828"
  },
  {
    "number":30396,
    "text":"ENH Allow disabling refitting of cross-validation estimators\n\n### Discussed in [URL] <div type='discussions-op-text'> <sup>Originally posted by AhmedThahir November 7, 2024<\/sup> Feature request: Allow disable refitting of cross-validation estimators (such as LassoCV, RidgeCV) on the full training set after finding the best hyperparameters? Sometimes I only want the optimal hyperparameter and do not want to waste resources on refitting. This is especially important for large datasets. <\/div> As @alifa98 has highlighted, this is the relevant code block. [URL] User should be allowed to toggle this behavior.",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30396"
  },
  {
    "number":28669,
    "text":"Polars not mentioned as requirement to build documentation\n\n### Describe the issue linked to the documentation The developer documentation [here]([URL] lists the dependencies required to build the documentation. However, [CODE] is not mentioned as a required dependency leading to the following error: [CODE_BLOCK] It seems that since version 1.4.0, polars is now also supported. ### Suggest a potential alternative\/fix The easiest fix would be to add [CODE] in the documentation [here]([URL] as a needed dependency. Alternatively, a [CODE] could be created with all the needed dependencies for development - although sure I'm this must have already been discussed in the past and there should be reasons against it.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28669"
  },
  {
    "number":28982,
    "text":"Add zero_division for single class prediction in MCC\n\n### Describe the bug I have found a potential edge case issue with _sklearn.metrics.matthews_corrcoef_. The example provided in the documentation works as expected: [CODE_BLOCK] However, edge cases appear when either _y_ or _y_true_ have only one single label. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Outputs should be 1, not 0. ### Actual Results Outputs are 0. ### Versions ```shell System: python: 3.11.9 (main, Apr 2 2024, 08:25:04) [Clang 16.0.6 ] executable: \/tmp\/sklearn-test\/.venv\/bin\/python machine: macOS-14.2.1-arm64-arm-64bit Python dependencies: sklearn: 1.4.2 pip: 24.0 setuptools: 69.5.1 numpy: 1.26.4 scipy: 1.13.0 Cython: None pandas: None matplotlib: None joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: openmp internal_api: openmp num_threads: 8 prefix: libomp filepath: \/tmp\/sklearn-test\/.venv\/lib\/python3.11\/site-packages\/torch\/lib\/libomp.dylib version: None user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/tmp\/sklearn-test\/.venv\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib version: 0.3.23.dev threading_layer: pthreads architecture: armv8 user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/tmp\/sklearn-test\/.venv\/lib\/python3.11\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib version: 0.3.26.dev threading_layer: pthreads architecture: neoversen1 user_api: openmp intern...",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28982"
  },
  {
    "number":27063,
    "text":"bench_20newsgroups.py\n\n### Describe the bug [CODE_BLOCK] An exception has occurred, use %tb to see the full traceback. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I dont Know ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27063"
  },
  {
    "number":24791,
    "text":"Example change produces warnings in documentation\n\n### Describe the issue linked to the documentation The tutorial about [Unsupervised learning]([URL] in the development version is missing images from the K-means clustering example. This is not the case for the stable version. The changes in [URL] join all the plots in one figure. ### Suggest a potential alternative\/fix We already had a similar issue in the past, [URL] and it was decided to regenerate the images. My opinion is that getting rid of the links to the missing images should be enough in this case.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24791"
  },
  {
    "number":31473,
    "text":"Add option to return final cross-validation score in SequentialFeatureSelector\n\n### Describe the workflow you want to enable Currently, when using [CODE], it internally performs cross-validation to decide which features to select, based on the scoring function. However, the final cross-validation score (e.g., recall) is not returned by the SFS object. ### Describe your proposed solution Add an attribute (e.g., [CODE]) that stores the mean cross-validation score of the final model with the selected features. This would avoid having to run another cross-validation externally to get the final performance score. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context This feature would be especially useful when the scoring metric is expensive to compute, as it would avoid redundant cross-validation runs.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31473"
  },
  {
    "number":32032,
    "text":"Setting weights on items when passing list of dicts to RandomizedSearchCV\n\n### Describe the workflow you want to enable We can pass a list of dictionaries to CODE], for example [CODE_BLOCK] If I understand correctly [here option but if I am equally interested in the Flat() one I might want to set weights [1.0, 0.5, 0.5] on the list of param dicts. Maybe this is not a great example but what I mean is the amount of trials spent on one option can be driven by the structure of the estimators and how they are combined and sometimes it would be helpful to be able to correct or control it. ### Describe your proposed solution Not sure what could be a nice API, maybe there would be a [CODE] parameter which can only b...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32032"
  },
  {
    "number":24952,
    "text":"[Question] - Is there any way to view feature names of Random Forest Model built in scikit learn ==0.21.3 version\n\n### Describe the issue linked to the documentation I have loaded a vendor model and want to review the feature names of the RF built in sklearn == 0.21.3 version, is there anyway to take a look? ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24952"
  },
  {
    "number":31403,
    "text":"[PCA] ValueError: too many values to unpack (expected 3)\n\n### Describe the bug I am getting the following error when running PCA with version 1.6.1: <img width=\"956\" alt=\"Image\" src=\"[URL] \/> ### Steps\/Code to Reproduce You can reproduce it with this snippet: [CODE_BLOCK] ### Expected Results This works with version [CODE]. [CODE_BLOCK] I tried using [CODE], but that does not help in my desperate attempts to solve the issue. Why did this stop working after [CODE]? For now, I just rolled back to [CODE]. Thanks ### Actual Results <img width=\"956\" alt=\"Image\" src=\"[URL] \/> ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31403"
  },
  {
    "number":24606,
    "text":"MiniBatchDictionaryLearning confusion over U,V\n\n### Describe the issue linked to the documentation [URL] The optimization problem in the documentation is described as $$ \\arg\\min \\frac12 \\Vert X-UV\\Vert_F^2+\\alpha \\Vert U\\Vert_1 $$ such that $\\Vert V_k\\Vert_2\\leq 1$ for all $k$. This is very confusing, because generally $U$ is the dictionary matrix, and $V$ is the representation matrix (e.g. see the paper linked in the documentation [URL] So it should read $$ \\arg\\min \\frac12 \\Vert X-UV\\Vert_F^2+\\alpha \\Vert V\\Vert_1 $$ such that $\\Vert U_k\\Vert_2\\leq 1$ for all $k$; here $U_k$ denotes the columns of $U$. In other words, the $L_1$ regularization should be performed on the representation matrix $V$, and the $L_2$ norm constraints on the columns (basis vectors) of the matrix $U$. ### Suggest a potential alternative\/fix It should read $$ \\arg\\min \\frac12 \\Vert X-UV\\Vert_F^2+\\alpha \\Vert V\\Vert_1 $$ such that $\\Vert U_k\\Vert_2\\leq 1$ for all $k$; here $U_k$ denotes the columns of $U$.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24606"
  },
  {
    "number":26784,
    "text":"RFC deprecate the SAMME.R algorithm in AdaBoostClassifier\n\nWhile working on the following example ([URL] I found a couple of issues regarding the SAMME.R algorithm, which is the default algorithm in [CODE]: - The algorithm was implemented based on the [following paper]([URL] However, this paper is a preprint. In the [final version of the paper]([URL] the SAMME.R algorithm is not presented. So we implemented an unpublished algorithm. - SAMME.R can show some diverging behaviour as shown in [URL] For these two reasons, I think that we should deprecate this algorithm and remove the parameter [CODE] from the [CODE]. In addition, I think that we should monitor the latest work on multiclass AdaBoost, where additional theoretical founding is revealed, cf. [URL]",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26784"
  },
  {
    "number":24888,
    "text":"'OrdinalEncoder' object has no attribute '_missing_indices'\n\n### Describe the bug I have fitted an OrdinalEncoder and saved the [CODE] attribute as a numpy array. I want to load these categories in, in a new module so I do not have to re-fit the model. When trying to transform the prefitted model, I get this Attribute Error. I see that [CODE] is not initialized in [CODE], but instead is created in the [CODE] function, which I intend to never call. ### Steps\/Code to Reproduce [CODE_BLOCK] The error with trace is below [CODE_BLOCK] ### Expected Results No error is thrown. Data is fitted with [CODE_BLOCK] ### Actual Results ``` AttributeError Traceback (most recent call last) <ipython-input-9-78339e8cfce1> in <module> ----> 1 new_encoder.transform(df) \/opt\/conda\/lib\/python3.7\/site-packages\/sklearn\/preprocessing\/_encoders.py in transform(self, X) 933 X_trans = X_int.astype(self.dtype, copy=False) 934 --> 935 for cat_idx, missing_idx in self._mi...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24888"
  },
  {
    "number":31725,
    "text":"Confusion around coef_ and intercept_ for Polynomial Ridge Regression inside a Pipeline\n\n### Describe the issue linked to the documentation When using a Pipeline with PolynomialFeatures and Ridge, it's unclear in the documentation how to extract the actual model coefficients and intercept to reproduce the regression equation manually (outside scikit-learn). For example, when fitting a polynomial regression with: make_pipeline(PolynomialFeatures(degree=3), Ridge()) Most users wrongly assume that coef_[0] is the intercept, which it is not. This behavior is not explained clearly in the Ridge or Pipeline documentation and led to confusion even after reading the docs and searching online. This is a common use case \u2014 for example, when exporting trained models to plain Python, Java, or C++. ### Suggest a potential alternative\/fix ### \u2705 Suggested Fix The coefficients returned by [CODE] include the weight for the constant basis function (created by [CODE]), but the actual y-intercept is stored separately in [CODE]. This makes it unclear how to reconstruct an equation like: y = a\u00b7x\u00b3 + b\u00b7x\u00b2 + c\u00b7x + d ### Suggested Fix: 1. In the [CODE], [CODE], and\/or [CODE] documentation, add a clear explanation that: - [CODE] creates features [CODE] - The intercept is not included in [CODE], but is returned separately as [CODE] - The first element of [CODE] corresponds to the coefficient of the constant term [CODE], not the model intercept 2. Provided a code snippet that reconstructs the polynomial us...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31725"
  },
  {
    "number":28147,
    "text":"AttributeError: 'BinomialDeviance' object has no attribute 'get_init_raw_predictions'\n\n### Describe the bug I save GradientBoostingClassifier model by pickle at sklearn==0.20 ,loaded model at sklearn=0.22 and use it happend after error. [CODE_BLOCK] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results e ### Actual Results ```python >>> models[\"model\"].predict_proba(data)[:,1] Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"C:\\Users\\ZJLAB\\anaconda3\\envs\\pch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 2214, in predict_proba raw_predictions = self.decision_function(X) File \"C:\\Users\\ZJLAB\\anaconda3\\envs\\pch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 2121, in decision_function raw_predictions = self._raw_predict(X) File \"C:\\Users\\ZJLAB\\anaconda3\\envs\\pch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1655, in _raw_predict raw_predictions = self._raw_predict_init(X) File \"C:\\Users\\ZJLAB\\anaconda3\\envs\\pch\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1649, in _raw_predict_init raw_predictions = self.loss_.get_init_raw_predictions( AttributeError: 'Binomial...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28147"
  },
  {
    "number":25935,
    "text":"BaggingClassifier throws ValueError: WRITEBACKIFCOPY base is read-only\n\n### Describe the bug When I use the bagging-classifier in conjunction with LinearSVC it throws [CODE] when [CODE]. Changing [CODE] to 1 removes the error ### Steps\/Code to Reproduce The issue is that I cannot reproduce this error with e.g iris-data but I can't share the dataset since it's company-classified. My code is as following: [CODE_BLOCK] ### Expected Results No error is thrown and it works ### Actual Results The error [CODE] is thrown with the following stack-trace ``` joblib.externals.loky.process_executor._RemoteTraceback: \"\"\" Traceback (most recent call last): File \"C:\\Users\\my_user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\transtotag-6uueIOAW-py3.10\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker r = call_item() File \"C:\\Users\\my_user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\transtotag-6uueIOAW-py3.10\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__ return self.fn(self.args, self.kwargs) File \"C:\\Users\\my_user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\transtotag-6uueIOAW-py3.10\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__ return self.func(args, kwargs) File \"C:\\Users\\my_user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\transtotag-6uueIOAW-py3.10\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__ return...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25935"
  },
  {
    "number":30767,
    "text":"DOC Add [CODE] example to [CODE]\n\nNoticed that the [CODE] page ([URL] could be improved while working on #30399  We should clarify that both [CODE] and [CODE] return the display object  Describe the purpose of the display object more generally (i.e., stores data for the plot)  Add an example section using [CODE] (currently we just describe in the text that we can get the same plot via [CODE]  Explicitly detail that we can add to existing plot via [CODE] by passing the [CODE] parameter I may have missed some points cc @DeaMariaLeon @glemaitre",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30767"
  },
  {
    "number":25210,
    "text":"ENH partial_dependece plot for HistGradientBoosting estimator fitted with [CODE]\n\n### Describe the workflow you want to enable As partial dependence of a model at a point is defined as an expectation. #25209 tries to solve this for [CODE] when you have new [CODE]. For older tree-based models trained with sample_weights, [[CODE]]([URL] keeps track of the training [CODE] and calculates the [CODE] with that into consideration. But, as discussed during the implementation of [CODE] on the HistGradientBoosting estimators ([URL] these models stores an attribute [CODE] and when [CODE] with recursion is asked, it throws an error: [URL] ### Describe your proposed solution As discussed in [URL] the big difference between other tree-based algorithms and HistGradientBoosting is that HistGradientBoosting does not save the [CODE] when building the tree. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25210"
  },
  {
    "number":30970,
    "text":"Allow for multiclass cost matrix in FixedThresholdClassifier and TunedThresholdClassifierCV\n\n### Describe the workflow you want to enable With #26120, we got [CODE] and [CODE] but only for binary classification. The next logical step would be to extend it to the multiclass setup. ### Describe your proposed solution For [CODE], one could allow for a cost matrix instead of a single threshold. [CODE] seems straight forward (or I'm missing something). ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30970"
  },
  {
    "number":30457,
    "text":"Add checking if tree criterion\/splitter are classes\n\n### Describe the workflow you want to enable In the process of creating custom splitters, criterions & models that inherit from the respective _scikit-learn_ classes, a very convenient (albeit currently impossible) solution is to add the splitter & criterion classes as parameters to the model constructor. The currently supported parameter types are strings (referencing splitters & criterions that are already in _scikit-learn_) or objects. Because the splitters & criterions depend on parameters from the fitting function, there is a need for class support in the process of parameter parsing. ### Describe your proposed solution Checking if the splitter\/criterion is a class and constructing it accordingly. A code solution is available [here]([URL] ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30457"
  },
  {
    "number":27463,
    "text":"CI Issues regarding conda lock files\n\nOpening a new issue regarding some of the discussions around [URL] @lesteve these are maybe what I have in mind: - they're generated files and usually it's a good idea not to have generated files in the repo - the files are generated, and their generation depends on the exact time when the conda repo data was downloaded in the user's machine. So there's basically no way to review those changes. Which means we have a ton of changed lines which we cannot review. - since they're generated and we don't review them, we can't really make sure somebody hasn't manually changed anything in the changed file and that the files are indeed generated and not touched afterwards. - a significant amount of changed lines has been coming from these files - we are not even consistent on the use of the same snapshot for different CI. We might be using a generated file from last week for circleCI, and one from a month ago for some of the Azure stuff, and one from two weeks ago for other Azure configs. - we don't really have good docs on how contributors could use the lock files or if they should. Those are only used in the CI, and for that they don't need to be in the same repo. A few things we could do: - only allow changes to those files via PRs done by a bot that we write, and generate them periodically, like weekly maybe, and daily for some which change often (like scipy-dev) - move them out of the main repo. - we could also move to a docker based system w...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27463"
  },
  {
    "number":32022,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Aug 28, 2025) \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev Unable to find junit file. Please see link for details.",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32022"
  },
  {
    "number":25767,
    "text":"Change default behavior of [CODE]'s [CODE] variable.\n\n### Describe the workflow you want to enable I think it is misleading to have the [CODE] method to default to reproduce the same sample every time it is called. The default value for [CODE] should be [CODE], and the documentation [CODE] should state that [CODE] ensures random behavior. ### Describe your proposed solution Please see above box ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25767"
  },
  {
    "number":27339,
    "text":"RFC should the scikit-learn metrics return a Python scalar or a NumPy scalar?\n\nWhile working on the representation imposed by NEP51, I found out that we recently made the [CODE] to return a Python scalar while, up-to-now, other metric are returning NumPy scalar. This change was made due to the array API work: [URL] I assume that we are getting to an intersection where we should make the output of our metrics consistent but also foresee potential requirements: as the comment indicate, calling [CODE] will be a sync point but it might not be the best strategy for lazy computation. This RFC is a placeholder to discuss what strategy we should be implementing.",
    "labels":[
      "RFC",
      "help wanted"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27339"
  },
  {
    "number":26241,
    "text":"AdaBoost: deprecation of \"base_estimator\" does not handle \"base_estimator=None\" setting properly\n\n### Describe the bug Scikit-learn 1.2 deprecated [CODE] 's [CODE] in favour of [CODE] (see #23819). Because there are also validators in place, old code that explicitly defined [CODE] stopped working. A solution that fixes the deprecation is to add a possible [CODE] to a list allowed values in [CODE]; I will do that in a PR. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown. ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26241"
  },
  {
    "number":29381,
    "text":"SimpleImputer's fill_value validation seems too strict\n\n### Describe the bug The CODE] checks whether the _type_ of the [CODE] can be cast with numpy to the dtype of the input data ([CODE]) using [CODE]: [URL] This seems too strict to me, and means one cannot impute a uint8 array with fill_value=0 (a python int). Replacing the validation with something like [CODE] would be more permissible, and without knowing all the details looks safe enough, but perhaps I'm missing something. ### Steps\/Code to Reproduce Though the example in isolation doesn't make a lot of sense (imputing data that doesn't contain missing values), this case might occur with generically defined transformation pipelines applied to unknown datasets. [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[19], [line 5 2 from sklearn import impute 4 df = pd.Series(0, 1, 2], dtype=\"uint8\").to_frame() ----> [5 impute.SimpleImputer(strategy=\"constant\", fill_value=0).fit_transform(df) File ~\/micromamba\/envs\/grapy\/lib\/python3.9\/site-packages\/sklearn\/utils\/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, args, *kwargs) [293]([URL]",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29381"
  },
  {
    "number":27725,
    "text":"BUG: pytest giving UnicodeDecodeError on Windows machine\n\n### Describe the bug When running the test suite on my Windows machine, I get the following error: [CODE_BLOCK] [URL] These lines are causing the error. Simply specifying [CODE] upon [CODE] solves my issue. Not sure if maintainers would accept this change. If so, I can make a simple one-line PR. Otherwise is there any suggested workaround for me? It is kinda annoying to add this keyword every time I run a test suite then remove when commit. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results Correctly runs the test suite. ### Actual Results ``` Traceback (most recent call last): File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\runpy.py\", line 197, in _run_module_as_main return _run_code(code, main_globals, None, File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\runpy.py\", line 87, in _run_code exec(code, run_globals) File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\Scripts\\pytest.exe\\__main__.py\", line 7, in <module> sys.exit(console_main()) File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 192, in console_main code = main() File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 150, in main config = _prepareconfig(args, plugins) File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 331, in _prepareconfig config = pluginmanager.hook.pytest_cmdline_parse( File \"D:\\Downloads...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27725"
  },
  {
    "number":28946,
    "text":"Yeo-Johnson inverse_transform fails silently on extreme skew data\n\n### Describe the bug The Yeo-Johnson is not a surjective transformation for negative lambdas. Therefore, the inverse transformation returns [CODE] when inverse transforming values outside the range of the transform. This failure is silent, so it took me quite a while of debugging to understand this behavior. The problematic lines are [URL] and [URL] in which we might compute [CODE], which of course returns [CODE] as per [URL] ### Steps\/Code to Reproduce To reproduce for positive values (there is a similar problem for negative values): [CODE_BLOCK] ### Expected Results The code should either: 1) validate its inputs and raise an exception 2) validate its inputs and raise a warning 3) fail silently, but have it documented behavior ### Actual Results It just prints [CODE_BLOCK] ### Versions ```shell System: python: 3.11.3 (main, Jan 18 2024, 19:07:12) [Clang 18.0.0 ([URL] 75501f53624de92aafce2f1da698 executable: \/home\/pyodide\/this.program machine: Emscripten-3.1.46-wasm32-32bit Pytho...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28946"
  },
  {
    "number":27038,
    "text":"Document developer utils for parameter validation\n\n### Describe the issue linked to the documentation [URL] and [URL] introduced parameter validation. These tools should be documented under [URL] In addition, a lot of tests for raising errors in case of bad input seems to disappear. Are there common tests for it or is it still recommended to write tests a la [CODE_BLOCK] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27038"
  },
  {
    "number":25109,
    "text":"Add link to license in the readme\n\n### Describe the issue linked to the documentation Thought it would be helpful\/convenient to link the license from the readme ### Suggest a potential alternative\/fix It is already available in the top right hand corner of the repository but this is often missed",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25109"
  },
  {
    "number":25616,
    "text":"Standard Deviation with GPR always between 0 and 1\n\n### Describe the bug I have been trying to fit a gpr interpolation to a set of data, but I keep finding that the standard deviation is always between 0 and 1. I have tried a 1-dimensional example which uses a sin graph and that produces std with a much greater range than my 16 variable problem, whose y values can range between -300 to 50. Is this down to the kernel I have selected or is there something else that I can do to rectify this issue? This is what I am using currently [CODE] Thanks ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect the standard deviation to go beyond 1 at some point. ### Actual Results ``` X = [[0.0999999557809357, 1.3299999629859667, 0.09999855094431032, 1.3299998916392302, 0.09999995283350248, 0.5000004336166886, -0.09999987344949994, 0.5000000291128839, 0.09999968571976785, 1.3299999913612839, -0.09999893694808444, 1.329999992898639, -0.09999962275499284, 1.3299999492423913, 0.09999936606708786, 1.329999994924743, -14.999999994821529]] GPR(X) = (array([0.45100584]), array([0.97051732])) X = [[-0.09999966505014607, 1.329999880690619, 0.09999945465135117, 0.5000000304384626, -0.09999976824132358, 0.5000000804875099, -0.09999761679762588, 0.5000017157059897, -0.09999988188916228, 1.3299997993502248, 0.09999882495398946, 0.5000000116867408, -0.09999975104800556, 1.3299998491078646, -0.09999969367805404, 0.5000005169037308, 12.074950768759113]] GPR(X) = (array([1.00625317]), ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25616"
  },
  {
    "number":30567,
    "text":"Provide wheel for Windows ARM64\n\n### Describe the workflow you want to enable Pretty simple, I want to be able to more easily use scikit-learn on my Windows ARM64 machine. ### Describe your proposed solution Build a wheel for Windows ARM64. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30567"
  },
  {
    "number":24369,
    "text":"DOC Clarify documentation writing guideline\n\n### Describe the issue linked to the documentation The ['Guidelines for writing documentation']([URL] section seems to be specifically about docstrings and the suggestions don't seem to be as relevant for other types of documentation, e.g., example, tutorial, usage pages. ### Suggest a potential alternative\/fix Clarify that this section is about docstrings. Potentially add sections on guidelines for writing examples, usage and tutorial pages?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24369"
  },
  {
    "number":25206,
    "text":"Unclear train_score_ attribute description for GradientBoostingClassifier\n\n### Describe the issue linked to the documentation I recently trained a binary gradient boosting model using the GradientBoostingClassifier. I wanted to plot the intermediate losses at each iteration on the test set while evaluating my results and stumbled upon the [CODE] attribute which is described as [CODE]. However, when plotting my test losses (calculated using normal log_loss using probabilities based on the [CODE] function) the training scores were not in the expected [0,1) interval and the resulting plot did not make a lot of sense (I would love a test set with the loss curve :D). Following the plot: ![loss_without_scalled_test_loss]([URL] After some digging, I found out that, at least for binomial log_loss, deviance is scaled by a factor of 2 using the following formula: [CODE_BLOCK] Knowing this, the fix for the problem is very easy but at least for me took quite some time to find the scaling factor. For completion, the resulting plot makes much more sense if we scale the test loss by the same factor of 2: ![loss_with_scaled_test_loss]([URL] ### Suggest a potential alternative\/fix I would suggest adding a hint to the [CODE] attribute that the deviance is not necessarily the loss but could be a scaled version of the loss and dependent on the loss function employed. In addition, I would somehow change the [CODE] part of the current documentation as the equality sign leaves room for interpretati...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25206"
  },
  {
    "number":28041,
    "text":"Random object not being passed from to Kmeans\n\n### Describe the bug scikit-learn version 1.3.2, file _discretization.py line 308, the random object is not being passed to KMeans. As a result, runs are not reproducible even if you pass a random seed to KBinsDiscretizer. ### Steps\/Code to Reproduce kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans', random_state=42) kdb.fit(X) # this will reproduce different results each time ### Expected Results Same results each time ### Actual Results Different results between runs ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28041"
  },
  {
    "number":27359,
    "text":"[CODE] does not return sparse matrix\n\n### Describe the bug I have been looking at [URL] and working on [CODE]. There is used [CODE] function, which, according to documentation, should return sparse matrix. But it returns [CODE]. Did not find any issue mentioned it, so I considered it to be a bug. Should it return scipy sparse matrix, or documentation should be changed? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results scipy sparse matrix ### Actual Results numpy.ndarray ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27359"
  },
  {
    "number":26013,
    "text":"Add per feature \"maximum category\" counts to [CODE]\n\n### Describe the workflow you want to enable This is a follow up task for #25677 It would be nice to allow users to [specify a per feature number of maxcategories]([URL] instead of having a global limit as implemented in #25677. More details in the linked comment. ### Describe your proposed solution Allow users to pass a list of shape [CODE] or a dict mapping column name to [CODE] values to specify the number of maximum categories per feature. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26013"
  },
  {
    "number":30452,
    "text":"Multiple thresholds in FixedThresholdClassifier\n\n### Describe the workflow you want to enable Currently FixedThresholdClassifier only allows for a unique threshold as a float. It would be nicer to be also able to accept a list of floats and that multiple classes would be produced accordingly. ### Describe your proposed solution Typically, default behaviour would be to cut the scores into bins and labels the bins accordingly from 0 to n, where n is the number of thresholds. Additional options could be welcome (providing a list of labels, option to starts counting at 1 for our non technical friends ... etc.) ### Describe alternatives you've considered, if relevant Current solutions is to cut (pd.cut) outputs myself, outside of the sklearn pipeline. ### Additional context There are industries where instances are expected to be binned in different risk classes. See for exemple the rating grades in the IRB Framework. See EBA Guidelines on PD estimation [...]\"",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30452"
  },
  {
    "number":31423,
    "text":"The libomp.dylib shipped with the macOS x86_64 package does not have an SDK version set\n\n### Describe the bug I want to build an macOS app that uses scikit-learn as a dependency. Using the arm64 package of scikit-learn for this works flawlessly. However, if I want to do the same using the macOS x86_64 packages Apple's notarizing step always breaks the app. This is likely due to the shipped libomp.dylib in the x86_64 package (installed using pip) does not have an SDK version set: [CODE_BLOCK] The arm64 version has this set: [CODE_BLOCK] It would be great, if you could set this (to at least 10.9; would probably need a rebuild of the dylib from source). I already tried some workarounds, but so far none have been successful. Is there any chance you would consider that :)? ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31423"
  },
  {
    "number":25533,
    "text":"Error while installing DeepLabCut: Collecting scikit-learn>=1.0\n\n### Describe the bug I'm trying to install DeeplLabCut and was encountering an error. The devs guided me over here to as it seems to be an error while installing scikit-learn. Issue for reference: [URL] I already installed Microsoft C++ Build Tools due to a previous error message, but it didn't fix the issue, just changed the error message. I also installed an additional Fortran compiler (via MinGW) since that pops up in the log, but no effect. ### Steps\/Code to Reproduce 1. Clone [URL] 2. Open console 3. cd Deeplabcut\/conda-environments 4. conda env create -f DEEPLABCUT.yaml ### Expected Results Successful installation ### Actual Results ``[CODE]kernel_spec_manager_class[CODE]EnableNBExtensionApp[CODE]kernel_spec_manager_class[CODE]EnableServerExtensionApp`. Enabling: nb_conda - Writing config: C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\etc\\jupyter - Validating... nb_conda 2.2.1 ok done Installing pip dependencies: | Ran pip subprocess with arguments: ['C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\DEEPLABCUT\\\\python.exe', '-m', 'pip', 'install', '-U', '-r', 'C:\\\\ProgramData\\\\DeepLabCut\\\\conda-environments\\\\condaenv.pjo7koro.requirements.txt', '--exists-action=b'] Pip subprocess output: Collecting deeplabcut[gui,tf] Using cached deeplabcut-2.3.0-py3-none-any.whl (1.4 MB) Collecting dlcl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25533"
  },
  {
    "number":30390,
    "text":"CI Replace pytorch conda channel in CI lock-files\n\nAfter a quick look, it seems like the only place we are using the pytorch channel is for the CUDA CI, cc @betatim. The easiest thing to try would be to use the conda-forge pytorch-gpu package? See URL] for more details. the main thing is: > 2.5 will be the last release of PyTorch that will be published to the [pytorch.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30390"
  },
  {
    "number":25364,
    "text":"Specifying 'cosine' as metric in KDTree throws error\n\n### Describe the bug I am trying to implement the [CODE] with [CODE] as a distance metric. I first started with [scipy's implementation]([URL] but it didn't support [CODE] as a metric. Then, I came across [sklearn implementation]([URL] The documentation indicates that the supported metric can be found ([scipy.spatial.distance]([URL] and ([distance_metrics]([URL] Both of these places show that the [CODE] metric is supported. Also, the [user guide]([URL] also suggests that cosine is supported as it tries to point out [CODE] as [CODE]. However, it still throws an error for me when I try to use that metric. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results [CODE_BLOCK] ### Version...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25364"
  },
  {
    "number":26530,
    "text":"TransformedTargetRegressor forces 1d y shape to regressor\n\n### Describe the bug I experience the following error when using TransformedTargetRegressor with my skorch model: ValueError: The target data shouldn't be 1-dimensional but instead have 2 dimensions, with the second dimension having the same size as the number of regression targets (usually 1). Please reshape your target data to be 2-dimensional (e.g. y = y.reshape(-1, 1). #### After checking the Source Code this lead me the the following unexpected behaivor which makes little sense: If TransformedTargetRegressor is fitted with with a 2d dimensional y, it will still be transformed to a 1d dimensional output y should have the same input and output shapes with a TransformedTargetRegressor or there should be an init argument to disable the change of the input shape (Yes, internally it gets casted to 2d, but I\u2019m talking about the In and Outputs) [URL] TransformedTargetRegressor-->fit [CODE_BLOCK] But in the end we squeeze it back into a 1d which causes issues for models which expect a 2d input of y y was 2d in the beginning for a reason ### The following code would solve this: [CODE_BLOCK] This could only create an issue where the y input was for some reason 2d but should be 1d for the regressor. In this case an attribute would be nice [CODE_BLOCK] Also in TransformedTargetRegressor-->predict the results dont get squeezed after the prediction of the estimator - only if ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26530"
  },
  {
    "number":29792,
    "text":"Discrepancy between .fit_transform() and .transform() methods in the LLE module\n\n### Describe the bug A user would expect the same result from - [CODE] and then [CODE] - [CODE] But this is not the case in the current code for [CODE]. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results [CODE_BLOCK] ### Actual Results [CODE_BLOCK] ### Versions ```shell System: python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ] executable: \/Users\/wonderman\/miniforge3\/envs\/sklearn_dev_env\/bin\/python machine: macOS-14.6.1-arm64-arm-64bit Python dependencies: sklearn: 1.6.dev0 pip: 24.2 setuptools: 72.2.0 numpy: 2.1.0 scipy: 1.14.1 Cython: 3.0.11 pandas: None matplotlib: None joblib: 1.4.2 threadpoolctl: 3.5.0 Built with OpenMP: True threadpoolctl info: user_api: blas internal_api: openblas num_threads: 8 prefix: libopenblas filepath: \/Users\/wonderman\/miniforge3\/envs\/sklearn...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29792"
  },
  {
    "number":28995,
    "text":"Add \"scoring\" argument to `[CODE]`\n\n### Describe the workflow you want to enable I want to enable non-accuracy metrics to `[CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE][CODE]` method. I can't think of any other alternatives tbh. ### Additional context I think in theory this requires a slep, as it's changing shared API, right?",
    "labels":[
      "RFC",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28995"
  },
  {
    "number":29291,
    "text":"[CODE] Fails with pyarrow String Types in sklearn\n\n### Describe the bug When using SimpleImputer from sklearn with pyarrow string types, the imputer fails with an error. This issue occurs when attempting to impute missing values in a DataFrame containing pyarrow string columns. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The SimpleImputer should handle pyarrow string types and impute the missing values without raising an error. ### Actual Results ``[CODE]transform`\\\" (...) 1093 UserWarning,...",
    "labels":[
      "Bug",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29291"
  },
  {
    "number":25627,
    "text":"OrdinalEncoder does not work with HistGradientBoostingClassifier when there are NULLs\n\n### Describe the bug If you use the ordinal encoder when there is NULLS you need to put them to a Negative Value otherwise you get. The following error _The used value for unknown_value is one of the values already used for encoding the seen categories._ For example this could be done as [CODE] This later causes an issue when predict_proba() is run on the trained HistGradientBoostingClassifier. _OverflowError: can't convert negative value to npy_uint8_ It seems odd to me that it can train but not predict The super hackish way I have been getting around this is to just add one to the value. However, this means I need to do it out of the pipeline and make my code less clean. [CODE] ### Steps\/Code to Reproduce #Modified from [URL] #This also happens in the classifier ```python from sklearn.datasets import fetch_openml from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder from time import time from sklearn.pipeline import make_pipeline from sklearn.ensemble import HistGradientBoostingRegressor import numpy as np bikes = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True, parser=\"pandas\") # Make an explicit copy to avoid \"SettingWithCopyWarning\" from pandas X, y = bikes.data.copy(), bikes.target X[\"weather\"].replace(to_replace=\"heavy_rain\", value=np.nan, inplace=True) mask_training = X[\"year\"] == 0.0 X = X.drop(columns=[\"year\"]) X_train, y_train...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25627"
  },
  {
    "number":29293,
    "text":"Nightly wheel upload has been broken for 11 days\n\n[URL] ![image]([URL] Maybe the v3 -> v4 artifact update oh well :sweat: [URL] [build log]([URL] [CODE_BLOCK]",
    "labels":[
      "Bug",
      "Build \/ CI"
    ],
    "label_count":2,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29293"
  },
  {
    "number":29533,
    "text":"Add FN and FP weight parameter in MCC\n\n### Describe the workflow you want to enable Introducing a weight parameter for false negatives (FN) and false positives (FP) in Matthews Correlation Coefficient (MCC) would enhance the metric\u2019s flexibility and applicability, particularly in contexts where the costs of different types of errors vary significantly. By allowing the impact of FN and FP to be adjusted, MCC can be tailored to better reflect the specific goals of the application. For example, in medical problems, minimizing false negatives might be crucial to avoid missing a critical condition, while in other contexts, reducing false positives might be more important to prevent unnecessary interventions. ### Describe your proposed solution Original: [CODE_BLOCK] Proposed: [CODE_BLOCK] ### Describe alternatives you've considered, if relevant - Instead of two parameters, one single parameter can be used for weighting, similarly to beta in fbeta_score. ### Additional cont...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29533"
  },
  {
    "number":31840,
    "text":"SkLearn IQR function\n\n### Describe the workflow you want to enable Recently, I was working on a machine learning project with a dataset that was quite skewed. I repeatedly had to compute the interquartile range (IQR), calculate the 25th and 75th percentiles, visualize the box plot, and then remove outliers \u2014 all manually. While this wasn't an issue at first, it became tedious to write the same code over and over again for different rows and columns. This made me wonder: Wouldn\u2019t it be much more efficient if scikit-learn offered a built-in utility to calculate the IQR and optionally remove or flag outliers? I believe this kind of functionality could significantly streamline the preprocessing workflow for many users. ### Describe your proposed solution I\u2019d like to suggest adding a simple utility class to scikit-learn (or as part of a preprocessing module), called OutlierRemoval. This class would encapsulate all IQR-related preprocessing logic and expose a clean interface for users to apply it. [CODE_BLOCK] Prevents redundant code when handling outliers across multiple projects Encourages best practices in preprocessing pipelines Makes exploratory data analysis (EDA) cleaner and more intuitive Aligns with scikit-learn\u2019s emphasis on reusable, composable preprocessing tools ### Describe alternatives you've considered, if relevant I've used pandas and numpy to manually calcu...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31840"
  },
  {
    "number":31621,
    "text":"ENH: Add MedianAbsoluteDeviationScaler (MADScaler) to sklearn.preprocessing\n\n### Describe the workflow you want to enable Today, if a user wants to centre features by the median and scale them by the median absolute deviation (MAD) they must hand-roll code like: mad = 1.4826  np.median(np.abs(X - np.median(X, axis=0)), axis=0) X_scaled = (X - np.median(X, axis=0)) \/ mad A built-in MedianAbsoluteDeviationScaler (or a statistic=\"mad\" option on RobustScaler) would let them write a single, self-documenting line: from sklearn.preprocessing import MedianAbsoluteDeviationScaler X_scaled = MedianAbsoluteDeviationScaler().fit_transform(X) That makes robust MAD scaling first-class, composable in pipelines, and reversible via inverse_transform(). ### Describe your proposed solution Add a new transformer: class MedianAbsoluteDeviationScaler(BaseEstimator, TransformerMixin): with_centering: bool = True with_scaling: bool = True copy: bool = True unit_variance: bool = False # learned in fit center_: ndarray scale_: ndarray Fit logic 1. center_ = np.median(X, axis=0) (if with_centering) 2. mad = np.median(np.abs(X - center_), axis=0)  1.4826 3. Guard against zeros with float_eps, store in scale_. transform() and inverse_transform() reuse the pattern from RobustScaler. Docs \/ tests - Unit tests for shape preservation, inverse-transform round-trip, and robustness to outliers. - A short subsection in preprocessing.rst and a gallery example comparing Standard, Robust (IQR) and MAD scalers. - Ch...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31621"
  },
  {
    "number":30624,
    "text":"Inconsistency in shapes of [CODE] attributes between [CODE] and [CODE] when parameter [CODE] is 2D with [CODE]\n\n### Describe the bug This issue comes from my (possibly incorrect) understanding that [CODE] and [CODE] classes should handle the dimensions of the [CODE] and [CODE] parameters to the [CODE] method in the same way in a sense that the same pair of [CODE] parameter values provided to both [CODE] and [CODE] methods should produce the [CODE] attribute values of the same shape in both classes. But it appears that in case of a 2D shaped parameter [CODE] of the form [CODE] with [CODE] passed into the [CODE] method, the resulting shapes of [CODE] attribute differ between [CODE] and [CODE] classes. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results The example code should produce no output and throw no error. According to the [[CODE] docs]([URL] the resulting value of the [CODE] attribute should be 2D shaped as [CODE]. This is what happens in my minimal code example, indeed. The [docs for the [CODE] class]([URL] are less detailed but the parameter and attribute names, types, and shapes are the same for [CODE], [CODE], and [CODE]. I can't think of a reason why the logic of how the shapes of [CODE] and [CODE] parameters tran...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30624"
  },
  {
    "number":31965,
    "text":"a11y - scikit-learn docs accessibility audit and remediation\n\n### Description Note: This is scoped as part of an ongoing NASA ROSES grant in collaboration with Quansight; as such, a couple of us at Quansight will take on the work outlined in this issue. Per the NASA ROSES grant, we will conduct an accessibility review of the [scikit-learn documentation site]([URL] and work on remediation of the flagged issues. Since scikit-learn uses the PyData Sphinx Theme, on which we have already conducted thorough accessibility audits and spent a substantial amount of work over the last couple of years to make this theme more accessible, the audit and remediation of scikit-learn will focus on customised\/custom features added to the scikit-learn documentation. ### Proposed implementation To achieve this goal, I propose the following approach: 1. Scope what needs to be audited\/tested - and update this issue to reflect this 2. Test\/audit components and report back on the findings in this issue 3. Iteratively work on any remediation tasks as needed. Please let me know if you have any questions or suggestions on how to approach this more effectively, so we can keep you all aligned and ensure a smooth contribution. Also, if y'all can assign me to this issue, it would be great! \u2728",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31965"
  },
  {
    "number":31679,
    "text":"AI tools like Copilot Coding Agent don't know about \/ don't respect our Automated Contributions Policy\n\n(I am creating an issue to a PR already opened (#31643), because there are many more ways to solve the problem.) AI tools many people use to create PRs don't care about our Automated Contributions Policy from our own repositories](URL] - so far without success. You can see that there is an increasing amount of partially or fully generated PRs and a decrease in overall quality for PRs on scikit-learn by looking at [the last closed PRs. It is not a flood yet, but bad enough to keep several maintainers busy for some extra hours a week. It could become a flood in the future. This is why it is important to find solutions. Quite some of the authors of these additional low-quality PRs on scikit-learn also spam llm-based PRs on other open source projects at the same time. I have added repeated cases to @adrinjalali's [agents-to-block]([URL] folder. The pattern of spammers is to open a PR with an unqualified guess of what the project needs or how an issue can be solved, and then not follow up after maintainers reviewed, close and try again. PRs can look like someone made a genuine attempt to address an open issue, and project maintainers start to interact with the \"authors\" - but then their review c...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31679"
  },
  {
    "number":24659,
    "text":"check_classifiers_train fails when tag requires_positive_X=True\n\n### Describe the bug The check CODE] fails for classifiers that need positive X, even when the tag [CODE] is set to [CODE]. In the example below, I copy\/paste the template from [skltemplate: self.demo_param = demo_param def fit(self, X, y): \"\"\"A reference implementation of a fitting function for a classifier. Parameters ---------- X : array-like, shape (n_samples, n_features) The training input samples. y : array-like, shape (n_samples,) The target values. An array ...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24659"
  },
  {
    "number":24725,
    "text":"[CODE] section for [CODE] and [CODE] does not link to referenced documentation.\n\n### Describe the issue linked to the documentation The current documentation of [pairwise_distances]([URL] and [paired_distances]([URL] does not contain hyperlinks working in the [CODE] section. Is this the intended behavior? ### Suggest a potential alternative\/fix I'm not actually sure why this would happen as the references seem to be correct. Is it because [CODE] has hyperlink to [CODE] and vice-versa is also done?",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24725"
  },
  {
    "number":29633,
    "text":"test_svm fails on i386 with scipy 1.13\n\n### Describe the bug scipy 1.13 is triggering test failure in test_svc_ovr_tie_breaking[NuSVC] on i386 architecture. The error can be seeing in debian CI tests, [URL] Full test log at [URL] or [URL] ### Steps\/Code to Reproduce On an i386 system with scipy 1.13 installed [CODE_BLOCK] ### Expected Results test should pass ### Actual Results ``` 1333s _______________________ test_svc_ovr_tie_breaking[NuSVC] _______________________ 1333s 1333s SVCClass = <class 'sklearn.svm._classes.NuSVC'> 1333s 1333s @pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC]) 1333s def test_svc_ovr_tie_breaking(SVCClass): 1333s \"\"\"Test if predict breaks ties in OVR mode. 1333s Related issue: [URL] 1333s \"\"\" 1333s X, y = make_blobs(random_state=0, n_samples=20, n_features=2) 1333s 1333s xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 100) 1333s ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 100) 1333s xx, yy = np.meshgrid(xs, ys) 1333s 1333s common_params = dict( 1333s kernel=\"rbf\", gamma=1e6, random_state=42, decision_function_shape=\"ovr\" 1333s ) 1333s svm = SVCClass( 1333s break_ties=False, 1333s common_params, 1333s ).fit(X, y) 1333s pred = svm.predict(np.c_[xx.ravel(), yy.ravel()]) 1333s dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()]) 1333s > assert not np.all(pred == np.argmax(dv, axis=1)) 1333s E assert not True 1333s E + where True = <function all at 0xf689d5e0>(array([1, 1, 1, ..., 1, 1, 1]) == array([1, 1, ..., dtype=int32) 1333s E + where...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29633"
  },
  {
    "number":26776,
    "text":"Document the class DistanceMetric\n\nRight now we don't have any documentation for the public class [CODE]. It would be nice to have a description. xref: [URL]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26776"
  },
  {
    "number":26885,
    "text":"HalvingRandomSearchCV does not support param_distribution as a list\n\n### Describe the bug Before scikit-learn version 1.3.0 (e.g. 1.2.0) HalvingRandomSearchCV could be used with a [CODE] as the input for param_distribution (similar to RandomizedSearchCV). The type hint in the documentation states that only [CODE] is possible but the description talks about the option to pass also a [CODE]. As for version 1.3.0 the parameters are validated so [CODE] no longer works. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results No error is thrown ### Actual Results ```py --------------------------------------------------------------------------- InvalidParameterError Traceback (most recent call last) Cell In[22], line 20 12 params = [ 13 {'clf': (DecisionTreeClassifier(),), 'clf__criterion': ['entropy', 'gini']}, 14 {'clf': (LogisticRegression(),), 'clf__C': [0.01, 0.1, 1, 10, 100], 'clf__max_iter': [10_000]} 15 ] 17 search = HalvingRandomSearchCV( 18 pipe, param_distributions=params, scoring=\"accuracy\") ---> 20 search.fit(X, y) File ~\\AppData\\Local\\miniconda3\\envs\\automl\\lib\\site-packages\\sklearn\\base.py:1144, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, args, *kwargs) 1139 partial_fit_and_fitted = ( 1140 fit_method.__name__ == \"partial_fit\" an...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26885"
  },
  {
    "number":27504,
    "text":"Returning number of samples in leaf nodes in decision trees.\n\n### Describe the workflow you want to enable In the paper \"Towards Practical Lipschitz Bandits\" by Wang, Ye, Geng and Rudin ([URL] the authors used a modified version of the DecisionTreeRegressor in their algorithm. More specifically, they used number of samples in the leaf nodes to quantify the uncertainty level of the predictions. It seems that the method in this paper is a bit useful. It would be great if \"DecisionTreeRegressor\" object in the official scikit-learn package could output number of samples in the leaf nodes. ### Describe your proposed solution When fitting the DecisionTreeRegressor, in addition to record mean of the leaf nodes, also record number of samples used to compute the mean. When output a prediction, in addition to output of mean of the corresponding leaf node, also output the number of training samples used in the prediction. I'm a coauthor of the paper \"Towards Practical Lipschitz Bandits\", and have a local copy of our solution when writing the paper. I just forked the master branch of the official scikit-learn repo, and replace the folder \"sklearn\" by our local copy. This fork is at: [URL] Our code is a bit old, and may not be optimized in some aspects. But maybe it's useful as a reference. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27504"
  },
  {
    "number":26850,
    "text":"v0.24.X fails to build because of new Cython 3.0.0 release\n\n### Describe the bug The build requirements in [CODE] specify a minimum Cython version but no maximum version. [URL] The recent release of Cython 3.0.0 is not compatible with the [CODE] [CODE] code. [URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I expect the package to build successfully. ### Actual Results Excerpt of output: [CODE_BLOCK] ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26850"
  },
  {
    "number":28618,
    "text":"Add a download_openml util\n\nWe should add a [CODE] utility in [CODE] which downloads the file, but doesn't return [CODE], and instead returns the paths to the downloaded data file (arff or parquet), and the metadata json file. This utility can then be internally called by [CODE]. A user, can then do something like: [CODE_BLOCK] This should be easy to implement, and should make a bunch of our examples closer to a real world kind of scenario. cc @glemaitre @ogrisel @GaelVaroquaux @MarcoGorelli",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28618"
  },
  {
    "number":31284,
    "text":"\u26a0\ufe0f CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: May 05, 2025) \u26a0\ufe0f\n\nCI is still failing on Linux_Nightly.pylatest_pip_scipy_dev - Test Collection Failure",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31284"
  },
  {
    "number":29813,
    "text":"Adding timeseries-tailored baseline strategy to Dummy* estimators. Making them more intelligent with strategy=\"best\".\n\n### Describe the workflow you want to enable While always predicting the mean (for regression) or the most frequent class (for classification) are solid baselines for many ML workloads, they are too weak for time series problems, where the target is constantly changing. A much better, natural baseline in such cases is a lagged series value or the mean of the series over a recent window. I would love to see a Dummy strategy tailored for time series, as sensible baselines are critical in these tasks to avoid optimistic bias. Moreover, all Dummy estimators are currently simple and extremely fast, giving us room to add more intelligent strategies. What\u2019s not very convenient in the current implementation is that it requires the user to manually test all simple strategies and choose the strongest to establish a reasonable baseline (which is something I always end up doing in practice). I think we can relieve users of this burden by introducing a \"best\" strategy that internally evaluates all existing strategies and returns the best one based on the provided scoring method. With these additions, we\u2019ll make Dummy estimators smarter and more useful, capable of highlighting weak user solutions early on. Disadvantages: 1)The typical runtime will increase from ~1\/100th of a second to ~1\/10th of a second, which I believe is negligible compared to the runtime of main models...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29813"
  },
  {
    "number":31872,
    "text":"Strange normalization of semi-supervised label propagation in [CODE]\n\nThe method CODE] on the [CODE] class in [CODE] [(line 455) Summary  Troubles with the current code normalization: - In the dense affinity_matrix case, the current code sums axis=0 and then divides the rows by these sums. Other normalizations in semi_supervised use axis=1 (as this case should). This does not cause incorrect result so long as we have symmetric affinity_matrices. The dense case arises for kernel \"rbf\" which provides symmetric matrices. But if someone provides their own kernel the normalization could be incorrect. - In the sparse affinity_matrix case, the current code divides all rows by the sum of the first row. This is not standard normalization, but does not cause errors so long as the row sums are all the same. The sparse case arises for kernel \"knn\" which has all rows sum to k. But if someone provides their own kernel the normalization could be incorrect. - The normalization is different for the dense and sparse cases, which could be confusing to someone writing their own kernel. The fix involves changing [CODE] to [CODE] and correcting the sparse case to divide each row by its sum when the row sums are not all equal. <details> <summary> original somewhat rambling description <\/summary>  Summary  The method returns a different [CODE] for sparse and for dense versions of the same kernel matrix. Neither sparse nor dense versions normalize the usual way (columns sum to 1). The dense case is c...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31872"
  },
  {
    "number":31365,
    "text":"TargetEncoder example code\n\n### Describe the issue linked to the documentation The example used in the stable TargetEncoder documentation is not coherent with the expected order of [CODE] (the 80 corresponds to 'dog' but is is in second order not first). Printing [CODE] reveal that the order is indeed coherent with [CODE]. However, as I was trying to understand where this difference of order came from, I wasn't able to find in [TargetEncoder class definition]([URL] where [CODE] was set. ### Suggest a potential alternative\/fix - make it more explicit in documentation, such as adding a print of [CODE] - make [CODE] preserve the columns order found in the dataset",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31365"
  },
  {
    "number":29554,
    "text":"RFECV cross-validation generator ([CODE]) parameter\n\n### Describe the issue linked to the documentation Hello, if I'm not mistaken, I think that the documentation of RFECV, it's possible to perform StratifiedKFold. In fact, code-wise, the CODE] parameter is processed by [check_cv. According to the code of [CODE], if [CODE] is supplied as an integer (and so a cv method must be chosen), and the estimator is a classifier, and y is binary or multiclass, then the cv generator will indeed be a [CODE]. ``` cv = 5 if cv is None else cv if isinstance(cv, numbers....",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29554"
  },
  {
    "number":29199,
    "text":"Inconsistency regarding Poisson regression in decision tree user guide doc\n\n### Inconsistency about criterion in Decision Tree Regressor Hi! I was revising scitkit-learn User guide about Decision Tree regressors and I found an inconsistency. In 1.10.7.2. Regression criteria it is mentioned Poisson Deviance. However, the formula showed is the one of Half Poisson Deviance. Furthermore, when I checked API section about decision tree regressor and poisson deviance is mentioned not half poisson deviance. So, I'm confused, what is the criterion used Poisson Deviance or Half Poisson Deviance? I need an answer as soon as posible as I'm using this function in my bachelor thesis. <img width=\"488\" alt=\"Captura de pantalla 2024-06-06 024245\" src=\"[URL] <img width=\"499\" alt=\"image\" src=\"[URL] ### Edit documentation Please can you fix this inconsistency? If poisson deviance is used change the formula in the guide but if half poisson deviance is used can you specify it? Thanks!",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29199"
  },
  {
    "number":27839,
    "text":"LocalOutlierFactor might not work with duplicated samples\n\nThis an investigation from the discussion in [URL] [CODE] might be difficult to use when there are duplicate values larger then [CODE]. In this case, the distance for these neighbors is [CODE], meaning that the local reachibility density is therefore infinite (or in the algorithm [CODE]). The issue starts for sample next to those local peaky density: they might use the [CODE] as measure, meaning that they will have a really negative [CODE] while the value of the sample could be really close to the one of the plateau. I will now provide a minimum sythetic example to show the issue: [CODE_BLOCK] [CODE_BLOCK] In the results above, we see that the first and last values should not be considered as outliers but because they have their neighbors coming from the constant part (i.e. plateau at 0.1), the local reachibility density is 1e10 and thus the negative outlier factor is set to -1e7. Running the same code without the constant part will give: ```python import numpy as np from sklearn.neighbors import LocalOutlierFactor rng = np.random.default_rng(0) x = rng.permutation(np.hstack([ np.linspace(0.1, 0.2, num=30), rng.random(5) * 1...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27839"
  },
  {
    "number":32161,
    "text":"Add an option to OrdinalEncoder to sort encoding by decreasing frequencies\n\n### Describe the workflow you want to enable At the moment, when no categories are provided, the default ordering of categories for a given categorical column is based on the lexicographical ordering of the categories observed in the training set. However, this is quite arbitrary and one could instead provide an option to encode such values based on their observed frequency in the training set. The motivation would be to nudge the inductive bias of tree-based models (and models that favor axis aligned decision functions) into separating nominal from rare values more easily (e.g. fewer splits in a tree). This might be especially useful for outlier detection models such as [CODE]. Related to #15796. ### Describe your proposed solution Extend the [CODE] option to have: - [CODE] (default for backward compat); - [CODE] (same as today); - [CODE] (the new option). If [CODE], then the generated category encodings would be: - 0 would encode the most frequent category observed in the training set, - 1 the second most frequent, - ... - and so on until the least frequent categories. Tie breaking could be based on the lexicographical order to ensure that the behavior of [CODE] remains invariant under a shuffling of the rows of the training set. ### Describe alternatives you've considered, if relevant - Use [CODE] that leverages some frequency info (mixed with the mean value of the target variable), but this is a s...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32161"
  },
  {
    "number":27307,
    "text":"I can not achieve inplace scaling by using sklearn.preprocessing.minmax_scale\n\n### Describe the bug ## By setting the copy=False, ndarray data has not changed unexpectedly ![image]([URL] ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results ## A reasonable explanation about the copy parameter of minmax_scala funciton ### Actual Results # There are no warings and errors, just the result is not wrong! ![image]([URL] ### Versions [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27307"
  },
  {
    "number":32062,
    "text":"Regressor Prediction Makes a Negative Y Offset\n\n### Describe the bug Hi, I've found a strange situation where regressor prediction makes a negative Y offset. See an orange line on my picture below. Here is my py file and json data: [test_scikit.zip]([URL] You will need to change JS_PATH to your path: JS_PATH = \"D:\/Projects\/Crpt\/CryptoMaiden\/Bot\/Base\/Test\/btc_data.json\" You will also need to install a poltly lib. <img width=\"1625\" height=\"921\" alt=\"Image\" src=\"[URL] \/> I'm new to the Scikit so I decided to report the issue. I tried RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor and all of them have this issue. To change a regressor in my code you can just comment\/uncomment it on lines 89-95. ### Steps\/Code to Reproduce # See my attached ZIP file! ### Expected Results No negative Y offset. ### Actual Results Just run my py file! ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/32062"
  },
  {
    "number":27482,
    "text":"ColumnTransformer converts pandas extension datatypes to [CODE]\n\n### Describe the bug pandas has some extension data types is not applied and if they contain missing values the conversion fails. ### Steps\/Code to Reproduce [CODE] produces a [CODE] array, but [CODE] produces an [CODE] array: [CODE_BLOCK] This causes a [CODE] if the array has missing values and is later passed to an estimator: [CODE_BLOCK] ### Expected Results The output of the ColumnTransformer for [CODE] inputs would ideally be [CODE], with missing values represented by [CODE], and fitting the [CODE] on them would not raise an error ### Actual Results ``` Float64 fl...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27482"
  },
  {
    "number":27564,
    "text":"Decision Rules in If\/Then format\n\n### Describe the workflow you want to enable Although Decision tree has the following to print rules, [CODE_BLOCK] Current Output: [CODE_BLOCK] Expected Output: It would be more better if we have human readable rules , something like this: [CODE_BLOCK] ### Describe your proposed solution I want to include this function to get human readable rules as i mentioned: ``` def get_rules(tree, feature_names, class_names): tree_ = tree.tree_ feature_name = [ feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\" for i in tree_.feature ] paths = [] path = [] def recurse(node, path, paths): if tree_.feature[node] != _tree.TREE_UNDEFINED: name = feature_name[node] threshold = tree_.threshold[node] p1, p2 = list(path), list(path) p1 += [f\"({name} <= {np.round(threshold, 3)})\"] recurse(tree_.children_left[node], p1, paths) p2 += [f\"({name} > {np.round(threshold, 3)})\"] recurse(tree_.children_right[node], p2, paths) else: path += [(tree_.value[node], tree_.n_node_samples...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27564"
  },
  {
    "number":25452,
    "text":"Issue building from source on Fedora 37 Python 3.11\n\nDear maintainers, I've recently updated my system to Fedora 37, shipping python 3.11. I've created and activated a new python venv [CODE_BLOCK] then build main at commit a7cd0ca44d0af64bc22a7213371e3c11d94dbbe8 in editable mode [CODE_BLOCK] The build fails with the following error [CODE_BLOCK] <details><summary>Full traceback<\/summary> ```trpy Obtaining file:\/\/\/home\/cmarmo\/software\/scikit-learn Installing build dependencies ... done Checking if build backend supports build_editable ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Requirement already satisfied: numpy>=1.17.3 in \/home\/cmarmo\/skldevenv\/lib64\/python3.11\/site-packages (from scikit-learn==1.3.dev0) (1.24.1) Requirement already satisfied: scipy>=1.3.2 in \/home\/cmarmo\/skldevenv\/lib64\/python3.11\/site-packages (from scikit-learn==1.3.dev0) (1.10.0) Requirement already satisfied: joblib>=1.1.1 in \/home\/cmarmo\/skldevenv\/lib64\/python3.11\/site-packages (from scikit-learn==1.3.dev0) (1.2.0) Requirement already satisfied: threadpoolctl>=2.0.0 in \/home\/cmarmo\/skldevenv\/lib64\/python3.11\/site-packages (from scikit-learn==1.3.dev0) (3.1.0) Installing collected packages: scikit-learn Running setup.py develop for scikit-learn error: subprocess-exited-with-error ...",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25452"
  },
  {
    "number":29390,
    "text":"Broken ref in datasets.rst\n\nThere's a reference in [CODE] that no longer points to anything after #29104 was merged: [URL] I don't understand why it wasn't caught by the CI in #29104. I saw it in the 1.5.1 release branch (#29382), see [URL] I will ignore it for the release, but we need to rephrase this paragraph because it references a part of the doc that no longer exists.",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29390"
  },
  {
    "number":24240,
    "text":"\u26a0\ufe0f CI failed on Linux_Docker.debian_atlas_32bit \u26a0\ufe0f\n\nCI is still failing on Linux_Docker.debian_atlas_32bit - test_fastica_simple[27-float32-True] - test_fastica_simple[27-float32-False]",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24240"
  },
  {
    "number":28162,
    "text":"Multiple GMMs with variable training samples lead to memory leak?\n\n### Describe the bug When i use sklearn GaussianMixture multiple times, while varying the the number of training pixels, it creates a memory leak. ### Steps\/Code to Reproduce CODE_BLOCK] ### Expected Results The \"leak\" function should not have a memory leak. it's memory footprint should be bounded by the sizes it needs when using the highest possible number of pixels, i.e 500000, which is the number used in the \"no_leak\" function ### Actual Results Here is the plot for the \"no_leak\" function: ![leak1 [GCC 9.4.0] executable: \/usr\/bin\/python3 machine: Linux-5.4.0-166-generic...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28162"
  },
  {
    "number":26157,
    "text":"RFECV crashes with a non-pickable error even if n_jobs is set to 1.\n\n### Describe the bug When using the .fit method, the code crashes with warnings related to jbolib and no errors, even when n_jobs is set to 1. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results A clear error message and the code crashes OR no error is returned and the code does not crash. Suggestion in: [URL] Change: [CODE_BLOCK] To: [CODE_BLOCK] Additionally, it could be good to have a try, except statement when dealing with JobLib, where the except would crash the code with some error message. ### Actual Results ``` \/python-data-2022-09\/bin\/python3: line 22: 2714665 Segmentation fault \/usr\/bin\/singularity --silent exec -B $DIR\/..\/$SQFS_IMAGE:$INSTALLATION_PATH:image-src=\/ $DIR\/..\/$CONTAINER_IMAGE bash -c \"eval \\\"\\$(\/CSC_CONTAIN...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26157"
  },
  {
    "number":27481,
    "text":"Homogeneity Score is Not Consistently Correct For Trivial Clustering\n\n### Describe the bug The homogeneity_score is not being computed consistently when you have a single truth label for different array sizes. It seems not to matter how many unique labels are in the predicted labels, just so long as there is only one truth label. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I would expect this to always be 1.0 for a single truth label, regardless of array size. ### Actual Results You will see that the homogeneity score bounces around among -1.0, 0.0, and 1.0 depending on array size. ### Versions [CODE_BLOCK]",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27481"
  },
  {
    "number":26941,
    "text":"Make sure [CODE] is not rendered by sphinx when disabled\n\nxref: [URL] When [CODE], there shouldn't be any documented [CODE] method since it doesn't exist. cc @thomasjpfan",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26941"
  },
  {
    "number":27882,
    "text":"[RFC] Varying the number of outputs considered for splitting in Multi Output Decision Trees\n\n### Describe the workflow you want to enable One strength of RFRs is that they are incredibly robust and therefore provide a strong baseline for many tasks without needing to consider normalization or scaling of either the inputs or outputs. In the case of multi-output RFRs this robustness towards the output space goes away due to the summing of impurities across different output dimensions which entails the need to standardize the output labels to ensure that undue attention isn't given to particular outputs. Currently the documentation doesn't readily inform the user of this artifact. In the spirit of the Random Forest one solution to avoiding this problem would be to randomly sample which output(s) to consider for the determination of the split. If the number of outputs was set to 1 then we would end up with a case where the normalization of the output space once again doesn't matter. ### Describe your proposed solution The introduction of a new kwarg [CODE] (by analogy to [CODE]) could allow users to control how many outputs were considered when selecting the optimal split. If set to [CODE] all outputs would be used as currently, if set to [CODE] then a single output would be used as described above. This seems like a relevant and natural hyper-parameter for the multi-output RFR. ### Describe alternatives you've considered, if relevant _No response_ ### Additional context I have n...",
    "labels":[
      "help wanted",
      "New Feature"
    ],
    "label_count":2,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27882"
  },
  {
    "number":24695,
    "text":"Does this line in `Classifier comparison' produce a test data leakage?\n\n### Describe the issue linked to the documentation I feel there's a line here: [CODE_BLOCK] from this page ([URL] that produces a test data leakage. Shouldn't it be something like: [CODE_BLOCK] as in this page, [URL] > This class implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. ### Suggest a potential alternative\/fix [CODE_BLOCK]",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24695"
  },
  {
    "number":31540,
    "text":"Make [CODE] part of the public API\n\n### Describe the workflow you want to enable This tool is great to run multiple scorers on a single estimator thanks to the caching mechanism. It is a bummer that it is not part of the public API. ### Describe your proposed solution Make it part of the public API ### Describe alternatives you've considered, if relevant _No response_ ### Additional context _No response_",
    "labels":[
      "Enhancement"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31540"
  },
  {
    "number":30339,
    "text":"DOC: clarify the documentation for the loss functions used in GBRT, and Absolute Error in particular.\n\n### Describe the bug From my understanding, currently there is no way to minimize the MAE (Mean Absolute Error). Quantile regression with quantile=0.5 will optimize for the Median Absolute Error. This would be different from optimizing the MAE when the conditional distribution of the response variable is not symmetrically-distributed. [URL] What I expect - Using [CODE] should optimize for the mean of absolute errors. - Using [CODE] should optimize for the median of absolute errors. [CODE_BLOCK] What happens Both give the same results - Using [CODE] optimizes for the median of absolute errors - Using [CODE] optimizes for the median of absolute errors Suggested Actions If this is intended behavior: - Feel free to close this issue marked as resolved. - Kindly add a note in the documentation that \"Absolute Error optimizes for Median Absolute Error, not Mean Absolute Error\" as \"absolute_error\" is not very clear. - I would appreciate if there was more explanation regarding on using custom loss functions #21614. This way, we could optimize for Mean Absolute Error, Median Absolute Error, Log Cosh, etc. as per the requirement. Note I have tried my best to go through the documentation prior to creating this issue. I am a fresh graduate in Computer Science, and if you believe this issue is not well-framed due to a misunderstanding of my concepts, kindly advise me and I'll work on it. #...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30339"
  },
  {
    "number":31912,
    "text":"Stable extender contract via [CODE] \/ [CODE] resp [CODE] \/ [CODE] separation\n\n### Describe the workflow you want to enable tl;dr, I am suggestion to refactor [CODE] internals to a layer separation with boilerplate between [CODE] and [CODE] resp [CODE] and [CODE] methods, to make extender interfaces more stable. Also see [URL] More background: Currently, every time [CODE] releases a new minor version - e.g., 1.5.0, 1.6.0, 1.7.0 - compliant extensions, e.g., custom transformers, classifiers, etc, break - specifically, referring to the API conformance as tested through [CODE] or [CODE]. These repeated breakages in the \"extender contract\" contrast the stability of the usage contract, which is stable and professionally managed. For a package like [CODE] which means to be a standard not just for ML algorithms but also an API standard that everyone uses, this is not a good state to be in - \"do not break user code\" is the maxim that gets broken for power users writing extensions. Of course maintaining downwards compatibility is not always possible, but nothing should break without a proper warning. ### Describe your proposed solution The [CODE]\/[CODE] separation would ensure stability of the extension contract - and would also allow to build secondary deprecation patterns in relation to it. The (oop) pattern this would implement is the so-called \"template pattern\". It would allow to remove likely changing parts such as the boilerplate (e.g., [CODE] vs [CODE] and such) from the extens...",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"open",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31912"
  },
  {
    "number":28392,
    "text":"Add transformations to target column\n\n### Describe the workflow you want to enable I would like Pipelines to allow the transformation of the target columns of a dataset. Additionally, to complement this, this transformation should be able to be reversed at the end of a pipeline, which is now impossible, as intermediate steps in a pipeline must implement transform, which most models don't implement, implying that there currently cannot be any step to transform the output of a model inside a pipeline. ### Describe your proposed solution This would involve making the transform method on classes like FunctionTransformer accept a 'y' parameter and return the dataset as a tuple (X_new, y_new). ### Describe alternatives you've considered, if relevant If this interferes too much with the existing implementation, a new method could be added that handles only target variables. ### Additional context This comes from a proyect i'm working on where i have to develop a regression model. It turns out transforming the output by applying a logarithm before predicting the output and reverting the transformation after the model gives an answer works really well, but I'm forced to do these steps manually. It would be really useful for me to have this functionality as part of this package. Edit: An aditional use for this is applying normalization to the output of a regression model. If the target value has values that are too high, models like MLPs might have trouble giving satisfactory results. ...",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28392"
  },
  {
    "number":28910,
    "text":"RFC Move [CODE] to \"developer API\" via [CODE]\n\nAs a part of making it easier and more \"standard\" to write scikit-learn estimators by third party developers, we have been slowly developing a \"developer API\" kind of thing, which are useful for third party developers, but not end users of the estimators. Some of the work has been: - [CODE] - [CODE], ... - [CODE] What I'm proposing here, is to create a new [CODE] method instead of the existing [CODE]. We've had a lot of discussions when we designed the current system, which goes through the MRO and the [CODE] _adds_ to the tag set instead of returning the tags. Now the question is do we want to keep the current system or do we want [CODE] to return the estimator's tags instead, and call parent's [CODE] inside itself? As in, instead of: [CODE_BLOCK] to have: [CODE_BLOCK] Inside our tags, we also have some starting with [CODE] such as [CODE], and the question is do we want to make those _public_. cc @scikit-learn\/core-devs",
    "labels":[
      "RFC"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28910"
  },
  {
    "number":29829,
    "text":"\u26a0\ufe0f CI failed on Linux_free_threaded.pylatest_pip_free_threaded (last failure: Sep 13, 2024) \u26a0\ufe0f\n\nCI is still failing on Linux_free_threaded.pylatest_pip_free_threaded Unable to find junit file. Please see link for details.",
    "labels":[
      "Build \/ CI"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29829"
  },
  {
    "number":31224,
    "text":"OneVsRestClassifier when all estimators predict a sample belongs to the other classes\n\n### Describe the bug Hello, I stumbled upon quite a funny case by accident. In OneVsRestClassifier, each classifier predicts whether a sample belongs to a specific class, or to any of the other class. For instance, if you have 3 classes, you will have 3 binary classifiers: - the first one says if the sample belongs to class 1 or to one of the two other classes. - the second one says if the sample belongs to class 2 or to one of the two other classes. - the third one says if the sample belongs to class 3 or to one of the two other classes. However, it creates an edge case where all of the estimators of OneVsRestClassifier predict a specific sample belongs to the other classes: - calling .predict() will mark the sample as belonging to the last class (which is of course wrong since in our example, the 3rd estimator said the sample did not belong in that class). - calling .predict_proba() will return NaNs values. ### Steps\/Code to Reproduce [CODE_BLOCK] ### Expected Results I guess .predict() should return NaNs, and .predict_proba() should return a vector of 0s for that sample. ### Actual Results ```python >>> clf.predict(X) array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/31224"
  },
  {
    "number":28935,
    "text":"VotingClassifier Doesn't work when use CatboostClassifier among estimators\n\n### Describe the bug VotingClassifier Doesn't work when using CatboostClassifier among estimators ### Steps\/Code to Reproduce here is my test case [CODE_BLOCK] ### Expected Results prediction and accuracy of classifier If I replace CatBoost by other classifier, it works perfectly can you help please? ### Actual Results ```python Traceback (most recent call last) Cell In[68], line 32 29 voting_clf.fit(X_train, y_train) 31 # Making predictions ---> 32 y_pred = voting_clf.predict(X_test) 34 # Evaluating the performance 35 accuracy = accuracy_score(y_test, y_pred) File ~\/anaconda3\/lib\/python3....",
    "labels":[
      "Bug"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28935"
  },
  {
    "number":29731,
    "text":"Request for Clarification on the Structure of tree_model._predictors[0][0].nodes in HistGradientBoosting Models\n\n### Describe the issue linked to the documentation Hello, I am currently developing a library for visualizing decision trees, which can be found [URL] I have already implemented support for most of the models in your library and would like to extend this support to the HistGradientBoostingClassifier and HistGradientBoostingRegressor. However, I am encountering difficulties in interpreting the data stored in tree_model._predictors[0][0].nodes. I have reviewed the documentation but could not find detailed explanations for the meaning of each column in this structure. While I have inferred the purpose of most columns, I would like to ensure that I fully understand what each column represents and whether any of them have been transformed by the model. Specifically, I need to know if any processing has been applied that I would need to reverse to use the data accurately in my visualizations. Below is an excerpt of sample data from _predictors[][] .nodes for reference: Classification (few first lines) [CODE_BLOCK] Regression ``` [( 0. , 100, 2, 0.00801952, 1, 1, 4, 1.00977398e+06, 0, 0, 50, 0, 0) (-98.49746157, 51, 3, -0.09872225, 1, 2, 3, 2.83967592e+05, 1, 0, 51, 0, 0) (-16.34897221, 29, 0, 0. , 0, 0, 0, -1.00000000e+00, 2, 1, 0, 0, 0) ( -1.28258454, 22, 0, 0. , 0, 0, 0, -1.000...",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29731"
  },
  {
    "number":29530,
    "text":"Community section: add link to GitHub discussions\n\n### Describe the issue linked to the documentation Can we add a link to GitHub discussions in the footer of the home page? - home page: [URL] - link to add: [URL] ### Suggest a potential alternative\/fix _No response_",
    "labels":[
      "Documentation"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29530"
  },
  {
    "number":27959,
    "text":"PR: Polynomial Chaos Expansions with no responses???\n\n### Describe the workflow you want to enable . ### Describe your proposed solution . ### Describe alternatives you've considered, if relevant _No response_ ### Additional context Why no one comment this PR [URL] ? Its a good work (yes\/no) ? If yes, when will implemented in scikit-learn? in the next scikit-learn 1.4 ? I'm not the author, but i think this work needs any word. The author have a lot of work and time in this PR.",
    "labels":[
      "New Feature"
    ],
    "label_count":1,
    "state":"closed",
    "url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27959"
  }
]