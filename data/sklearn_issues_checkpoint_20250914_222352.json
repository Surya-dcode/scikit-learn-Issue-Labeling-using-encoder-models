{
  "timestamp": "2025-09-14T22:23:52.372225",
  "total_issues": 229,
  "issues": [
    {
      "number": 32178,
      "title": "Trees: impurity decrease calculation is buggy when there are missing values",
      "body": "### Describe the bug\n\nIn decision trees (both classif. and regression), the impurity decrease calculation is sometimes wrong when there are missing values in X.\n\nThis can lead to unexpectedly shallow trees when using `min_impurity_decrease` to control depth.\n\nThis was discovered by investigations started by this issue: #32175\n\n### Steps/Code to Reproduce\n\n```Python\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nX = np.vstack([\n    [0, 0, 0, 0, 1, 2, 3, 4],\n    [1, 2, 1, 2, 1, 2, 1, 2]\n]).swapaxes(0, 1).astype(float)\ny = [0, 0, 0, 0, 1, 1, 1, 1]\n\nn_leaves = []\nfor _ in range(1000):\n    tree = DecisionTreeRegressor(max_depth=1, min_impurity_decrease=0.25).fit(X, y)\n    # all the trees have two leaves\n    assert tree.tree_.n_leaves == 2\n\nX[X == 0] = np.nan\nn_leaves_w_missing = []\nfor _ in range(1000):\n    tree = DecisionTreeRegressor(max_depth=1, min_impurity_decrease=0.25).fit(X, y)\n    n_leaves_w_missing.append(tree.tree_.n_leaves)\n\nprint(np.bincount(n_leaves_w_missing))\n# prints [0 ~500 ~500]\n```\n\nThe last print shows that in approx. half of the cases, the tree has only one leaf (i.e. no split).\n\n### Expected Results\n\nChaning 0 by nan should have no impact on the tree construction in this example.\n\nThe tree should always have one split (and hence two leaves).\n\n### Actual Results\n\nIn approx. half of the cases, the tree has only one leaf (i.e. no split).\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ]\nexecutable: /home/arthur/dev-perso/scikit-learn/sklearn-env/bin/python\n   machine: Linux-6.14.0-29-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.8.dev0\n          pip: None\n   setuptools: 80.9.0\n        numpy: 2.3.3\n        scipy: 1.16.2\n       Cython: 3.1.3\n       pandas: None\n   matplotlib: 3.10.6\n       joblib: 1.5.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_op...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-13T16:12:22Z",
      "updated_at": "2025-09-13T16:12:22Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32178"
    },
    {
      "number": 32176,
      "title": "⚠️ CI failed on Ubuntu_Atlas.ubuntu_atlas (last failure: Sep 13, 2025) ⚠️",
      "body": "**CI failed on [Ubuntu_Atlas.ubuntu_atlas](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79967&view=logs&j=689a1c8f-ff4e-5689-1a1a-6fa551ae9eba)** (Sep 13, 2025)\n- test_fit_transform[98]",
      "labels": [
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-13T03:02:07Z",
      "updated_at": "2025-09-14T03:01:46Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32176"
    },
    {
      "number": 32175,
      "title": "Unexepected behavior of tree splits: missing values handling is buggy?",
      "body": "### Describe the bug\n\nWhen adding a sanity check in the best split function (`_splitter.pxy`), I get a bunch of tests failing. This probably reveal a bug in missing values handling.\n\n### Steps/Code to Reproduce\n\nAdd those lines after [this line](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_splitter.pyx#L517).\n```Python\ncurrent_proxy_improvement = criterion.proxy_impurity_improvement()\nif current_proxy_improvement < best_proxy_improvement - 1:\n    raise ValueError(f\"Unconsistent improvement {current_proxy_improvement} < {best_proxy_improvement}\" )\n```\n\nAnd then run the following tests: `pytest sklearn/ensemble/tests/test_forest.py sklearn/tree/tests/`\n\n### Expected Results\n\nNo error is thrown, proving the final partitionning of samples is optimal and the children impurities are the correct/optimal ones.\n\n### Actual Results\n\nMany tests fail. I will split them into three categories, based on the alleged cause:\n\n\n1. **MAE criterion**\nErrors that have likely the same cause to than those two issues: #32099 #10725. The current implementation of the MAE criterion is slightly buggy. My PR https://github.com/scikit-learn/scikit-learn/pull/32100 will fix it.\n\n```\nFAILED sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-absolute_error-float64] - ValueError: Unconsistent improvement -9.0 < -4.0\nFAILED sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-absolute_error-float32] - ValueError: Unconsistent improvement -9.0 < -4.0\n```\n\nOn the branch of my PR those tests don't fail.\n \n2. **Missing values**\nMany tests related to missing values are failing. As explained in my PR https://github.com/scikit-learn/scikit-learn/pull/32119, the current way missing values are handled is a bit convoluted, and probably a bit buggy\n\n```\nFAILED sklearn/ensemble/tests/test_forest.py::test_missing_values_is_resilient[make_regression-ExtraTreesRegressor] - ValueError: Unconsistent improvement 222739.5025534111 < 230516.59488172...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-12T21:41:27Z",
      "updated_at": "2025-09-13T15:53:56Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32175"
    },
    {
      "number": 32174,
      "title": "Fix rendering of D2 Brier score section in User Guide",
      "body": "_This is an issue for a contributor who has worked with rst and sphinx documentation before, or who wants to spend 10 hours to learn on the task. It is not suitable for ai agents._\n\nThe newly added section about **D2 Brier score** (added via #28971) doesn't render correctly in the User Guide.\nIn the 1.8 dev version documentation it renders as \n\n```\n|details-start| D2 Brier score |details-split|\n...\n|details-end|\n```\n\nWe probably need to use .`. dropdown::` like in the section above.\n\nMaybe @elhambb, do you want to take care of it?\nAlso @star1327p or @EmilyXinyi, if that's not too boring for you.",
      "labels": [
        "Easy",
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-09-12T20:16:03Z",
      "updated_at": "2025-09-13T17:46:17Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32174"
    },
    {
      "number": 32171,
      "title": "⚠️ CI failed on Wheel builder (last failure: Sep 14, 2025) ⚠️",
      "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/17706003886)** (Sep 14, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-12T15:46:41Z",
      "updated_at": "2025-09-14T15:57:57Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32171"
    },
    {
      "number": 32168,
      "title": "Unexpected behavior when combining FunctionTransformer and pandas DataFrames",
      "body": "### Describe the bug\n\nWhen using a pandas `DataFrame` with the `FunctionTransformer(func=..., feature_names_out='one-to-one')` on data with the wrong column order, the column names are mixed. While this behavior might be derived from the documentation, it still felt unexpected.\n\n### Steps/Code to Reproduce\n\n```py\nfrom sklearn.preprocessing import FunctionTransformer\nimport pandas as pd\n\nnewdf = pd.DataFrame({\"a\": [1,2], \"b\": [True, False], \"c\": [\"x\", \"y\"]})\nnewdf_shuffled_cols = newdf[[\"c\", \"a\", \"b\"]]\n\ndef testfun(x):\n    return x\n\n# From the docs:\n# If func returns an output with a columns attribute, then the columns is \n# enforced to be consistent with the output of get_feature_names_out.\n\nfunctrans = FunctionTransformer(func=testfun, feature_names_out='one-to-one')\nfunctrans.fit(newdf)\ntransformed = functrans.transform(newdf_shuffled_cols)\nprint(newdf[\"c\"])\nprint(transformed[\"c\"])\n```\n\n### Expected Results\n\nThe same column is returned.\n\n### Actual Results\n\n```\n0    x\n1    y\nName: c, dtype: object\n0     True\n1    False\nName: c, dtype: bool\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.7 | packaged by conda-forge | (main, Sep  3 2025, 14:30:35) [GCC 14.3.0]\nexecutable: /opt/conda/envs/sklearn_burg/bin/python3\n   machine: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.7.2\n          pip: 25.2\n   setuptools: 80.9.0\n        numpy: 2.3.3\n        scipy: 1.16.1\n       Cython: None\n       pandas: 2.3.2\n   matplotlib: None\n       joblib: 1.5.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 28\n         prefix: libopenblas\n       filepath: /opt/conda/envs/sklearn_burg/lib/libopenblasp-r0.3.30.so\n        version: 0.3.30\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 28\n         prefix: libgomp\n       filepath: /opt/conda/envs/sklearn_burg/lib/libgomp.so.1.0.0\n      ...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-12T11:05:12Z",
      "updated_at": "2025-09-12T11:12:17Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32168"
    },
    {
      "number": 32167,
      "title": "`permutation_importance` errors with `polars` dataframe and `ColumnTransformer`",
      "body": "### Describe the bug\n\nHaving polars dataframe with `ColumnTransformer` lets `permutation_importance` crash.\n\nMaybe related to the warnings reported in this issue https://github.com/scikit-learn/scikit-learn/issues/28488. But here, `permuation_importance` errors.\n\n### Steps/Code to Reproduce\n\nA MWE\n```\nimport polars as pl\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\n\nX, y = make_classification(n_samples=100, n_features=5, random_state=42)\nfeature_names = [f'feature_{i}' for i in range(X.shape[1])]\n\ndf = pl.DataFrame({name: X[:, i] for i, name in enumerate(feature_names)})\ndf = df.with_columns(pl.Series(\"target\", y))\n\nX_train, X_test, y_train, y_test = train_test_split(df.select(feature_names), df['target'], test_size=0.2, random_state=42)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('scaler', StandardScaler(), feature_names)  # using feature names here\n    ]\n)\npreprocessor.set_output(transform=\"polars\")\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])\n\nmodel = pipeline.fit(X_train, y_train)\n\npermutation_importance(model, X_test, y_test)\n```\n\n### Expected Results\n\nNo error is shown.\n\n### Actual Results\n\n```\n.../python3.12/site-packages/sklearn/utils/_indexing.py:259, in _safe_indexing(X, indices, axis)\n    248     raise ValueError(\n    249         \"'X' should be a 2D NumPy array, 2D sparse matrix or \"\n    250         \"dataframe when indexing the columns (i.e. 'axis=1'). \"\n    251         \"Got {} instead with {} dimension(s).\".format(type(X), len(X.shape))\n    252     )\n    254 if (\n    255     axis == 1\n    256     and indices_dtype == \"str\"\n    257     and not (_is_pandas_df(X) or _use_interchange_p...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-09-12T07:59:26Z",
      "updated_at": "2025-09-12T13:47:53Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32167"
    },
    {
      "number": 32162,
      "title": "Deprecate n_jobs in LogisticRegression and evaluate multi-threading",
      "body": "When https://github.com/scikit-learn/scikit-learn/pull/32073 is merged, `n_jobs` will have no effect in `LogisticRegression` so should be deprecated.\n\nIt's also a good time to consider enabling multi-threading to compute the logistic regression path here https://github.com/scikit-learn/scikit-learn/blob/21e0df780772e9567b09249a05a42dfde6de465d/sklearn/linear_model/_logistic.py#L1373-L1388\n\nSo far it was disabled because parallelism was happening at a higher level using joblib. Now we should benchmark the impact of enabling multi-threading to see if it's positive for all the solvers and a wide range of problems.\nLike for other estimators using OpenMP-based multi-threading, the number of thread should not be controlled by `n_jobs` but instead use all available core (`n_threads = _openmp_effective_n_threads()`).",
      "labels": [
        "API",
        "Needs Benchmarks"
      ],
      "state": "open",
      "created_at": "2025-09-11T15:06:38Z",
      "updated_at": "2025-09-12T16:33:37Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32162"
    },
    {
      "number": 32161,
      "title": "Add an option to OrdinalEncoder to sort encoding by decreasing frequencies",
      "body": "### Describe the workflow you want to enable\n\nAt the moment, when no categories are provided, the default ordering of categories for a given categorical column is based on the lexicographical ordering of the categories observed in the training set.\n\nHowever, this is quite arbitrary and one could instead provide an option to encode such values based on their observed frequency in the training set.\n\nThe motivation would be to nudge the inductive bias of tree-based models (and models that favor axis aligned decision functions) into separating nominal from rare values more easily (e.g. fewer splits in a tree). This might be especially useful for outlier detection models such as `IsolationForest`.\n\nRelated to #15796.\n\n### Describe your proposed solution\n\n\nExtend the `categories` option to have:\n\n- `categories=\"lexigraphical\"` (default for backward compat);\n- `categories=user_provided_list` (same as today);\n- `categories=\"frequency\"` (the new option).\n\nIf `categories=\"frequency\"`, then the generated category encodings would be:\n\n- 0 would encode the most frequent category observed in the training set,\n- 1 the second most frequent,\n- ...\n- and so on until the least frequent categories.\n\nTie breaking could be based on the lexicographical order to ensure that the behavior of `OrdinalEncoder` remains invariant under a shuffling of the rows of the training set.\n\n\n### Describe alternatives you've considered, if relevant\n\n- Use `TargetEncoder` that leverages some frequency info (mixed with the mean value of the target variable), but this is a supervised method and would therefore not be suitable for unsupervised anomaly detection tasks, for instance.\n\n- Introduce a new dedicated class, e.g. `FrequencyEncoder`.\n  - pro: allow using the observed relative frequency (between 0 and 1) as value (but would collapse equally frequent categories into the same numerical value).\n  - con: introduces yet another estimator class in our public API.\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-09-11T14:27:27Z",
      "updated_at": "2025-09-13T17:43:47Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32161"
    },
    {
      "number": 32155,
      "title": "ColumnTransformer.fit() fails on polars.DataFrame: AttributeError: 'DataFrame' object has no attribute 'size'",
      "body": "### Describe the bug\n\nFitting a sklearn.compose.ColumnTransformer with *more than one* transformer on a polars.DataFrame yields the error:\n\n> AttributeError: 'DataFrame' object has no attribute 'size'\n\n* Fitting works fine when converting the DataFrame to pandas beforehand\n* Fitting also works fine with a *polars* DataFrame for as long as only a *single* transformer is passed to ColumnTransformer\n\nI am using the latest stable version of sklearn (1.7.2) and polars (1.33.1).\n\nThank you so much for looking into this!\n\n### Steps/Code to Reproduce\n\n```python\nimport polars as pl\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n## Generate toy data (polars DataFrame)\ndf = pl.DataFrame({\n    'some_categories': list('abc'),\n    'some_numbers': range(3)\n})\n\nprint(df)\nshape: (3, 2)\n┌─────────────────┬──────────────┐\n│ some_categories ┆ some_numbers │\n│ ---             ┆ ---          │\n│ str             ┆ i64          │\n╞═════════════════╪══════════════╡\n│ a               ┆ 0            │\n│ b               ┆ 1            │\n│ c               ┆ 2            │\n└─────────────────┴──────────────┘\n\n## Define a ColumnTransformer and fit on polars df -> AttributeError\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), ['some_categories']),\n        ('num', 'passthrough', [\"some_numbers\"])\n    ])\n\n## Fit on polars df\npreprocessor.fit(df) ## AttributeError: 'DataFrame' object has no attribute 'size'\n\n## Fit on pandas df\npreprocessor.fit(df.to_pandas()) ## works fine for pandas df\n\n\n## Define ColumnTransformers with only one transformer each and fit on polars df -> works fine\npreprocessor1 = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), ['some_categories'])\n    ])\n\npreprocessor2 = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', [\"some_numbers\"])\n    ])\n\npreprocessor1.fit(df) ## works\npreproce...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-09-11T07:51:18Z",
      "updated_at": "2025-09-11T10:15:47Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32155"
    },
    {
      "number": 32154,
      "title": "GridSearchCV cannot obtain the optimal parameter results.",
      "body": "### Describe the bug\n\nWhen I used GridSearchCV to obtain the optimal parameters, I found that different cross-validation folds produced inconsistent results.\n\n### Steps/Code to Reproduce\n\n```python3\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.7, random_state=0)\nparam_grid_rf = {'n_estimators': [5, 10, 15, 35, 50, 70]}\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\nprint(f'{roc_auc_score(y_test, y_proba):.4f}')\n```\n> auc: 0.8859\nbest_params is 50\n\n```python3\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.7, random_state=0)\nparam_grid_rf = {'n_estimators': [50, 70, 80, 90]}\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\nprint(f'{roc_auc_score(y_test, y_proba):.4f}')\n```\n> auc: 0.8725\nbest_params is 80\n\n### Expected Results\n\nBoth results above should be identical.\n>auc: 0.8859\nbest_params is 50\n\n### Actual Results\n\nBetween the two parameter lists, we should obtain the parameters with the best AUC, not inconsistent ones.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\nexecutable: /work/users/suny/mm/.venv/bin/python\n   machine: Linux-5.15.0-143-generic-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.7.2\n          pip: 25.2\n   setuptools: 80.9.0\n        numpy: 2.2.6\n        scipy: 1.15.3\n       Cython: None\n       pandas: 2.3.2\n   matplotlib: 3.10.6\n       joblib: 1.5.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 64\n         prefix: libscipy_openblas\n       filepath: /work/users/suny/mm/.venv/lib/python3.10/site-packages/numpy.libs/libscipy_openblas64_-56d6093b.so\n        version: 0.3.29\nthreading_layer: pthreads\n ...",
      "labels": [
        "Bug",
        "Needs Info",
        "Needs Reproducible Code"
      ],
      "state": "open",
      "created_at": "2025-09-11T07:34:48Z",
      "updated_at": "2025-09-11T15:54:05Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32154"
    },
    {
      "number": 32153,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Sep 11, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79876&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Sep 11, 2025)\n- Test Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-11T02:34:50Z",
      "updated_at": "2025-09-11T14:23:54Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32153"
    },
    {
      "number": 32152,
      "title": "Allow `categories` parameter in `OrdinalEncoder` to accept a dict of column names → categories",
      "body": "### Describe the workflow you want to enable\n\nCurrently, the `categories` parameter in `OrdinalEncoder` only accepts:\n\n- `\"auto\"`, or\n- a list of lists, where the position of each list corresponds to the order of columns in the input.\n\nThis makes it _somewhat inconvenient_ when working with pandas DataFrames, since one must manually align the category lists with the column order.\n\n### Describe your proposed solution\n\nAllow `categories` to also accept a dictionary mapping column names to category lists. For example:\n\n```python\nencoder = OrdinalEncoder(categories={\n    \"size\": [\"small\", \"medium\", \"large\"],\n    \"priority\": [\"low\", \"medium\", \"high\"]\n})\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n#### Motivation\n\n- Improves ergonomics when working with pandas (very common in scikit-learn pipelines).\n- Reduces potential bugs from mismatched column ordering.\n- Makes the API consistent with the way many users already think about preprocessing (column → transformation).",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-09-10T16:38:59Z",
      "updated_at": "2025-09-11T17:05:30Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32152"
    },
    {
      "number": 32150,
      "title": "Latex not correctly rendered for ridge",
      "body": "### Describe the issue linked to the documentation\n\nDescription\nIn the online API reference, formula is displayed as:\n`||y - Xw||^2_2 + alpha * ||w||^2_2`\n\nit should instead look like\n\n<img width=\"299\" height=\"77\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f2f77288-d7a6-4ece-8f06-4e2b076c032d\" />\n\nSteps to Reproduce the Issue:\nPlease see [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html](url)\n\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-10T15:09:57Z",
      "updated_at": "2025-09-10T22:28:30Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32150"
    },
    {
      "number": 32146,
      "title": "Unexpected behavior of the HTML repr of meta-estimators",
      "body": "Here's a list of some unexpected behaviors of the HTML repr of meta-estimators\n- `Pipeline` doesn't display its named steps\n\n  <img width=\"164\" height=\"95\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c368188d-544f-4bb1-bfeb-d76490f5146a\" />\n\n  This was maybe intentional ? In comparison `FeatureUnion` does for instance\n\n  <img width=\"282\" height=\"91\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2a6ded8c-d148-4852-a03c-0a88c1b8bc49\" />\n\n- a `Pipeline` in a meta-estimator doesn't render properly; it doesn't have the dashed border\n  \n  <img width=\"186\" height=\"120\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/471ce468-8597-4fbc-b718-870c4f11a1fd\" />\n\n  Another meta-estimator renders properly\n\n  <img width=\"294\" height=\"116\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cb467dcd-63c2-4a54-b92a-d6686b3b3bc8\" />\n\n- transformers of `ColumnTransformer` are expandable to show the selected columns, but there's no additional info. I think it should be explicit that these are the selected columns.\n\n  <img width=\"274\" height=\"106\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e44d47fb-c784-4631-9a73-3d274671430e\" />\n\n  Note that this could be fixed by https://github.com/scikit-learn/scikit-learn/pull/31937\n\n- When the inner estimator of a meta-estimator is not a meta-estimator itself, it's expandable but the dropdown is not useful anymore (it's the non-html repr of the estimator basically):\n\n  <img width=\"169\" height=\"129\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/21723076-ae7e-46e3-8d22-3561aef1b1ec\" />\n\n  Now that we have the parameter table, it's not useful anymore to have the additional repr which is redundant and less informative. I'd be in favor of removing the dropdown\n\n- When the inner estimator of a meta-estimator is a meta-estimator itself, it's expandable but sometimes there's no dropdown or the dropdown is the non-html repr of the meta-estimator, and there's no parameter table....",
      "labels": [
        "Documentation",
        "frontend"
      ],
      "state": "open",
      "created_at": "2025-09-10T10:15:18Z",
      "updated_at": "2025-09-11T09:19:16Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32146"
    },
    {
      "number": 32145,
      "title": "New min dependencies broke the doc build",
      "body": "We didn't run a dock build before merging https://github.com/scikit-learn/scikit-learn/pull/31656 which bumps min dependencies, and it broke the CI, see https://app.circleci.com/pipelines/github/scikit-learn/scikit-learn/70723/workflows/2064650c-116a-405d-9b1d-9cd469b8804f/jobs/317740.\n\ncc/ @GaetandeCast @ogrisel",
      "labels": [
        "Build / CI",
        "Blocker"
      ],
      "state": "closed",
      "created_at": "2025-09-10T09:46:01Z",
      "updated_at": "2025-09-11T12:14:03Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32145"
    },
    {
      "number": 32125,
      "title": "Tree module - Broken test: test fails when changing random_state=0 to =1",
      "body": "In the test `tree/tests/test_tree.py::test_regression_tree_missing_values_toy`, in this [line](https://github.com/scikit-learn/scikit-learn/blob/0033630cd35d5945ea8c1b5beff6efe9583cd523/sklearn/tree/tests/test_tree.py#L2695C1-L2696C1):\n\n```\n    tree = Tree(criterion=criterion, random_state=0).fit(X, y)\n```\n\nif you change `random_state=0` to `random_state=1` all the tests with `Tree` being `ExtraTreeRegressor` fails.\n\nIt is completly logical when you look at the code: when there are missing values the random split (which is what `ExtraTreeRegressor`  does) *randomly* put them to the left or the right. The *randomly* part here is not compatible the test logic. It's a lucky 1/16 chance in the choice of the random_seed that made this test passed until now (I ran the test with 1000 seeds, and it's indeed 1/16, because it's tested on 4 arrays, hence proba = 1/2^4).\n\nI'm willing to open a PR to fix this. My suggestion is to stop running this test on `ExtraTreeRegressor`, there are alreay many tests on `ExtraTreeRegressor`s and the issues linked in the docstring of this test aren't mentionning `ExtraTreeRegressor` anywhere.",
      "labels": [
        "module:tree"
      ],
      "state": "closed",
      "created_at": "2025-09-07T18:33:08Z",
      "updated_at": "2025-09-11T13:26:04Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32125"
    },
    {
      "number": 32122,
      "title": "⚠️ CI failed on Wheel builder (last failure: Sep 07, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/17523435997)** (Sep 07, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-07T10:00:35Z",
      "updated_at": "2025-09-08T05:15:52Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32122"
    },
    {
      "number": 32121,
      "title": "\"Improve documentation on FeatureUnion behavior with polars DataFrame output causing duplicate column names\"",
      "body": "### Describe the issue linked to the documentation\n\nThe current documentation for FeatureUnion describes its behavior with pandas DataFrames but does not mention how it behaves when used with polars DataFrames. Specifically, FeatureUnion concatenates outputs of its transformers before the set_output wrapper renames columns based on get_feature_names_out. This works fine with pandas but causes issues with polars since polars does not allow creating a DataFrame with duplicate column names. This leads to errors when using FeatureUnion with polars outputs.\n\n### Suggest a potential alternative/fix\n\nThe documentation should mention the behavior difference when using FeatureUnion with polars DataFrames, specifically that concatenation occurs before column renaming. This can cause duplicate column names errors in polars.\n\nIt would be helpful to add a warning or note about this limitation, and suggest possible workarounds, such as manually renaming columns or converting to pandas DataFrame before using FeatureUnion.\n\nIncluding a minimal example demonstrating the issue and how to avoid it would further improve clarity for users.",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-07T07:41:58Z",
      "updated_at": "2025-09-08T07:39:38Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32121"
    },
    {
      "number": 32115,
      "title": "[ENH] Adding KModes and KPrototypes clustering algorithms",
      "body": "### Describe the workflow you want to enable\n\nCurrently, scikit-learn users working with datasets that contain categorical features (e.g., `country`, `profession`, `product_type`) face a significant hurdle. The standard practice is to use one-hot encoding before applying algorithms like K-Means.\n\nThis workflow is problematic because:\n1.  **High-Dimensionality:** It drastically increases the dimensionality of the dataset (the \"curse of dimensionality\").\n2.  **Sparsity:** It creates a sparse matrix that is computationally inefficient and poorly suited for distance-based algorithms like K-Means, which are designed for dense, numerical data.\n3.  **Interpretability:** The resulting clusters are based on a transformed version of the data, making the centroids and the cluster logic difficult to interpret in terms of the original categorical features.\n\nThe workflow I want to enable is a seamless and native experience for clustering categorical and mixed data:\n*   **For fully categorical data:** A user should be able to call `KModes(n_clusters=5).fit(X_categorical)` directly, without any pre-processing.\n*   **For mixed data types:** A user should be able to call `KPrototypes(n_clusters=5, categorical=[0, 2]).fit(X_mixed)`, where they simply specify which columns are categorical. The algorithm would then automatically use an appropriate dissimilarity measure for each data type.\n\nThis integrates categorical clustering directly into the robust and familiar scikit-learn API, eliminating the need for external dependencies and inefficient pre-processing.\n\n### Describe your proposed solution\n\nI propose implementing the well-established K-Modes and K-Prototypes algorithms as new classes within the `sklearn.cluster` module. These algorithms are the canonical equivalents of K-Means for categorical and mixed data, respectively.\n\n**Proposed API Design (following scikit-learn conventions):**\n\n```python\nclass KModes(BaseEstimator, ClusterMixin):\n    \"\"\"\n    K-Modes clustering for categori...",
      "labels": [
        "New Feature",
        "module:cluster",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-09-05T13:02:23Z",
      "updated_at": "2025-09-12T11:51:30Z",
      "comments": 9,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32115"
    },
    {
      "number": 32112,
      "title": "RFC Deprecate FeatureUnion and make_union",
      "body": "Unless I'm missing something, to me `FeatureUnion` is just a `ColumnTransformer` where all transformers are applied to all features. So it's just a special case of `ColumnTransformer`.\n\n```py\nimport pandas as pd\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [1, 2, 3, 4]})\n\nfu = FeatureUnion([(\"std_1\", StandardScaler()), (\"std_2\", StandardScaler())])\nfu.set_output(transform=\"pandas\")\nprint(fu.fit_transform(df))\n   std_1__a  std_1__b  std_2__a  std_2__b\n0 -1.341641 -1.341641 -1.341641 -1.341641\n1 -0.447214 -0.447214 -0.447214 -0.447214\n2  0.447214  0.447214  0.447214  0.447214\n3  1.341641  1.341641  1.341641  1.341641\n\nall_cols = slice(None)\nct = ColumnTransformer([(\"std_1\", StandardScaler(), all_cols), (\"std_2\", StandardScaler(), all_cols)])\nct.set_output(transform=\"pandas\")\nprint(ct.fit_transform(df))\n   std_1__a  std_1__b  std_2__a  std_2__b\n0 -1.341641 -1.341641 -1.341641 -1.341641\n1 -0.447214 -0.447214 -0.447214 -0.447214\n2  0.447214  0.447214  0.447214  0.447214\n3  1.341641  1.341641  1.341641  1.341641\n```\n\nIn addition, the parameters of `FeatureUnion` is a subset of the parameters of `ColumnTransformer`, so I don't see anything that one would be able to do with `FeatureUnion` but not with `ColumnTransformer`.\n\nFrom a maintenance view, it duplicates the burden because they share almost no code and it's common that they suffer from the same bugs and  fixes have to be repeated in both classes. And usually one or the other keeps the bug for a while because we only implement a fix for one and forget about the other. In general the forgotten one is `FeatureUnion` btw :) (e.g. the latest one https://github.com/scikit-learn/scikit-learn/issues/32104 for `FeatureUnion` that was detected and fixed a while ago for `ColumnTransformer` https://github.com/scikit-learn/scikit-learn/issues/28260)\n\nFinally, `FeatureUnion` has unresolved long st...",
      "labels": [
        "API",
        "RFC",
        "module:compose",
        "module:pipeline"
      ],
      "state": "open",
      "created_at": "2025-09-05T11:27:26Z",
      "updated_at": "2025-09-08T14:43:07Z",
      "comments": 14,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32112"
    },
    {
      "number": 32110,
      "title": "Optimize Performance of SGDOptimizer and AdamOptimizer with Vectorized Operations",
      "body": "### Describe the workflow you want to enable\n\nI aim to enable a more efficient training workflow for Multilayer Perceptrons (MLPs) in scikit-learn by optimizing the performance of the `SGDOptimizer` and `AdamOptimizer` classes. Currently, these optimizers use list comprehensions in their `_get_updates` methods to compute parameter updates, which can be computationally expensive for large neural networks with many parameters (e.g., hidden layers with thousands of neurons). The proposed vectorized operations will allow users to train larger MLPs faster, particularly on datasets requiring extensive iterations, without altering the existing API or user experience. Additionally, the optimization will address redundant computations in `SGDOptimizer` when using Nesterov’s momentum, further improving training speed. This enhancement will benefit users working on deep learning tasks within scikit-learn, such as image classification or regression with complex models, by reducing training time and improving scalability.\n\n### Describe your proposed solution\n\nTo optimize the performance of `SGDOptimizer` and `AdamOptimizer`, I propose the following changes to `sklearn/neural_network/_stochastic_optimizers.py`:\n\n1. **Vectorized Operations**:\n   - Replace list comprehensions in `_get_updates` with in-place NumPy vectorized operations. This will leverage NumPy’s optimized C-based implementation, reducing Python loop overhead. For example, in `AdamOptimizer._get_updates`, the current list comprehension for updating first and second moments can be replaced with a single vectorized operation across all parameters.\n   - Example implementation for `AdamOptimizer._get_updates`:\n ```python\n\ndef _get_updates(self, grads: List[np.ndarray]) -> List[np.ndarray]:\n         self.t += 1\n         lr_t = self.learning_rate_init * np.sqrt(1 - self.beta_2**self.t) / (1 - self.beta_1**self.t)\n         for m, v, grad in zip(self.ms, self.vs, grads):\n             np.multiply(self.beta_1, m, out=m)\n     ...",
      "labels": [
        "Performance",
        "Needs Benchmarks",
        "module:neural_network"
      ],
      "state": "open",
      "created_at": "2025-09-05T09:29:33Z",
      "updated_at": "2025-09-05T18:44:23Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32110"
    },
    {
      "number": 32109,
      "title": "Add inner max_iter or a smart automatic setting to Lasso inside graphical lasso",
      "body": "`GraphicalLasso` and `GraphicalLassoCV` expose `enet_tol`. They should also expose `enet_max_iter`.\nCurrently, the `max_iter` of the *outer iteration* is also used for this inner iteration. This is unfortunate, e.g., if you set a small number of outer iterations.\n\nPopped up in https://github.com/scikit-learn/scikit-learn/pull/31987#discussion_r2324154906.",
      "labels": [
        "Enhancement",
        "API",
        "Needs Decision",
        "module:covariance",
        "module:linear_model"
      ],
      "state": "open",
      "created_at": "2025-09-05T07:00:22Z",
      "updated_at": "2025-09-14T09:29:07Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32109"
    },
    {
      "number": 32104,
      "title": "FeatureUnion with polars output can error due to duplicate column names",
      "body": "### Describe the bug\n\nFeatureUnion concatenates outputs of its transformers _before_ the `set_output` wrapper renames columns based on `get_feature_names_out` (adding the transformer name prefix). This works with pandas but not polars which does not allow creating a dataframe with duplicate feature names\n\nin addition to the reproducer below, `pytest sklearn/tests/test_pipeline.py::test_feature_union_set_output` fails if we replace \"pandas\" with \"polars\" in the test\n\n### Steps/Code to Reproduce\n\n```python\nimport polars as pl\nimport pandas as pd\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [1, 2, 3, 4]})\nfu = FeatureUnion([(\"std_1\", StandardScaler()), (\"std_2\", StandardScaler())])\nfu.set_output(transform=\"pandas\")\nfu.fit_transform(df) # OK\n\n# result:\n#\n#    std_1__a  std_1__b  std_2__a  std_2__b\n# 0 -1.341641 -1.341641 -1.341641 -1.341641\n# 1 -0.447214 -0.447214 -0.447214 -0.447214\n# 2  0.447214  0.447214  0.447214  0.447214\n# 3  1.341641  1.341641  1.341641  1.341641\n\ndf = pl.from_pandas(df)\nfu.set_output(transform=\"polars\")\nfu.fit_transform(df) # ERROR during hstack step as both transformers have output column names ['a', 'b']\n\n# error:\n# Traceback (most recent call last):\n#     ...\n# polars.exceptions.DuplicateError: column with name 'a' has more than one occurrence\n```\n\n### Expected Results\n\ndataframe with column names `std_1__a  std_1__b  std_2__a  std_2__b`\n\n### Actual Results\n```\nTraceback (most recent call last):\n  File \".../feature_union.py\", line 26, in <module>\n    fu.fit_transform(df) # ERROR during hstack step as both transformers have output column names ['a', 'b']\n    ~~~~~~~~~~~~~~~~^^^^\n  File \".../scikit-learn/sklearn/utils/_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \".../scikit-learn/sklearn/pipeline.py\", line 1970, in fit_transform\n    return self._hstack(Xs)\n           ~~~~~~~~~~~~^^^^\n  File \".../scikit-learn/s...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-09-04T10:09:30Z",
      "updated_at": "2025-09-07T14:10:20Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32104"
    },
    {
      "number": 32099,
      "title": "DecisionTreeRegressor with absolute error criterion: non-optimal split",
      "body": "### Describe the bug\n\nWhile working on fixing the issue https://github.com/scikit-learn/scikit-learn/issues/9626, I noticed that in some cases, the current implementation of `DecisionTreeRegressor(criterion=\"absolute_error\")` doesn't not find the optimal split in some cases, when sample weights are given.\n\nIt seems to only happen with a small number of points, and the chosen split is not too far from the optimal split.\n\nMy PR for https://github.com/scikit-learn/scikit-learn/issues/9626 will fix this one too. I'm openning this issue only to document the current behavior.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef abs_error_of_a_leaf(y, w):\n    return min((np.abs(y - yi) * w).sum() for yi in y)\n\ndef abs_error_of_leaves(leaves, y, w):\n    return sum(abs_error_of_a_leaf(y[leaves == i], w[leaves == i]) for i in np.unique(leaves))\n\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 1, 3, 1, 2])\nw = np.array([3., 3., 2., 1., 2.])\n\nreg = DecisionTreeRegressor(max_depth=1, criterion='absolute_error')\nsk_leaves = reg.fit(X, y, sample_weight=w).apply(X)\nprint(\"leaves:\", sk_leaves, \"total abs error:\", abs_error_of_leaves(sk_leaves, y, w))\n# prints [1 1 1 1 2] and 4.0\n# If you look at the values of X, y, w, it's easy enough to doubt this split is the best\n\nexpected_leaves = np.array([1, 1, 2, 2, 2])\nprint(\"total abs error:\", abs_error_of_leaves(expected_leaves, y, w))\n# prints 3.0 => indeed, the split returned by sklearn is not the best\n```\n\n### Expected Results\n\nChooses a split that minimizes the AE.\n\n### Actual Results\n\nPrints:\n```\nleaves: [1 1 1 1 2] total abs error: 4.0\ntotal abs error: 3.0\n```\n\nShowing the chosen split is not optimal.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.11 (main, Aug 18 2025, 19:19:11) [Clang 20.1.4 ]\nexecutable: /home/arthur/dev-perso/fast-mae-split/.venv/bin/python\n   machine: Linux-6.14.0-29-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.7.1\n     ...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-09-03T17:48:02Z",
      "updated_at": "2025-09-08T14:38:40Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32099"
    },
    {
      "number": 32095,
      "title": "Using `fetch_20newsgroups` with multiple pytest workers leads to race",
      "body": "### Describe the bug\n\nWhen using `pytest-xdist` with several workers to run a test suite that uses `fetch_20newsgroups` as a fixture (`scope=\"session\") the dataset shape is sometimes wrong. For example I just had a run where `X.shape=(5902, 68435) y.shape=(5902,)` - it should be something like ~11000 samples for the \"train\" subset.\n\nSome test functions will see the \"shorter\" dataset and some the full dataset.\n\nI think the problem is caused by using `pytest-xdist` where each worker will download the dataset itself. The download itself will work in parallel (though wasteful) but then when the file gets `shutil.move`d to the final location things get broken? Or one of the workers sees a partial file somehow.\n\nThis is the relevant code\n\nhttps://github.com/scikit-learn/scikit-learn/blob/be9dd4d4c1f03b8d27311f2d43fcb3c88bdea55c/sklearn/datasets/_base.py#L1499\n\nI'm wondering if the fix is to not use `NamedTempFile` to create the filename to download to, but instead use a name like `fname + '.part'` and check if that file exists before starting a download. That way only one process would start downloading the file.\n\nThe problem is that we would need a `check_or_create(path)` function that will perform the check and creation of a path in an atomic operation. Not sure that exists :-/\n\n### Steps/Code to Reproduce\n\nIf you put this snippet into a file and run it with `python your_file.py <n_procs>` it reproduces a different version (I think) of the problem.\n\n```python\nimport multiprocessing as mp\nimport random\nimport sys\nimport time\n\nfrom sklearn.datasets import fetch_20newsgroups\n\n\ndef fetch_data(i):\n    time.sleep(random.random())\n    data = fetch_20newsgroups(subset=\"train\", shuffle=True, random_state=42)\n    return (i, len(data.data))\n\nif __name__ == \"__main__\":\n    n_processes = int(sys.argv[1])\n\n    with mp.Pool(processes=n_processes) as pool:\n        results = pool.map(fetch_data, range(n_processes))\n    print(results)\n```\n\nMake sure to delete the 20newsgroups file(s) fro...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-09-03T14:02:07Z",
      "updated_at": "2025-09-04T14:56:32Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32095"
    },
    {
      "number": 32090,
      "title": "Unpickling ColumnTransformer fitted in 1.6.1 fails in 1.7.1 with AttributeError: _RemainderColsList",
      "body": "### Describe the bug\n\n**Summary** \n\nA `ColumnTransformer` pickled with **scikit-learn 1.6.1** cannot be unpickled with **1.7.1** (and other versions > 1.6.1). The unpickling fails before any method call with:\n\n```bash\nAttributeError: Can't get attribute '_RemainderColsList' on <module 'sklearn.compose._column_transformer' from '.../site-packages/sklearn/compose/_column_transformer.py'>\n```\n\nThis makes it impossible to load persisted pipelines across these versions when ColumnTransformer was used.\n\n### Steps/Code to Reproduce\n\n## Run it with scikit-learn==1.6.1\n```\nimport pickle\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Minimal toy data\ndf = pd.DataFrame({\"num\": [1.0, 2.0, 3.5], \"cat\": [\"a\", \"b\", \"a\"]})\n\n# Minimal ColumnTransformer\nct = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), [\"num\"]),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"cat\"]),\n    ], remainder=\"passthrough\"\n)\n\n# Fit and pickle\nct.fit(df)\nwith open(\"column_transformer.pkl\", \"wb\") as f:\n    pickle.dump(ct, f)\n\n```\n## Run with scikit-learn > 1.6.1\n```\nimport pickle\nimport pandas as pd\n\n# Unpickle the fitted transformer\nwith open(\"column_transformer.pkl\", \"rb\") as f:\n    ct = pickle.load(f)\n\n# Use on small test data (includes an unseen category \"c\")\ndf2 = pd.DataFrame({\"num\": [0.0, 1.0], \"cat\": [\"a\", \"c\"]})\nX = ct.transform(df2)\n\n```\n\n### Expected Results\n\nA ColumnTransformer fitted and persisted in 1.6.1 can be loaded in 1.7.1 and used normally (e.g., transform), or—if cross-version unpickling is intentionally unsupported—clear guidance in release notes and/or a compatibility shim to avoid a hard failure on import.\n\n### Actual Results\n\nUnpickling fails immediately with AttributeError (below), seemingly because a private helper `_RemainderColsList` referenced in the pickle no longer exists / was moved in `sklearn.compose._column_transformer` in 1.7.x.\n\n`At...",
      "labels": [
        "Bug",
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-09-03T09:33:57Z",
      "updated_at": "2025-09-11T16:18:05Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32090"
    },
    {
      "number": 32087,
      "title": "⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: Sep 14, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79978&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (Sep 14, 2025)\n- test_multi_metric_search_forwards_metadata[GridSearchCV-param_grid]",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-09-03T03:00:07Z",
      "updated_at": "2025-09-14T02:56:10Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32087"
    },
    {
      "number": 32086,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Sep 03, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79590&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Sep 03, 2025)\n- Test Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-03T02:34:09Z",
      "updated_at": "2025-09-03T08:24:44Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32086"
    },
    {
      "number": 32083,
      "title": "1.1.8 LARS Lasso at Mathematical Formulation",
      "body": "### Describe the issue linked to the documentation\n\nInstead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the l1 norm of the parameter vector.\n\n* not a curve\n* \"curve\" is not computed at every point\n* infinitely many solutions of the l1 norm of the parameter vector\n\n### Suggest a potential alternative/fix\n\nInstead of returning one vector of parameters, the LARS solution returns the 2D array coef_path_ of shape (n_features, max_features + 1). The values within the 2D array are the parameters of the model at each critical point on the path drawn by the l1 norm as the alpha parameter is decreased. The first column is always zero.\n\nMight not be the clearest either tbh, you guys can probably come up with something much nicer.",
      "labels": [
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-09-03T01:02:18Z",
      "updated_at": "2025-09-09T01:37:51Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32083"
    },
    {
      "number": 32076,
      "title": "```TargetEncoder``` should take ```groups``` as an argument",
      "body": "### Describe the workflow you want to enable\n\nThe current implementation of TargetEncoder uses ```KFold```-cross-validation to avoid data leakage. In cases of longitudinal or clustered data, it is desirable to ensure that rows belonging to the same group or cluster belong to the same train-folds to avoid data-leakage.\n\n### Describe your proposed solution\n\n This could be achieved by introducing an optional```group``` parameter and the use of ```GroupKFold```-cross-validation if the ```group``` is not ```None```.\n\n### Describe alternatives you've considered, if relevant\n\nThe alternative is to continue ignoring group structure. \n\n\n### Additional context\n\n_No response_",
      "labels": [
        "Enhancement",
        "help wanted"
      ],
      "state": "open",
      "created_at": "2025-09-02T09:59:39Z",
      "updated_at": "2025-09-11T16:00:53Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32076"
    },
    {
      "number": 32075,
      "title": "RFC new fitted attributes for LogisticRegressionCV",
      "body": "Contributes to #11865.\n\n### Fitted Attributes\nAfter the removal of `multi_class` and any OvR-logic in `LogisticRegressionCV` in #32073, there are a few fitted attributes that have now (or always had) a strange data format (I neglect l1_ratios in the following for ease of reading):\n- `coefs_paths_` is a dictionary with class labels as keys and arrays of shape (n_folds, n_cs, n_features) or similar as values.\n  As `coef_` is an array of shape (n_classes, n_features), `coefs_paths_` should be an array of shape (n_folds, n_cs, n_classes, n_features), such that `coefs_paths_[idx_fold, idx_cs, :, :]` gives comparable coefficients. Maybe the intercept should be separated as `intercepts_paths_`.\n- `scores_` is a dictionary with class labels as keys and arrays of shape (n_folds, n_cs) or similar as values. All values are the same regardless of the key (class label). This is a relict from OvR.\n  A good value would be just an array of shape (n_folds, n_cs)\n- `C_` is an array of shape (n_classes)\n  As the different penalties for classes are gone with the removal of OvR, `C_` should be a single float: the single best penalty parameter.\n- `l1_ratio_` same as `C_`\n- `n_iter_` is an array of shape (1, n_folds, n_cs) or similar\n  The first dimension should be removed, i.e. shape (n_folds, n_cs)\n\n### Deprecation strategy\nIt is unclear to me how to accomplish the above. Options:\n1. Deprecate old attributes and introduce new ones with new names. (time = 2 releases)\n2. Same as 1. but then deprecate new ones and reintroduce the old names. (time = 4 releases)\n3. Deprecate old attributes and switch behavior in after the deprecation cycle (time = 2 releases)\n4. Another option?\n\nUsually, we avoided deprecations options like 3.\n@scikit-learn/core-devs recall for comments",
      "labels": [
        "RFC"
      ],
      "state": "open",
      "created_at": "2025-09-02T07:42:23Z",
      "updated_at": "2025-09-05T12:15:05Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32075"
    },
    {
      "number": 32072,
      "title": "LogisticRegressionCV intercept is wrong",
      "body": "### Describe the bug\n\nThe intercept calculated by `LogisticRegressionCV` is wrong.\nA bit related to #11865.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import StratifiedKFold\n\niris = load_iris()\nX, y = iris.data, iris.target\nlrcv = LogisticRegressionCV(solver=\"newton-cholesky\").fit(X, y)\n\n# exact same split as default LogisticRegressionCV\ncv = StratifiedKFold(5)\nfolds = list(cv.split(X, y))\n\n# First fold (index 0) and second C (index 1)\ntrain_fold_0 = folds[0][0]\nlr = LogisticRegression(\n    solver=\"newton-cholesky\", C=lrcv.Cs_[1]\n).fit(X[train_fold_0], y[train_fold_0])\n\n# Compare coefficients without intercept for class 0\nnp.testing.assert_allclose(lrcv.coefs_paths_[0][0, 1, :-1], lr.coef_[0], rtol=1e-5)\n\n# Now the intercept of class 0\nnp.testing.assert_allclose(lrcv.coefs_paths_[0][0, 1, -1], lr.intercept_[0], rtol=1e-5)\n```\n\nIt is also not related to the freedom to add a constant to coefficients: Probabilities are invariant under shifting all coefficients of a single feature j for all classes by the same amount c:\n`coef[k, :] -> coef[k, :] + c    =>    proba stays the same`\nSee\n```python\n# Intercept for all classes\nlr.intercept_\n# array([ 0.35141429, -0.02662967, -0.32478462])\n\n[lrcv.coefs_paths_[cla][0, 1, -1] for cla in range(3)]\n# [0.33603135678054513, -0.04201515149357693, -0.2940162052869682]\n# These are not related by a single constant.\n```\n\n### Expected Results\n\nThe `LogisticRegression` should reproduce the same result as the selected on from `LogisticRegressionCV`.\n\n### Actual Results\n\nAssertionError: \nNot equal to tolerance rtol=1e-05, atol=0\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference among violations: 0.01538293\nMax relative difference among violations: 0.04377435\n ACTUAL: array(0.336031)\n DESIRED: array(0.351414)\n\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 (main, Feb  4 2025...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-01T14:09:05Z",
      "updated_at": "2025-09-02T13:37:33Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32072"
    },
    {
      "number": 32067,
      "title": "Enhance the warning message for metadata default value change",
      "body": "### Describe the workflow you want to enable\n\nCurrently the warning raised for [Deprecation / Default Value Change](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#deprecation-default-value-change)\nis quite generic\n```\nSupport for sample_weight has recently been added to this class. To maintain backward compatibility, ...\n```\n\nWould it be possible to specify the class in question ?  Something like\n```\nSupport for sample_weight has recently been added to ExampleRegressor. To maintain backward compatibility, ...\n```\n\n### Describe your proposed solution\n\nI think we can get the class through the owner attribute of the MethodMetadataRequest which raises the warning.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-09-01T09:03:14Z",
      "updated_at": "2025-09-01T12:17:29Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32067"
    },
    {
      "number": 32062,
      "title": "Regressor Prediction Makes a Negative Y Offset",
      "body": "### Describe the bug\n\nHi, I've found a strange situation where regressor prediction makes a negative Y offset. See an orange line on my picture below.\nHere is my py file and json data:\n[test_scikit.zip](https://github.com/user-attachments/files/22069020/test_scikit.zip)\n\nYou will need to change JS_PATH  to your path:\nJS_PATH = \"D:/Projects/Crpt/CryptoMaiden/Bot/Base/Test/btc_data.json\"\n\nYou will also need to install a poltly lib.\n\n<img width=\"1625\" height=\"921\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e8ac2c73-c556-4570-be28-6bfaedd7ba82\" />\n\nI'm new to the Scikit so I decided to report the issue.\n\nI tried RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor and all of them have this issue. \nTo change a regressor in my code you can just comment/uncomment it on lines 89-95.\n\n### Steps/Code to Reproduce\n\n# See my attached ZIP file!\n\n### Expected Results\n\nNo negative Y offset.\n\n### Actual Results\n\nJust run my py file!\n\n### Versions\n\n```shell\n1.7.1\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-08-31T22:44:45Z",
      "updated_at": "2025-09-03T11:33:26Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32062"
    },
    {
      "number": 32049,
      "title": "The dcg_score and ndcg_score documentation are hard to understand",
      "body": "### Describe the issue linked to the documentation\n\nThe documentation for the `dcg_score` and `ndcg_score` leave much to be desired.\n\nI believe this is also a by-product of competing definitions of the discount cumulative gains (DCG) and normalised DCG (nDCG) in literature. Namely\n\n$$\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)}$$\n\nand\n\n$$\\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)}.$$\n\n\nThe `dcg_score` uses the former definition, I do not think this is very clear.\n\nThe description for the DCG score (`dcg_score`) says \"Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount\". While this is technically correct to what `dcg_score` does, like many maths equations, it is hard to understand without using maths notation.\n\nIf a user wants to clarify the exact equation of the DCG used past the description they might go to the references, however,\n\n- the first reference is the Wikipedia which offers both definitions;\n\n- the third reference, \"Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)\", defines the discount function an a much more general way. While interesting does not give the user any insight into how the `dcg_score` is actually implemented.\n\nMy criticism of the `ndcg_score` is the same.\n\n### Suggest a potential alternative/fix\n\nI would propose giving an explicit definition of the DCG along the lines of\n\n$$\\mathrm{DCG_{k}} = \\sum_{i=1}^{k} \\frac{rel_{i}}{\\log_{2}(i+1)}$$\n\nwhere each $rel_i$ is the true score ranked in the order induced by the predicted scores.\n\nAlso, to do something similar for the `ndcg_score`.",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "open",
      "created_at": "2025-08-29T15:25:55Z",
      "updated_at": "2025-08-30T04:33:59Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32049"
    },
    {
      "number": 32048,
      "title": "Leiden Clustering",
      "body": "### Describe the workflow you want to enable\n\nThe \"Leiden\" Clustering algorithm is considered one of the most powerful clustering algorithms, often outperforming competitors by a wide margin. \nThe algorithm fulfils the inclusion criteria: its now 6 years old, has some 5200 citations. \n\nCurrently, it is implemented in scanpy and cugraph where the latter includes a fast, gpu-enabled implementation. Due to its empirical performance, inclusion in scikit-learn would be a welcome addition for practitioners as it is vastly superior to most clustering algorithms currently included in scikit learn (on non-trivial datasets).\n\n### Describe your proposed solution\n\nI propose to include the Leiden algorithm as a clustering algorithm in scikit-learn.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n[From Louvain to Leiden: guaranteeing well-connected communities](https://www.nature.com/articles/s41598-019-41695-z)",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-08-29T13:04:59Z",
      "updated_at": "2025-09-09T15:36:50Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32048"
    },
    {
      "number": 32046,
      "title": "rendering of 'routing' note in the documentation",
      "body": "### Describe the issue linked to the documentation\n\nthe rendering of this section seems to be over-indented leading to some funky rendering in html:\n\nexample:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA.set_transform_request\n\n<img width=\"1174\" height=\"683\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/740398db-819e-4df4-aa67-b274aa75412f\" />\n\nsource code:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/2e4e40babb3ab86d2ed2185bc0dba7fdba9414f1/sklearn/utils/_metadata_requests.py#L1215\n\n### Suggest a potential alternative/fix\n\nremove one level of indent source code:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/2e4e40babb3ab86d2ed2185bc0dba7fdba9414f1/sklearn/utils/_metadata_requests.py#L1215",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-29T10:31:51Z",
      "updated_at": "2025-09-02T10:15:52Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32046"
    },
    {
      "number": 32045,
      "title": "make sphinx directive about version more sklearn specific",
      "body": "### Describe the issue linked to the documentation\n\nThis is minor issue mostly affecting the rendering of the documentation of downstream libraries.\n\nFor example in Nilearn we use the TransformerMixin in quite a few of our estimators.\n\nBut when viewing the doc of our estimators, the sklearn methods of that mixin may have things like 'Added in version 1.3'\n\nhttps://nilearn.github.io/stable/modules/generated/nilearn.maskers.SurfaceLabelsMasker.html#nilearn.maskers.SurfaceLabelsMasker.set_transform_request\n\n<img width=\"1209\" height=\"574\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/38c33e22-6cef-4cc3-9656-3255cd9f029a\" />\n\nHowever Nilearn does not have a version 1.3 so this kind of look confusing.\n\n### Suggest a potential alternative/fix\n\nI am wondering if it would be possible to mention 'scikit-learn' in the sphinx directives that are about version (added, deprecated...)",
      "labels": [
        "Bug",
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-08-29T10:20:34Z",
      "updated_at": "2025-09-02T13:52:49Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32045"
    },
    {
      "number": 32044,
      "title": "PyTorch tensor failed with SVM",
      "body": "### Describe the bug\n\nPyTorch tensor failed with SVM: `TypeError: asarray(): argument 'dtype' must be torch.dtype, not type`\n\n### Steps/Code to Reproduce\n\n```python\nimport torch\nfrom sklearn import config_context\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\nX_torch = torch.randn(100, 4, dtype=torch.float32)\ny_torch = torch.randint(0, 2, (100,), dtype=torch.int32)\n\nprint(f\"输入数据形状: {X_torch.shape}\")\nprint(f\"标签数据形状: {y_torch.shape}\")\nprint(f\"输入数据类型: {X_torch.dtype}\")\nprint(f\"标签数据类型: {y_torch.dtype}\")\n\nwith config_context(array_api_dispatch=True):\n    svm = SVC(kernel='linear', random_state=42)\n    svm.fit(X_torch, y_torch)\n    \n\n    y_pred = svm.predict(X_torch)\n    \n    accuracy = accuracy_score(y_torch.cpu().numpy(), y_pred)\n    print(f\"SVM 准确率: {accuracy:.4f}\")\n    \n    print(f\"支持向量数量: {len(svm.support_vectors_)}\")\n    print(f\"模型参数形状: {svm.coef_.shape if hasattr(svm, 'coef_') else 'No coefficients'}\")\n```\n\n### Expected Results\n\nNo error is thrown.\n\n### Actual Results\n\n```\n输入数据形状: torch.Size([100, 4])\n标签数据形状: torch.Size([100])\n输入数据类型: torch.float32\n标签数据类型: torch.int32\nTraceback (most recent call last):\n  File \"/mnt/workspace/scikit-learn/bug.py\", line 19, in <module>\n    svm.fit(X_torch, y_torch)\n  File \"/mnt/workspace/scikit-learn/sklearn/base.py\", line 1373, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/scikit-learn/sklearn/svm/_base.py\", line 205, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/mnt/workspace/scikit-learn/sklearn/utils/validation.py\", line 3024, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/scikit-learn/sklearn/utils/validation.py\", line 1383, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/mnt/workspace/scikit-learn/sklearn/utils/validation.py\", line 1068, in check_array\n ...",
      "labels": [
        "module:svm",
        "Array API"
      ],
      "state": "closed",
      "created_at": "2025-08-29T01:21:01Z",
      "updated_at": "2025-08-29T03:41:00Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32044"
    },
    {
      "number": 32043,
      "title": "Failed to build scikit learn with cython.",
      "body": "### Describe the bug\n\nI run the command in https://scikit-learn.org/stable/developers/advanced_installation.html , but it built failed: \n\n```\nroot@dsw-1307236-5f5f447cdf-xs4m5:/mnt/workspace/scikit-learn# pip install --editable .    --verbose --no-build-isolation    --config-settings editable-verbose=true\nUsing pip 25.2 from /usr/local/lib/python3.11/site-packages/pip (python 3.11)\nLooking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\nObtaining file:///mnt/workspace/scikit-learn\n  Checking if build backend supports build_editable ...   Running command Checking if build backend supports build_editable\ndone\n  Preparing editable metadata (pyproject.toml) ...   Running command Preparing editable metadata (pyproject.toml)\n  + meson setup --reconfigure /mnt/workspace/scikit-learn /mnt/workspace/scikit-learn/build/cp311 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/mnt/workspace/scikit-learn/build/cp311/meson-python-native-file.ini\n  The Meson build system\n  Version: 1.9.0\n  Source dir: /mnt/workspace/scikit-learn\n  Build dir: /mnt/workspace/scikit-learn/build/cp311\n  Build type: native build\n  Project name: scikit-learn\n  Project version: 1.8.dev0\n  C compiler for the host machine: cc (gcc 11.4.0 \"cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\")\n  C linker for the host machine: cc ld.bfd 2.38\n  C++ compiler for the host machine: c++ (gcc 11.4.0 \"c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\")\n  C++ linker for the host machine: c++ ld.bfd 2.38\n  Cython compiler for the host machine: cython (cython 3.1.3)\n  Host machine cpu family: x86_64\n  Host machine cpu: x86_64\n  Compiler for C supports arguments -Wno-unused-but-set-variable: YES (cached)\n  Compiler for C supports arguments -Wno-unused-function: YES (cached)\n  Compiler for C supports arguments -Wno-conversion: YES (cached)\n  Compiler for C supports arguments -Wno-misleading-indentation: YES (cached)\n  Library m found: YES\n  Program sklearn/_build_utils/tempita.py found: YES (/usr/local/bin/pyt...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-29T00:59:49Z",
      "updated_at": "2025-08-29T01:04:25Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32043"
    },
    {
      "number": 32036,
      "title": "Classification metrics don't seem to support sparse?",
      "body": "While working on #31829, I noticed that although most metrics in `_classification.py` say they support sparse in the docstring (and include \"sparse matrix\" in `validate_params`), when you actually try, you get an error.\n\nEssentially in `_check_targets`, we do:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/726ed184ed80b0191732baaaf5825b86b41db4d2/sklearn/metrics/_classification.py#L128-L131\n\n`column_or_1d` then calls `check_array` with `accept_sparse` set to the default `False`.\n\n```python\nfrom sklearn.metrics import accuracy_score\nfrom scipy import sparse\nimport numpy as np\n\ny = [0, 2, 1, 3]\ny_sparse = sparse.csr_matrix(np.array(y).reshape(-1, 1))\n\naccuracy_score(y_sparse, y_sparse)\n```\n\nGives the following error:\n\n<details open>\n<summary>Error</summary>\n\n```\nTypeError                                 Traceback (most recent call last)\nCell In[11], line 1\n----> 1 accuracy_score(sparse_col, sparse_col)\n\nFile ~/Documents/dev/scikit-learn/sklearn/utils/_param_validation.py:218, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    212 try:\n    213     with config_context(\n    214         skip_parameter_validation=(\n    215             prefer_skip_nested_validation or global_skip_validation\n    216         )\n    217     ):\n--> 218         return func(*args, **kwargs)\n    219 except InvalidParameterError as e:\n    220     # When the function is just a wrapper around an estimator, we allow\n    221     # the function to delegate validation to the estimator, but we replace\n    222     # the name of the estimator by the name of the function in the error\n    223     # message to avoid confusion.\n    224     msg = re.sub(\n    225         r\"parameter of \\w+ must be\",\n    226         f\"parameter of {func.__qualname__} must be\",\n    227         str(e),\n    228     )\n\nFile ~/Documents/dev/scikit-learn/sklearn/metrics/_classification.py:373, in accuracy_score(y_true, y_pred, normalize, sample_weight)\n    371 # Compute accuracy for each possible representati...",
      "labels": [
        "Bug",
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-08-28T11:02:13Z",
      "updated_at": "2025-09-09T12:02:25Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32036"
    },
    {
      "number": 32032,
      "title": "Setting weights on items when passing list of dicts to RandomizedSearchCV",
      "body": "### Describe the workflow you want to enable\n\nWe can pass a list of dictionaries to `RandomizedSearchCV`, for example\n\n```python\n[\n    {\"dim_reduction\": \"passthrough\"},\n    {\n        \"dim_reduction\": PCA(),\n        \"dim_reduction__n_components\": [10, 20, ...,]\n    }\n]\n```\n\nIf I understand correctly [here](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/model_selection/_search.py#L335) to get a set of hyperparameters one of the items in the list is chosen with equal probabilities, then a set of parameters is sampled from that dict.\n\nIn some cases it would be convenient to control the probability of choosing each list item. For example above I might want to invest more computation time in the \"not passthrough\" branch. Or if I have a column that can be either dropped or transformed I may want to explore more the \"not drop\" branch.\n\nIn other cases we have to create multiple list items due to nested estimators but that results in one choice for a hyperparameter to be over-represented. For example:\n\n```python\n[\n    {\n        \"transformer\": Flat(),\n        \"transformer__a\": uniform(0.0, 1.0),\n        \"transformer__b\": uniform(0.0, 1.0),\n    },\n    {\n        \"transformer\": Nested(),\n        \"transformer__part\": A(),\n        \"transformer__part__a\": uniform(0.0, 1.0),\n    },\n    {\n        \"transformer\": Nested(),\n        \"transformer__part\": B(),\n        \"transformer__part__b\": uniform(0.0, 1.0),\n    },\n]\n```\n\nI have to create 2 grid items for the Nested() option but if I am equally interested in the Flat() one I might want to set weights [1.0, 0.5, 0.5] on the list of param dicts. Maybe this is not a great example but what I mean is the amount of trials spent on one option can be driven by the structure of the estimators and how they are combined and sometimes it would be helpful to be able to correct or control it.\n\n\n### Describe your proposed solution\n\nNot sure what could be a nice API, maybe there would be a `distribution_weights` parameter which can only b...",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-08-27T18:06:56Z",
      "updated_at": "2025-09-02T19:22:57Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32032"
    },
    {
      "number": 32022,
      "title": "⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Aug 28, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79396&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (Aug 28, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-08-27T02:35:55Z",
      "updated_at": "2025-08-29T03:33:03Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32022"
    },
    {
      "number": 32003,
      "title": "`OrdinalEncoder` transformed validation dataset still contains null / missing values",
      "body": "### Describe the bug\n\nI use `OrdinalEncoder` to `fit_trainsform` a training dataset which is properly cleaned up. However, using the same encoder to `transform` validation / test dataset still contains null / missing values.\n\n### Steps/Code to Reproduce\n\n```\nordinal_encoder = OrdinalEncoder(categories=\"auto\",\n                                 handle_unknown=\"use_encoded_value\",\n                                 unknown_value=numpy.nan,\n                                 encoded_missing_value=numpy.nan) # treat unknown categories as np.nan (or None)\nX_train[categorical_features] = ordinal_encoder.fit_transform(X_train[categorical_features].astype(str)) # OrdinalEncoder expects all values as the same type (e.g. string or numeric only)\nX_validation[categorical_features] = ordinal_encoder.transform(X_validation[categorical_features].astype(str)) # only use `transform` on the validation data\n```\nThe following pass:\n```\nassert not X_train[categorical_features].isnull().values.any()\nassert not X_train[categorical_features].isna().values.any()\n```\nThe following fails!:\n```\nassert not X_validation[categorical_features].isnull().values.any()\nassert not X_validation[categorical_features].isna().values.any()\n```\n\n\n### Expected Results\n\n`transform` on validation dataset should clean up the values, leaving no missing and/or null values.\n\n### Actual Results\n\nThe assertion code on validation dataset fails!\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.3 (main, Aug 14 2025, 11:53:40) [GCC 14.2.0]\nexecutable: /home/khteh/.local/share/virtualenvs/JupyterNotebooks-uVG1pv5y/bin/python\n   machine: Linux-6.14.0-28-generic-x86_64-with-glibc2.41\n\nPython dependencies:\n      sklearn: 1.7.1\n          pip: 25.0\n   setuptools: 80.9.0\n        numpy: 2.3.2\n        scipy: 1.16.1\n       Cython: None\n       pandas: 2.3.1\n   matplotlib: 3.10.5\n       joblib: 1.5.1\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n     ...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-24T10:58:47Z",
      "updated_at": "2025-08-24T11:23:28Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/32003"
    },
    {
      "number": 31990,
      "title": ".",
      "body": "",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-22T10:01:19Z",
      "updated_at": "2025-08-22T10:57:40Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31990"
    },
    {
      "number": 31989,
      "title": "Implementing Divisive Analysis",
      "body": "### Describe the workflow you want to enable\n\nI want to add Divisive Analysis Clustering to base scikit-learn in order to provide more options to developers.\n\"Divisive methods start when all objects are together (that is, at step 0 there is one cluster) and in each following step a cluster is split up, until there are _n_ of them.\" (Kaufman and Rousseeuw 1990). \n\n### Describe your proposed solution\n\nImplement a class that performs divisive clustering extending BaseEstimatior and ClusterMixin.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nIt has been implemented in R (cluster package). It's commonly used for marketing purposes and document and topic classification.\n\nKaufman, L., & Rousseeuw, P. J. (1990). Finding Groups in Data. En Wiley series in probability and statistics. https://doi.org/10.1002/9780470316801",
      "labels": [
        "New Feature",
        "Hard",
        "module:cluster",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-08-21T23:31:50Z",
      "updated_at": "2025-08-27T15:13:49Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31989"
    },
    {
      "number": 31988,
      "title": "Different Results on ARM and x86 when using `RFECV(RandomForestClassifier())`",
      "body": "### Describe the bug\n\nWhen using  `RFECV(RandomForestClassifier())` with `sklearn=1.7.1` with `numpy>=2.0.0`, I am seeing significant discrepancies in floating point results between ARM Macs and x86 Macs/Linux machines. This discrepancy goes away when I downgrade to `numpy=1.26.4`\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\n\ndef _extract_rfe_scores(rfecv):\n    grid_scores_ = rfecv.cv_results_['mean_test_score']\n    n_features = len(rfecv.ranking_)\n    # If using fractional step, step = integer of fraction * n_features\n    if rfecv.step < 1:\n        rfecv.step = int(rfecv.step * n_features)\n    # Need to manually calculate x-axis, grid_scores_ is a 1-d array\n    x = [n_features - (n * rfecv.step)\n         for n in range(len(grid_scores_)-1, -1, -1)]\n    if x[0] < 1:\n        x[0] = 1\n    return pd.Series(grid_scores_, index=x, name='Accuracy')\n\nnp.random.seed(0)\nX = np.random.rand(50, 20)\ny = np.random.randint(0, 2, 50)\n\nexp = pd.Series([\n            0.4999999999999999, 0.52, 0.52, 0.5399999999999999,\n            0.44000000000000006, 0.52, 0.4600000000000001,\n            0.5599999999999998, 0.52, 0.52, 0.5, 0.5399999999999999, 0.54,\n            0.5599999999999999, 0.47999999999999987, 0.6199999999999999,\n            0.5399999999999999, 0.5, 0.4999999999999999, 0.45999999999999996],\n            index=pd.Index(range(1, 21)), name='Accuracy')\n\nselector = RFECV(RandomForestClassifier(\n    random_state=123, n_estimators=2), step=1, cv=10)\nselector = selector.fit(X, y.ravel())\nselector_series = _extract_rfe_scores(selector)\n\npd.testing.assert_series_equal(selector_series, exp)\n```\n\n### Expected Results\n\nI expect the resulting `selector_series` to be equal to `exp` or\n\n```\n [0.4999999999999999, 0.52, 0.52, 0.5399999999999999,\n  0.44000000000000006, 0.52, 0.4600000000000001,\n  0.5599999999999998, 0.52, 0.52, 0.5, 0.5399999999999999, 0.54,\n  0.55...",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-08-21T21:29:41Z",
      "updated_at": "2025-09-11T17:05:40Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31988"
    },
    {
      "number": 31980,
      "title": "Add beginner-friendly examples",
      "body": "## 🎯 Beginner Examples Request\n\n### Description\nIt would be great to have more beginner-friendly examples in the project.\n\n### Suggested additions:\n- Simple \"Hello World\" examples\n- Step-by-step tutorials\n- Common use case demonstrations\n- Code comments for clarity\n\n### Why this matters:\n- Helps new developers get started\n- Makes the project more inclusive\n- Encourages community growth\n\n### Type\n- [ ] Documentation\n- [ ] Enhancement\n- [ ] Good first issue\n\n---\n*I'm a beginner and would love to help create examples that help others like me!*",
      "labels": [
        "spam"
      ],
      "state": "closed",
      "created_at": "2025-08-20T16:59:19Z",
      "updated_at": "2025-08-20T22:34:28Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31980"
    },
    {
      "number": 31979,
      "title": "Create troubleshooting guide",
      "body": "## 🔧 Troubleshooting Guide\n\n### Description\nA troubleshooting guide would help users solve common problems.\n\n### Suggested content:\n- Common error messages and solutions\n- Installation troubleshooting\n- Configuration issues\n- Performance problems\n\n### Benefits:\n- Reduces support burden\n- Improves user experience\n- Self-service help\n\n### Type\n- [ ] Documentation\n- [ ] Enhancement\n- [ ] Good first issue\n\n---\n*I'd like to help create a comprehensive troubleshooting guide!*",
      "labels": [
        "spam"
      ],
      "state": "closed",
      "created_at": "2025-08-20T16:36:02Z",
      "updated_at": "2025-08-20T22:33:15Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31979"
    },
    {
      "number": 31978,
      "title": "Create troubleshooting guide",
      "body": "## 🔧 Troubleshooting Guide\n\n### Description\nA troubleshooting guide would help users solve common problems.\n\n### Suggested content:\n- Common error messages and solutions\n- Installation troubleshooting\n- Configuration issues\n- Performance problems\n\n### Benefits:\n- Reduces support burden\n- Improves user experience\n- Self-service help\n\n### Type\n- [ ] Documentation\n- [ ] Enhancement\n- [ ] Good first issue\n\n---\n*I'd like to help create a comprehensive troubleshooting guide!*",
      "labels": [
        "spam"
      ],
      "state": "closed",
      "created_at": "2025-08-20T16:01:04Z",
      "updated_at": "2025-08-20T22:30:57Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31978"
    },
    {
      "number": 31976,
      "title": "Add beginner-friendly examples",
      "body": "## 🎯 Beginner Examples Request\n\n### Description\nIt would be great to have more beginner-friendly examples in the project.\n\n### Suggested additions:\n- Simple \"Hello World\" examples\n- Step-by-step tutorials\n- Common use case demonstrations\n- Code comments for clarity\n\n### Why this matters:\n- Helps new developers get started\n- Makes the project more inclusive\n- Encourages community growth\n\n### Type\n- [ ] Documentation\n- [ ] Enhancement\n- [ ] Good first issue\n\n---\n*I'm a beginner and would love to help create examples that help others like me!*",
      "labels": [
        "spam"
      ],
      "state": "closed",
      "created_at": "2025-08-20T14:14:01Z",
      "updated_at": "2025-08-20T22:34:52Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31976"
    },
    {
      "number": 31974,
      "title": "⚠️ CI failed on Wheel builder (last failure: Aug 22, 2025) ⚠️",
      "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/17145548838)** (Aug 22, 2025)",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-08-20T04:39:41Z",
      "updated_at": "2025-08-22T08:45:00Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31974"
    },
    {
      "number": 31971,
      "title": "ValueError in PLSRegression.fit() with zero-variance predictor",
      "body": "### Describe the bug\n\nRelated: https://github.com/scipy/scipy/commit/5bc3d8814d566ef328f41cfa69ccd797c68b0d02\n\nWhen fitting a PLSRegression model, if the input array X contains a feature with zero variance (i.e., a constant column), the fit method raises a ValueError: illegal value in 4th argument of internal gesdd.\n\nThis results in a division by zero when a predictor has no variance, creating NaN values likely in the intermediate matrices. These NaN values are then passed to the SciPy function, which in turn calls the LAPACK gesdd routine for SVD, causing it to crash.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.cross_decomposition import PLSRegression\n\nn_samples = 20\ny = np.arange(n_samples, dtype=float)\n\n# This feature has zero variance.\nX = np.ones((n_samples, 1))\n\n# This will raise the error.\npls = PLSRegression(n_components=1)\npls.fit(X, y)\n```\n\n### Expected Results\n\nThe model should either fit successfully (e.g., perhaps assigning a zero weight to the zero-variance feature) or raise a more informative ValueError indicating that a predictor has zero variance.\n\n### Actual Results\n\nWe get \"ValueError: illegal value in 4th argument of internal gesdd\"\n\n```python\n/home/user/.local/lib/python3.13/site-packages/sklearn/cross_decomposition/_pls.py:99: RuntimeWarning: invalid value encountered in divide\n  y_weights = np.dot(Y.T, x_score) / np.dot(x_score.T, x_score)\n/home/user/.local/lib/python3.13/site-packages/sklearn/cross_decomposition/_pls.py:368: RuntimeWarning: invalid value encountered in divide\n  x_loadings = np.dot(x_scores, Xk) / np.dot(x_scores, x_scores)\n/home/user/.local/lib/python3.13/site-packages/sklearn/cross_decomposition/_pls.py:377: RuntimeWarning: invalid value encountered in divide\n  y_loadings = np.dot(x_scores, yk) / np.dot(x_scores, x_scores)\nTraceback (most recent call last):\n  File \"/home/user/agents/test/f.py\", line 14, in <module>\n    pls.fit(X, y)\n    ~~~~~~~^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-p...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-08-19T18:44:24Z",
      "updated_at": "2025-09-09T01:38:18Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31971"
    },
    {
      "number": 31970,
      "title": "Create troubleshooting guide",
      "body": "## 🔧 Troubleshooting Guide\n\n### Description\nA troubleshooting guide would help users solve common problems.\n\n### Suggested content:\n- Common error messages and solutions\n- Installation troubleshooting\n- Configuration issues\n- Performance problems\n\n### Benefits:\n- Reduces support burden\n- Improves user experience\n- Self-service help\n\n### Type\n- [ ] Documentation\n- [ ] Enhancement\n- [ ] Good first issue\n\n---\n*I'd like to help create a comprehensive troubleshooting guide!*",
      "labels": [
        "spam"
      ],
      "state": "closed",
      "created_at": "2025-08-19T16:25:44Z",
      "updated_at": "2025-08-20T05:07:17Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31970"
    },
    {
      "number": 31968,
      "title": "⚠️ CI failed on Linux.pylatest_pip_openblas_pandas (last failure: Aug 19, 2025) ⚠️",
      "body": "**CI failed on [Linux.pylatest_pip_openblas_pandas](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79175&view=logs&j=78a0bf4f-79e5-5387-94ec-13e67d216d6e)** (Aug 19, 2025)\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csc_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csc_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csc_array-False]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csc_array-True]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csr_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csr_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csr_array-False]\n- test_sparse_matmul_to_dense[23-float32-csc_matrix-csr_array-True]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csc_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csc_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csc_array-False]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csc_array-True]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csr_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csr_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csr_array-False]\n- test_sparse_matmul_to_dense[23-float32-csc_array-csr_array-True]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csc_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csc_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csc_array-False]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csc_array-True]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csr_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csr_matrix-True]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csr_array-False]\n- test_sparse_matmul_to_dense[23-float32-csr_matrix-csr_array-True]\n- test_sparse_matmul_to_dense[23-float32-csr_array-csc_matrix-False]\n- test_sparse_matmul_to_dense[23-float32-csr_array-csc_matrix-True]\n- test_sparse_matmu...",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-19T03:08:50Z",
      "updated_at": "2025-08-22T08:54:27Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31968"
    },
    {
      "number": 31965,
      "title": "a11y - scikit-learn docs accessibility audit and remediation",
      "body": "### Description\n\n**Note:** This is scoped as part of an ongoing NASA ROSES grant in collaboration with Quansight; as such, a couple of us at Quansight will take on the work outlined in this issue.\n\nPer the NASA ROSES grant, we will conduct an accessibility review of the [scikit-learn documentation site](https://scikit-learn.org/stable/) and work on remediation of the flagged issues. \n\nSince scikit-learn uses the PyData Sphinx Theme, on which we have already conducted thorough accessibility audits and spent a substantial amount of work over the last couple of years to make this theme more accessible, the audit and remediation of scikit-learn will focus on customised/custom features added to the scikit-learn documentation. \n\n### Proposed implementation \n\nTo achieve this goal, I propose the following approach:\n\n1. Scope what needs to be audited/tested - and update this issue to reflect this\n2. Test/audit components and report back on the findings in this issue\n3. Iteratively work on any remediation tasks as needed.\n\nPlease let me know if you have any questions or suggestions on how to approach this more effectively, so we can keep you all aligned and ensure a smooth contribution. \n\nAlso, if y'all can assign me to this issue, it would be great! ✨",
      "labels": [
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-08-18T15:20:25Z",
      "updated_at": "2025-08-29T15:05:28Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31965"
    },
    {
      "number": 31955,
      "title": "⚠️ CI failed on Wheel builder (last failure: Aug 19, 2025) ⚠️",
      "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/17059042784)** (Aug 19, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-16T04:54:50Z",
      "updated_at": "2025-08-19T11:37:17Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31955"
    },
    {
      "number": 31947,
      "title": "UserWarning: X has feature names, but PowerTransformer was fitted without feature names",
      "body": "### Describe the bug\n\nWhen using pandas dataframes and a `TransformedTargetRegressor` with `PowerTransformer` with `set_output(transform=\"pandas\")`, I get this warning:\n\n> UserWarning: X has feature names, but PowerTransformer was fitted without feature names\n\nThe warning does not arise when using other estimators (e.g. `StandardScaler`) but only with `PowerTransformer`.\n\nThe problem seems to originate from the `inverse_transform` implementation of `PowerTransformer`.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\n\nX, y = load_iris(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# This works fine:\npipeline = TransformedTargetRegressor(\n    regressor=LinearRegression(),\n    transformer=StandardScaler().set_output(transform=\"pandas\")\n)\npipeline.fit(X_train, y_train)\ny_test_pred = pipeline.predict(X_test)\n\n# But this gets a warning:\npipeline = TransformedTargetRegressor(\n    regressor=LinearRegression(),\n    transformer=PowerTransformer().set_output(transform=\"pandas\")\n)\npipeline.fit(X_train, y_train)\ny_test_pred = pipeline.predict(X_test)\n```\n\n### Expected Results\n\nNo warning\n\n### Actual Results\n\n> UserWarning: X has feature names, but PowerTransformer was fitted without feature names\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.9\n\nPython dependencies:\n      sklearn: 1.7.1\n          pip: 25.2\n   setuptools: 65.5.0\n        numpy: 2.0.2\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n```",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-08-14T09:52:18Z",
      "updated_at": "2025-08-27T15:49:02Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31947"
    },
    {
      "number": 31940,
      "title": "Diabetes data should match the original source",
      "body": "### Describe the bug\n\nWhen `load_diabetes` is called with `scaled=False`, the `s5` attribute has some values with insufficient precision:\nAll values should stay equal when rounded to 4 decimals, but 11 of them don't.\n\nThis is caused by the fact that the unpacked `sklearn/datasets/data/diabetes_data_raw.csv.gz` has some numeric differences to the original data.\nE.g. entry nr. 147 contains `4.803999999999999` here (in line 147), and `4.804` in the original (line 148 because of header).\n\nThe following example shows different behavior when the data source is toggled with `use_internal`.\n\nI need the correct data because I have code that tries to autodetect the precision – which currently cannot detect the correct precision of `s5`.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.datasets import load_diabetes\nimport requests\nfrom io import StringIO\nimport pandas as pd\n\nuse_internal = True\nif use_internal:\n    diabetes = load_diabetes(as_frame=True, scaled=False)\n    s5 = diabetes.frame['s5']\nelse:\n    # diabetes.DESCR names https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n    # as source URL.\n    # There the following orig_url is linked as the original data set.\n    orig_url = 'https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt'\n    response = requests.get(orig_url)\n    response.raise_for_status()\n    data = StringIO(response.text)\n    diabetes = pd.read_csv(data, sep='\\t')\n    s5 = diabetes['S5']\n\nrounded = s5.round(4)\npd.set_option('display.precision', 16)\ndiff = s5[s5 != rounded] \nprint(diff)\nassert(diff.empty)\n\n```\n\n### Expected Results\n\n`Series([], Name: S2, dtype: float64)`\n\nand no assertion.\n\n(As with `use_internal = False`)\n\n\n### Actual Results\n\n```\n146    4.8039999999999994\n239    5.3660000000000005\n265    4.8039999999999994\n303    5.4510000000000005\n313    5.2470000000000008\n324    5.3660000000000005\n359    4.8039999999999994\n364    4.8039999999999994\n410    5.3660000000000005\n415    4.8039999999999994\n428    5.3660000000000005\nName: s5, ...",
      "labels": [
        "module:datasets"
      ],
      "state": "open",
      "created_at": "2025-08-13T11:11:37Z",
      "updated_at": "2025-08-25T13:35:44Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31940"
    },
    {
      "number": 31931,
      "title": "Allow common estimator checks to use `xfail_strict=True`",
      "body": "### Describe the workflow you want to enable\n\nI'd like to be able to use [`parametrize_with_checks`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.parametrize_with_checks.html) and use \"strict mode\" to notice when checks that are marked as xfail start passing. But I don't want to turn on strict mode for my whole test suite (`xfail_strict = true` in `pytest.ini`)\n\n### Describe your proposed solution\n\nWe use `pytest.mark.xfail` internally when generating all the estimator + check combinations. I think we could pass `strict=True` there to make it a failure for a test, that is marked as xfail, to pass.\n\nhttps://github.com/scikit-learn/scikit-learn/blob/c5497b7f7eacfaff061cf68e09bcd48aa93d4d6b/sklearn/utils/estimator_checks.py#L456\n\nI think we want to make this behaviour configurable, so we need a new parameter for `parametrize_with_checks`, something like `strict=None` with the option to set it to `True`/`False`.\n\nI'd set the default to `None` so that not setting it does not override the setting in `pytest.ini` (to be checked if this actually works). If you are using `pytest.ini` to control strict mode then not passing `strict` to `parametrize_with_checks` should not change anything.\n\n### Describe alternatives you've considered, if relevant\n\nI tried layering `@pytest.mark.xfail(strict=True)` on top of `@parametrize_with_checks` but that doesn't seem to work.\n\n```python\n@pytest.mark.xfail(strict=True)\n@parametrize_with_checks(...)\ndef test_sklearn_compat(estimator, check):\n   ...\n```\n\n### Additional context\n\n_No response_",
      "labels": [
        "Enhancement"
      ],
      "state": "closed",
      "created_at": "2025-08-12T13:02:24Z",
      "updated_at": "2025-09-01T10:15:04Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31931"
    },
    {
      "number": 31930,
      "title": "Docs instructions for installing  LLVM OpenMP with Homebrew may need updating",
      "body": "### Describe the issue linked to the documentation\n\nEnvironment variables CFLAGS, CXXFLAGS, CXXFLAGS mentioned here:\nhttps://scikit-learn.org/dev/developers/advanced_installation.html#compiler-macos:~:text=Set%20the%20following%20environment%20variables%3A\nmay be for Intel-based Macs only.\n\nSo when trying to do this:\n```\nmake clean\npip install --editable . \\\n    --verbose --no-build-isolation \\\n    --config-settings editable-verbose=true\n```\nI got  `../../meson.build:1:0: ERROR: Compiler /usr/bin/clang cannot compile programs.`\n\nThe reason being that `Homebrew` installed `libomp` here: `/opt/homebrew/opt/libomp` and not here`/usr/local/opt/libomp/`.\n\n\n### Suggest a potential alternative/fix\n\nModify the env variables that I mentioned above to the right path to `libomp` for M2 macs.\n\nPlease note:\n\n- I'm not sure if the variables should be updated or have the two mac versions (Intel vs M1/M2).\n- I didn't test that all works for an Intel mac. \n- Modifying the variables to the correct path, I was able to make the new environment.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-08-12T10:12:30Z",
      "updated_at": "2025-08-13T13:36:06Z",
      "comments": 12,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31930"
    },
    {
      "number": 31925,
      "title": "Add a better implementation of Latent Dirichlet Allocation",
      "body": "### Describe the workflow you want to enable\n\nWhile this remains to be rigorously tested, the scikit-learn implementation of Latent Dirichlet Allocation is, in the [unanimous experience of topic modelling scholars](https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html), outperformed by Gibbs-Sampling implementations, such as the ones in MALLET and tomotopy when it comes to topic quality. I have personally been criticised for using the scikit-learn implementation of LDA in my publications as a baseline, since other scholars do not think this implementation does justice to how well LDA can actually work in practice.\nThis is quite sad, since scikit-learn otherwise has a very authoritative position when it comes to machine learning, and many research and industry workflows build on your well-thought out and convenient API.\n\nIt would be of immense value for both industry and academia if Latent Dirichlet Allocation had multiple implementations, and preferably another one were the default.\n\n### Describe your proposed solution\n\nInclude the implementation of LDA from the following publication:\n[Distributed Algorithms for Topic Models](https://jmlr.org/papers/volume10/newman09a/newman09a.pdf)\n\nThis implementation has been around for a while, is used both in tomotopy and MALLET, is published in a reputable journal and has been cited more than 600 times according to Google Scholar.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Info",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-08-11T14:43:45Z",
      "updated_at": "2025-09-03T06:09:40Z",
      "comments": 16,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31925"
    },
    {
      "number": 31923,
      "title": "404 when fetching datasets with sklearn.datasets.fetch_openml",
      "body": "### Describe the bug\n\nMy Azure DevOps pipeline started failing to fetch data from OpenML with 404 as of 9 August. My original line in a Jupyter notebook uses `fetch_openml(name='SPECT', version=1, parser='auto')`; but I've not been able to download any other dataset either (e.g., iris, miceprotein).\n\nThe SPECT dataset at OpenML [here ](https://www.openml.org/search?type=data&status=active&id=336) looks ok. So is this a scikit-learn bug rather than an OpenML one? I can't find any reported issues about this at https://github.com/openml/openml.org/issues either.\n\n### Steps/Code to Reproduce\n\n```\nfrom sklearn.datasets import fetch_openml\nfetch_openml(name='SPECT', version=1, parser='auto')\n```\n\n### Expected Results\n\nData should be fetched with no error.\n\n### Actual Results\n\nThis is from scikit-learn 1.5.1 and Python 3.9.20 in my local Windows Python interpreter:\n```\nC:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\datasets\\_openml.py:107: UserWarning: A network error occurred while downloading https://api.openml.org/data/v1/download/52239. Retrying...\n  warn(\nTraceback (most recent call last):\n  File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-de4cc69a81bb>\", line 1, in <module>\n    fetch_openml(name='SPECT')\n  File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\datasets\\_openml.py\", line 1127, in fetch_openml\n    bunch = _download_data_to_bunch(\n  File \"C:\\Apps\\Miniconda3\\v3_8_5_x64\\Local\\envs\\prodaps-dev-py39\\lib\\site-packages\\sklearn\\datasets\\_openml.py\", line 681, in _download_data_to_bunch\n    X, y, frame, categories = _retry_with_clean_cache(\n  File \"C:\\...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-11T12:35:23Z",
      "updated_at": "2025-08-14T08:39:06Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31923"
    },
    {
      "number": 31913,
      "title": "⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Aug 10, 2025) ⚠️",
      "body": "**CI failed on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78962&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (Aug 10, 2025)\n- test_dtype_preprocess_data[73-True-True]",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-10T03:03:23Z",
      "updated_at": "2025-08-13T12:36:40Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31913"
    },
    {
      "number": 31912,
      "title": "Stable extender contract via `fit` / `_fit` resp `predict` / `_predict` separation",
      "body": "### Describe the workflow you want to enable\n\ntl;dr, I am suggestion to refactor `scikit-learn` internals to a layer separation with boilerplate between `fit` and `_fit` resp `predict` and `_predict` methods, to make extender interfaces more stable. Also see https://github.com/scikit-learn/scikit-learn/issues/31728\n\nMore background: Currently, every time `scikit-learn` releases a new minor version - e.g., 1.5.0, 1.6.0, 1.7.0 - compliant extensions, e.g., custom transformers, classifiers, etc, break - specifically, referring to the API conformance as tested through `check_estimator` or `parametrize_with_checks`.\n\nThese repeated breakages in the \"extender contract\" contrast the stability of the usage contract, which is stable and professionally managed.\n\nFor a package like `scikit-learn` which means to be a standard not just for ML algorithms but also an API standard that everyone uses, this is not a good state to be in - \"do not break user code\" is the maxim that gets broken for power users writing extensions.\n\nOf course maintaining downwards compatibility is not always possible, but nothing should break without a proper warning.\n\n### Describe your proposed solution\n\nThe `fit`/`_fit` separation would ensure stability of the extension contract - and would also allow to build secondary deprecation patterns in relation to it.\n\nThe (oop) pattern this would implement is the so-called \"template pattern\".\n\nIt would allow to remove likely changing parts such as the boilerplate (e.g., `validate_data` vs `_validate_data` and such) from the extension locus, and thus completely prevent breakage in relation to boilerplate changes.\nReference: https://refactoring.guru/design-patterns/template-method\n\nExamples of how this can be used to improve stability:\n\n* `sktime`, for a different API, has a separation between `fit` calling an internal `_fit`, where change-prone boilerplate is sandwiched between a stable user contract (`fit`) and a stable extender contract (`_fit`); similarly `pr...",
      "labels": [
        "RFC",
        "Developer API"
      ],
      "state": "open",
      "created_at": "2025-08-09T22:02:48Z",
      "updated_at": "2025-08-30T16:35:47Z",
      "comments": 19,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31912"
    },
    {
      "number": 31907,
      "title": "HDBSCAN modifies input precomputed distance matrix",
      "body": "### Describe the bug\n\nWhen using `sklearn.cluster.HDBSCAN` with `metric=\"precomputed\"`, the input distance matrix is modified after calling `fit_predict()`. The original `hdbscan` package (v0.8.40) works correctly.  \n\n### Steps/Code to Reproduce\n```py\nimport numpy as np\nfrom sklearn.cluster import HDBSCAN\n\nrmsd_matrix = np.random.rand(5, 5)\nrmsd_matrix = (rmsd_matrix + rmsd_matrix.T) / 2\nnp.fill_diagonal(rmsd_matrix, 0)\n\nprint(\"Before HDBSCAN:\")\nprint(rmsd_matrix)\n\nhdb = HDBSCAN(metric=\"precomputed\", min_cluster_size=2)\nhdb.fit_predict(rmsd_matrix)\n\nprint(\"\\nAfter HDBSCAN:\")\nprint(rmsd_matrix)  # Matrix is changed!\n```\n\n### Expected Results\n\nInput matrix should remain unchanged (as in original hdbscan).\n\n### Actual Results\n\nInput matrix is changed\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\nexecutable: /home/username/project/bin/python3\n   machine: Linux-6.14.0-27-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.7.0\n          pip: 24.0\n   setuptools: 80.9.0\n        numpy: 2.2.6\n        scipy: 1.16.0\n       Cython: None\n       pandas: 2.3.0\n   matplotlib: 3.10.3\n       joblib: 1.5.1\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 18\n         prefix: libscipy_openblas\n       filepath: /home/username/project/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-56d6093b.so\n        version: 0.3.29\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 18\n         prefix: libscipy_openblas\n       filepath: /home/username/project/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 18\n         prefix: libgomp\n       filepath: /home/username/project/lib/python3.12/site-packages/scikit_learn.libs/lib...",
      "labels": [
        "Bug",
        "module:cluster"
      ],
      "state": "closed",
      "created_at": "2025-08-09T12:22:53Z",
      "updated_at": "2025-09-09T13:30:38Z",
      "comments": 14,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31907"
    },
    {
      "number": 31904,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Aug 17, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=79126&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Aug 17, 2025)\n- Test Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-09T02:51:16Z",
      "updated_at": "2025-08-22T10:59:31Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31904"
    },
    {
      "number": 31901,
      "title": "QuantileTransformer is incredibly slow",
      "body": "### Describe the workflow you want to enable\n\nThis is a feature request to improve performance of the QuantileTransformer. It takes ~60 minutes to fit, uses a huge amount of memory when transforming large non-sparse dataframes with 30M+ rows and 500 columns. It also does not support sample_weight.  Ideally it should be as fast as catboost's Pool quantize method, which does many of the same computations in a fraction of the time:\nhttps://catboost.ai/docs/en/concepts/python-reference_pool_quantized\n\n\n### Describe your proposed solution\n\nSee source code for https://catboost.ai/docs/en/concepts/python-reference_pool_quantized\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-08T23:10:38Z",
      "updated_at": "2025-08-27T06:40:57Z",
      "comments": 13,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31901"
    },
    {
      "number": 31899,
      "title": "Add `covariance_estimator` to `QuadraticDiscriminantAnalysis`?",
      "body": "### Describe the workflow you want to enable\n\n`LinearDiscriminantAnalysis` has an optional `covariance_estimator` parameter, while the similar `QuadraticDiscriminantAnalysis` does not. QDA is even more sensitive than LDA to covariance estimation.\n\nWould it be desirable to add the `covariance_estimator` parameter to `QuadraticDiscriminantAnalysis`? \n\n### Describe your proposed solution\n\nI can try to implement this. I would look at how it is done in `LinearDiscriminantAnalysis`, and just copy that implementation into `QuadraticDiscriminantAnalysis`.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-08-08T15:03:02Z",
      "updated_at": "2025-09-05T13:44:52Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31899"
    },
    {
      "number": 31896,
      "title": "⚠️ CI failed on Wheel builder (last failure: Aug 08, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/16821604494)** (Aug 08, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-08T04:32:32Z",
      "updated_at": "2025-08-08T13:27:43Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31896"
    },
    {
      "number": 31894,
      "title": "TunedThreasholdClassiffierCV not understanding `func(y_pred, y_true, ...)` as a valid `scoring`",
      "body": "This code\n\n```py\nfrom sklearn.model_selection import TunedThresholdClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nimport sklearn\nimport numpy as np\n\nsklearn.set_config(enable_metadata_routing=True)\n\ndef my_metric(y_true, y_pred, sample_weight=None):\n    assert sample_weight is not None\n    return np.mean(y_pred)\n\nX, y = make_classification(random_state=0)\nsample_weight = np.random.rand(len(y))\n\nest = TunedThresholdClassifierCV(LogisticRegression(), cv=2, scoring=my_metric)\nest.fit(X, y, sample_weight=sample_weight)\n```\n\ngives this:\n\n```py\nTraceback (most recent call last):\n  File \"/tmp/2.py\", line 17, in <module>\n    est.fit(X, y, sample_weight=sample_weight)\n  File \"/path/to/scikit-learn/sklearn/base.py\", line 1366, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/scikit-learn/sklearn/model_selection/_classification_threshold.py\", line 129, in fit\n    self._fit(X, y, **params)\n  File \"/path/to/scikit-learn/sklearn/model_selection/_classification_threshold.py\", line 742, in _fit\n    routed_params = process_routing(self, \"fit\", **params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/scikit-learn/sklearn/utils/_metadata_requests.py\", line 1636, in process_routing\n    request_routing = get_routing_for_object(_obj)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/scikit-learn/sklearn/utils/_metadata_requests.py\", line 1197, in get_routing_for_object\n    return deepcopy(obj.get_metadata_routing())\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/scikit-learn/sklearn/model_selection/_classification_threshold.py\", line 871, in get_metadata_routing\n    scorer=self._get_curve_scorer(),\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/scikit-learn/sklearn/model_selection/_classification_threshold.py\", line 880, in _get_curve_scorer\n    curve_scorer = _CurveScorer.fr...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-08-07T12:50:24Z",
      "updated_at": "2025-08-11T13:01:50Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31894"
    },
    {
      "number": 31889,
      "title": "We don't support `func(estimator, X, y, ...)` across the board as a scorer",
      "body": "Our documentation [here](https://scikit-learn.org/stable/modules/model_evaluation.html#custom-scorer-objects-from-scratch) states a callable with a `(estimator, X, y)` is a valid scorer. However, it isn't.\n\nIn https://github.com/scikit-learn/scikit-learn/issues/31599, it is observed that passing such an object fails in the context of a `_MultimetricScorer`.\n\nWhile working on other metadata routing issues, I found that `TunedThresholdClassifierCV` also fails with such an object, since it creates a `_CurveScorer` which ignores the object and expects to just use the `_score_func` of a given _scorer_ object.\n\nConsider the following script:\n\n```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TunedThresholdClassifierCV, cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics._scorer import _Scorer, mean_squared_error, make_scorer\n\n\nclass MyScorer(_Scorer):\n    def _score(self, *args, **kwargs):\n        print(\"I'm logging stuff\")\n        return super()._score(*args, **kwargs)\n\ndef my_scorer(estimator, X, y, **kwargs):\n    print(\"I'm logging stuff in my_scorer\")\n    return mean_squared_error(estimator.predict(X), y, **kwargs)\n\ndef my_metric(y_pred, y_true, **kwargs):\n    print(\"I'm logging stuff in my_metric\")\n    return mean_squared_error(y_pred, y_true, **kwargs)\n\nmy_second_scorer = make_scorer(my_metric)\n\nX, y = make_classification()\n\n# this prints logs\nprint(\"cross_val_score'ing\")\ncross_val_score(\n    LogisticRegression(),\n    X,\n    y,\n    scoring=MyScorer(mean_squared_error, sign=1, kwargs={}, response_method=\"predict\"),\n)\n\nprint(\"1. TunedThresholdClassifierCV'ing\")\nmodel = TunedThresholdClassifierCV(\n    LogisticRegression(),\n    # scoring=MyScorer(mean_squared_error, sign=1, kwargs={}, response_method=\"predict\"),\n    # scoring=my_scorer,\n    scoring=my_second_scorer,\n)\nmodel.fit(X, y)\n\nprint(\"2. TunedThresholdClassifierCV'ing\")\nmodel = TunedThresholdClassifierCV(\n    LogisticRegression(),\n    s...",
      "labels": [
        "Bug",
        "API",
        "module:metrics"
      ],
      "state": "open",
      "created_at": "2025-08-07T10:50:45Z",
      "updated_at": "2025-08-20T19:16:08Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31889"
    },
    {
      "number": 31885,
      "title": "`SVC(probability=True)`  is not thread-safe",
      "body": "This was discovered while running:\n\n```\npytest -v --parallel-threads=4 --iterations=2 sklearn/svm/tests/test_sparse.py\n```\n\nbefore including the fix pushed to #30041 under https://github.com/scikit-learn/scikit-learn/pull/30041/commits/bce2b4eb7d5ab49cf758f98c667e86243883d1de.\n\nI suspect the problem is that the built-in Platt scaling implementation of the vendored C++ code base of libsvm that uses a singleton pseudo random generator. Therefore, seeding the shared RNG state from competing threads prevents getting reproducible results and hence the test failure.",
      "labels": [
        "Bug",
        "Moderate"
      ],
      "state": "open",
      "created_at": "2025-08-06T15:37:02Z",
      "updated_at": "2025-08-29T03:53:15Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31885"
    },
    {
      "number": 31884,
      "title": "pairwise_distances_argmin_min / ArgKMin64 is not thread-safe",
      "body": "### Describe the bug\n\nProblem found while test investigating failures found in #30041. I crafted a minimal reproducer below. It might be caused by a race condition (corruption) of shared intermediate buffers used in OpenMP threads.\n\nSome remarks:\n\n- the problem happens with either strategy (\"parallel_on_X\" vs \"parallel_on_Y\");\n- running the reproducer with `OMP_NUM_THREADS=1` hides the problem;\n- running the reproducer with a lower than default value for `OMP_NUM_THREADS` makes the problem less likely to happen;\n- using `threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")` does not hide the problem for some reason...\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.metrics._pairwise_distances_reduction._argkmin import ArgKmin64\nimport numpy as np\nfrom joblib import delayed, Parallel\nfrom threadpoolctl import threadpool_info\nfrom pprint import pprint\n\npprint(threadpool_info())\nrng = np.random.RandomState(0)\nX = rng.randn(97, 149)\nY = rng.randn(111, 149)\n\n\n# Note: strategy does not matter.\nshared_kwargs = dict(\n    k=1, metric=\"euclidean\", strategy=\"parallel_on_X\", return_distance=True\n)\nreference_results = ArgKmin64.compute(X, Y, **shared_kwargs)\n\nfor n_iter in range(10):\n    print(\".\", end=\"\")\n    for results in Parallel(n_jobs=4, backend=\"threading\")(\n        delayed(ArgKmin64.compute)(X, Y, **shared_kwargs) for _ in range(100)\n    ):\n        if shared_kwargs[\"return_distance\"]:\n            result_distances, result_indices = results\n            np.testing.assert_allclose(result_distances, reference_results[0])\n            np.testing.assert_array_equal(result_indices, reference_results[1])\n        else:\n            np.testing.assert_array_equal(results, reference_results)\n```\n\n### Expected Results\n\nNo error.\n\n### Actual Results\n\n```python-traceback\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[20], line 27\n     25 if shared_kwargs[\"return_di...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-08-06T14:10:35Z",
      "updated_at": "2025-08-22T08:13:22Z",
      "comments": 15,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31884"
    },
    {
      "number": 31883,
      "title": "Fitting different instances of `LinearSVR` is not thread-safe",
      "body": "### Describe the bug\n\nFound while working on #30041.\n\nSee the reproducer below. Fitting `LinearSVR` probably relies on a shared global state in the C++ code and that introduces a race condition when fitting several models concurrently in different threads. As a result, the outcomes are randomly corrupted.\n\n`LinearSVC` does not seem to have the problem (or at least not with its default solver).\n\n### Steps/Code to Reproduce\n\n```python\n# %%\nimport numpy as np\nfrom sklearn.svm import LinearSVR, LinearSVC\nfrom sklearn.datasets import make_regression\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom joblib import Parallel, delayed\n\n\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n\nX, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n\n\nC_range = np.logspace(-6, 6, 13)\n\nmodel_class = LinearSVR\nif model_class == LinearSVC:\n    y = np.sign(y)  # Convert to binary classification for LinearSVC\n\n\nsequential_results = [\n    model_class(C=C, random_state=0).fit(X, y).coef_.copy() for C in C_range\n]\n\n\nparallel_results = Parallel(n_jobs=4, backend=\"threading\")(\n    delayed(lambda C: model_class(C=C, random_state=0).fit(X, y).coef_.copy())(C)\n    for C in C_range\n)\nnp.testing.assert_array_equal(\n    sequential_results,\n    parallel_results,\n    err_msg=\"Parallel and sequential results differ.\",\n)\n```\n\n### Expected Results\n\nNothing.\n\n### Actual Results\n\n```python\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[22], line 32\n     23 sequential_results = [\n     24     model_class(C=C, random_state=0).fit(X, y).coef_.copy() for C in C_range\n     25 ]\n     28 parallel_results = Parallel(n_jobs=4, backend=\"threading\")(\n     29     delayed(lambda C: model_class(C=C, random_state=0).fit(X, y).coef_.copy())(C)\n     30     for C in C_range\n     31 )\n---> 32 np.testing.assert_array_equal(\n     33     sequential_results,\n...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-08-06T09:26:48Z",
      "updated_at": "2025-08-27T12:37:43Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31883"
    },
    {
      "number": 31872,
      "title": "Strange normalization of semi-supervised label propagation in `_build_graph`",
      "body": "The method `_build_graph` on the `LabelPropagation` class in `sklearn/semi_supervised/_label_propagation.py` [(line 455)](https://github.com/scikit-learn/scikit-learn/blob/7d1d96819172e2a7c826f04c68b9d93188cf6a92/sklearn/semi_supervised/_label_propagation.py#L455) treats normalization differently for sparse and dense kernels. I have questions about both of them.\n\n** (Edited) Summary **\nTroubles with the current code normalization:\n- In the dense affinity_matrix case, the current code sums axis=0 and then divides the rows by these sums. Other normalizations in semi_supervised use axis=1 (as this case should). This does not cause incorrect result so long as we have symmetric affinity_matrices. The dense case arises for kernel \"rbf\" which provides symmetric matrices. But if someone provides their own kernel the normalization could be incorrect.\n- In the sparse affinity_matrix case, the current code divides all rows by the sum of the first row. This is not standard normalization, but does not cause errors so long as the row sums are all the same. The sparse case arises for kernel \"knn\" which has all rows sum to k. But if someone provides their own kernel the normalization could be incorrect.\n- The normalization is different for the dense and sparse cases, which could be confusing to someone writing their own kernel.\n\nThe fix involves changing `axis=0` to `axis=1` and correcting the sparse case to divide each row by its sum when the row sums are not all equal.\n\n<details>\n\n<summary> original somewhat rambling description </summary>\n\n** Summary **\nThe method returns a different `affinity_matrix` for sparse and for dense versions of the same kernel matrix. Neither sparse nor dense versions normalize the usual way (columns sum to 1). The dense case is correct for symmetric input kernels. The sparse case scales all values by a constant instead of by column sums.\n\nI suspect the results still converge in most non-symmetric cases. That's probably why this hasn't caused any issue...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-08-03T21:59:31Z",
      "updated_at": "2025-08-11T13:05:09Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31872"
    },
    {
      "number": 31871,
      "title": "Proposal to Contribute Uncertainty Quantification via Aleatoric/Epistemic Decomposition to scikit-learn",
      "body": "### Describe the workflow you want to enable\n\nHi,\n\nWhile ensemble methods like RandomForestRegressor are widely used, scikit-learn currently lacks native support for estimating and exposing predictive uncertainty—an increasingly essential feature in many applied domains such as healthcare, scientific modeling, and decision support systems.\n\n### Describe your proposed solution\n\n\nI propose adding functionality to expose both:\n\n    Aleatoric uncertainty (data-driven),\n    Epistemic uncertainty (model-driven).\n\n\nImportantly, this is not just a concept—I have already implemented this wrapper as part of my ongoing PhD research. The approach is detailed in a preprint available here:\n\nhttp://dx.doi.org/10.22541/au.175373261.14525669/v1 . \n\nThe implementation is functional, tested, and used in geophysical mapping described in the paper.\n\nThis contribution builds on established research by Mohammad Hossein Shaker and Eyke Hüllermeier in uncertainty estimation for Random Forest Classification, and I have extended those principles to Random Forest Regression.\n\nThe approach is detailed in this article available here:\n\nhttp://dx.doi.org/10.1007/978-3-030-44584-3_35\n\nThanks\n\n### Describe alternatives you've considered, if relevant\n\n\n\n\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-08-03T07:09:08Z",
      "updated_at": "2025-08-04T16:55:35Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31871"
    },
    {
      "number": 31870,
      "title": "Faster algorithm for KMeans",
      "body": "### Describe the workflow you want to enable\n\nDear community and developers, \n\nI think [this work](https://arxiv.org/abs/2308.09701) might be interesting to the scikit-community.  In this work, we discuss 2 classical algorithms for an sampling-based version of k-means, which return an epsilon-approximation of the centroids (which is user-determined). \n\nI was wondering if this could be an interesting addition to your (great) library, as it shows practical advantages already on small datasets.\n\n### Describe your proposed solution\n\nAlgorithm 1 of  [this work](https://arxiv.org/abs/2308.09701) can result in a faster k-mean algorithm. \n\nI implemented the algorithm, which can be found [here](\nhttps://github.com/Scinawa/do-you-know-what-q-means). However, as it is just a proof of concept, is not ready to be merged in scikit-learn. \n\n\n\n### Describe alternatives you've considered, if relevant\n\nThere are other fast coreset-based algorithms, which are much more complicated to implement, and are practically slower than our algorithm. \n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-08-03T05:18:33Z",
      "updated_at": "2025-08-04T11:14:48Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31870"
    },
    {
      "number": 31869,
      "title": "Array API support for CalibratedClassifierCV",
      "body": "### Describe the workflow you want to enable\n\nTowards #26024. \nUse `CalibratedClassifierCV` with pytorch or tensorflow models.\nThis has become even more interesting use case with #31068.\n\n### Describe your proposed solution\n\nIn line with out Array API adoption path.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "help wanted",
        "Hard",
        "module:calibration",
        "Array API"
      ],
      "state": "open",
      "created_at": "2025-08-02T10:02:10Z",
      "updated_at": "2025-09-05T02:20:31Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31869"
    },
    {
      "number": 31862,
      "title": "Ordinal Encoder Type Hints State unknown_value should be float, but this produces an error.",
      "body": "### Describe the bug\n\nFollowing the type hints of the OrdinalEncoder I set the unknown_value parameter to -1.0.\n\n<img width=\"507\" height=\"146\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b9c86ab1-7a23-47b3-ad89-9de091c8d81e\" />\n\nThis produces an error when handle_unknown='use_encoded_value' as it needs and int. Should hopefully just be as easy as updating the type hints unless there is something I'm missing?\n\n### Steps/Code to Reproduce\n\n```python\nordinal_encoder = OrdinalEncoder(\n                handle_unknown=\"use_encoded_value\", unknown_value=-1\n            )\n\nordinal_encoder.fit_transform(...)\n```\n\n### Expected Results\n\nExpected result would be to not get an error when following type hints.\n\n### Actual Results\n\nAn error is raised about the type of the unknown_value\n\n### Versions\n\n```shell\ninternal_api: openblas\n    num_threads: 12\n         prefix: libscipy_openblas\n       filepath: /databricks/python3/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 6\n         prefix: libgomp\n       filepath: /databricks/python3/lib/python3.12/site-packages/torch/lib/libgomp-a34b3233.so.1\n        version: None\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 12\n         prefix: libgomp\n       filepath: /databricks/python3/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-31T21:57:19Z",
      "updated_at": "2025-08-01T07:27:02Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31862"
    },
    {
      "number": 31859,
      "title": "Intercepts of Newton-Cholesky logistic regression get corrupted when warm starting",
      "body": "### Describe the bug\n\nWhen using multinomial logistic regression with warm starts from a previous iteration, the final coefficients in the model are correct, but the intercepts somehow get filled with incorrect numbers somewhere.\n\nAs a result, predictions from a warm-started model differ from those of a cold-start model that has more iterations on the same data.\n\nThe issue appears to have been introduced recently as it works fine with version 1.5, but not with 1.6 or 1.7.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\n\nmodel1 = LogisticRegression(\n    solver=\"newton-cholesky\",\n    max_iter=2\n).fit(X, y)\nmodel2 = LogisticRegression(\n    solver=\"newton-cholesky\",\n    max_iter=1,\n    warm_start=True\n).fit(X, y).fit(X, y)\n\nnp.testing.assert_almost_equal(\n    model1.coef_,\n    model2.coef_\n)\n\nnp.testing.assert_almost_equal(\n    model1.predict_proba(X[:5]),\n    model2.predict_proba(X[:5])\n)\n```\n\n### Expected Results\n\nIntercepts should be the same, up to shifting by a constant if needed.\n\n### Actual Results\n\nIntercepts are different, as are predicted probabilities\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]\nexecutable: /home/david/miniforge3/bin/python\n   machine: Linux-6.12.33+deb12-amd64-x86_64-with-glibc2.36\n\nPython dependencies:\n      sklearn: 1.7.1\n          pip: 24.2\n   setuptools: 74.1.2\n        numpy: 2.0.1\n        scipy: 1.14.1\n       Cython: 3.1.0\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 20\n         prefix: libscipy_openblas\n       filepath: /home/david/.local/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Has...",
      "labels": [
        "Bug",
        "module:linear_model"
      ],
      "state": "closed",
      "created_at": "2025-07-31T11:26:16Z",
      "updated_at": "2025-08-11T08:18:13Z",
      "comments": 14,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31859"
    },
    {
      "number": 31849,
      "title": "Extend make file to inlcude initial setup installations.",
      "body": "### Describe the workflow you want to enable\n\nI recently made my first contribution to sklearn and found it a bit tidious to do the initial setup after cloning the repo. I think that extending the make file to include something similar to `make inital setup` to install the dependencies in [step 4 of the Contributing > Contributing code > How to contribute](https://scikit-learn.org/stable/developers/contributing.html) would be benefitial. Additionnaly adding a script ot run the git commands. I'd love to implement this so please, let me know if this is something of interest! \n\n### Describe your proposed solution\n\nExtending the make file to include something similar to `make inital setup` to install the dependencies in [step 4 of the Contributing > Contributing code > How to contribute](https://scikit-learn.org/stable/developers/contributing.html)\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-28T19:02:54Z",
      "updated_at": "2025-07-29T07:40:22Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31849"
    },
    {
      "number": 31840,
      "title": "SkLearn IQR function",
      "body": "### Describe the workflow you want to enable\n\nRecently, I was working on a machine learning project with a dataset that was quite skewed. I repeatedly had to compute the interquartile range (IQR), calculate the 25th and 75th percentiles, visualize the box plot, and then remove outliers — all manually.\n\nWhile this wasn't an issue at first, it became tedious to write the same code over and over again for different rows and columns. This made me wonder: Wouldn’t it be much more efficient if scikit-learn offered a built-in utility to calculate the IQR and optionally remove or flag outliers?\n\nI believe this kind of functionality could significantly streamline the preprocessing workflow for many users.\n\n### Describe your proposed solution\n\nI’d like to suggest adding a simple utility class to scikit-learn (or as part of a preprocessing module), called OutlierRemoval. This class would encapsulate all IQR-related preprocessing logic and expose a clean interface for users to apply it.\n\n```py\nclass OutlierRemoval:\n    def __init__(self, multiplier: float = 1.5):\n        # Multiplier for the IQR rule (default is 1.5)\n        ...\n\n    def get_q1(self, X, column):\n        # Returns the 25th percentile for a column\n        ...\n\n    def get_q3(self, X, column):\n        # Returns the 75th percentile for a column\n        ...\n\n    def calculate_iqr(self, X, column):\n        # Returns IQR = Q3 - Q1\n        ...\n\n    def plot_boxplot(self, X, column):\n        # Displays a boxplot for the column\n        ...\n\n    def remove_outliers(self, X, column):\n        # Removes rows with outliers from the dataset\n        ...\n```\n\nPrevents redundant code when handling outliers across multiple projects\n\nEncourages best practices in preprocessing pipelines\n\nMakes exploratory data analysis (EDA) cleaner and more intuitive\n\nAligns with scikit-learn’s emphasis on reusable, composable preprocessing tools\n\n### Describe alternatives you've considered, if relevant\n\nI've used pandas and numpy to manually calcu...",
      "labels": [
        "New Feature"
      ],
      "state": "closed",
      "created_at": "2025-07-27T03:39:59Z",
      "updated_at": "2025-08-04T11:37:11Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31840"
    },
    {
      "number": 31834,
      "title": "Resource cleanup issues in dataset loaders: files opened but not closed.",
      "body": "### Describe the bug\n\nTwo dataset loader functions in `sklearn.datasets` have resource cleanup issues where files are opened but not properly closed using context managers, potentially leading to resource leaks.\n\nThe first one is more important:\n### in _lfw.py:\nLine 172:  `pil_img = Image.open(file_path)` -  an image is opened each iteration of the loop.\nThe file handle is never explicitly closed. \nPIL does not always immediately close the file. This can exhaust file descriptors.\n\nThis one is less severe:\n### In _kddcup99.py:\nLines 390 - 394: The file is opened and manually closed using `file_.close()`, but not inside a `try`/`finally` or `with` block.\nfile_.close() appears after a loop without exceptions. This means that if an error occurs in the loop, the file remains open.\n\n### Steps/Code to Reproduce\n\nCode snippet shouldn't be necessary - \n### Primary Issue in _lfw.py\nOpening many images without closing can exhaust system file descriptors\nUnclosed file handles can prevent garbage collection\nApplications or notebooks that repeatedly fetch the dataset could accumulate thousands of unclosed files\n### Secondary Issue in _kddcup99.py\nIf line.decode() fails (encoding issues), file remains open.\nIf Xy.append() fails (memory constraints), file remains open.\nKeyboard interruption during process, file remains open.\n\n### Expected Results\n\nAll files should be opened using context managers, or \n```python\nwith Image.open(file_path) as pil_img:\n    # processing\n```\nensuring proper closure even if exceptions are raised. This ensures file handles are released immediately, and code is safe under interruption or failure.\n\n### Actual Results\n\nFiles are opened without being explicitly closed, leading to:\nExhaustion of file descriptors when loading the dataset multiple times, unexpected behavior under memory pressure or long sessions.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\nexecutable: C:python.exe\n ...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-25T01:30:26Z",
      "updated_at": "2025-07-25T10:26:47Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31834"
    },
    {
      "number": 31811,
      "title": "Bug: StackingRegressor serialization error with custom neural network regressors (TabTransformer, ANN, DNN)",
      "body": "### Describe the bug\n\nBug Report: StackingRegressor in scikit-learn Fails with Custom Neural Network Regressors\nDear scikit-learn Maintainers,\nI am Dr. Mohsen Jahan, Professor of Agroecology and Instructor of Artificial Intelligence and Digital Transformation at Ferdowsi University of Mashhad, Iran. While conducting a research project on multi-objective feature selection using the NSGA-III algorithm and stacking models, I encountered an issue with the StackingRegressor implementation in scikit-learn (version 1.5.2). Specifically, this module exhibits compatibility issues with custom regression models, particularly those based on neural networks such as TabTransformerRegressor, ANNRegressor, and DNNRegressor.\nIssue Description\nWhen using StackingRegressor in scikit-learn with custom regression models that adhere to the standard scikit-learn API (e.g., implementing fit and predict methods) but rely on complex internal structures (e.g., based on tensorflow or pytorch), serialization or cloning errors occur. These errors manifest particularly when such models are used as regressors or meta_regressor in StackingRegressor, affecting processes like GridSearchCV or model persistence with joblib. For instance, in our project, employing a custom SimpleDNNRegressor (built with tensorflow) as the meta-regressor in StackingRegressor resulted in serialization errors. This issue was not observed when using mlxtend.regressor.StackingRegressor (version 0.23.1), which handles custom models more robustly due to its more flexible cloning/serialization mechanisms.\nTechnical Details\n\nscikit-learn Version: 1.5.2\nAffected Models: TabTransformerRegressor, ANNRegressor, DNNRegressor, and likely other neural network-based regressors\nAffected Module: sklearn.ensemble.StackingRegressor\nObserved Errors:\nSerialization errors during GridSearchCV or model saving with joblib.\nIncompatibility with custom models leveraging external libraries (e.g., tensorflow).\n\n\nWorkaround: Using mlxtend.regressor.St...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-22T04:57:19Z",
      "updated_at": "2025-07-22T04:57:59Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31811"
    },
    {
      "number": 31810,
      "title": "CI: Enable GitHub Actions App for ppc64le (Power architecture) support",
      "body": "Hi scikit-learn team,\n\nWe’re reaching out to propose enabling CI support for the ppc64le (IBM Power) architecture in your repository, as part of a broader effort to ensure cross-platform compatibility in the scientific Python ecosystem.\n\nWe’re using a GitHub Actions (GHA)-based runner service provided and maintained by IBM to run jobs for the ppc64le architecture. This setup has already been successfully integrated into projects like:\n\n✅ [cryptography](https://github.com/pyca/cryptography/issues/13086)\n\n📌 [Tracking issue in NumPy](https://github.com/numpy/numpy/issues/29125)\n\nWe’d now like to propose enabling the GitHub Actions app in this repository to allow running CI jobs for ppc64le directly via GitHub Actions. This would support upstream compatibility and help ensure continued support for the Power architecture in scikit-learn.\n\nKey Benefits:\n🔒 Ephemeral and secure runners, isolated per job\n\n🛠️ Maintained by IBM, requires no setup effort from your side\n\n🔁 Integrates with existing GitHub Actions workflows\n\n📚 Technical documentation and usage details:\nhttps://github.com/IBM/actionspz/tree/main/docs\n\nWe’re happy to assist with the setup or provide any additional details the team may need.\n\nThanks so much!",
      "labels": [
        "Build / CI",
        "Needs Decision"
      ],
      "state": "open",
      "created_at": "2025-07-21T17:36:40Z",
      "updated_at": "2025-08-13T08:53:27Z",
      "comments": 12,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31810"
    },
    {
      "number": 31808,
      "title": "Handle new `pd.StringDtype` that is coming in pandas 3",
      "body": "This issue is the result of investigating https://github.com/scikit-learn/scikit-learn/issues/31778\n\nThe failures in the nightlies are due to changes coming in pandas 3.0. In particular the switch to using `StringDtype` as the type for string columns. The old behaviour was to use `object`.\n\nThis has a few effects:\n- can no longer use `np.issubdtype` because the new dtype isn't one known to numpy\n- selecting columns in `ColumnTransformer` doesn't select the right columns anymore\n\nThese are the failing tests:\n```\nFAILED compose/tests/test_column_transformer.py::test_make_column_transformer_pandas - TypeError: Cannot interpret '<StringDtype(storage='python', na_value=nan)>'...\nFAILED compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[pd-index-expected_cols4] - TypeError: Cannot interpret '<StringDtype(storage='python', na_value=nan)>'...\nFAILED compose/tests/test_column_transformer.py::test_make_column_selector_with_select_dtypes[cols1-None-None-object] - AssertionError: \nFAILED compose/tests/test_column_transformer.py::test_make_column_selector_with_select_dtypes[cols3-None-include3-None] - AssertionError: \nFAILED compose/tests/test_column_transformer.py::test_make_column_selector_with_select_dtypes[cols4-None-object-None] - AssertionError: \nFAILED compose/tests/test_column_transformer.py::test_make_column_selector_with_select_dtypes[cols12-None-include12-None] - AssertionError: \nFAILED compose/tests/test_column_transformer.py::test_column_transformer_with_make_column_selector - AssertionError: \nFAILED preprocessing/tests/test_encoders.py::test_encoder_dtypes_pandas - assert False\nFAILED preprocessing/tests/test_function_transformer.py::test_function_transformer_with_dataframe_and_check_inverse_True - TypeError: Cannot interpret '<StringDtype(storage='python', na_value=nan)>'...\n```\n\nThree of these (first one and last two) are due to using `issubdtype`. The other failures are due to not selecting the right columns (n.b. the way the test...",
      "labels": [
        "Enhancement",
        "Moderate",
        "module:compose",
        "module:preprocessing",
        "Pandas compatibility"
      ],
      "state": "closed",
      "created_at": "2025-07-21T12:21:44Z",
      "updated_at": "2025-07-23T05:51:08Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31808"
    },
    {
      "number": 31806,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jul 21, 2025) ⚠️",
      "body": "CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78376&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a) (Jul 21, 2025)\n\nTest Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-21T08:34:55Z",
      "updated_at": "2025-07-21T08:35:47Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31806"
    },
    {
      "number": 31804,
      "title": "DOC metadata docstrings generator has wrong indentation",
      "body": "### Describe the issue linked to the documentation\n\nI am a maintainer of a third party package [fastcan](https://github.com/scikit-learn-contrib/fastcan).\n\nAfter I update the scikit-learn version from 1.7.0 to 1.7.1, the Sphinx document generation gives the following error.\n\n```\nParameters\n---------- [docutils]\n<SOME PY SCRIPT>:docstring of sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>.func:38: CRITICAL: Unexpected section title.\n```\n\nThe raw error log of readthedocs build can be found [here](https://app.readthedocs.org/api/v2/build/28919247.txt).\n\nIt is suspected the error is caused by the wrong indentation in `sklearn.utils._metadata_requests.py` as below.\n\n```python\nREQUESTER_DOC = \"\"\"\nConfigure whether metadata should be requested to be passed to the ``{method}`` method.\n```\n\n### Suggest a potential alternative/fix\n\nThe correct indentation should be as below\n\n```python\nREQUESTER_DOC = \"\"\"        Configure whether metadata should be requested to be \\\npassed to the ``{method}`` method.\n```\n\nI am not sure why the official documents of scikit-learn does not have this error. However, at least for consistence with `REQUESTER_DOC_PARAM` and `REQUESTER_DOC_RETURN`, which have 8 spaces indentation, `REQUESTER_DOC` should also have 8 spaces indentation.",
      "labels": [
        "Documentation",
        "Metadata Routing"
      ],
      "state": "closed",
      "created_at": "2025-07-21T06:30:14Z",
      "updated_at": "2025-07-22T05:53:37Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31804"
    },
    {
      "number": 31799,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jul 21, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78376&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Jul 21, 2025)\n- Test Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-21T02:33:56Z",
      "updated_at": "2025-07-22T08:37:38Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31799"
    },
    {
      "number": 31789,
      "title": "⚠️ CI failed on Wheel builder (last failure: Jul 19, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/16384706430)** (Jul 19, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-19T04:25:21Z",
      "updated_at": "2025-07-20T04:53:11Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31789"
    },
    {
      "number": 31781,
      "title": "Documentation may be inaccurate regarding deprecation of `multi_class` in LogisticRegression",
      "body": "### Describe the issue linked to the documentation\n\nIn the documentation for `LogisticRegression`  under `multi_class`, there is a [note:](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=Deprecated%20since%20version%201.5%3A%20multi_class%20was%20deprecated%20in%20version%201.5%20and%20will%20be%20removed%20in%201.7.) \n\"Deprecated since version 1.5: `multi_class` was deprecated in version 1.5 and will be removed in 1.7. \" \n\nHowever, I think this will be removed in version 1.8, based on this PR: https://github.com/scikit-learn/scikit-learn/pull/31241\n\n\n### Suggest a potential alternative/fix\n\nChange the docs to 1.8 version - if that is correct.",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-18T08:39:11Z",
      "updated_at": "2025-07-21T09:05:36Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31781"
    },
    {
      "number": 31776,
      "title": "Documentation Bug: Warning about \"unstable development version\"",
      "body": "### Describe the issue linked to the documentation\n\nWhen browsing the scikit-learn documentation, I selected a stable version (e.g., 1.7.0) from the versions. However, I still see the warning banner at the top of the page: **This is documentation for an unstable development version.**\n\nThis is a bit confusing, as I'm clearly viewing a stable release. \n\n<img width=\"1748\" height=\"830\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3e236cbb-31cd-4e77-aead-05cdee6408c9\" />\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Bug",
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-07-17T14:02:05Z",
      "updated_at": "2025-07-18T09:28:38Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31776"
    },
    {
      "number": 31773,
      "title": "Anaconda new ToS causing CI failures",
      "body": "New Anaconda ToS: https://www.anaconda.com/legal/terms/terms-of-service , effective 15 July 2025, is causing the follow error in our CIs:\n\n```\nCondaToSNonInteractiveError: Terms of Service have not been accepted for the following channels. Please accept or remove them before proceeding:\n    • https://repo.anaconda.com/pkgs/main\n    • https://repo.anaconda.com/pkgs/r\n\nTo accept a channel's Terms of Service, run the following and replace `CHANNEL` with the channel name/URL:\n    ‣ conda tos accept --override-channels --channel CHANNEL\n\nTo remove channels with rejected Terms of Service, run the following and replace `CHANNEL` with the channel name/URL:\n    ‣ conda config --remove channels CHANNEL\n```\n\nWe can use [`conda-anaconda-tos`](https://www.anaconda.com/docs/getting-started/tos-plugin) or potentially switch to miniforge ?\n\n@scikit-learn/core-devs @scikit-learn/communication-team @scikit-learn/documentation-team \n\n(Of interest here is corresponding issue in pytorch https://github.com/pytorch/pytorch/issues/158438)",
      "labels": [
        "High Priority"
      ],
      "state": "closed",
      "created_at": "2025-07-17T03:36:55Z",
      "updated_at": "2025-07-22T21:50:54Z",
      "comments": 20,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31773"
    },
    {
      "number": 31769,
      "title": "⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Jul 16, 2025) ⚠️",
      "body": "**CI failed on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78222&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (Jul 16, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-16T02:33:49Z",
      "updated_at": "2025-07-16T15:13:39Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31769"
    },
    {
      "number": 31768,
      "title": "⚠️ CI failed on Ubuntu_Jammy_Jellyfish.pymin_conda_forge_openblas_ubuntu_2204 (last failure: Jul 16, 2025) ⚠️",
      "body": "**CI failed on [Ubuntu_Jammy_Jellyfish.pymin_conda_forge_openblas_ubuntu_2204](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78222&view=logs&j=f71949a9-f9d9-549e-cf45-2e99c7b412d1)** (Jul 16, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-16T02:33:33Z",
      "updated_at": "2025-07-16T15:13:38Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31768"
    },
    {
      "number": 31767,
      "title": "⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: Jul 16, 2025) ⚠️",
      "body": "**CI failed on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78222&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (Jul 16, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-16T02:33:26Z",
      "updated_at": "2025-07-16T15:13:38Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31767"
    },
    {
      "number": 31766,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jul 16, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78222&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Jul 16, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-16T02:32:08Z",
      "updated_at": "2025-07-16T15:13:38Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31766"
    },
    {
      "number": 31761,
      "title": "y_pred changed to y_true in RocCurveDisplay.from_predictions, but not in DetCurveDisplay.from_predictions",
      "body": "The parameter `y_pred` was deprecated in `RocCurveDisplay.from_predictions` and replaced by `y_score`. Although the  `y_pred` parameter in `DetCurveDisplay.from_predictions`  has an identical docstring (except for details about the name change), it was not renamed. \n\nIt seems to me that both signatures should match in that regard.\n\nI'm not sure if it applies to other binary display parameters, but this relates to https://github.com/scikit-learn/scikit-learn/issues/30717.",
      "labels": [
        "API"
      ],
      "state": "closed",
      "created_at": "2025-07-15T14:23:05Z",
      "updated_at": "2025-07-25T18:14:15Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31761"
    },
    {
      "number": 31754,
      "title": "In Balltree, filter out/mask specific points in query",
      "body": "### Describe the workflow you want to enable\n\nI would like to be able to query nearest points within a Balltree but excluding some of them.\nE.g. I create a Balltree on 60k points. I want to find the k nearest neighbour points but within a subset of the 60k points. \nExample case: I have N clusters of points. I build a Balltree with all the points of the N clusters (e.g. 60k points). Then I want to find for each of the points of a given cluster the closest point from the other clusters (i.e. excluding itself).\n\n### Describe your proposed solution\n\n I would like to pass an extra mask argument (e.g. array of 60k elements) to the query with True for the points in the other clusters and False for the points in the specific cluster.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature"
      ],
      "state": "closed",
      "created_at": "2025-07-13T20:32:13Z",
      "updated_at": "2025-07-30T15:13:44Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31754"
    },
    {
      "number": 31750,
      "title": "Full Python/sklearn Adaptation of py-earth",
      "body": "### Describe the workflow you want to enable\n\nA full Python (not c or cython) port of py-earth, an archived sklearn project.\n\n### Describe your proposed solution\n\n- MARS regression is a great and really practical technique.\n- py-earth implemented this, based in the R earth library.\n- The archived state of py-earth means it's only possible to get working with old dependencies which limits the ability to use it with newer tools and in more current workflows..\n\n### Describe alternatives you've considered, if relevant\n\n- I tried to modernise py-earth, but got tripped up on lots of issues such as Python 2 to 3 conversion, the old scipy dependencies etc.\n- py-earth was mostly consistent with sklearn, but not completely.\n- I've created a full Python port (repo still private, as the repo is still a bit messy), as a secondary output of my PhD.\n- I would like to try introduce it as a 'spiritual' successor to py-earth and collaborate with the sklearn community.\n- Keen to get some guidance on approaching this, as I'm relatively new to contributing.\n\n### Additional context\n\n- For policy and decision contexts, the stepwise linear approach and combination of a visualisable model and change points, means MARS regression has advantages over other modelling methods.\n- For changepoint analysis involving gradients, MARS is easier and nicer to work with than PELT-based changepoints (ruptures).\n- What this means is that in sklearn workflows, it's potentially a useful prediction method for decision-analysis and forecasting.\n- Whilst the performance of the resulting models may not be as good as other techniques, that's made up for by the advantage of explainability and the adaptive approach.",
      "labels": [
        "New Feature",
        "Needs Decision"
      ],
      "state": "open",
      "created_at": "2025-07-13T01:27:13Z",
      "updated_at": "2025-07-16T12:39:42Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31750"
    },
    {
      "number": 31738,
      "title": "Present parameters and attributes sorted alphabetically to make it easier to find them on the documentation pages.",
      "body": "### Describe the issue linked to the documentation\n\n## Example\nOn documentation page https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html the parameters are listed out of order, with \"hidden_layer_sizes\" being shown at the top, followed by \"activation\", that should be the first parameters among the three visible on this screenshot. The \"solver\" parameter is kind of better positioned than the other two, but it's actually not well positioned at all, because after it we have the \"alpha\" parameter, which should be at the top of the list since it starts with \"a\". \"batch_size\" should appear after the parameters that start with \"a\", and so on.\n\n<img width=\"992\" height=\"862\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/74910bb8-3c5f-41db-ba5d-f78e09a40c14\" />\n\n### Suggest a potential alternative/fix\n\nSort the parameters and attributes alphabetically by name before presenting them on the documentation pages.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-07-10T23:50:59Z",
      "updated_at": "2025-07-31T06:50:16Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31738"
    },
    {
      "number": 31733,
      "title": "Add More Data to the RidgeCV, LassoCV, and ElasticNetCV Path",
      "body": "### Describe the workflow you want to enable\n\nCurrently, the mse_path_ is available from the above models, which lets you inspect/plot the mse for all folds, alphas, and l1_ratios for elasticnet for instance. It would be very nice to record not only the mse in this way, but also the coefficients and possibly the in-sample/validation score.\n\n### Describe your proposed solution\n\nAdd variables that include the coefficients and maybe the score.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "spam",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-09T19:53:32Z",
      "updated_at": "2025-07-10T03:56:41Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31733"
    },
    {
      "number": 31731,
      "title": "`scipy.minimize(method=’L-BFGS-B’)` deprecation warning for `iprint` and `disp` arguments",
      "body": "### Describe the bug\n\nWhen upgrading to scipy 1.16, fitting a LogisticRegression raises a deprecation warning:\n\n```\nDeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n```\n\nThe [documentation page of scipy.minimize](https://docs.scipy.org/doc/scipy-1.16.0/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb) mentions this double deprecation.\n\n### Steps/Code to Reproduce\n\n`python -Wd`\n```python\n>>> from sklearn.linear_model import LogisticRegression\n>>> import numpy as np\n>>> X = np.array([[1], [0]])\n>>> y = np.array([1, 0])\n>>> LogisticRegression().fit(X, y)\nDeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n  opt_res = optimize.minimize(\n```\n\n### Expected Results\n\nNo deprecation warning\n\n### Actual Results\n\nSee above\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.4 (main, Jul 25 2024, 22:11:22) [Clang 18.1.8 ]\nexecutable: /Users/vincentmaladiere/dev/inria/skrub/.venv/bin/python\n   machine: macOS-14.0-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.7.0\n          pip: None\n   setuptools: 80.9.0\n        numpy: 2.3.1\n        scipy: 1.16.0\n       Cython: None\n       pandas: 2.3.1\n   matplotlib: 3.10.3\n       joblib: 1.5.1\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /Users/vincentmaladiere/dev/inria/skrub/.venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-07-09T13:32:38Z",
      "updated_at": "2025-07-09T14:25:27Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31731"
    },
    {
      "number": 31728,
      "title": "Making the extension contract stable through version upgrades",
      "body": "### Describe the workflow you want to enable\n\nCurrently, every time `scikit-learn` releases a new minor version - e.g., 1.5.0, 1.6.0, 1.7.0 - compliant extensions, e.g., custom transformers, classifiers, etc, break - specifically, referring to the API conformance as tested through `check_estimator` or `parametrize_with_checks`.\n\nThese repeated breakages in the \"extender contract\" contrast the stability of the usage contract, which is stable and professionally managed.\n\nFor a package like `scikit-learn` which means to be a standard not just for ML algorithms but also an API standard that everyone uses, this is not a good state to be in - \"do not break user code\" is the maxim that gets broken for power users writing extensions.\n\nOf course maintaining downwards compatibility is not always possible, but nothing should break without a proper warning.\n\n### Describe your proposed solution\n\nThe main reason imo why this keeps happening is that `scikit-learn` is not using a proper pattern that ensures stability of the extension contract - and also no secondary deprecation patterns in relation to it.\n\nA simple pattern that could improve a lot would be the \"template pattern\", in a specific form to separate likely changing parts such as the boilerplate (e.g., `validate_data` vs `_validate_data` and such) from the extension locus.\nReference: https://refactoring.guru/design-patterns/template-method\n\nExamples of how this can be used to improve stability:\n\n* `sktime`, for a different API, has a separation between `fit` calling an internal `_fit`, where change-prone boilerplate is sandwiched between a stable user contract (`fit`) and a stable extender contract (`_fit`); similarly `predict` and `_predict`\n* `feature-engine` overrides the `BaseTransformer` `scikit-learn` extension contract with a similar pattern using `super()` calls in `fit` etc.\n\nIn particular the `fit`/`_fit` pairing that combines strategy and template pattern can be introduced easily via pure internal refactoring -...",
      "labels": [
        "New Feature",
        "Developer API"
      ],
      "state": "closed",
      "created_at": "2025-07-09T10:26:51Z",
      "updated_at": "2025-08-09T22:03:07Z",
      "comments": 13,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31728"
    },
    {
      "number": 31725,
      "title": "Confusion around coef_ and intercept_ for Polynomial Ridge Regression inside a Pipeline",
      "body": "### Describe the issue linked to the documentation\n\nWhen using a Pipeline with PolynomialFeatures and Ridge, it's unclear in the documentation how to extract the actual model coefficients and intercept to reproduce the regression equation manually (outside scikit-learn).\n\nFor example, when fitting a polynomial regression with:\n\nmake_pipeline(PolynomialFeatures(degree=3), Ridge())\n\nMost users wrongly assume that coef_[0] is the intercept, which it is not. This behavior is not explained clearly in the Ridge or Pipeline documentation and led to confusion even after reading the docs and searching online.\n\nThis is a common use case — for example, when exporting trained models to plain Python, Java, or C++.\n\n### Suggest a potential alternative/fix\n\n### ✅ Suggested Fix\n\n\nThe coefficients returned by `.coef_` include the weight for the constant basis function (created by `PolynomialFeatures`), but the actual y-intercept is stored separately in `.intercept_`. This makes it unclear how to reconstruct an equation like:\n\ny = a·x³ + b·x² + c·x + d\n\n### Suggested Fix:\n\n1. In the `Ridge`, `Pipeline`, and/or `PolynomialFeatures` documentation, add a clear explanation that:\n   - `PolynomialFeatures(degree=n)` creates features `[1, x, x², ..., xⁿ]`\n   - The intercept is **not** included in `.coef_`, but is returned separately as `.intercept_`\n   - The first element of `.coef_` corresponds to the coefficient of the constant term `1`, not the model intercept\n\n2. Provided a code snippet that reconstructs the polynomial using both:\n\n```python\ncoefs = model.named_steps['ridge'].coef_\nintercept = model.named_steps['ridge'].intercept_\n```\n\nThis change would help students and developers trying to reproduce the regression manually in another language or platform.",
      "labels": [
        "Documentation",
        "spam",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-09T04:41:13Z",
      "updated_at": "2025-07-26T16:03:23Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31725"
    },
    {
      "number": 31724,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Jul 09, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=78075&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Jul 09, 2025)\n- Test Collection Failure",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-07-09T02:34:25Z",
      "updated_at": "2025-07-10T13:07:08Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31724"
    },
    {
      "number": 31722,
      "title": "`test_unsorted_indices` for `SVC` may fail randomly with sparse vs dense data",
      "body": "### Describe the bug\n\nThe [<code>test_unsorted_indices</code>](https://github.com/scikit-learn/scikit-learn/blob/cfd5f7833dfb3794e711e79e4a3373e599d5a1f0/sklearn/svm/tests/test_sparse.py#L121) function occasionally fails on CI when comparing the coefficients of `SVC(kernel=\"linear\", probability=True, random_state=0)` trained on dense vs sparse data.\n\nI suspect this is due to additional randomness introduced by the internal cross-validation and Platt scaling when `probability=True` is set. See the [SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for reference.\n\n### Steps/Code to Reproduce\n\nUnfortunately, I haven't been able to reproduce the failure reliably. I've only seen it fail three times when creating or reviewing PRs, but the error disappears after re-running CI.\n\nI've also tried looping through various `random_state` values without triggering a failure locally.\n\nFor now, I'm labelling this with \"Hard\" and \"Needs Reproducible Code.\"\n\n### Expected Results\n\n```python\ndef test_unsorted_indices(csr_container):\n    # test that the result with sorted and unsorted indices in csr is the same\n    # we use a subset of digits as iris, blobs or make_classification didn't\n    # show the problem\n    X, y = load_digits(return_X_y=True)\n    X_test = csr_container(X[50:100])\n    X, y = X[:50], y[:50]\n    tols = dict(rtol=1e-12, atol=1e-14)\n\n    X_sparse = csr_container(X)\n    coef_dense = (\n        svm.SVC(kernel=\"linear\", probability=True, random_state=0).fit(X, y).coef_\n    )\n    sparse_svc = svm.SVC(kernel=\"linear\", probability=True, random_state=0).fit(\n        X_sparse, y\n    )\n    coef_sorted = sparse_svc.coef_\n    # make sure dense and sparse SVM give the same result\n    assert_allclose(coef_dense, coef_sorted.toarray(), **tols)\n```\nshould consistently pass.\n\n### Actual Results\n\nIn rare cases, the assertion fails:\n\n```console\nAssertionError: \nNot equal to tolerance rtol=1e-07, atol=0       \nMismatched elements: 2 / 2880 (0.0694%...",
      "labels": [
        "Bug",
        "Hard",
        "module:svm",
        "Needs Reproducible Code"
      ],
      "state": "closed",
      "created_at": "2025-07-08T00:16:35Z",
      "updated_at": "2025-07-08T20:05:39Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31722"
    },
    {
      "number": 31719,
      "title": "What are the coefficients returned by Polynomial Ridge Regression (or any regression)?",
      "body": "### Describe the issue linked to the documentation\n\nI asked and answered a question about Regression in [Stack Overflow](https://stackoverflow.com/questions/79691953/ridge-polynomial-regression-how-to-get-parameters-for-equation-found).  Here is a summary and question.\n\nI ran several regressions using pipeline and gridsearch.  The winning regression was polynomial ridge regression.  What I then wanted to do was extract the coefficients of the successful regression so I could pass them on for an implementation that uses just python (no libraries) and Java (no libraries).  That was not straightforward.\n\nI eventually found the coefficients under `steps` after someone pointed that out.  Even the answers I got on Google indicated they were under the attribute `coef` but I couldn't find them though I thought I had read the docs sufficiently.\n\nAs explained at the link above, I expected coefficients for an equation: `a + bx + cx^2 + dx^3`.  If I looked at the coefficients under the attribute `coef_` I got: `[ 0.00000000e+00  9.17291774e-01 -4.25186367e-09  9.06355625e-18]`, from which I assumed that meant that `a=0`,` b=9.17291774e-01`, etc.  It turned out that was only partially true, `b-d` are correct but `a` is not.  `a` is actually the interecept which is another attribute `intercept_`.  At least, that is how I got things to work (code below for an example)\n\nQuestion:  what is the first element in the coefficients from Polynomial Ridge Regression or have I completely misunderstood?\n\n```\nimport pandas as pd\nimport warnings\n\n# regression libs\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# useful initializations\nwarnings.filterwarnings('ignore')\n\np = [0, 10, -20, .30]\n\n# Create fake data using the preceding coefficients with some noise\ndef regr_noise(x, p):\n    mu = np.random.uniform(0,50E6)\n    return (p[0] + p[1]*x + p[2]*x**2 + p[3]*x**3 + mu)...",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-07-07T19:24:47Z",
      "updated_at": "2025-07-09T13:38:43Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31719"
    },
    {
      "number": 31717,
      "title": "SimpleImputer fails in \"most_frequent\" if incomparable types only if ties",
      "body": "### Describe the bug\n\n### Observed behavior\n\nWhen using the \"most_frequent\" strategy from SimpleImputer and there is a tie, the code takes the minimum values among all ties. This crashes if the values are not comparable such as `str` and `NoneType`.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\n\nX1 = np.asarray(['a', None])[:, None]\nX2 = np.asarray(['a', None, None])[:, None]\n\nimputer = SimpleImputer(add_indicator=True, strategy=\"most_frequent\")\n\ntry:\n    imputer.fit_transform(X1)\n    print('X1 processed successfully')\nexcept Exception as e:\n    print('Error while processing X1:', e)\n\n\ntry:\n    imputer.fit_transform(X2)\n    print('X2 processed successfully')\nexcept Exception as e:\n    print('Error while processing X2:', e)\n```\n\n### Expected Results\n\nI would expect the Imputer to have a consistant behavior not depending on whether or not a tie is presente. Namely:\n* Run whether or not values are comparable\n* Crashes if values are not comparable, wheter there is a tie or not.\n\nNote that the code claims to process data like `scipy.stats.mode` but `mode` only processes numeric values since scipy 1.9.0, it therefore crashed on this example and redirect the user toward `np.unique`:\n\n```\nTraceback (most recent call last):\n  File \"/Users/aabraham/NeuralkFoundry/tutorials/repro.py\", line 11, in <module>\n    print(scipy.stats.mode(X1))\n          ~~~~~~~~~~~~~~~~^^^^\n  File \"/Users/aabraham/.local/share/mamba/envs/skle/lib/python3.13/site-packages/scipy/stats/_axis_nan_policy.py\", line 611, in axis_nan_policy_wrapper\n    res = hypotest_fun_out(*samples, axis=axis, **kwds)\n  File \"/Users/aabraham/.local/share/mamba/envs/skle/lib/python3.13/site-packages/scipy/stats/_stats_py.py\", line 567, in mode\n    raise TypeError(message)\nTypeError: Argument `a` is not recognized as numeric. Support for input that cannot be coerced to a numeric array was deprecated in SciPy 1.9.0 and removed in SciPy 1.11.0. Please consider `np.unique`....",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-07-07T09:43:04Z",
      "updated_at": "2025-08-21T15:18:34Z",
      "comments": 19,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31717"
    },
    {
      "number": 31708,
      "title": "Frisch-Newton Interior Point Solver for Quantile Regression",
      "body": "### Describe the workflow you want to enable\n\nHi @ scikit-learn devs! \n\nOver at [pyfixest](https://github.com/py-econometrics/pyfixest), we have implemented a Frisch-Newton Interior Point solver to fit quantile regressions. The algorithm goes back to work from Koenker. In practice, we have followed Koenker and Ng [\"A Frisch-Newton Algorithm for Sparse Quantile Regression\". ](https://link.springer.com/article/10.1007/s10255-005-0231-1)\n\nThe code is licensed under MIT and available [here](https://github.com/py-econometrics/pyfixest/blob/master/pyfixest/estimation/quantreg/frisch_newton_ip.py#L70). \n\nWe (@apoorvalal) have collected some benchmarks [here](https://gist.github.com/apoorvalal/3e18eea79c6e9e8e8ee380e0fc0bab1f) - the FN solver seems to outperform the scikit default solver by an order of a magnitude.  \n\n<img width=\"1362\" height=\"534\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f25eb315-60d8-464f-80f8-bf1c6aedce3b\" />\n\nWould you be interested in a PR that adds the FN solver as a new estimation method to the quantile regression class? \n\nWe've also implemented algorithms from [Chernozhukov et al ](https://arxiv.org/abs/1909.05782)that can drastically speed up estimation of the entire **quantile regression process**. \n\nAll the best, Alex\n\n### Describe your proposed solution\n\nI open a PR and add a new solver \"fn\" to `QuantileRegressor`.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nRelated to https://github.com/scikit-learn/scikit-learn/issues/20132",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-07-05T10:00:37Z",
      "updated_at": "2025-09-09T15:27:32Z",
      "comments": 17,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31708"
    },
    {
      "number": 31705,
      "title": "EmpiricalCovariance user guide assume_centered tip incorrect",
      "body": "### Describe the issue linked to the documentation\n\nThe [user guide documentation](https://scikit-learn.org/stable/modules/covariance.html#empirical-covariance) for EmpiricalCovariance currently states:\n\n> More precisely, if `assume_centered=False`, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and `assume_centered=True` should be used.\n\nIt doesn't make sense, however, that `assume_centered=False` would require data to be centered.  Likewise, it would seem that the user would need to center the data OR use `assume_centered=True` -- not both.\n\nAdditionally, it doesn't seem like there are separate training and testing data for this.\n\n### Suggest a potential alternative/fix\n\nI think it should read:\n\n>More precisely, if `assume_centered=True`, then the data set's mean vector should be zero. If not, the data should be centered by the user, or `assume_centered=False` should be used.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-07-04T20:46:17Z",
      "updated_at": "2025-07-22T12:30:14Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31705"
    },
    {
      "number": 31700,
      "title": "Pipelines are permitted to have no steps and are displayed as fitted",
      "body": "### Describe the bug\n\nPipeline without defined steps is displayed in HTML as fitted.  \n\n\n\n\n### Steps/Code to Reproduce\n\n\n```\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([])\n\npipe\n```\n\n### Expected Results\n\nMaybe empty list should not be accepted. And it should rise a ValueError with a message asking to add steps.\n\n\n\n\n### Actual Results\n\nUsing vscode jupyter extension:\n\n<img width=\"401\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f0ad0033-1b86-4a91-a30b-969a5d2ea22e\" />\n\nNote: Accepting an empty list is one issue, and showing that it is fitted is another.\nThe former occurs when a `Pipeline` is initialized. The latter, I believe, is a design flaw in `sklearn/utils/_repr_html/estimator.py.`\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)]\nexecutable: /Users/dealeon/projects/scikit-learn/sklearn-env/bin/python\n   machine: macOS-15.5-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.8.dev0\n          pip: 25.1\n   setuptools: 75.8.0\n        numpy: 2.1.1\n        scipy: 1.14.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /opt/homebrew/Cellar/libomp/19.1.7/lib/libomp.dylib\n        version: None\n```",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-07-04T09:24:26Z",
      "updated_at": "2025-07-14T13:02:42Z",
      "comments": 11,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31700"
    },
    {
      "number": 31679,
      "title": "AI tools like Copilot Coding Agent don't know about / don't respect our Automated Contributions Policy",
      "body": "(I am creating an issue to a PR already opened (#31643), because there are many more ways to solve the problem.)\n\nAI tools many people use to create PRs don't care about our [Automated Contributions Policy](https://scikit-learn.org/dev/developers/contributing.html#automated-contributions-policy). \n\nSince [GitHub Copilot Coding Agent Has Arrived!](https://github.com/orgs/community/discussions/159068) and people build [Github-MCP](https://github.com/dhyeyinf/Github-MCP)s that can be integrated with LLM clients, scikit-learn and other open source projects get an increasing amount of AI spam. Many people who care about open source are unhappy about it and request an option to block AI-generated PRs and issues on their projects ([Allow us to block Copilot-generated issues (and PRs) from our own repositories](https://github.com/orgs/community/discussions/159749)) - so far without success.\n\nYou can see that there is an increasing amount of partially or fully generated PRs and a decrease in overall quality for PRs on scikit-learn by looking at [the last closed PRs](https://github.com/scikit-learn/scikit-learn/pulls?q=is%3Apr+is%3Aclosed) (as of June 30th 2025). It is not a flood yet, but bad enough to keep several maintainers busy for some extra hours a week. It could become a flood in the future. This is why it is important to find solutions.\n\nQuite some of the authors of these additional low-quality PRs on scikit-learn also spam llm-based PRs on other open source projects at the same time. I have added repeated cases to @adrinjalali's [agents-to-block](https://github.com/adrinjalali/agents-to-block/pull/1/files) folder. The pattern of spammers is to open a PR with an unqualified guess of what the project needs or how an issue can be solved, and then not follow up after maintainers reviewed, close and try again. \n\nPRs can look like someone made a genuine attempt to address an open issue, and project maintainers start to interact with the \"authors\" - but then their review c...",
      "labels": [
        "RFC"
      ],
      "state": "open",
      "created_at": "2025-06-30T08:23:42Z",
      "updated_at": "2025-07-10T11:49:45Z",
      "comments": 27,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31679"
    },
    {
      "number": 31672,
      "title": "ENH Add clip parameter to MaxAbsScaler",
      "body": "### Describe the workflow you want to enable\n\nAdd a `clip` parameter to `MaxAbsScaler` that will allow for clipping values that exceed the maximum value seen during the training stage.\n\n### Describe your proposed solution\n\nSimilar to `MinMaxScaler`, but in this case it will clip [-1, +1].\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nI'm not sure if it is possible to implement it without breaking sparsity of the inputs, which is the main problem.",
      "labels": [
        "Enhancement",
        "API",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-06-28T05:22:49Z",
      "updated_at": "2025-07-25T17:08:54Z",
      "comments": 9,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31672"
    },
    {
      "number": 31668,
      "title": "memory leak for QuantileTransformer",
      "body": "### Describe the bug\n\nThere is a doubling of the memory footprint when QuantileTransformer is called on a dataframe and old references to the dataframe are discarded. See repro.\n\n\n### Steps/Code to Reproduce\n\n\n```python\nimport sys, os, gc, psutil\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.compose import ColumnTransformer\n\n\ndf_train = pd.DataFrame(np.random.randn(1000000, 500), columns = [\"C\"+str(x) for x in np.arange(500)], ).astype('float32')\nordered_columns = df_train.columns\nN, p = df_train.shape\ngc.collect()\n\n\ndef current_mem():\n    process = psutil.Process(os.getpid())\n    rssgb = process.memory_info().rss / 2 ** 30\n    print(rssgb)\n    return rssgb\n\n\ndef fit_apply_scaler(df_train, columns=ordered_columns):\n    if not isinstance(df_train, pd.DataFrame):\n        df_train = pd.DataFrame(df_train, columns=columns)\n    ordered_columns = df_train.columns\n    current_mem()\n    columns_to_scale = [\"C1\", \"C2\", \"C3\", \"C4\", \"C73\", \"C77\" , \"C10\", \"C20\"]\n    scaler = ColumnTransformer([('qts', QuantileTransformer(n_quantiles=20, output_distribution=\"normal\", subsample=N, copy=False, random_state=0), columns_to_scale)], remainder='passthrough', n_jobs=None, verbose=False, verbose_feature_names_out=False).set_output(transform='pandas')\n    df_train = scaler.fit_transform(df_train)[ordered_columns].astype('float32').values\n    gc.collect()\n    current_mem()\n    return scaler, df_train\n\n\ndef outerfunc(df_train, ordered_columns=ordered_columns):\n    current_mem()\n    scaler, df_train = fit_apply_scaler(df_train)\n    print(sys.getsizeof(df_train))\n    current_mem()\n    gc.collect()\n    return df_train\n\n\nfor i in range(5):\n    df_train = outerfunc(df_train)\n    gc.collect()\n    current_mem()\n\n\nsys.getsizeof(df_train)\n```\n\n### Expected Results\n\nMemory footprint should not exceed 4GB\n\n### Actual Results\n\nUsed memory grows to 6GB upon repetition. Df_train is replaced within the outerfunc by the scaled and transformed arr...",
      "labels": [
        "Bug",
        "Performance",
        "module:preprocessing",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-06-27T17:49:33Z",
      "updated_at": "2025-07-08T13:12:11Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31668"
    },
    {
      "number": 31659,
      "title": "Metadata not routed to transformers in pipeline during cross validation.",
      "body": "### Describe the bug\n\nWhen using a pipeline with transformers in combination with cross validation, it seems that metadata is not correctly routed to the transformers during prediction. I would expect, that if `set_transform_request` is set, that this is honored when calling predict on the pipeline.\n\n**Edit:** At least according to the code this is a known limitation. Although I couldn't find an issue tracking the progress on this.\nhttps://github.com/scikit-learn/scikit-learn/blob/9028b518e7a906a806a1dc8994f2714cc980c941/sklearn/model_selection/_validation.py#L362C1-L367C14\n\n### Steps/Code to Reproduce\n\n```python\nimport sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_regression\nfrom sklearn.base import TransformerMixin, _MetadataRequester\nfrom sklearn.model_selection import cross_validate\n\nprint(sklearn.__version__)\n\nsklearn.set_config(enable_metadata_routing=True)\n\n\nclass DummyTransfomerWithMetadata(TransformerMixin, _MetadataRequester):\n\n    def fit(self, X, y=None, metadata=None):\n        return self\n\n    def transform(self, X, y=None, metadata=None):\n        print(f\"Received {metadata=}\")\n        return X\n\n    # We need to explicitly implement fit_transform,\n    # otherwise transform will not receive metadata during fit\n    def fit_transform(self, X, y=None, metadata=None):\n        return self.transform(X, y, metadata)\n\n\nX, y = make_regression()\n\ntransformer = DummyTransfomerWithMetadata()\ntransformer.set_fit_request(metadata=True)\ntransformer.set_transform_request(metadata=True)\n\n\npipe = Pipeline([\n    (\"transformer\", transformer),\n    (\"clf\", LinearRegression())\n])\n\n\nprint(f\"--- Cross validation ---\")\ncross_validate(\n    pipe, X, y, params={\"metadata\": \"Some metadata\"}, cv=2\n)\n```\n\n### Expected Results\n\n```\n1.7.0\n--- Cross validation ---\nReceived metadata='Some metadata'   # Fit \nReceived metadata='Some metadata'   # Predict\nReceived metadata='Some metadata'\nReceived metadata='So...",
      "labels": [
        "Bug",
        "Metadata Routing"
      ],
      "state": "open",
      "created_at": "2025-06-25T11:47:15Z",
      "updated_at": "2025-07-03T08:56:35Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31659"
    },
    {
      "number": 31657,
      "title": "DOC Managing huntr security vulnerability reports",
      "body": "### Describe the issue linked to the documentation\n\n### Issues\n- The project receives reports from huntr that are not useful.\n- The reports from huntr are time consuming and use up limited maintainer resources.\n\n### Discussion / Proposal\n- Update our [SECURITY.md](http://security.md/) file to indicate how we are dealing with huntr reports\n- Direct security reporters to provide more detailed information on security vulnerability including proof of concept (POC) and proof of impact (POI)\n- Once POC and POI is established, can direct people to report issue via the GitHub Security Advisory: https://github.com/scikit-learn/scikit-learn/security/advisories/new\n- Remove scikit-learn from the huntr bug bounty program\n\n\n### Proposed text for huntr reports\nDraft text for huntr submissions: \n>The scikit-learn project is not reviewing reports submitted to huntr. Please use our SECURITY.md to submit reports. For security reports, provide both a POC (proof of concept) and POI (proof of impact). If your report is deemed impactful, you can then report it to huntr to collect a bounty.\n\n### References\n- [Scientific Python SPEC](https://github.com/scientific-python/specs/pull/391/)\n- [NumPy discussion on security](https://github.com/numpy/numpy/issues/29178)\n- [Dask: comment from huntr person](https://github.com/dask/community/issues/415#issuecomment-2755046159)\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-25T09:45:00Z",
      "updated_at": "2025-08-05T14:02:53Z",
      "comments": 12,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31657"
    },
    {
      "number": 31653,
      "title": "DOC About Us page - clarify team descriptions",
      "body": "### Describe the issue linked to the documentation\n\nReferences #31430 \n\n[@thomasjpfan note](https://github.com/scikit-learn/scikit-learn/pull/31430#issuecomment-2914501524):\n>With the current governance, all the named roles are considered \"core\". Specifically, the contributor experience, communication, documentation, and maintainer teams are all \"core contributors\".\n\n>Before this PR, only the maintainer team can approve PRs. With this PR, any of the other teams can approve PRs. Although, in practice, I think we normally considered the other approvals as valid.\n\n[@ArturoAmorQ note](https://github.com/scikit-learn/scikit-learn/pull/31430#pullrequestreview-2874019392):\n>Honest question, shall we modify the terminology across the documentation e.g. in about.rst? Such that it's clear who are those referred here.\n\nTeam names and descriptions are not consistent.\n\n1. GitHub Teams: https://github.com/orgs/scikit-learn/teams\n- Communication Team\n- Contributor Experience Team\n- Core-devs\n- Documentation Team\n\n2. About Us page: https://scikit-learn.org/dev/about.html\n\nActive Core Contributors\n- Maintainers Team\n- Documentation Team\n- Contributor Experience Team\n- Communication Team\n\n\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-06-25T08:43:29Z",
      "updated_at": "2025-07-10T03:57:09Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31653"
    },
    {
      "number": 31635,
      "title": "Two bugs in `sklearn.metrics.roc_curve`: `drop_intermediate=True` option",
      "body": "### Describe the bug\n\nThe function `sklearn.metrics.roc_curve` contains two separate (but potentially interacting) bugs related to the `drop_intermediate=True` option.  This report describes both.\n\n---\n\n### Bug 1: Incorrect Ordering of `drop_intermediate` Relative to Initial Point Prepending\n\nWhen `drop_intermediate=True` (the default), `roc_curve` attempts to simplify the ROC curve by removing intermediate points—those that are collinear with their neighbors and therefore do not affect the curve's shape.\n\nHowever, intermediate points are dropped **before** the initial point `(0, 0)` and the threshold `inf` are prepended to the results.  This causes incorrect retention of points that would otherwise be considered intermediate if the full curve were evaluated from the start.\n\n#### Example:\n\n```python\ny_true  = numpy.array([0, 0, 0, 0, 1, 1, 1, 1])\ny_score = numpy.array([0, 1, 2, 3, 4, 5, 6, 7])\n```\n\nIn this case, a threshold of 4 perfectly separates class 0 from class 1.  The expected simplified ROC curve should be:\n\n```python\nfpr = [0., 0., 1.]\ntpr = [0., 1., 1.]\nthresholds = [inf, 4., 0.]\n```\n\nInstead, the actual output is:\n\n```python\nfpr = [0., 0., 0., 1.]\ntpr = [0., 0.25, 1., 1.]\nthresholds = [inf, 7., 4., 0.]\n```\n\nThe point `(0., 0.25)` is redundant but retained, because it is evaluated before `(0., 0.)` is prepended—leading to an incorrect assessment of its relevance.\n\n#### Root Cause:\n\n```python\n# Incorrect order: intermediates dropped before prepending\nfps, tps, thresholds = _binary_clf_curve(...)\n\nif drop_intermediate:\n    # identify and drop intermediates\n    ...\n\n# only afterward:\nfps = numpy.r_[0, fps]\ntps = numpy.r_[0, tps]\nthresholds = numpy.r_[inf, thresholds]\n```\n\n#### Recommended Fix:\n\nReorder the operations so that the initial point is prepended before identifying intermediate points:\n\n```python\nfps, tps, thresholds = _binary_clf_curve(...)\n\n# Prepend start of curve\nfps = numpy.r_[0, fps]\ntps = numpy.r_[0, tps]\nthresholds = numpy.r_[numpy.inf, thres...",
      "labels": [
        "Bug",
        "module:metrics",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-23T13:00:58Z",
      "updated_at": "2025-09-11T00:08:14Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31635"
    },
    {
      "number": 31633,
      "title": "Check `pos_label` present in `y_true` in metric functions",
      "body": "Noticed while working on https://github.com/scikit-learn/scikit-learn/pull/30508#discussion_r2158871194\n\nCurrently the following metric functions do not explicitly check that `pos_label` is present in `y_true`:\n\n* `roc_curve`\n* `precision_recall_curve`\n* `det_curve`\n* `brier_score_loss`\n\nAFAICT all (?) other classification metrics (e.g., `recall_score`, `precision_score`), including ranking metric `average_precision_score` explicitly check that `pos_label` is present in `y_true`:\n\ne.g. this is the error from `recall_score`/`precision_score`/`f1` family:\n```\n        if y_type == \"binary\":\n            if len(present_labels) == 2 and pos_label not in present_labels:\n>               raise ValueError(\n                    f\"pos_label={pos_label} is not a valid label. It should be \"\n                    f\"one of {present_labels}\"\n                )\nE               ValueError: pos_label=2 is not a valid label. It should be one of [0, 1]\n```\n\n`roc_curve` and `precision_recall_curve` do not explicitly check this, they do *warn* (no error) that there are no 'positive' samples in `y_true`:\n\n```\n        if tps[-1] <= 0:\n>           warnings.warn(\n                \"No positive samples in y_true, true positive value should be meaningless\",\n                UndefinedMetricWarning,\n            )\nE           sklearn.exceptions.UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n```\n\nSimilarly, for `det_curve` this results in an invalid divide warning (we divide by 0):\n```\nFile ~/Documents/dev/scikit-learn/sklearn/metrics/_ranking.py:418, in det_curve(y_true, y_score, pos_label, sample_weight, drop_intermediate)\n    415 sl = slice(first_ind, last_ind)\n    417 # reverse the output such that list of false positives is decreasing\n--> 418 return (fps[sl][::-1] / n_count, fns[sl][::-1] / p_count, thresholds[sl][::-1])\n\nRuntimeWarning: invalid value encountered in divide\n```\n\n`brier_score_loss` gives no error and no warning. `_validate_binary_probabi...",
      "labels": [
        "Bug",
        "Needs Decision",
        "module:metrics"
      ],
      "state": "open",
      "created_at": "2025-06-23T05:10:01Z",
      "updated_at": "2025-06-24T04:51:53Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31633"
    },
    {
      "number": 31628,
      "title": "DOC: Glossary contains several FIXME tags",
      "body": "### Describe the issue linked to the documentation\n\nThe Glossary for scikit-learn contains several FIXME tags.\nhttps://scikit-learn.org/dev/glossary.html\n\n### Suggest a potential alternative/fix\n\nFIXME tags can be used for future improvement, but I think they should belong in code comments instead of the Glossary page.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-06-23T01:09:22Z",
      "updated_at": "2025-06-29T18:15:22Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31628"
    },
    {
      "number": 31624,
      "title": "⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: Jun 29, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=77785&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (Jun 29, 2025)\n- Test Collection Failure\n- test_ensemble_heterogeneous_estimators_behavior[stacking-classifier]\n- test_ensemble_heterogeneous_estimators_behavior[voting-classifier]\n- test_heterogeneous_ensemble_support_missing_values[StackingClassifier-LogisticRegression-X0-y0]\n- test_heterogeneous_ensemble_support_missing_values[VotingClassifier-LogisticRegression-X2-y2]\n- test_stacking_classifier_iris[False-None-3]\n- test_stacking_classifier_iris[False-None-cv1]\n- test_stacking_classifier_iris[False-final_estimator1-3]\n- test_stacking_classifier_iris[False-final_estimator1-cv1]\n- test_stacking_classifier_iris[True-None-3]\n- test_stacking_classifier_iris[True-None-cv1]\n- test_stacking_classifier_iris[True-final_estimator1-3]\n- test_stacking_classifier_iris[True-final_estimator1-cv1]\n- test_stacking_classifier_drop_column_binary_classification\n- test_stacking_classifier_sparse_passthrough[coo_matrix]\n- test_stacking_classifier_sparse_passthrough[coo_array]\n- test_stacking_classifier_sparse_passthrough[csc_matrix]\n- test_stacking_classifier_sparse_passthrough[csc_array]\n- test_stacking_classifier_sparse_passthrough[csr_matrix]\n- test_stacking_classifier_sparse_passthrough[csr_array]\n- test_stacking_classifier_drop_binary_prob\n- test_stacking_classifier_error[y1-params1-ValueError-does not implement the method predict_proba]\n- test_stacking_classifier_error[y2-params2-TypeError-does not support sample weight]\n- test_stacking_classifier_error[y3-params3-TypeError-does not support sample weight]\n- test_stacking_randomness[StackingClassifier]\n- test_stacking_classifier_stratify_default\n- test_stacking_with_sample_weight[StackingClassifier]\n- test_stacking_cv_influence[StackingClassifier]\n- test_stacking_prefit[StackingClassifier-DummyClassifier-predict_proba-final_estimator0-X0-y0]...",
      "labels": [
        "Needs Triage",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-06-22T02:56:32Z",
      "updated_at": "2025-06-29T16:34:09Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31624"
    },
    {
      "number": 31621,
      "title": "ENH: Add MedianAbsoluteDeviationScaler (MADScaler) to sklearn.preprocessing",
      "body": "### Describe the workflow you want to enable\n\nToday, if a user wants to centre features by the median and scale them by the median absolute deviation (MAD) they must hand-roll code like:\nmad = 1.4826 * np.median(np.abs(X - np.median(X, axis=0)), axis=0)\nX_scaled = (X - np.median(X, axis=0)) / mad\n\nA built-in MedianAbsoluteDeviationScaler (or a statistic=\"mad\" option on RobustScaler) would let them write a single, self-documenting line:\nfrom sklearn.preprocessing import MedianAbsoluteDeviationScaler\nX_scaled = MedianAbsoluteDeviationScaler().fit_transform(X)\n\nThat makes robust MAD scaling first-class, composable in pipelines, and reversible via inverse_transform().\n\n### Describe your proposed solution\n\nAdd a new transformer:\nclass MedianAbsoluteDeviationScaler(BaseEstimator, TransformerMixin):\n    with_centering: bool = True\n    with_scaling:   bool = True\n    copy:           bool = True\n    unit_variance:  bool = False\n\n    # learned in fit\n    center_: ndarray\n    scale_: ndarray\n\nFit logic\n1. center_ = np.median(X, axis=0) (if with_centering)\n\n2. mad = np.median(np.abs(X - center_), axis=0) * 1.4826\n\n3. Guard against zeros with float_eps, store in scale_.\n\ntransform() and inverse_transform() reuse the pattern from RobustScaler.\n\nDocs / tests\n\n- Unit tests for shape preservation, inverse-transform round-trip, and robustness to outliers.\n\n- A short subsection in preprocessing.rst and a gallery example comparing Standard, Robust (IQR) and MAD scalers.\n\n- Changelog bullet in whats_new/v1.5.rst.\n\nI am happy to implement this within ~2 weeks.\n\n### Describe alternatives you've considered, if relevant\n\n- Keep user-land recipes – fragments the ecosystem and lacks inverse_transform().\n\n- Extend RobustScaler with statistic={\"iqr\",\"mad\"} (default \"iqr\"). This also works, but changes a long-standing API and may require a deprecation cycle.\n\n### Additional context\n\n- MAD is a well-known σ-consistent robust scale estimator, more efficient than IQR for symmetric heavy-tailed or L...",
      "labels": [
        "New Feature",
        "module:preprocessing",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-21T21:26:54Z",
      "updated_at": "2025-07-01T01:45:53Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31621"
    },
    {
      "number": 31620,
      "title": "ENH: Add MedianAbsoluteDeviationScaler (MADScaler) to sklearn.preprocessing",
      "body": "",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-21T21:21:25Z",
      "updated_at": "2025-06-21T21:23:23Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31620"
    },
    {
      "number": 31608,
      "title": "(Perhaps) safer version of halving search",
      "body": "### Describe the workflow you want to enable\n\nI find the current experimental implementation of HalvingGridSearchCV problematic. At the first rounds it tends to select candidates with hyperparameters adapted to small sample sizes that are bad in hindsight, when it's too late. Think of regularization, tree depth, number of leaves, etc. This is a problem with CV in general, but 4/5 or 9/10 are a far cry from #samples / #candidates.\n\n### Describe your proposed solution\n\nI've the following suggestion, although TBH I haven't thoroughly thought about it: take a splitter as usual and in each iteration of the splitter reduce the candidates, say by 2 or 3. So, for example, you start with cv=5 and 100 candidates, fit them on folds 2-5, compute scores on fold 1, discard half the candidates, proceed to the next split with test fold = 2 and 50 remaining candidates, etc. It obviously requires more resources than the current implementation, but early selected candidates would be better adapted to the last rounds.\n\n### Describe alternatives you've considered, if relevant\n\nImplementing the above on top of GridSearchCV.\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-20T18:54:31Z",
      "updated_at": "2025-07-22T09:58:40Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31608"
    },
    {
      "number": 31604,
      "title": "`_safe_indexing` fails with pyarrow==16.0.0",
      "body": "### Describe the bug\n\n`_safe_indexing` fails with pyarrow==16.0.0 because `filter()` expects a pyarrow boolean type and cannot handle getting a numpy boolean array or a list passed.\n\nI found apache/arrow#42013 addressing this and it was fixed for version 17.0.0. Upgrading my pyarrow version has resolved the issue for me.\n\nWe accept pyarrow==12.0.0 as a minimum (optional) dependency.\nIn the CI, we test in `pylatest_conda_forge_mkl_linux` with pyarrow==20.0.0 (only). \n\n### Steps/Code to Reproduce\n\nRun `test_safe_indexing_1d_container_mask`.\n\n### Expected Results\n\nno errors\n\n### Actual Results\n\nTraceback:\n\n```pytb\narray_type = 'pyarrow_array', indices_type = 'series'\n\n    @pytest.mark.parametrize(\n        \"array_type\", [\"list\", \"array\", \"series\", \"polars_series\", \"pyarrow_array\"]\n    )\n    @pytest.mark.parametrize(\"indices_type\", [\"list\", \"tuple\", \"array\", \"series\"])\n    def test_safe_indexing_1d_container_mask(array_type, indices_type):\n        indices = [False] + [True] * 2 + [False] * 6\n        array = _convert_container([1, 2, 3, 4, 5, 6, 7, 8, 9], array_type)\n        indices = _convert_container(indices, indices_type)\n>       subset = _safe_indexing(array, indices, axis=0)\n\nsklearn/utils/tests/test_indexing.py:229: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsklearn/utils/_indexing.py:323: in _safe_indexing\n    return _pyarrow_indexing(X, indices, indices_dtype, axis=axis)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nX = <pyarrow.lib.Int64Array object at 0x7f08c2789c60>\n[\n  1,\n  2,\n  3,\n  4,\n  5,\n  6,\n  7,\n  8,\n  9\n]\nkey = array([False,  True,  True, False, False, False, False, False, False]), key_dtype = 'bool', axis = 0\n\n    def _pyarrow_indexing(X, key, key_dtype, axis):\n        \"\"\"Index a pyarrow data.\"\"...",
      "labels": [
        "Bug",
        "module:utils"
      ],
      "state": "closed",
      "created_at": "2025-06-20T10:15:33Z",
      "updated_at": "2025-06-26T09:06:46Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31604"
    },
    {
      "number": 31601,
      "title": "Implement row-wise prediction skipping",
      "body": "### Describe the workflow you want to enable\n\nIn lines like:\n\n```python\nmodel.predict_proba(df)\n```\n\nI know that certain rows do not need the probability to be predicted. So I would need to:\n\n- Filter the dataframe\n- Store the indexes at which I do not want evaluation\n- Evaluate the filtered dataframe\n- Put back the whole dataframe with the probabilities of the dropped data as -1, NaN or some other reasonable value.\n\n### Describe your proposed solution\n\nI would like to have a `skip_at` argument like:\n\n```python\nindexes=numpy.array[1, 20, 40])\n\nprobabilities = model.predict_proba(df, skip_at=indexes)\n```\n\nSuch that probabilities is NaN at 1, 20 and 40 do not get added and **specially** the model does not waste time evaluating the probability there.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "API"
      ],
      "state": "closed",
      "created_at": "2025-06-20T08:44:44Z",
      "updated_at": "2025-06-24T05:46:38Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31601"
    },
    {
      "number": 31599,
      "title": "`_MultimetricScorer` deals with `_accept_sample_weights` inconsistently",
      "body": "### Describe the bug\n\nWhen one of the scorers in `_MultimetricScorer` is not a `Scorer` object, it is handled incorrectly.\n\nSee line [here](https://github.com/scikit-learn/scikit-learn/blob/0fc081a4e131b08cb6d22f77f250733f265097b4/sklearn/metrics/_scorer.py#L143). If the scorers passed to `MultimetricScorer` are of the following type: [`Scorer`, `function`], it raises an error because the attribute `_accept_sample_weight` does not exist for the second scorer (`function` in this case). This (possibly) bug is present only in 1.7.0 since before this, the `sample_weight` kwarg was being passed to all functions without a check of accepting sample weights.\n\nPossible fix: Use `if hasattr(scorer, '_accept_sample_weight'`) or `if isinstance(scorer, _BaseScorer)` _before_ checking for the `_accept_sample_weight` attribute.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics._scorer import _BaseScorer, _MultimetricScorer\n\n# Step 1: Define a simple estimator\nclass SimpleEstimator(BaseEstimator, RegressorMixin):\n    def fit(self, X, y):\n        self.mean_ = np.mean(y)\n        return self\n\n    def predict(self, X):\n        return np.full(X.shape[0], self.mean_)\n\n# Step 2: Define a custom scorer inheriting from _BaseScorer and a function which estimates score\nclass SimpleScorer(_BaseScorer):\n    def _score(self, method_caller, estimator, X, y_true, sample_weight=None):\n        y_pred = method_caller(estimator, \"predict\", X)\n        return self._score_func(y_true, y_pred, **self._kwargs)\n\ndef default_score(estimator, X, y, sample_weight=None, **kws):\n    return estimator.score(X, y, sample_weight=sample_weight)\n\ndef mse(y, y_pred):\n    return np.mean((y - y_pred)**2)\n\n# Step 3: Create a _MultimetricScorer with multiple scorers\nscorers = {\n    \"mse\": SimpleScorer(mse, sign=1, kwargs={}),\n    \"default\": default_score\n}\nmulti_scorer = _MultimetricScorer(scorers=scorers)\n\n# Step 4: Generate sample data\nX...",
      "labels": [
        "Bug",
        "module:metrics",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-20T08:12:08Z",
      "updated_at": "2025-08-18T23:16:55Z",
      "comments": 14,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31599"
    },
    {
      "number": 31595,
      "title": "Attribute docstring does not show properly when there is a property with the same name",
      "body": "### Describe the issue linked to the documentation\n\nWhen a @property is documented by a docstring and when the corresponding fitted attribute with the same name is also documented in the docstring of the class, the documentation only displays the first line of the docstring of the @property. The name of the property is also not properly rendered.\nFor example `estimators_samples_` of `BaggingClassifier` is displayed like this in the documentation:\n\n![Image](https://github.com/user-attachments/assets/e062a3ca-541b-413d-884b-3bc31d1f54a2)\n\nAlthough its docstring is:\n\n![Image](https://github.com/user-attachments/assets/825ec69d-32b5-44d0-a1dd-39de378e5c93)\n\nAnd the docstring of the `@property` is:\n\n![Image](https://github.com/user-attachments/assets/72a1a343-aba9-43e3-b53a-67339e5e4689)\n\nThis was probably introduced here : #30989 \n\n### Suggest a potential alternative/fix\n\nOne solution is to remove the docstring of the property, in which case the docstring of the attribute will be rendered properly. But it would have to be done in all such cases. I discovered it while working on RandomForestClassifier.feature_importances_ that suffers from the same issue.\n\nCc: @antoinebaker @lesteve What do you think would be the right way to document an attribute coming from a property?",
      "labels": [
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-06-19T13:43:00Z",
      "updated_at": "2025-08-20T13:25:15Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31595"
    },
    {
      "number": 31593,
      "title": "scikit-learn API reference on the website not rendering LaTeX correctly",
      "body": "### Describe the bug\n\nOn the API reference on the web, formulas are shown as: \n\n`a * ||w||_1 + 0.5 * b * ||w||_2^2`\n\nInstead of \n\n<img width=\"232\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8f45c9d9-3ff9-48ae-b904-d6d1286f9f89\" />\n\n(Unless it's expected!)  \n\n### Steps/Code to Reproduce\n\nPlease see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html \n\n\n\n### Expected Results\n\nI think the formulas should look like mathematical formulas, not like LaTeX:\n\n<img width=\"232\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8f45c9d9-3ff9-48ae-b904-d6d1286f9f89\" />\n\n\n\n### Actual Results\n\n`a * ||w||_1 + 0.5 * b * ||w||_2^2`\n\n### Versions\n\n```shell\nAll releases on the website\n```",
      "labels": [
        "Documentation"
      ],
      "state": "open",
      "created_at": "2025-06-19T12:24:03Z",
      "updated_at": "2025-09-08T08:25:48Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31593"
    },
    {
      "number": 31592,
      "title": "Compilation \"neighbors/_kd_tree.pyx\" crashes on ARM",
      "body": "### Describe the bug\n\nHi. I rebuilt scikit-learn from source, but the compiler crashed.\n\n### Steps/Code to Reproduce\n\n```shell\n$ cat /etc/debian_version\n\n12.11\n```\n\n```shell\n$ cat /etc/os-release\n\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n```\n\n```shell\n$ gcc --version\n\ngcc (Debian 12.2.0-14+deb12u1) 12.2.0\n```\n\n```shell\n$ cat requirements | grep scikit\nscikit-learn==1.5.2 ; python_version >= \"3.12\" and python_version < \"3.13\"\n\n$ pip3 install -r requirements.txt --no-deps --no-binary \":all:\" -vvv\n```\n\n\n\n### Expected Results\n\nBuild without problmes\n\n### Actual Results\n\n```\n[205/249] Compiling C object sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o\n  FAILED: sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o\n  cc -Isklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p -Isklearn/neighbors -I../sklearn/neighbors -I../../../pip-build-env-0jwmo4n5/overlay/lib/python3.12/site-packages/numpy/_core/include -I/usr/local/include/python3.12 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c11 -O3 -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -MD -MQ sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o -MF sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o.d -o sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/meson-generated_sklearn_neighbors__kd_tree.pyx.c.o -c sklearn/neighbors/_kd_tree.cpython-312-aarch64-linux-gnu.so.p/sklearn/neighbors/_k...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-19T11:20:38Z",
      "updated_at": "2025-07-24T07:40:39Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31592"
    },
    {
      "number": 31587,
      "title": "Can't create exe-file with this module",
      "body": "### Describe the bug\n\n```py\nfrom sklearn.neighbors import NearestNeighbors\n\n--hidden-import=sklearn.neighbors\n\nFile \"C:\\Python\\lib\\site-packages\\PyInstaller\\lib\\modulegraph\\modulegraph.py\", line 2537, in _scan_bytecode\n    for inst in util.iterate_instructions(module_code_object):\n  File \"C:\\Python\\lib\\site-packages\\PyInstaller\\lib\\modulegraph\\util.py\", line 13, in iterate_instructions\n    yield from (i for i in dis.get_instructions(code_object) if i.opname != \"EXTENDED_ARG\")\n  File \"C:\\Python\\lib\\site-packages\\PyInstaller\\lib\\modulegraph\\util.py\", line 13, in <genexpr>\n    yield from (i for i in dis.get_instructions(code_object) if i.opname != \"EXTENDED_ARG\")\n  File \"C:\\Python\\lib\\dis.py\", line 338, in _get_instructions_bytes\n    argval, argrepr = _get_const_info(arg, constants)\n  File \"C:\\Python\\lib\\dis.py\", line 292, in _get_const_info\n    argval = const_list[const_index]\nIndexError: tuple index out of range\n```\n\nProject output will not be moved to output folder\n\n### Steps/Code to Reproduce\n\n```py\nfrom sklearn.neighbors import NearestNeighbors\npoints = np.array([[pt.x, pt.y, pt.z] for pt in face_centers])\nif len(points) < 2:\n      return 0\nnbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(points)\n```\n\n### Expected Results\n\nI expected to create exe-file with this module imported using pyInstaller\n\n### Actual Results\n\n```py\nFile \"C:\\Python\\lib\\dis.py\", line 292, in _get_const_info\n    argval = const_list[const_index]\nIndexError: tuple index out of range\n```\n\nProject output will not be moved to output folder\n\n### Versions\n\n```shell\nsklearn: 1.4.2\n          pip: 21.3.1\n   setuptools: 60.2.0\n        numpy: 1.26.4\n        scipy: 1.15.3\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-18T20:08:35Z",
      "updated_at": "2025-06-19T09:16:50Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31587"
    },
    {
      "number": 31583,
      "title": "Unjustified \"number of unique classes > 50%\" warning in CalibratedClassifierCV",
      "body": "### Describe the bug\n\nWhile using CalibratedClassifierCV with a multiclass dataset, I noticed that the following warning is raised, even though the number of classes is much smaller than the number of samples:\n\n```\nUserWarning: The number of unique classes is greater than 50% of the number of samples.\n```\n\nThis seems unexpected, so I tried to reproduce the situation with synthetic data. From what I can tell, the number of classes is well below 50% of the number of training samples passed to fit().\n\nIt’s possible I’m misunderstanding the intended behavior, but based on reading the source code, it looks like this might be caused by a call to type_of_target(classes_) (instead of y), which could falsely trigger the condition if classes_ is treated like data.\n\n(The same happens with GridSearchCV, for example).\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndef main():\n\t# Simulate 1000 samples, 40 features, 30 classes (<< 50%)\n\tn_samples = 1000\n\tn_features = 40\n\tn_classes = 30\n\n\trng = np.random.RandomState(42)\n\tx = rng.rand(n_samples, n_features)\n\ty = np.tile(np.arange(n_classes), int(np.ceil(n_samples / n_classes)))[:n_samples]\n\n\tprint(f\"Samples: {len(y)}\")\n\tprint(f\"Unique classes: {len(np.unique(y))}\")\n\tprint(f\"Class/sample ratio: {len(np.unique(y)) / len(y):.2%}\")\n\n\tbase_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\tcal_clf = CalibratedClassifierCV(base_clf, method='isotonic', cv=2)\n\tcal_clf.fit(x, y)\n\n\nif __name__ == '__main__':\n\tmain()\n```\n\n### Expected Results\n\nI expected no warning to be raised, as the class/sample ratio is only ~3% (well under the 50% threshold). There are no rare classes, and the splits from CV should still contain enough samples.\n\n### Actual Results\n\n```\nSamples: 1000\nUnique classes: 30\nClass/sample ratio: 3.00%\n/miniconda3/envs/sklearn_check/lib/python3.13/site-packages/sklearn/utils/_response.py:203: UserW...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-06-18T10:41:31Z",
      "updated_at": "2025-07-14T01:21:59Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31583"
    },
    {
      "number": 31572,
      "title": "Documentation improvement (LogisticRegression): display a note as a note",
      "body": "### Describe the issue linked to the documentation\n\nA note in the description of the parameter `intercept_scaling` should be displayed as a note in [LogisticRegression](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html), just as any other note.  \n\n![Image](https://github.com/user-attachments/assets/b434d4e8-8e13-4fbd-a3a0-c4191571aeb2)\n\n### Suggest a potential alternative/fix\n\nChange the code [in this file](https://github.com/scikit-learn/scikit-learn/blob/031d2f83b/sklearn/linear_model/_logistic.py#L883).   \nSee example below [here](https://github.com/scikit-learn/scikit-learn/blob/031d2f83b/sklearn/linear_model/_logistic.py#L948).",
      "labels": [
        "Easy",
        "Documentation",
        "module:linear_model"
      ],
      "state": "closed",
      "created_at": "2025-06-17T14:52:03Z",
      "updated_at": "2025-06-18T12:46:11Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31572"
    },
    {
      "number": 31571,
      "title": "Several Doc improvement for whats_new",
      "body": "### Describe the issue linked to the documentation\n\nI found some bugs or unclear areas that need further improvement in several versions of whats_new documentation.\n\n### [v1.5.0](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.5.rst#version-150)\n\n- \"Deprecates `Y` in favor of `y` in the methods fit, transform and inverse_transform of: :class:`cross_decomposition.PLSRegression`, :class:`cross_decomposition.PLSCanonical`, :class:`cross_decomposition.CCA`, and :class:`cross_decomposition.PLSSVD`.\"[(link)](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.5.rst#modsklearncross_decomposition) However, class`cross_decomposition.PLSSVD` doesn‘t seem to have the `inverse_transform` method (refer to [class PLSSVD](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD))\n- \"store_cv_values and cv_values_ are deprecated in favor of store_cv_results and cv_results_ in ~linear_model.RidgeCV and ~linear_model.RidgeClassifierCV.\"[(link)](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.5.rst#modsklearnlinear_model) I recommend to make it clear that the `store_cv_values` and `cv_values_` are Parameters (like the previous item), otherwise it will be misleading to know whether they are parameters or methods.\n\n### [v1.4.0](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst)\n\n- \":func:`sklearn.extmath.log_logistic` is deprecated and will be removed in 1.6. Use `-np.logaddexp(0, -x)` instead.\"[(link)](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.4.rst#modsklearnutils-1) The full qualified name of function `log_logistic` should be `sklearn.utils.extmath.log_logistic`.\n\n### [v1.3.0](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/v1.3.rst)\n\n- \"The parameter log_scale in the class :class:`model_selection.LearningCurveDisplay` has been deprecated in 1.3 and will be removed in 1.5....",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-17T14:41:51Z",
      "updated_at": "2025-06-18T09:30:40Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31571"
    },
    {
      "number": 31566,
      "title": "⚠️ CI failed on Wheel builder (last failure: Jun 17, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15697733135)** (Jun 17, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-17T10:25:50Z",
      "updated_at": "2025-06-17T12:02:44Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31566"
    },
    {
      "number": 31555,
      "title": "is_classifier returns False for custom classifier wrappers in scikit-learn 1.6.1, even with ClassifierMixin and _estimator_type",
      "body": "### Describe the bug\n\n#### Describe the bug\n\nSince upgrading to scikit-learn 1.6.1, the utility function `is_classifier` always returns `False` for custom classifier wrappers, even if they inherit from `ClassifierMixin` and explicitly define `_estimator_type = \"classifier\"`.\n\nThis was not the case in previous versions (<=1.5.x), and breaks many downstream code patterns relying on `is_classifier`, as well as certain custom scorer usages and checks.\n\n\n### Steps/Code to Reproduce\n\n```python\nimport sklearn\nprint(\"scikit-learn version:\", sklearn.__version__)\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin, is_classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclass BinCls(BaseEstimator, ClassifierMixin):\n    _estimator_type = \"classifier\"\n    def __init__(self, model=None):\n        self.model = model\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        self.classes_ = self.model.classes_\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\nrng = RandomForestClassifier()\nclf = BinCls(rng)\nprint(\"is_classifier(clf) =\", is_classifier(clf))  # Expect True, but gets False\n\n\n### Expected Results\n\nprint(\"is_classifier(clf) =\", is_classifier(clf))  # Expect True, but gets False\n\n### Actual Results\n\nprint(\"is_classifier(clf) =\", is_classifier(clf)) # Expect True, but gets False\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\nexecutable: C:\\Users\\Greg\\anaconda3\\envs\\ml_trade\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0\n   setuptools: 72.1.0\n        numpy: 2.1.3\n        scipy: 1.15.2\n       Cython: 3.1.1\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: mkl\n    num_threa...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-16T12:26:15Z",
      "updated_at": "2025-06-16T12:42:51Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31555"
    },
    {
      "number": 31554,
      "title": "Allow batch based metrics calculation of sklearn.metrics",
      "body": "### Describe the workflow you want to enable\n\nI have a lot of data and need to calculate metrics such as accuracy_score, jaccard_score, f1_score, recall, precision etc.\n\n### Describe your proposed solution\n\n When I try to calculate these it can literally take days, so i created a small solution which can batch and avg in the end, or for the weighted metrics it can do a weighted avg of each, this accelerated the calculation to just a couple of minutes, because I have a 32 core CPU. I'm willing to contribute with the proper guidance as I'm unfamiliar with the codebase, but I think many people can benefit from this. I'm unsure if there is already a work around of this present in the codebase, but if there is one do let me know, thanks a lot.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "Performance",
        "help wanted",
        "module:metrics",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-16T10:14:05Z",
      "updated_at": "2025-08-08T15:00:08Z",
      "comments": 15,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31554"
    },
    {
      "number": 31548,
      "title": "DOC About Us page: multi-column list for emeritus contributors",
      "body": "References #31519 \nReferences #30826 \n\n---\n\nIt would be good to keep the file. The new proposed layout looks like this, and it save 28 lines of whitespace. So users can get to the important section faster, how to support scikit-learn.\n\n### Before\n<img width=\"1145\" alt=\"Screenshot 2025-06-12 at 6 53 17 AM\" src=\"https://github.com/user-attachments/assets/59db6862-580d-41d2-ac0e-b5fd6629ee79\" />\n\n### After\n<img width=\"970\" alt=\"Screenshot 2025-06-12 at 6 52 40 AM\" src=\"https://github.com/user-attachments/assets/371d8d38-1f47-4251-8753-445f363071c3\" />\n\n_Originally posted by @reshamas in https://github.com/scikit-learn/scikit-learn/pull/31519#discussion_r2142352666_",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-06-15T15:51:33Z",
      "updated_at": "2025-06-18T16:50:41Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31548"
    },
    {
      "number": 31546,
      "title": "Regression in `DecisionBoundaryDisplay.from_estimator` with `colors` and `plot_method='contour'` after upgrading to v1.7.0",
      "body": "### Describe the bug\n\nHello. Recently, after upgrading to scikit-learn v1.7.0, I encountered an issue when using `DecisionBoundaryDisplay.from_estimator` with the `colors` keyword argument. Specifically, the following error is raised:\n```python\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\sklearn\\inspection\\_plot\\decision_boundary.py\", line 276, in plot\n    plot_func(self.xx0, self.xx1, response, cmap=cmap, **safe_kwargs)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\matplotlib\\contour.py\", line 689, in __init__\n    raise ValueError('Either colors or cmap must be None')\nValueError: Either colors or cmap must be None\n```\nHowever, in v1.6.0, everything works fine.\n\nAfter further investigation, it seems this issue was introduced by PR #29797, where both `cmap` and `colors` are passed to `plot_func` unconditionally, without explicit conflict handling:\nhttps://github.com/scikit-learn/scikit-learn/blob/d4d4af8c471c60d183d0cb67e14e6434b0ebb9fb/sklearn/inspection/_plot/decision_boundary.py#L276\nAdditionally, when setting `plot_method='contour'` in multiclass classification scenarios, the decision boundary is no longer shown as it was in v1.6.0. It appears that this regression is due to the switch in v1.7.0 to always using a cmap to plot the entire decision surface in multiclass scenarios.\n\nHere are the visual differences:\n- v1.6.0 with `plot_method='contour'`:\n![Image](https://github.com/user-attachments/assets/858d2540-47d5-4637-b992-89dc9b196b08)\n- v1.7.0 with the same code:\n![Image](https://github.com/user-attachments/assets/6d7a5c0e-2df9-47c0-bc9c-3a4e0e5dbac4)\n## Suggestion\nTo preserve backward compatibility and expected behavior:\n- Check for mutual exclusivity of `colors` and `cmap` and raise a clear warning/error;\n- Retain the old behavior when `plot_method='contour'`.\n\nI'd be happy to open a PR to help address this regression if the core team is supportive.\n\n### Steps/Code t...",
      "labels": [
        "Bug",
        "Regression"
      ],
      "state": "closed",
      "created_at": "2025-06-14T16:22:44Z",
      "updated_at": "2025-07-15T12:53:33Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31546"
    },
    {
      "number": 31542,
      "title": "Huber Loss for HistGradientBoostingRegressor",
      "body": "### Describe the workflow you want to enable\n\nHuber loss is available as an option for `GradientBoostingRegressor` and works great when training on data with frequent outliers (thank you!). `HistGradientBoostingRegressor` however does not support Huber loss, which may be required when scaling to larger datasets. \n\n### Describe your proposed solution\n\nAdd HuberLoss as an option for the `HistGradientBoostingRegressor` class. \n\n### Describe alternatives you've considered, if relevant\n\nPossibly allow custom loss functions for the `HistGradientBoostingRegressor`\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "help wanted",
        "Hard"
      ],
      "state": "open",
      "created_at": "2025-06-13T13:24:16Z",
      "updated_at": "2025-06-27T08:18:40Z",
      "comments": 23,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31542"
    },
    {
      "number": 31540,
      "title": "Make `sklearn.metrics._scorer._MultimetricScorer` part of the public API",
      "body": "### Describe the workflow you want to enable\n\nThis tool is great to run multiple scorers on a single estimator thanks to the caching mechanism. It is a bummer that it is not part of the public API.\n\n### Describe your proposed solution\n\nMake it part of the public API\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "Enhancement",
        "API",
        "Needs Decision"
      ],
      "state": "open",
      "created_at": "2025-06-13T09:18:28Z",
      "updated_at": "2025-08-13T07:05:35Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31540"
    },
    {
      "number": 31538,
      "title": "当selector = VarianceThreshold(threshold=0.1)和selector = VarianceThreshold()输出的结果不一样",
      "body": "### Describe the bug\n\nimport numpy as np\nX = np.arange(30,dtype=float).reshape((10, 3))\nX[:,1] = 1\nfrom sklearn.feature_selection import VarianceThreshold\nvt = VarianceThreshold(threshold=0.01)\nxt = vt.fit_transform(X)\n# 未设置阈值时，可能未实际计算方差\nvt1 = VarianceThreshold()\nvt1.fit(X)                # 先调用fit方法\nprint(vt1.variances_)     # 现在可以安全访问\n\n# 设置阈值后强制计算\nvt2 = VarianceThreshold(threshold=0.01)\nvt2.fit(X)  # 实际执行计算\nprint(vt2.variances_)     # 输出正确值\nvt = VarianceThreshold(threshold=0.01)\nvt.fit(X)  # 确保实际计算\nprint(vt.variances_)\n# 检查方差计算一致性\nmanual_var = np.var(X, axis=0, ddof=0)\nsklearn_var = vt.variances_\nif not np.allclose(manual_var, sklearn_var):\n    print(f\"警告：方差计算不一致！手动:{manual_var}，sklearn:{sklearn_var}\")\n    # 确保使用最新稳定版\nimport sklearn\nprint(\"scikit-learn版本:\", sklearn.__version__)  # 应 ≥ 1.0\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learn版本: 1.7.0\n\n### Steps/Code to Reproduce\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learn版本: 1.7.0\n\n### Expected Results\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learn版本: 1.7.0\n\n### Actual Results\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learn版本: 1.7.0\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\nexecutable: c:\\Users\\wp\\Desktop\\python312\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.7.0\n          pip: 25.1.1\n   setuptools: 78.1.0\n        numpy: 1.26.0\n        scipy: 1.13.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 4\n         prefix: libopenblas\n...\n       filepath: C:\\Users\\wp\\Desktop\\python312\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.27--3aa239bc726cfb0bd8e5330d8d4c15c6.dll\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Haswell\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-13T00:58:02Z",
      "updated_at": "2025-06-13T10:28:25Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31538"
    },
    {
      "number": 31536,
      "title": "Improve sample_weight handling in sag(a)",
      "body": "### Describe the bug\n\nThis may be more of a discussion, but overall I am not sure what treatment of weighting would preserve the convergence guarantees for the SAG(A) solver. So far as I see it, at each update step we uniformly select some index $i_j$ such that the update steps can be generalised as:\n\n$x^{k+1} = x^{k} - \\sum_{j=1}^{k} \\alpha_{j} S(j, i_{1:k}) f'_{i_j}(x^j)$\n\nWhere $S(j, i_{1:k}) = 1/n$ if $j$ is the maximum iteration at which $i_j$ is selected. \n\nFor frequency based weighting, one could sample $i_j$ using weights as a probability, and under non-uniform sampling the SAG(A) convergence guarantees still seem to hold, (see [here]([https://inria.hal.science/hal-00860051/document])).\n\n Alternatively as currently done, the weights could be multiplied through with the gradient update and that could also work, however I am not sure which method is best (we also here need to additionally consider the division by the cardinality of the set of \"seen\" elements within each update step).\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom scipy.stats import kstest\nfrom sklearn.linear_model.tests.test_sag import sag, squared_dloss\nfrom sklearn.datasets import make_regression\nfrom sklearn.utils._testing import assert_allclose_dense_sparse\n\nstep_size=0.01\nalpha=1\n\nn_features = 1\n\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples=10000,random_state=77,n_features=n_features)\nweights = rng.randint(0,5,size=X.shape[0])\n\nX_repeated = np.repeat(X,weights,axis=0)\ny_repeated = np.repeat(y,weights,axis=0)\n\nweights_w_all = np.zeros([n_features,100])\nweights_r_all = np.zeros([n_features,100])\n\nfor random_state in np.arange(100):\n\n    weights_w, int_w = sag(X,y,step_size=step_size,alpha=alpha,sample_weight=weights,dloss=squared_dloss,random_state=random_state)\n    weights_w_all[:,random_state] = weights_w\n    weights_r, int_r = sag(X_repeated,y_repeated,step_size=step_size,alpha=alpha,dloss=squared_dloss,random_state=random_state)\n    weights_r_all[:,ra...",
      "labels": [
        "Enhancement"
      ],
      "state": "open",
      "created_at": "2025-06-12T16:04:19Z",
      "updated_at": "2025-06-28T14:33:36Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31536"
    },
    {
      "number": 31533,
      "title": "RFC: stop using scikit-learn `stable_cumsum` and instead use `np.cumsum/xp.cumulative_sum` directly",
      "body": "As discussed in https://github.com/scikit-learn/scikit-learn/pull/30878/files#r2142562746, our current `stable_cumsum` function brings very little value to the user: it does extra computation to check that `np.allclose(np.sum(x), np.cumsum(x)[-1])` and raises a warning otherwise. However, in most cases, users can do nothing about the warning.\n\nFurthermore, as seen in the CI of #30878, the array API compatible libraries we test against do not have the same numerical stability behavior for `sum` and `cumsum`, so it makes it challenging to write a test for the occurrence of this warning that is consistent across libraries.\n\nSo I would rather not waste the overhead of computing `np.sum(x)` and just always directly call `np.cumsum` or `xp.cumsum` and deprecate `sklearn.utils.extmath.stable_cumsum`.",
      "labels": [
        "RFC",
        "Array API"
      ],
      "state": "open",
      "created_at": "2025-06-12T12:11:30Z",
      "updated_at": "2025-08-22T10:25:13Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31533"
    },
    {
      "number": 31527,
      "title": "⚠️ CI failed on Wheel builder (last failure: Jun 12, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15601223966)** (Jun 12, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-12T04:36:25Z",
      "updated_at": "2025-06-12T15:23:10Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31527"
    },
    {
      "number": 31525,
      "title": "Issue with the `RidgeCV` diagram representation with non-default alphas",
      "body": "It seems that we introduced a regression in the HTML representation. The following code is failing:\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import RidgeCV\n\nRidgeCV(np.logspace(-3, 3, num=10)\n```\n\nleads to the following error:\n\n```pytb\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/IPython/core/formatters.py:406, in BaseFormatter.__call__(self, obj)\n    404     method = get_real_method(obj, self.print_method)\n    405     if method is not None:\n--> 406         return method()\n    407     return None\n    408 else:\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_repr_html/base.py:145, in ReprHTMLMixin._repr_html_inner(self)\n    140 def _repr_html_inner(self):\n    141     \"\"\"This function is returned by the @property `_repr_html_` to make\n    142     `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n    143     on `get_config()[\"display\"]`.\n    144     \"\"\"\n--> 145     return self._html_repr()\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_repr_html/estimator.py:480, in estimator_html_repr(estimator)\n    469 html_template = (\n    470     f\"<style>{style_with_id}</style>\"\n    471     f\"<body>\"\n   (...)    476     '<div class=\"sk-container\" hidden>'\n    477 )\n    479 out.write(html_template)\n--> 480 _write_estimator_html(\n    481     out,\n    482     estimator,\n    483     estimator.__class__.__name__,\n    484     estimator_str,\n    485     first_call=True,\n    486     is_fitted_css_class=is_fitted_css_class,\n    487     is_fitted_icon=is_fitted_icon,\n    488 )\n    489 with open(str(Path(__file__).parent / \"estimator.js\"), \"r\") as f:\n    490     script = f.read()\n\nFile ~/Documents/teaching/demo_data_science_a...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-06-11T20:41:12Z",
      "updated_at": "2025-06-19T09:12:13Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31525"
    },
    {
      "number": 31521,
      "title": "TarFile.extractall() got an unexpected keyword argument 'filter'",
      "body": "### Describe the bug\n\nFor the latest version `1.7.0`, it can be installed with Python 3.10, but the parameter `filter` is available starting from Python 3.12 (See: https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractall ). \nhttps://github.com/scikit-learn/scikit-learn/blob/5194440b5d41e73ff436c45e35aa1476223f753c/sklearn/datasets/_twenty_newsgroups.py#L87\n\nAs a result, when I attempted to download the `20newsgroups` dataset, an error occurred:\n\n```\n  File \"\\xxx\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n    return func(*args, **kwargs)\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 322, in fetch_20newsgroups\n    cache = _download_20newsgroups(\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 87, in _download_20newsgroups\n    fp.extractall(path=target_dir, filter=\"data\")\nTypeError: TarFile.extractall() got an unexpected keyword argument 'filter'\n```\n\n### Steps/Code to Reproduce\n\n```\nfrom sklearn.datasets import fetch_20newsgroups\ncats = ['alt.atheism', 'sci.space']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n```\n\n### Expected Results\n\n```\nlist(newsgroups_train.target_names)\nnewsgroups_train.filenames.shape\nnewsgroups_train.target.shape\nnewsgroups_train.target[:10]>>> cats = ['alt.atheism', 'sci.space']\n```\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\\xxx\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n    return func(*args, **kwargs)\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 322, in fetch_20newsgroups\n    cache = _download_20newsgroups(\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 87, in _download_20newsgroups\n    fp.extractall(path=target_dir, filter=\"data\")\nTypeError: TarFile.extractall() got an unexpected keyword argument 'filter'\n```\n\n### Versions\n\n```shell\n`1.7.0`\n```",
      "labels": [
        "Bug",
        "Regression"
      ],
      "state": "closed",
      "created_at": "2025-06-11T05:12:28Z",
      "updated_at": "2025-07-07T09:10:43Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31521"
    },
    {
      "number": 31520,
      "title": "32-Bit Raspberry Pi OS Installation Issues with UV",
      "body": "### Describe the bug\n\nWhen attempting to install scikit-learn==1.4.2 - 1.6.1 on Raspberry Pi OS Lite 32-Bit (Bookworm) or Raspberry Pi OS Lit 32-Bit (Bullseye) with UV, the following error is given:\n```\n  × Failed to download and build `scikit-learn==1.4.2`\n  ├─▶ Failed to resolve requirements from `build-system.requires`\n  ├─▶ No solution found when resolving: `setuptools`, `wheel`, `cython>=3.0.8`, `numpy==2.0.0rc1`, `scipy>=1.6.0`\n  ╰─▶ Because there is no version of numpy==2.0.0rc1 and you require numpy==2.0.0rc1, we can conclude that your\n      requirements are unsatisfiable.\n```\n\nIf I had to guess, it's that the numpy==2.0.0rc1 is the issue, but I'm not sure.  \n\nBullseye is also on Python 3.9 so the last version we can install is v1.6.1.  \n\n\n\n### Steps/Code to Reproduce\n\n```bash\n# 1. Install UV\n# 2. Create Virtual Environment\nuv venv --system-site-packages test \n# 3. Start venv\nsource test/bin/activate\n# 4. Install scikit-learn\nuv pip install scikit-learn==1.6.1\n```\n\n### Expected Results\n\nExpect that it should install correctly without errors. \n\n### Actual Results\n\n```\n  × Failed to download and build `scikit-learn==1.4.2`\n  ├─▶ Failed to resolve requirements from `build-system.requires`\n  ├─▶ No solution found when resolving: `setuptools`, `wheel`, `cython>=3.0.8`, `numpy==2.0.0rc1`, `scipy>=1.6.0`\n  ╰─▶ Because there is no version of numpy==2.0.0rc1 and you require numpy==2.0.0rc1, we can conclude that your\n      requirements are unsatisfiable.\n```\n\n### Versions\n\n```shell\n1.4.2\n1.6.0\n1.6.1\n```",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-06-11T01:05:52Z",
      "updated_at": "2025-06-12T15:04:52Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31520"
    },
    {
      "number": 31512,
      "title": "Add free-threading wheel for Linux arm64 (aarch64)",
      "body": "### Describe the workflow you want to enable\n\nI am a maintainer for the third-party package [fastcan](https://github.com/scikit-learn-contrib/fastcan). I tested the package on the free-threading Python (cp313t), and found scikit-learn missing a wheel for Linux arm64 (aarch64) on PyPI.\n\nI would like to have the official release wheel rather than building it from source.\n\n### Describe your proposed solution\n\nI tested scikit-learn on my own fork, and the free-threading wheel for Linux arm64 (scikit_learn-1.8.dev0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl) can be successfully built. So I suppose that wheel is just mistakenly missed.\n\nJust add that wheel in wheel.yml should be fine.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature"
      ],
      "state": "closed",
      "created_at": "2025-06-10T01:59:41Z",
      "updated_at": "2025-06-10T10:02:32Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31512"
    },
    {
      "number": 31503,
      "title": "HDBSCAN performance issues compared to original hdbscan implementation (likely because Boruvka algorithm is not implemented)",
      "body": "### Describe the bug\n\nWhen switching from Sklearn HDBSCAN implementation to original one from `hdbscan` library, I've notice that Sklearn's implementation has much worse implementation. I've tried investigating different parameters but it doesn't seem to have an effect on the performance.\n\nI've created synthetic benchmark using `make_blobs` function.  And those are my results:\n\nCPU: Ryzen 5 1600, 12 Threads@3.6Ghz*\nRAM: 32GB DDR4\n\n```python\n# dataset\nX, y = make_blobs(n_samples=10000, centers=5, cluster_std=0.60, random_state=0, n_features=10)\n\n# hdbscan params \nog_hdbscan = OGHDBSCAN(core_dist_n_jobs=-1)\nsk_hdbscan = SKHDBSCAN(n_jobs=-1)\n```\n\n![Image](https://github.com/user-attachments/assets/42bc818c-8547-4297-9020-e87a02b7bd90)\n\n* Tested out on Google Collab with similar results\n\n### Steps/Code to Reproduce\n\nI am starting both algorithms with `n_jobs=-1` to rule out the difference that may occure because of default setting of `core_dist_n_jobs=4` in `hdbscan`\n\n```python\nfrom hdbscan import HDBSCAN as OGHDBSCAN\nfrom sklearn.cluster import HDBSCAN as SKHDBSCAN\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=10000, centers=5, cluster_std=0.60, random_state=0, n_features=10)\n\nog_hdbscan = OGHDBSCAN(core_dist_n_jobs=-1)\nsk_hdbscan = SKHDBSCAN(n_jobs=-1)\n\nRUNS = 10\n\ndef time_hdbscan(hdbscan, X, runs):\n    times = []\n    for _ in range(runs):\n        start = time.time()\n        hdbscan.fit(X)\n        end = time.time()\n        times.append(end - start)\n    return times\n\ntimes_og = time_hdbscan(og_hdbscan, X, RUNS)\ntimes_sk = time_hdbscan(sk_hdbscan, X, RUNS)\n\nprint(\"Mean time OGHDBSCAN: \", np.mean(times_og))\nprint(\"Mean time SKHDBSCAN: \", np.mean(times_sk))\n\nplt.plot(range(RUNS), times_og, label='OGHDBSCAN', marker='o')\nplt.plot(range(RUNS), times_sk, label='SKHDBSCAN', marker='x')\nplt.xlabel('Run')\nplt.ylabel('Time (seconds)')\nplt.title('HDBSCAN Timing Comparison')\nplt.legend()\nplt.sh...",
      "labels": [
        "New Feature",
        "help wanted",
        "Hard"
      ],
      "state": "open",
      "created_at": "2025-06-08T14:53:52Z",
      "updated_at": "2025-06-13T12:37:39Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31503"
    },
    {
      "number": 31498,
      "title": "Doc website incorrectly flags stable as unstable",
      "body": "### Describe the bug\n\nCurrent website gives:\n![Image](https://github.com/user-attachments/assets/78ec363e-92cf-4a3f-afc5-68639078d9b3)\n\nI tried having a look on how to fix this, but went in a rabbit hole that the version switcher is generated by \"list_versions.py\" in the circle-ci scripts and this exceeded the time that I have. IMHO, such automation is over-engineered and does not make things more reliable, as we are seeing currently\n\n### Steps/Code to Reproduce\n\nGo to https://scikit-learn.org/stable/\n\n### Expected Results\n\nNot having the banner on top\n\n### Actual Results\n\nThe banner of the top of the website displays\n\n### Versions\n\n```shell\nstable\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-06-06T09:06:38Z",
      "updated_at": "2025-06-06T09:20:18Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31498"
    },
    {
      "number": 31475,
      "title": "MultiOutputRegressor can't process estimators with synchronization primitives",
      "body": "### Describe the bug\n\n[MultiOutputRegressor ](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html) can't process estimators with threading/multiprocessing synchronization primitives\n\nI want to propagate stop_event to the callback of regressor. I think the issue is because MultiOutputRegressor is trying to pickle each of estimator to move it to another thread/process. And if the estimator contains any synchronization primitives - they can't be pickled, so it fails. Maybe the solution might be to allow to provide pre-created estimators (for each of output) and provide them to the init of MultiOutputRegressor.\n\nI need to use MultiOutputRegressor because I need to export XGBoost model into onnx with a help of [skl2onnx](https://onnx.ai/sklearn-onnx/). If I don't use MultiOutputRegressor  - skl2onnx doesn't allow me to export, despite XGBoost has an [experimental way of multiple outputs](https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html).\n\nOr maybe I missed something. Please help.\n\n\nPackages:\n\n```\nxgboost                   3.0.0\n```\n\n### Steps/Code to Reproduce\n\n```python\nfrom threading import Event\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom xgboost import XGBRegressor, Booster\nfrom xgboost import callback as xgb_callbacks\n\n\nclass Callback(xgb_callbacks.TrainingCallback):\n    def __init__(self, stop_event: Event):\n        super().__init__()\n        self.stop_event = stop_event\n\n    def after_iteration(self, model: Booster, epoch: int, evals_log: dict[str, dict]) -> bool:\n        print(f\"xgboost training: epoch {epoch}, evals_log {evals_log}\")\n        return False\n\n\ndef train_xgboost(X_train, y_train):\n    stop_event = Event()\n\n    base_model = XGBRegressor(n_estimators=45, callbacks=[Callback(stop_event)])\n    model = MultiOutputRegressor(base_model)\n    # base_model.callbacks = [Callback(stop_eve...",
      "labels": [
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-06-03T10:30:37Z",
      "updated_at": "2025-06-10T13:07:23Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31475"
    },
    {
      "number": 31473,
      "title": "Add option to return final cross-validation score in SequentialFeatureSelector",
      "body": "### Describe the workflow you want to enable\n\nCurrently, when using `SequentialFeatureSelector`, it internally performs cross-validation to decide which features to select, based on the scoring function. However, the final cross-validation score (e.g., recall) is not returned by the SFS object.\n\n\n\n### Describe your proposed solution\n\nAdd an attribute (e.g., `final_cv_score_`) that stores the mean cross-validation score of the final model with the selected features. This would avoid having to run another cross-validation externally to get the final performance score.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nThis feature would be especially useful when the scoring metric is expensive to compute, as it would avoid redundant cross-validation runs.",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-06-02T23:20:53Z",
      "updated_at": "2025-06-03T09:08:55Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31473"
    },
    {
      "number": 31462,
      "title": "Feat: DummyClassifier strategy that produces randomized probabilities",
      "body": "### Describe the workflow you want to enable\n\n# Motivation\n\nThe `dummy` module is fantastic for testing pipelines all the way up through enterprise scales. The [strategies](https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73/sklearn/dummy.py#L374) offered in the `DummyClassifier` are excellent for testing corner cases. However, the strategies offered fall short when testing pipelines that include downstream tasks that depend on moments of the predicted probabilities (e.g. gains charts).\n\nThis is because the existing strategies do not include sampling _random probabilities_.\n\n## Proposed API:\n\nConsider adding a new strategy with a name like `uniform-proba` or `score-random` or something similar that results in this behavior for binary classification:\n\n```python\nprint(DummyClassifier(strategy=\"uniform-proba\").fit(X, y).predict_proba(X))\n\"\"\"\n[[0.5651713  0.4348287 ]\n [0.36557341 0.63442659]\n [0.42386353 0.57613647]\n ...\n [0.30348692 0.69651308]\n [0.59589879 0.40410121]\n [0.32664176 0.67335824]]\n\"\"\"\n```\n\n### Describe your proposed solution\n\n## Proposed implementation\n\nI had something like this in mind:\n```python\nclass DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n    ...\n\n    def predict_proba(self, X):\n        ...\n        for k in range(self.n_outputs_):\n            if self._strategy == \"uniform-proba\":\n                out = rs.dirichlet([1] * n_classes_[k], size=n_samples)\n                out = out.astype(np.float64)\n            ...\n```\n\nSimilar to the `\"stratified\"` strategy, this simple implementation relies on `numpy.random`, in this case the [`dirichlet`](https://numpy.org/doc/2.0/reference/random/generated/numpy.random.RandomState.dirichlet.html) distribution. By setting all the `alpha`s to 1, we are specifying that the probabilities of each class are equally distributed -- in contrast, the `\"stratified\"` strategy effectively samples from a dirichlet distribution with one alpha equal to 1 and the rest equal to 0.\n\n\n### Describe altern...",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-06-01T17:27:18Z",
      "updated_at": "2025-07-07T13:20:14Z",
      "comments": 9,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31462"
    },
    {
      "number": 31450,
      "title": "Spherical K-means support (unit norm centroids and input)",
      "body": "### Describe the workflow you want to enable\n\nHi,\nI was wondering if there is—or has been—any initiative to support cosine similarity in the KMeans implementation (i.e., spherical KMeans). I find the algorithm quite useful and would be happy to propose an implementation. The addition should be relatively straightforward.\n\n### Describe your proposed solution\n\nEnable the use of cosine similarity with KMeans or implement a separate SphericalKMeans class.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-05-28T20:47:24Z",
      "updated_at": "2025-06-13T11:59:45Z",
      "comments": 10,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31450"
    },
    {
      "number": 31444,
      "title": "⚠️ CI failed on Wheel builder (last failure: May 28, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15291085639)** (May 28, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-28T09:53:22Z",
      "updated_at": "2025-05-29T04:40:36Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31444"
    },
    {
      "number": 31443,
      "title": "Folder/Directory descriptions not present",
      "body": "### Describe the issue linked to the documentation\n\nI was navigating through the codebase, trying to find source code for some algorithms. I noticed that there are no descriptions of files present within a folder, which would actually make it easier to navigate through the codebase. We can have a small readme file within folders which would describe what is present in that folder. \n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-28T07:46:08Z",
      "updated_at": "2025-06-04T14:04:09Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31443"
    },
    {
      "number": 31441,
      "title": "Regression error characteristic curve",
      "body": "### Describe the workflow you want to enable\n\nAdd more fine-grained diagnostic, similar to ROC or Precision-Recall curves, to regression problems. It appears that this library has a lot of excellent tools for classification, and I believe it would benefit from some additional tools for regression.\n\n### Describe your proposed solution\n\nCompute Regression Error Characteristic (REC) [1] curve - for each error threshold the percentage of samples whose error is below that threshold. This is essentially the CDF of the regression errors. Its function is similar to that of ROC curves - allows comparing performance profiles of regressors beyond just one summary statistic, such as RMSE or MAE.\n\nI already implement a pull-request:\nhttps://github.com/scikit-learn/scikit-learn/pull/31380\n\nScreenshot from the merge request:\n\n![Image](https://github.com/user-attachments/assets/1974e8e7-03da-47c7-adb5-5c75eb24d61e)\n\nIf you believe this feature is useful, please help me with reviewing and merging it.\n\n### Describe alternatives you've considered, if relevant\n\nRegression Receiver Operating Characteristic (RROC) curves, proposed [2], which plot over-prediction vs under-prediction, are a different form of diagnostic curves for regression. They may also be useful, but I think we should begin from somewhere, and I belive it's better to begin from REC, both because the paper has more citations, and because it turned out to be very useful for me at work, and I believe it can be similarly useful to other scientists.\n\n### Additional context\n\n**References**\n---\n\n[1]: Bi, J. and Bennett, K.P., 2003. Regression error characteristic curves. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 43-50).\n[2]: Hernández-Orallo, J., 2013. ROC curves for regression. Pattern Recognition, 46(12), pp.3395-3411.",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-28T05:40:22Z",
      "updated_at": "2025-07-03T05:33:18Z",
      "comments": 9,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31441"
    },
    {
      "number": 31423,
      "title": "The libomp.dylib shipped with the macOS x86_64 package does not have an SDK version set",
      "body": "### Describe the bug\n\nI want to build an macOS app that uses scikit-learn as a dependency. Using the arm64 package of scikit-learn for this works flawlessly. However, if I want to do the same using the macOS x86_64 packages Apple's notarizing step always breaks the app. This is likely due to the shipped libomp.dylib in the x86_64 package (installed using pip) does not have an SDK version set:\n```\notool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 3 LC_VERSION_MIN_MACOSX\n      cmd LC_VERSION_MIN_MACOSX\n  cmdsize 16\n  version 10.9\n      sdk n/a\n```\nThe arm64 version has this set:\n```\notool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 4 LC_BUILD_VERSION\n      cmd LC_BUILD_VERSION\n  cmdsize 32\n platform 1\n    minos 11.0\n      sdk 11.0\n```\nIt would be great, if you could set this (to at least 10.9; would probably need a rebuild of the dylib from source). I already tried some workarounds, but so far none have been successful. Is there any chance you would consider that :)?\n\n### Steps/Code to Reproduce\n\n```\n% otool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 3 LC_VERSION_MIN_MACOSX\n      cmd LC_VERSION_MIN_MACOSX\n```\n\n### Expected Results\n\n```\n  cmdsize 16\n  version 10.9\n      sdk 10.9\n```\n\n### Actual Results\n\n```\n  cmdsize 16\n  version 10.9\n      sdk n/a\n```\n\n### Versions\n\n```shell\nscikit-learn==1.6.1 (from pip freeze)\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-24T23:02:36Z",
      "updated_at": "2025-06-04T13:31:29Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31423"
    },
    {
      "number": 31415,
      "title": "Discrepancy between output of classifier feature_importances_ with different sklearn installations",
      "body": "### Describe the bug\n\nI am currently using `scikit-learn` classifier `feature_importances_` attribute on a project to rank important features from my model, and my `CI` pipeline runs the project test-suite using instances of `scikit-learn==1.3.2` and `scikit-learn==1.5.2` on a remote linux host. I am experiencing some discrepancies in the output of the relevant test (for which I have provided a minimal viable reproducer below) on different machines/installations/sklearn versions. \n\nThere are a few specific problems I am experiencing:\n\n1. Locally, the test will pass using a binary installation of `scikit-learn==1.3.2` and fail using `scikit-learn==1.5.2`. With the help of my team, we have traced this error back and found the earliest failing version to be `1.4.1.post1`.  We suspect that the error originates from a change made in https://github.com/scikit-learn/scikit-learn/pull/27639 that has to do with the switch from absolute counts to store proportions in `tree_.values` but have not determined a root cause for the discrepancy.\n2. As mentioned in (1) when running the test-suite locally on my `Mac-ARM64` machine, the test will fail as described, however, when running the test on a remote linux machine, the test will pass with both sklearn versions\n3. The test will fail when I build the code from source vs. from the binary distribution of `scikit-learn==1.3.2`\n\nMy main question is, what could be the cause of these observed discrepancies between sklearn version, installation type and environment and which output is most \"correct\"?\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\nfrom pandas.testing import assert_frame_equal\nimport pdb\n\n\n# this test serves as a minimal viable reproducer for the\n# difference observed in output of tree values between\n# sklearn versions 1.3.2 and 1.4.2. this test should pass\n# when using sklearn==1.3.2 and fail when using sklearn==1.4.2\n\n# first create a min...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-22T23:58:23Z",
      "updated_at": "2025-06-04T13:23:10Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31415"
    },
    {
      "number": 31412,
      "title": "SimpleImputer converts `int32[pyarrow]` extension array to `float64`, subsequently crashing with numpy `int32` values",
      "body": "### Describe the bug\n\nWhen using the `SimpleImputer` with a pyarrow-backed pandas DataFrame, any float/integer data is converted to `None`/`float64` instead.\nThis causes the imputer to be fitted to `float64`, crashing on a dtype assertion when passing it a numpy-backed `int32` DataFrame after fitting.\n\nThe flow is the following:\n1. The imputer calls `_validate_input`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L319\n2. This calls `validate_data`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L344-L353\n3. This calls `check_array`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L2951-L2952\n4. Our input is a pandas dataframe:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L909\n5. This now checks if the dtypes need to be converted:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L925-L927\n6. Our input is backed by an extension array _and_ `int32[pyarrow]` is an integer datatype, so we return `True` here:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L714-L724\n7. Finally we pass the \\\"needs conversion\\\" check and convert the dataframe to `dtype` (which is `None` here, which apparently means `float64`):\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L966-L971\n\n### Steps/Code to Reproduce\n\n```py\nimport polars as pl\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\\\"a\\\": [10]}, schema={\\\"a\\\": pl.Int32}).to_pandas(use_pyarrow_extension_array=False))\n   ...",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-21T23:34:26Z",
      "updated_at": "2025-05-21T23:34:45Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31412"
    },
    {
      "number": 31408,
      "title": "estimator_checks_generator does not return (estimator, check) when hitting an expected failed check",
      "body": "### Describe the bug\n\nCurrently running sklearn_check_generator with mark=\"skip\" on our estimators.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.estimator_checks_generator.html\n\nI would like to start running those checks with \"xfail\".\n\nBut when I do so any test 'marked' via the `expected_failed_checks` dictionary gives a \n`ValueError: too many values to unpack (expected 2)`\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.utils.estimator_checks import estimator_checks_generator\nfrom sklearn.base import BaseEstimator\n\nexpected_failed_checks = {\"check_complex_data\": \"some reason\"}\n\nestimator = BaseEstimator()\n\n# fine\nfor e, check in estimator_checks_generator(\n                estimator=estimator,\n                expected_failed_checks=expected_failed_checks, \n                mark=\"skip\"\n            ):\n    print(check)\n\n# error\nfor e, check in estimator_checks_generator(\n                estimator=estimator,\n                expected_failed_checks=expected_failed_checks, \n                mark=\"xfail\"\n            ):\n    print(check)\n```\n\n### Expected Results\n\nestimator_checks_generator to yield (estimator, check) tuples whether xfail or skip was requested\n\n### Actual Results\n\n```\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_tags_renamed at 0x7ec1883e62a0>, 'BaseEstimator')\nfunctools.partial(<function check_valid_tag_types at 0x7ec1883e6200>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_repr at 0x7ec1883e51c0>, 'BaseEstimator')\nfunctools.partial(<function check_no_attributes_set_in_init at 0x7ec1883e4b80>, 'BaseEstimator')\nfunctools.partial(<function check_fit_score_takes_y at 0x7ec1883da0c0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_overwrite_params at 0x7ec1883e4a40>, 'BaseEstimator')\nfunctools.partial(<function chec...",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-05-21T14:15:00Z",
      "updated_at": "2025-06-04T12:17:22Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31408"
    },
    {
      "number": 31407,
      "title": "Cannot recover DBSCAN from memory-overuse",
      "body": "### Describe the bug\n\nI also just ran into this issue that the program gets killed when running DBSCAN, similar to:\nhttps://github.com/scikit-learn/scikit-learn/issues/22531\n\nThe documentation update already helps and I think it's ok for the algorithm to fail. But currently there is no way for me to recover, and a more informative error message would be useful. Since now DBSCAN just reports `killed` and it requires a bit of search to see what fails:\n```\n>>> DBSCAN(eps=1, min_samples=2).fit(np.random.rand(10_000_000, 3))\nKilled\n```\n\ne.g., something like how `numpy` does it:\n```\n>>> n = int(1e6)\n>>> np.random.rand(n, n)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"numpy/random/mtrand.pyx\", line 1219, in numpy.random.mtrand.RandomState.rand\n  File \"numpy/random/mtrand.pyx\", line 437, in numpy.random.mtrand.RandomState.random_sample\n  File \"_common.pyx\", line 307, in numpy.random._common.double_fill\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 7.28 TiB for an array with shape (1000000, 1000000) and data type float64\n```\n\nAdditionally, I noted that the memory accumulated with consecutive calling of DBSCAN. Which can lead to a killed program even though there is enough memory when running a single fit.\nI was able to resolve this by explicitly calling `import gc; gc.collect()` after each run. Maybe this could be invoked at the end of each DBSCAN fit?\n\n### Steps/Code to Reproduce\n\n```python\ntry:\n    DBSCAN(eps=1, min_samples=2).fit(np.random.rand(10_000_000, 3))\nexcept:\n    print(\"Caught exception\")\n```\n\n\n### Expected Results\n\n```python\nCaught exception\n```\n\n### Actual Results\n\n```python\nKilled\n```\n\n### Versions\n\n```shell\n>>> import sklearn; sklearn.show_versions()\n\nSystem:\n    python: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nexecutable: /usr/bin/python3\n   machine: Linux-6.14.6-arch1-1-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: 80.7.1\n        numpy: 1.26.4...",
      "labels": [
        "Bug",
        "help wanted",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-05-21T11:38:43Z",
      "updated_at": "2025-06-12T13:13:19Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31407"
    },
    {
      "number": 31403,
      "title": "[PCA] ValueError: too many values to unpack (expected 3)",
      "body": "### Describe the bug\n\nI am getting the following error when running PCA with version 1.6.1:\n\n<img width=\"956\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4a576ca3-2268-45c0-8fa8-cccea16fce6d\" />\n\n\n\n### Steps/Code to Reproduce\n\nYou can reproduce it with this snippet: \n\n```\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nX = np.random.choice([0, 1], size=(10, 2048), p=[0.7, 0.3])\nprint(X.shape, X.dtype)\n\npca = PCA(n_components=2)\npca.fit_transform(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n```\n\n\n\n### Expected Results\n\nThis works with version `1.3.2`. \n\n```\n(10, 2048) int64\n[0.12276184 0.11835199]\n[21.7604452  21.36603173]\n```\n\n\n\nI tried using `svd_solver='arpack'`, but that does not help in my desperate attempts to solve the issue.  Why did this stop working after `1.3.2`?  For now, I just rolled back to `1.3.2`. \n\n\nThanks\n\n### Actual Results\n\n<img width=\"956\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eaea3acd-7429-4497-9b14-b2c121fe1918\" />\n\n### Versions\n\n```shell\nVersion with the error `1.6.1`. Version that works for me: `1.3.2`.\n```",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-20T17:54:44Z",
      "updated_at": "2025-05-21T13:11:39Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31403"
    },
    {
      "number": 31399,
      "title": "DOC Jupyterlite raises a ValueError when using plotly",
      "body": "### Describe the issue linked to the documentation\n\nRunning for instance `plot_forest_hist_grad_boosting_comparison` in jupyterlite raises a `ValueError: Mime type rendering requires nbformat>=4.2.0 but it is not installed`. I tried adding `%pip install nbformat` at the top of the notebook cell but that doesn't seem to work. As per [this post in stackoverflow](https://stackoverflow.com/questions/69304838/plotly-cannot-find-nbformat-even-though-its-there-jupyter-notebook), downgrading `nbformat` to `5.1.2` solved this issue for me.\n\n### Suggest a potential alternative/fix\n\nAdd a magic function `%pip install nbformat==5.1.2` whenever plotly is imported.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-20T10:06:51Z",
      "updated_at": "2025-05-20T14:19:10Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31399"
    },
    {
      "number": 31395,
      "title": "RuntimeWarnings: divide by zero, overflow, invalid value encountered in matmul",
      "body": "### Describe the bug\n\nWhile running feature selection, I get the following warnings:\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n ret = a @ b\n\n\n\n### Steps/Code to Reproduce\n\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nX, y = make_friedman1(n_samples=500, n_features=100, random_state=0)\nestimator = SVR(kernel=\"linear\")\nselector = RFECV(estimator, step=1, cv=5)\nselector = selector.fit(X, y)\nprint(selector.support_)\n\n### Expected Results\n\n[ True  True False  True  True False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False]\n\n### Actual Results\n\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n...\n[ True  True False  True  True False False False False False False False\n False False False False False False False False False False False False...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-19T18:02:46Z",
      "updated_at": "2025-05-20T16:26:30Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31395"
    },
    {
      "number": 31391,
      "title": "Avoid bundling tests in wheels",
      "body": "### Describe the bug\n\nThe wheels currently include tests and test data. These usually are of no additional value outside of the source distributions and thus just bloat the distribution and complicate reviews. For this reasons, I recommend excluding them from future wheels.\n\nThis matches the official recommendation for Python packaging as well (see https://packaging.python.org/en/latest/discussions/package-formats/#what-is-a-wheel):\n\n> Wheels are meant to contain exactly what is to be installed, and nothing more. In particular, wheels should never include tests and documentation, while sdists commonly do.\n\n### Steps/Code to Reproduce\n\nDownload the current wheels and look for `sklearn/datasets/tests`\n\n### Expected Results\n\nThe directory is absent.\n\n### Actual Results\n\nThe directory exists.\n\n### Versions\n\n```shell\n1.6.1\n```",
      "labels": [
        "Needs Decision"
      ],
      "state": "closed",
      "created_at": "2025-05-19T13:55:41Z",
      "updated_at": "2025-06-04T13:35:14Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31391"
    },
    {
      "number": 31390,
      "title": "Contains code not allowed for commercial use",
      "body": "### Describe the bug\n\nhttps://github.com/scikit-learn/scikit-learn/blob/ff6bf36f06ca80bf505f37a8c5c42047129952ec/sklearn/datasets/_samples_generator.py#L1900 refers to code at https://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch6/lle.py, which contains the following notice (emphasis mine):\n\n> You are free to use, change, or redistribute the code in any way you wish for **non-commercial purposes**, but please maintain the name of the original author. This code comes with no warranty of any kind.\n\nThis might be problematic for anyone using *scikit-learn* in a commercial context.\n\n### Steps/Code to Reproduce\n\nNot required.\n\n### Expected Results\n\nThe code does not restrict commercial usage hidden deeply inside the code or external references.\n\n### Actual Results\n\nThe code restricts commercial usage hidden deeply inside an external reference.\n\n### Versions\n\n```shell\n1.6.1 and main\n```",
      "labels": [
        "Needs Investigation"
      ],
      "state": "closed",
      "created_at": "2025-05-19T13:26:08Z",
      "updated_at": "2025-06-26T10:03:52Z",
      "comments": 10,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31390"
    },
    {
      "number": 31389,
      "title": "Incomplete cleanup of Boston dataset",
      "body": "### Describe the bug\n\nIn #24603, the Boston dataset has been removed. Nevertheless, the corresponding dataset apparently is still being distributed with the package: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/data/boston_house_prices.csv This does not look correct.\n\n### Steps/Code to Reproduce\n\nNot required.\n\n### Expected Results\n\nThe corresponding data file is removed as well.\n\n### Actual Results\n\nThe corresponding data file is still distributed.\n\n### Versions\n\n```shell\n1.6.1 and `main`.\n```",
      "labels": [
        "good first issue",
        "help wanted"
      ],
      "state": "closed",
      "created_at": "2025-05-19T12:32:09Z",
      "updated_at": "2025-05-20T12:45:17Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31389"
    },
    {
      "number": 31382,
      "title": "ENH assert statement using AssertionError for `_agglomerative.py` file",
      "body": "### Describe the workflow you want to enable\n\nAccording to the [Bandit Developers document](https://bandit.readthedocs.io/en/latest/plugins/b101_assert_used.html#module-bandit.plugins.asserts), assert is removed with compiling to optimised byte code (python -O producing *.opt-1.pyc files). This caused various protections to be removed. Consider raising a semantically meaningful error or AssertionError instead.\nAs `_agglomerative.py` has the assert keyword, I would like to update the assert statement.\n\n### Describe your proposed solution\n\nMy proposed solution is to use AssertionError instead of assert.\nCurrent (Line 616):\n`assert n_clusters <= n_samples`\n\nProposal:\n`if not (n_clusters <= n_samples):`\n            `raise AssertionError`\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nIf you accept my offer, I will make a PR.",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-19T04:34:29Z",
      "updated_at": "2025-05-20T09:45:22Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31382"
    },
    {
      "number": 31377,
      "title": "⚠️ CI failed on Wheel builder (last failure: May 18, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15092078672)** (May 18, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-18T04:40:56Z",
      "updated_at": "2025-05-19T04:39:43Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31377"
    },
    {
      "number": 31374,
      "title": "Suggested fix: GaussianProcessRegressor.predict wastes significant time when both `return_std` and `return_cov` are `False`",
      "body": "### Describe the workflow you want to enable\n\nhttps://github.com/scikit-learn/scikit-learn/commit/7b715111bff01e836fcd3413851381c6a1057ca4 moved duplicated code above the conditional statements, but this means that an expensive step for computing GPR variances is executed even if both `return_std` and `return_cov` are `False`. Profiling shows this takes ~96% of the computation time. I would like to see the `y_mean` value returned before this step to save time.\n\n### Describe your proposed solution\n\nAbove `V = solve_triangular` https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py#L454, add\n\n```\nif not return_std and not return_cov:\n    return y_mean\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "Easy"
      ],
      "state": "closed",
      "created_at": "2025-05-16T19:39:14Z",
      "updated_at": "2025-07-01T11:44:59Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31374"
    },
    {
      "number": 31373,
      "title": "SimpleImputer converts `int32[pyarrow]` extension array to `float64`, subsequently crashing with numpy `int32` values",
      "body": "### Describe the bug\n\nWhen using the `SimpleImputer` with a pyarrow-backed pandas DataFrame, any float/integer data is converted to `None`/`float64` instead.\nThis causes the imputer to be fitted to `float64`, crashing on a dtype assertion when passing it a numpy-backed `int32` DataFrame after fitting.\n\nThe flow is the following:\n1. The imputer calls `_validate_input`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L319\n2. This calls `validate_data`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L344-L353\n3. This calls `check_array`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L2951-L2952\n4. Our input is a pandas dataframe:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L909\n5. This now checks if the dtypes need to be converted:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L925-L927\n6. Our input is backed by an extension array _and_ `int32[pyarrow]` is an integer datatype, so we return `True` here:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L714-L724\n7. Finally we pass the \"needs conversion\" check and convert the dataframe to `dtype` (which is `None` here, which apparently means `float64`):\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L966-L971\n\n### Steps/Code to Reproduce\n\n```py\nimport polars as pl\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\"a\": [10]}, schema={\"a\": pl.Int32}).to_pandas(use_pyarrow_extension_array=False))\n      ._f...",
      "labels": [
        "Bug",
        "module:impute"
      ],
      "state": "open",
      "created_at": "2025-05-16T16:30:30Z",
      "updated_at": "2025-07-09T16:31:19Z",
      "comments": 12,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31373"
    },
    {
      "number": 31368,
      "title": "`_weighted_percentile` NaN handling with array API",
      "body": "There isn't *necessarily* anything to fix here, but I thought it would be useful to open this for documentation, at least.\n\n---\n\n`_weighted_percentile` added support for NaN in #29034 and support for array APIs in #29431.\n\nOur implementation relys on `sort` putting NaN values at the end:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/8cfc72b81f7f19a03b5316440efc7d6bebd3c27c/sklearn/utils/stats.py#L70-L74\n\nAFAICT (confirmed by @ev-br) array API specs do not specify how `sort` should handle NaN, which means it is left to individual packages to determine.\n\n* torch seems to follow numpy and sort NaN to the end (tested manually with `float('nan')` and `torch.nan`) but this is not mentioned in the [docs](https://docs.pytorch.org/docs/stable/generated/torch.sort.html). There is some discussion of ordering NaN as the largest value here: https://github.com/pytorch/pytorch/issues/46544#issuecomment-883356705 and a related issue about negative NaN here: https://github.com/pytorch/pytorch/issues/116567\n* CuPy seems to follow numpy behaviour as well (relevant issues: https://github.com/cupy/cupy/issues/3324, and they seem to have [tests](https://github.com/cupy/cupy/blob/66820586ee1c41013868a8de4977c84f29180bc8/tests/cupy_tests/sorting_tests/test_sort.py#L161) to check that their results are the same as numpy with nan sorting )\n\nAs everything works, I don't think we need to do anything here (especially as we ultimately want to drop maintaining our own quantile function), but just thought it would be useful to document.\n\ncc @StefanieSenger @ogrisel",
      "labels": [
        "Array API"
      ],
      "state": "open",
      "created_at": "2025-05-15T12:10:19Z",
      "updated_at": "2025-09-11T14:05:15Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31368"
    },
    {
      "number": 31367,
      "title": "Inconsistent `median`/`quantile` behaviour now `_weighted_percentile` ignores NaNs",
      "body": "As of https://github.com/scikit-learn/scikit-learn/pull/29034, `_weighted_percentile` handles NaNs by ignoring them when calculating `percentile`.\n`np.median` and `np.percentile` on the other hand, will return NaN if a NaN is present in the input (`np.nanmedian` and `np.nanpercentile` will ignore nans).\n\nThere are many cases in the codebase where, if `sample_weight` is `None`, a `np` function is used (NaN returned), if `sample_weight` is given, `_weighted_percentile` used and NaNs ignored.\n\nSummary of affected cases:\n\n* `DummyRegressor.fit`\n* `AbsoluteError`/`PinballLoss`/`HuberLoss`  - `fit_intercept_only` method\n* `median_absolute_error`\n* `d2_pinball_score`\n* `SplineTransformer._get_base_knot_positions` - I think this was the original reason for https://github.com/scikit-learn/scikit-learn/pull/29034\n\nMaybe we could assess on a case by case basis whether it makes sense to return NaN if present in the input? @ogrisel suggested that we may want to raise a warning in some cases as well.\n\ncc @StefanieSenger",
      "labels": [
        "module:utils"
      ],
      "state": "closed",
      "created_at": "2025-05-15T11:32:37Z",
      "updated_at": "2025-05-21T08:40:04Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31367"
    },
    {
      "number": 31366,
      "title": "Gaussian Process Log Likelihood Gradient Incorrect",
      "body": "### Describe the bug\n\nThe gradient function of in the GaussianProcessRegressor Class is incorrect. This leads to inefficiencies fitting kernel (hyper) parameters.\nThe root of the issue is in that the gradient of the kernel function is made with respect to the log of the kernel parameter.\n\nSee the plot on the right showcasing the incorrect gradient currently being used in the optimization step:\n\n![Image](https://github.com/user-attachments/assets/ec7f5582-b928-4738-9371-02ad1391c7c4)\n\n\nTo fix this apply the chain rule giving:\n\n$\\frac{\\partial k(x)}{\\partial \\ln(x)} \\frac{\\partial \\ln(x)}{\\partial x} = \\frac{\\partial k(x)}{\\partial x}$\n$\\frac{\\partial k(x)}{\\partial \\ln(x)} \\frac{1}{x} = \\frac{\\partial k(x)}{\\partial x}$\n\nWhen implementing this the correct gradient is given (middle plot).\n\n\nTO REPRODUCE THIS BUG - see [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](https://github.com/conradstevens/scikit-learn/blob/gpr-log-likelihood-check/sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py)\n\nI am happy to fix this bug by modifying the `_gpr` class's `log_marginal_likelihood` function. Keeping in mind things may get trickier when kernel functions have multiple hyper parameters (likely resulting in multiple iterations of the chain rule). \n\n### Steps/Code to Reproduce\n\nTO REPRODUCE THIS BUG - see [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](https://github.com/conradstevens/scikit-learn/blob/gpr-log-likelihood-check/sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py)\n\n### Expected Results\n\nCRRECTION AND GROUND TRUTH TANGENTS CAN BE FOUND IN [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](h...",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-05-15T07:48:10Z",
      "updated_at": "2025-07-15T16:56:32Z",
      "comments": 14,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31366"
    },
    {
      "number": 31365,
      "title": "TargetEncoder example code",
      "body": "### Describe the issue linked to the documentation\n\nThe example used in the [stable TargetEncoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) is confusing as the order of the label (that is: dog, cat, snake) is not coherent with the expected order of `enc_low_smooth.encodings_` (the 80 corresponds to 'dog' but is is in second order not first). \n\nPrinting `TargetEncoder.categories_` reveal that the order is indeed coherent with `TargetEncoder.encodings_`. However, as I was trying to understand where this difference of order came from, I wasn't able to find in [TargetEncoder class definition](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/preprocessing/_target_encoder.py) where `self.categories_` was set.  \n\n### Suggest a potential alternative/fix\n\n- make it more explicit in documentation, such as adding a print of `enc_auto.categories_`\n- make `TargetEncoder()` preserve the columns order found in the dataset",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-14T22:11:18Z",
      "updated_at": "2025-05-15T06:03:08Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31365"
    },
    {
      "number": 31364,
      "title": "Tfidf no genera los cluster correctos para oraciones con poco significado y palabras repetidas",
      "body": "### Describe the workflow you want to enable\n\nDado el sigueinte csv:\ntexto,categoria\n\"el gato el gato el gato el gato el gato\",\"gato\"\n\"el perro el perro el perro el perro el perro\",\"perro\"\n\"la casa la casa la casa la casa la casa\",\"casa\"\n\"el avión el avión el avión el avión el avión\",\"avión\"\n\"la playa la playa la playa la playa la playa\",\"playa\"\n\"el gato el perro el gato el perro el gato\",\"mezcla\"\n\"el perro el gato el perro el gato el perro\",\"mezcla\"\n\"la playa la casa la playa la casa la playa\",\"mezcla\"\nAl usar tfidf con stop words y ngramas el cluster de la ultima oracion de nuestro csv no lo agrupa en el cluster correcto, que en este caso deberia estar con la oracion 6 y 7\n\n### Describe your proposed solution\n\nPodemos mencionar las limitaciones con textos repetidos en la documentacion o mejorar los calculos para poder manejar textos con poco significado semantico y palabras repetidas.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-14T20:10:20Z",
      "updated_at": "2025-05-20T07:48:26Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31364"
    },
    {
      "number": 31360,
      "title": "Describe `set_{method}_request()` API, expose `_MetadataRequester`, or expose `_BaseScorer`",
      "body": "### Describe the issue linked to the documentation\n\nTL;DR: Metadata routing for scoring could either use a base class or documentation of how to write `set_score_request()`.\n\nCurrently the [Metadata Estimator Dev Guide](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#metadata-routing) has examples of a metadata-consuming estimator and a metadata-routing estimator.  However, the metadata routing is also designed for scorers and CV splitters which may or may not be estimators.   Fortunately, `sklearn.model_selection` exposes `BaseCrossValidator`, which like `BaseEstimator`, subclasses `_MetadataRequester`.  Unfortunately, ~there's no base class for scorers.~ the base class for scorers, `_BaseScorer`, is not public.\n\nI don't understand how to string together the relevant methods that should be a part of `set_score_params`, The current workaround is to simply subclass `BaseEstimator`, even if I'm not making an estimator, or to subclass `_MetadataRequester`, even though its not part of the public API.  ~Or use `make_scorer` to pin the kwargs when instantiating the meta-estimator, rather than in `fit()`~\n\nMy use case is for scoring a time-series model where the data generating mechanism is known to the experiment, but not the model, and I need to compare the fit model to the true data generating mechanism.  I understand how to use a custom scorer in `RandomizedSearchCV`, and the [metadata API](https://scikit-learn.org/stable/metadata_routing.html#api-interface) explains how meta estimators like `RandomizedSearchCV` can pass additional arguments to my custom scorer.\n\n### Suggest a potential alternative/fix\n\n* publicly exposing `_MetadataRequester`\n* publicly exposing `_BaseScorer`\n* Document how to write the `set_{method}_request()` methods.  It looks like `_MetadataRequester` uses a descriptor `RequestMethod`, which relies on an instance having a `_get_metadata_request` method and a `_metadata_request` attribute (which it doesn't rea...",
      "labels": [
        "Documentation",
        "Needs Investigation",
        "Metadata Routing"
      ],
      "state": "open",
      "created_at": "2025-05-13T10:14:39Z",
      "updated_at": "2025-06-01T10:36:29Z",
      "comments": 15,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31360"
    },
    {
      "number": 31359,
      "title": "Documentation improvement for macOS Homebrew libomp installation",
      "body": "### Describe the issue linked to the documentation\n\nThe current documentation in `doc/developers/advanced_installation.rst` under the \"macOS compilers from Homebrew\" section provides environment variable examples using the path `/usr/local/opt/libomp/`. While this is correct for Intel-based Macs, Homebrew on Apple Silicon (arm64) Macs installs packages, including `libomp`, to `/opt/homebrew/opt/libomp/`.\n\nThis can lead to confusion and build issues for users on Apple Silicon hardware who follow the documentation to install from source.\n\nThe documentation will improve from mentioning that `libomp` is often installed as \"keg-only\" by Homebrew, which is why explicitly setting these paths is necessary. Homebrew's own output (`brew info libomp`) often provides guidance on the necessary `CPPFLAGS` and `LDFLAGS`.\n\n\n### Suggest a potential alternative/fix\n\nThe documentation could be updated to:\n1.  Mention the different Homebrew base paths for Intel (`/usr/local`) and Apple Silicon (`/opt/homebrew`).\n2.  Update the example environment variable settings to reflect the `/opt/homebrew/opt/libomp` path as a common case for Apple Silicon, or provide instructions for users to identify and use the correct path for their system.\n3.  Optionally, We could briefly explain the \"keg-only\" nature of `libomp` from Homebrew and how it relates to needing these environment variables.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-13T09:59:24Z",
      "updated_at": "2025-08-13T10:10:23Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31359"
    },
    {
      "number": 31356,
      "title": "Benchmark Function",
      "body": "### Describe the workflow you want to enable\n\nI would like to define multiple pipelines and compare them against each other on multiple datasets.\n\n### Describe your proposed solution\n\nA single helper function that executes this benchmark fully in parallel. This would allow \n\n### Describe alternatives you've considered, if relevant\n\nThere is an [MLR3 function](https://mlr3.mlr-org.com/reference/benchmark.html) that inspired this issue. \n\n### Additional context\n\nReasoning: I'm currently co-teaching a course where students can do the exercises in R using MLR3 or Python using scikit-learn. Doing the exercises in R appears to be less repetitive overall, as for example, there is a simple function for benchmarking. Also, it would require less time to actually wait for the results to finish as one could make more use of parallelism.",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-12T08:32:05Z",
      "updated_at": "2025-05-12T10:32:47Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31356"
    },
    {
      "number": 31350,
      "title": "SimpleImputer casts `category` into `object` when using \"most_frequent\" strategy",
      "body": "### Describe the bug\n\nThe column `dtype` changes from `category` to `object` when I transform it using `SimpleImputer`.\n\nHere is a list of related Issues and PRs that I found while trying to solve this problem:\n#29381 \n#18860\n#17625 \n#17526\n#17525\n\nIf this is truly a bug, I would like to work on a fix.\n\n### Steps/Code to Reproduce\n\n```python\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\ndf = pd.DataFrame(data=['A', 'B', 'C', 'A', pd.NA], columns=['column_1'], dtype='category')\n\ndf.info()\n\nimputer = SimpleImputer(missing_values=pd.NA, strategy=\"most_frequent\").set_output(transform='pandas')\n\noutput = imputer.fit_transform(df)\n\noutput.info()\n```\n\n### Expected Results\n\nThis is the output I expected to see on the terminal\n```\n> > > df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   column_1  4 non-null      category\ndtypes: category(1)\nmemory usage: 269.0 bytes\n\n>>> output.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   column_1  5 non-null      category\ndtypes: object(1)\nmemory usage: 172.0+ bytes\n```\n\nI expected `output` to keep the same `dtype` as the original `pd.DataFrame`.\n\n### Actual Results\n\nThe actual results for when `output.info()` is called is:\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   column_1  5 non-null      object\ndtypes: object(1)\nmemory usage: 172.0+ bytes\n```\nObserve that the `Dtype` for `column_1` is now object instead of category.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0]\nexecutable: /home/user/miniconda3/envs/prod/bin/python\n   mach...",
      "labels": [
        "Bug",
        "API",
        "Needs Decision",
        "module:impute"
      ],
      "state": "open",
      "created_at": "2025-05-11T03:55:11Z",
      "updated_at": "2025-09-11T00:07:50Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31350"
    },
    {
      "number": 31349,
      "title": "Add Multiple Kernel Learning (MKL) for Support Vector Machines (SVM)",
      "body": "### Describe the workflow you want to enable\n\nI propose adding a [Multiple Kernel Learning (MKL)](https://en.wikipedia.org/wiki/Multiple_kernel_learning) module for kernel optimization in kernel-based methods (such as SVM) to scikit-learn. MKL is a more advanced approach compared to GridSearchCV, offering a way to combine multiple kernels into a single, optimal kernel. In the worst case, MKL will behave like GridSearchCV by assigning a weight of 1 to the best kernel, but in the other cases, it will provide a weighted combination of kernels for better generalization.\n\n### Describe your proposed solution\n\nI have already implemented a complete MKL solution for regression, binary and multi-class classification, and clustering (One-Class). This implementation includes the [SimpleMKL algorithm](https://www.jmlr.org/papers/volume9/rakotomamonjy08a/rakotomamonjy08a.pdf), which optimizes the weights of the kernels, as well as the AverageMKL (simply averages the kernels) and SumMKL (simply sums the kernels) algorithms. This implementation is available on a [previously closed pull request](https://github.com/scikit-learn/scikit-learn/pull/31166).\n\n### Describe alternatives you've considered, if relevant\n\nAn alternative would be to continue relying on GridSearchCV for kernel selection. However, GridSearchCV is limited to selecting only one kernel and does not consider the possibility of combining multiple kernels, which can result in suboptimal performance. MKL provides a more sophisticated approach by optimizing kernel weights, leading to better performance in many machine learning tasks.",
      "labels": [
        "New Feature",
        "Needs Decision"
      ],
      "state": "open",
      "created_at": "2025-05-10T08:24:28Z",
      "updated_at": "2025-05-15T16:53:23Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31349"
    },
    {
      "number": 31348,
      "title": "⚠️ CI failed on Wheel builder (last failure: May 10, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14941597365)** (May 10, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-10T04:32:09Z",
      "updated_at": "2025-05-11T04:38:40Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31348"
    },
    {
      "number": 31344,
      "title": "Add MultiHorizonTimeSeriesSplit for Multi-Horizon Time Series Cross-Validation",
      "body": "### Describe the workflow you want to enable\n\nThe current `TimeSeriesSplit` in scikit-learn supports cross-validation for time series data with a single prediction horizon per split, which limits its use for scenarios requiring forecasts over multiple future steps (e.g., predicting 1, 3, and 5 days ahead). I propose adding a new class, `MultiHorizonTimeSeriesSplit`, to enable cross-validation with multiple prediction horizons in a single split.\n\nThis would allow users to:\n- Specify a list of horizons (e.g., `[1, 3, 5]`) to generate train-test splits where the test set includes indices for multiple future steps.\n- Evaluate time series models for short, medium, and long-term forecasts simultaneously.\n- Simplify workflows for applications like demand forecasting, financial modeling, or weather prediction, avoiding manual splitting.\n\nExample usage with daily temperatures:\n```\nfrom sklearn.model_selection import MultiHorizonTimeSeriesSplit\nimport numpy as np\n\n# Daily temperatures for 10 days (in °C)\nX = np.array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\ncv = MultiHorizonTimeSeriesSplit(n_splits=2, horizons=[1, 2])\nfor train_idx, test_idx in cv.split(X):\n    print(f\"Train indices: {train_idx}, Test indices: {test_idx}\")\n```\nExpected output:\n```\nTrain indices: [0 1 2 3 4], Test indices: [5 6]\nTrain indices: [0 1 2 3 4 5 6], Test indices: [7 8]\n```\n\n### Describe your proposed solution\n\nI propose implementing a new class, `MultiHorizonTimeSeriesSplit`, inheriting from `TimeSeriesSplit`. The class will:\n- Add a `horizons` parameter (list of integers) to specify prediction steps.\n- Modify the `split` method to generate test indices for each horizon while preserving temporal order.\n- Include input validation to ensure valid horizons and splits.\n\nTo ensure the correctness of MultiHorizonTimeSeriesSplit, we will develop unit tests covering various configurations and edge cases. For benchmarking, we will assess the computational efficiency and correctness of the new class compared...",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-05-09T14:36:35Z",
      "updated_at": "2025-06-15T15:57:33Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31344"
    },
    {
      "number": 31334,
      "title": "Title: Clarify misleading threshold implication in \"ROC with Cross-Validation\" example",
      "body": "### Describe the issue linked to the documentation\n\nLocation of the issue:\nThe example titled \"Receiver Operating Characteristic (ROC) with cross validation\" [(link)](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html) can lead to misunderstanding regarding decision threshold selection.\n\n🔍 Description of the problem\nThe example uses RocCurveDisplay.from_estimator() to plot ROC curves for each test fold in cross-validation,\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[test],                           the test set is used here instead of train \n        y[test],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\n**here is no warning or clarification that:**\n\n1)Users should not select thresholds based on predictions from these test folds.\n\n2)Even for ROC visualization, using predictions from training folds (via cross_val_predict) avoids potential bias and better simulates threshold tuning workflows.\n\nWithout this guidance, users may mistakenly tune thresholds by inspecting ROC curves on test sets — leading to data leakage and over-optimistic results.\n\n✅ Proposed solution\nreplace the test set with the train set in this code\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[train],                           train set is used here\n        y[train],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(me...",
      "labels": [
        "Documentation",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-07T14:46:24Z",
      "updated_at": "2025-05-07T15:31:40Z",
      "comments": 0,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31334"
    },
    {
      "number": 31327,
      "title": "⚠️ CI failed on Wheel builder (last failure: May 07, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14874735765)** (May 07, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-07T04:44:27Z",
      "updated_at": "2025-05-07T10:13:26Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31327"
    },
    {
      "number": 31326,
      "title": "⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 07, 2025) ⚠️",
      "body": "**CI failed on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76323&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 07, 2025)\nUnable to find junit file. Please see link for details.",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-07T02:33:31Z",
      "updated_at": "2025-05-08T08:15:43Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31326"
    },
    {
      "number": 31323,
      "title": "Add train_validation_test_split for three-way dataset splits",
      "body": "### Describe the workflow you want to enable\n\nEnable the user to divide the dataset into 3 parts (train, validation and test) instead of only two (train and test) using only one method. This would present a more elegant solution than using the method train_test_split twice.\n\n```python \nfrom sklearn.model_selection import train_val_test_split\nX_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n    X, y,\n    train_size=0.6,\n    val_size=0.2,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True,\n    stratify=y\n)\n```\n\n### Describe your proposed solution\n\nAdd a new method called train_test_validation_split where the dataset is divided into train, validation and test set. The arguments would be the same as the train_test_split method with the additional  val_size, similar to test_size and train_size but for the validation set.\n\n### Describe alternatives you've considered, if relevant\n\nUsing train_test_split twice works, but having a dedicated train_validation_test_split function would be cleaner and more concise.\n\n### Additional context\n\nUsing a validation set helps avoiding both overfitting aswell as underfitting.",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-06T15:07:03Z",
      "updated_at": "2025-05-07T08:56:30Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31323"
    },
    {
      "number": 31319,
      "title": "Argument order in haversine_distances for latitude/longitude",
      "body": "### Describe the issue linked to the documentation\n\nHello! I frequently use [sklearn.metrics.pairwise.haversine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html) to estimate distances on the globe. The example on the linked page uses a geographic example, but it does not specify whether geographic coordinates are in (latitude, longitude) or (longitude, latitude) form. From context, one can infer that the correct order is (latitude,longitude); however, it would be useful to explicitly state the order.\n\nThis is my first issue submission; please let me know if there is something more that might be useful for resolution!\n\n### Suggest a potential alternative/fix\n\nThere are two ways that this could be resolved:\n1. Include a short note stating that geographic coordinates should be input as (latitude,longitude)\n2. Include a comment in the example code describing the coordinate order.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-06T11:56:26Z",
      "updated_at": "2025-05-07T11:39:33Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31319"
    },
    {
      "number": 31318,
      "title": "`ValueError` raised by `FeatureUnion._set_output` with `FunctionTransform` that outputs a pandas `Series` in scikit-learn version 1.6",
      "body": "### Describe the bug\n\nHello,\n\nI'm currently working with scikit-learn version 1.6, and I encountered a regression that wasn't present in version 1.4.\n\nThe following minimal code computes two features — the cumulative mean of age and weight grouped by id. Each transformation function returns a pandas.Series:\n\n\n\nWhen I run this code with scikit-learn 1.6, I get the following error:\n\nAfter investigation, I found that the issue occurs because each transformer returns a Series, not a DataFrame. If I update the functions to return DataFrame objects instead, the error disappears.\n\nInterestingly, in scikit-learn 1.4, the same code works correctly even when the functions return Series.\n\n\nDo you have any explanation for why this changed between version 1.4 and 1.6 ?\n\nThanks in advance for your help!\n\n### Steps/Code to Reproduce\n\n\n```python\nimport pandas as pd\nfrom sklearn.pipeline import FunctionTransformer, FeatureUnion\nimport numpy as np\n\ndef compute_cumulative_mean_age(df: pd.DataFrame) -> pd.Series:\n    return (\n        df[\"age\"]\n        .astype(float)\n        .groupby(df[\"id\"])\n        .expanding()\n        .mean()\n        .droplevel(level=\"id\")\n        .reindex(df.index)\n        .rename(\"cumulative_mean_age\")\n    )\n\ndef compute_cumulative_mean_weight(df: pd.DataFrame) -> pd.Series:\n    return (\n        df[\"poids\"]\n        .astype(float)\n        .groupby(df[\"id\"])\n        .expanding()\n        .mean()\n        .droplevel(level=\"id\")\n        .reindex(df.index)\n        .rename(\"cumulative_mean_weight\")\n    )\n\ndef compute_features(df: pd.DataFrame) -> pd.DataFrame:\n    feature_union = FeatureUnion(\n        [\n            (\"cumulative_mean_age\", FunctionTransformer(compute_cumulative_mean_age)),\n            (\"cumulative_mean_weight\", FunctionTransformer(compute_cumulative_mean_weight))\n        ]\n    ).set_output(transform=\"pandas\")\n\n    return feature_union.fit_transform(X=df).astype(float)\n\ndef transform(df: pd.DataFrame) -> pd.DataFrame:\n    return compute_features(df)\n\nif __n...",
      "labels": [
        "Bug",
        "good first issue"
      ],
      "state": "closed",
      "created_at": "2025-05-06T11:44:35Z",
      "updated_at": "2025-07-19T21:40:46Z",
      "comments": 13,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31318"
    },
    {
      "number": 31315,
      "title": "SGDRegressor is not inheriting from LinearModel",
      "body": "### Describe the bug\n\nI wanted to rely on the base class [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267) to identify linear models, but I found out that [SGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1757) (nor any of its sub classes) is not inheriting this class. However, SGDClassifier is (through LinearClassifierMixin).\n\nIs there any reason for [BaseSGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1383) to not inherit [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267)? Is it because it overloads all of LinearModel's methods?\n\n### Steps/Code to Reproduce\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n\n### Expected Results\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n# True\n\n### Actual Results\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n# False\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]\nexecutable: /usr/local/bin/python3.10\n   machine: macOS-14.4.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.5.0\n          pip: 24.2\n   setuptools: 74.0.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.12\n       pandas: 1.5.3\n   matplotlib: 3.8.4\n       joblib: 1.2.0\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n ...",
      "labels": [
        "Enhancement"
      ],
      "state": "open",
      "created_at": "2025-05-06T09:05:13Z",
      "updated_at": "2025-05-11T05:17:58Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31315"
    },
    {
      "number": 31311,
      "title": "Reference CalibrationDisplay from calibration_curve's docstring in a \"See also section\"",
      "body": "### Describe the issue linked to the documentation\n\nEnrich documentation like proposed in #31302 for calibration_curve's\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-05T19:34:17Z",
      "updated_at": "2025-05-06T15:31:50Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31311"
    },
    {
      "number": 31304,
      "title": "DOC Link Visualization tools to their respective interpretation in the User Guide",
      "body": "### Describe the issue linked to the documentation\n\nAs of today, some of our [Display objects](https://scikit-learn.org/dev/visualizations.html#display-objects) point towards the [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) section of the User Guide, some of them point toward the respective plotted function, some of them do both.\n\nAs sometimes users want to know how to interpret the plot and sometimes they want to understand the plot API, we've resorted to linking both, e.g. for the [RocCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.RocCurveDisplay.html) we have:\n\n```\n    For general information regarding `scikit-learn` visualization tools, see\n    the :ref:`Visualization Guide <visualizations>`.\n    For guidance on interpreting these plots, refer to the :ref:`Model\n    Evaluation Guide <roc_metrics>`.\n```\n\nContributors willing to address this issue, please fix **one** of the following listed Display Objects **per pull request**.\n\n- [x] [inspection.PartialDependenceDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) points to [`partial-dependence`](https://scikit-learn.org/dev/modules/partial_dependence.html#partial-dependence). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) as well. #31313\n\n- [x] [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) points only to point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html). It should point to [`confusion-matrix`](https://scikit-learn.org/dev/modules/model_evaluation.html#confusion-matrix) as well. #31306\n\n- [x] [metrics.DetCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.DetCurveDisplay.html) points to [`det-curve`](https://scikit-learn.org/dev/modules/model_evaluation.html#det-curve). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizati...",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-05T15:51:12Z",
      "updated_at": "2025-05-12T08:42:08Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31304"
    },
    {
      "number": 31302,
      "title": "Reference `ValidationCurveDisplay` from `validation_curve`'s docstring in a \"See also section\"",
      "body": "### Describe the issue linked to the documentation\n\nThe docstring of `validation_curve` should point to the `ValidationCurveDisplay.from_estimator` factory method as a complementary tool that both computes the curves points and display them using matplotlib.\n\nIf you want to open a PR for this, please review some \"See also\" sections in other sections using `git grep` or the search feature of your IDE or github code search:\n\nhttps://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+%22See+also%22+language%3APython+path%3A%2F%5Esklearn%5C%2F%2F&type=code",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-05-05T12:50:38Z",
      "updated_at": "2025-06-16T06:56:58Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31302"
    },
    {
      "number": 31290,
      "title": "`_safe_indexing` triggers `SettingWithCopyWarning` when used with `slice`",
      "body": "### Describe the bug\n\nHere's something I noticed while looking into https://github.com/scikit-learn/scikit-learn/pull/31127\n\nThe test\n```\npytest sklearn/utils/tests/test_indexing.py::test_safe_indexing_pandas_no_settingwithcopy_warning\n```\nchecks that a copy is produced, and that no `SettingWithCopyWarning` is produced\n\nIndeed, no copy is raised, but why is using `_safe_indexing` with a slice allowed to not make a copy? Is this intentional?\n\nBased on responses, I can suggest what to do instead in https://github.com/scikit-learn/scikit-learn/pull/31127\n\n(I am a little surprised that this always makes copies, given that a lot of the discussion in https://github.com/scikit-learn/scikit-learn/issues/28341 centered around wanting to avoid copies)\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\n\nfrom sklearn.utils import _safe_indexing\nimport pandas as pd\n\nX = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [3, 4, 5]})\nsubset = _safe_indexing(X, slice(0, 2), axis=0)\nsubset.iloc[0, 0] = 10\n```\n\n### Expected Results\n\nNo `SettingWithCopyWarning`\n\n### Actual Results\n\n```\n/home/marcogorelli/scikit-learn-dev/t.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset.iloc[0, 0] = 10\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nexecutable: /home/marcogorelli/scikit-learn-dev/.venv/bin/python\n   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 24.2\n   setuptools: None\n        numpy: 2.1.0\n        scipy: 1.14.0\n       Cython: 3.0.11\n       pandas: 2.2.2\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n ...",
      "labels": [
        "Bug"
      ],
      "state": "open",
      "created_at": "2025-05-01T14:17:02Z",
      "updated_at": "2025-06-13T01:01:17Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31290"
    },
    {
      "number": 31288,
      "title": "`make_scorer(needs_sample_weight=True)` wrongly injects `needs_sample_weight` into the scoring function",
      "body": "### Describe the bug\n\n\nWhen using `make_scorer(..., needs_sample_weight=True)`, the generated scorer unexpectedly passes `needs_sample_weight=True` as a keyword argument to the scoring function itself, leading to `TypeError` unless **kwargs is manually added.\n\n\n### Steps/Code to Reproduce\n\n\nMinimal example:\n```\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\ndef weighted_mape(y_true, y_pred, sample_weight=None):\n    return np.average(np.abs((y_true - y_pred) / (y_true + 1e-8)), weights=sample_weight)\n\nscoring = make_scorer(weighted_mape, greater_is_better=False, needs_sample_weight=True)\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=0)\nweights = np.random.rand(100)\n\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(model, param_grid={\"n_estimators\": [10]}, scoring=scoring, cv=3)\ngrid.fit(X, y, sample_weight=weights)\n```\n\n\n\n### Expected Results\n\n**Expected behavior:**\n\n`make_scorer(..., needs_sample_weight=True)` should cause `sample_weight` to be passed during cross-validation scoring.\n\nThe scoring function should not receive `needs_sample_weight=True` as a kwarg.\n\n\n\n\n### Actual Results\n\n**Actual behavior:**\n\nThe scoring function raises:\n\n>TypeError: weighted_mape() got an unexpected keyword argument 'needs_sample_weight'\nunless manually patched with **kwargs.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nexecutable: /home/X/XX/pax_env/bin/python\n   machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.2.2\n          pip: 22.0.2\n   setuptools: 59.6.0\n        numpy: 1.26.0\n        scipy: 1.15.2\n       Cython: None\n       pandas: 1.5.3\n   matplotlib: 3.7.1\n       joblib: 1.2.0\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-05-01T07:52:49Z",
      "updated_at": "2025-05-05T08:33:27Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31288"
    },
    {
      "number": 31286,
      "title": "Clarification of output array type when metrics accept multiclass/multioutput",
      "body": "Clarification of how we should handle array output type when a metric outputs several values (i.e. accepts multiclass or multioutput input).\n\nThe issue was summarised succinctly in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196:\n\n> Not sure what should be the output namespace / device in case we output an array, e.g. roc_auc_score with average=None on multiclass problems...\n\nCurrently all regression/classification metrics that support array API and multiclass or multioutput, all output an array in the same namespace and device as the input (checked code and manually). Summary of these metrics :\n\n### Regression metrics\n\nReturns array in same namespace/device:\n* [explained_variance_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)\n* [r2_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) \n* [mean_absolute_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)\n* [mean_absolute_percentage_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error)\n* [mean_pinball_loss](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_pinball_loss.html#sklearn.metrics.mean_pinball_loss)\n* [mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)\n* [mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error)\n* [root_mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.root_mean_squared_error.html#sklearn.metrics.root_mean_squared_error)\n* [root_mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/s...",
      "labels": [
        "Needs Decision",
        "Array API"
      ],
      "state": "closed",
      "created_at": "2025-05-01T05:28:13Z",
      "updated_at": "2025-06-26T14:15:37Z",
      "comments": 33,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31286"
    },
    {
      "number": 31284,
      "title": "⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: May 05, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (May 05, 2025)\n- Test Collection Failure",
      "labels": [
        "Bug",
        "cython"
      ],
      "state": "closed",
      "created_at": "2025-05-01T02:52:32Z",
      "updated_at": "2025-05-05T12:54:29Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31284"
    },
    {
      "number": 31283,
      "title": "⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 05, 2025) ⚠️",
      "body": "**CI is still failing on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 05, 2025)\n- Test Collection Failure",
      "labels": [
        "Build / CI",
        "cython"
      ],
      "state": "closed",
      "created_at": "2025-05-01T02:51:48Z",
      "updated_at": "2025-05-05T12:55:15Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31283"
    },
    {
      "number": 31274,
      "title": "Automatically move `y_true` to the same device and namespace as `y_pred` for metrics",
      "body": "This is closely linked to #28668 but separate enough to warrant it's own issue (https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2814771519). This is mostly a summary of discussions so far. If we are happy with a decision, we can move to updating the documentation.\n\n---\n\nFor classification metrics to support array API, there is a problem in the case where `y_pred` is not in the same namespace/device as `y_true`.\n\n`y_pred` is likely to be the output of `predict_proba` or `decision_function` and would be in the same namespace/device as `X` (if we decide in #28668 that \"everything should follow X\").\n`y_true` could be an integer array or a numpy array or pandas series (this is pertinent as `y_true` may be string labels)\n\nMotivating use case:\n\nUsing e.g., `GridSearchCV` or `cross_validate` with a pipeline that moves `X` to GPU.\nConsider a pipeline like below (copied from https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2154958666): \n\n```python\npipeline = make_pipeline(\n   SomeDataFrameAwareFeatureExtractor(),\n   MoveFeaturesToPyTorch(device=\"cuda\"),\n   SomeArrayAPICapableClassifier(),\n)\n```\n\nPipelines do not ever touch `y` so we are not able to alter `y` within the pipeline.\nWe would need to pass a metric to `GridSearchCV` or `cross_validate`, which would be passed `y_true` and `y_pred` on different namespace / devices.\n\nThus the motivation to automatically move `y_true` to the same namespace / device as `y_pred`, in metrics functions.\n\n(Note another example is discussed in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2531072292)\n\nAs it is more likely that `y_pred` is on GPU, `y_true` follow `y_pred` was slightly preferred over `y_pred` follows `y_true`. Computation wise, CPU vs GPU is probably similar for metrics like log-loss, but for metrics that require sorting (e.g., ROC AUC) GPU may be faster? (see https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196 for more discussion o...",
      "labels": [
        "API",
        "Array API"
      ],
      "state": "open",
      "created_at": "2025-04-30T05:41:14Z",
      "updated_at": "2025-06-04T13:40:06Z",
      "comments": 8,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31274"
    },
    {
      "number": 31269,
      "title": "⚠️ CI failed on Wheel builder (last failure: May 05, 2025) ⚠️",
      "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14828681637)** (May 05, 2025)",
      "labels": [
        "Build / CI"
      ],
      "state": "closed",
      "created_at": "2025-04-29T04:32:02Z",
      "updated_at": "2025-05-05T12:55:34Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31269"
    },
    {
      "number": 31267,
      "title": "Change the default data directory",
      "body": "### Describe the workflow you want to enable\n\nIt's not a good practice to put files directly into the home directory.\n\n### Describe your proposed solution\n\nA more common way is to put them into the standard cache directories recommended by operating systems:\n\n| OS | Path |\n| -- | ---- |\n| Linux | `$XDG_CACHE_HOME` (if the env var presents) or `~/.cache` |\n| macOS | `~/Library/Caches` |\n| Windows | `%LOCALAPPDATA%` (`~/AppData/Local`) |\n\n### Describe alternatives you've considered, if relevant\n\nPut into `~/.cache/scikit-learn` for all operating systems. Though not being standard, it's still better than the home dir.\n\n### Additional context\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.get_data_home.html",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-04-28T21:22:54Z",
      "updated_at": "2025-05-27T21:20:46Z",
      "comments": 3,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31267"
    },
    {
      "number": 31257,
      "title": "⚠️ CI failed on Wheel builder (last failure: Apr 28, 2025) ⚠️",
      "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14699848568)** (Apr 28, 2025)",
      "labels": [
        "Bug",
        "free-threading"
      ],
      "state": "closed",
      "created_at": "2025-04-27T04:31:01Z",
      "updated_at": "2025-04-28T15:05:43Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31257"
    },
    {
      "number": 31256,
      "title": "⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Apr 26, 2025) ⚠️",
      "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75987&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Apr 26, 2025)\n- test_precomputed_nearest_neighbors_filtering[60]",
      "labels": [
        "module:test-suite"
      ],
      "state": "closed",
      "created_at": "2025-04-26T02:50:37Z",
      "updated_at": "2025-04-30T08:45:23Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31256"
    },
    {
      "number": 31248,
      "title": "Hangs in LogisticRegression with high intercept_scaling number",
      "body": "### Describe the bug\n\nWhen using the `LogisticRegression` model with the solver set to `liblinear` and specifying the `intercept_scaling` parameter, the model hangs without any clear reason. The processing time does not increase gradually with the size of the `intercept_scaling` parameter.\n\n### Steps/Code to Reproduce\n\nWhen running on my machine, the code below complete in around 7 seconds.\n```python\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n        intercept_scaling=1.0e+77,\n        solver='liblinear',\n        )\n\nmodel.fit([[0], [5]], [0, 6])\n```\n\nHowever, increasing `intercept_scaling` by just one decimal place causes the model to hang indefinitely:\n```python\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n        intercept_scaling=1.0e+78,\n        solver='liblinear',\n        )\n\nmodel.fit([[0], [5]], [0, 6])\n```\n\n### Expected Results\n\nI expect the code to finish running in a reasonable time.\n\nWhen `intercept_scaling` is set as 1.0e+77, the program finished in around 7 sec.\n\n### Actual Results\n\nI terminated the process by after a day, no error trace was given by the program.\n\n```javascript\nCommand terminated by signal 15\n\tCommand being timed: \"python Aidan2.py\"\n\tUser time (seconds): 94481.23\n\tSystem time (seconds): 12.59\n\tPercent of CPU this job got: 99%\n\tElapsed (wall clock) time (h:mm:ss or m:ss): 26:15:08\n\tAverage shared text size (kbytes): 0\n\tAverage unshared data size (kbytes): 0\n\tAverage stack size (kbytes): 0\n\tAverage total size (kbytes): 0\n\tMaximum resident set size (kbytes): 142780\n\tAverage resident set size (kbytes): 0\n\tMajor (requiring I/O) page faults: 3\n\tMinor (reclaiming a frame) page faults: 25293\n\tVoluntary context switches: 69\n\tInvoluntary context switches: 537213\n\tSwaps: 0\n\tFile system inputs: 256\n\tFile system outputs: 0\n\tSocket messages sent: 0\n\tSocket messages received: 0\n\tSignals delivered: 0\n\tPage size (bytes): 4096\n\tExit status: 0\n```\n\n### Versions\n\n```shell\nSystem:\n    p...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-04-24T18:48:45Z",
      "updated_at": "2025-04-28T09:12:55Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31248"
    },
    {
      "number": 31246,
      "title": "Faster Eigen Decomposition for Isomap & KernelPCA",
      "body": "(disclaimer: this issue and associated PR are part of a student project supervised by @smarie )\n\n### Summary\n\nEigendecomposition is slow when number of samples is large. This impacts decomposition models such as KernelPCA and Isomap. A \"randomized\" eigendecomposition method (from [Halko et al](https://arxiv.org/abs/0909.4061)) [has been introduced for KernelPCA](https://scikit-learn.org/stable/modules/decomposition.html#choice-of-solver-for-kernel-pca) leveraging Halko's algorithm 4.3 for randomized SVD decomposition (also used in [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca-using-randomized-svd)).\n\nUnfortunately, the current approach is only valid for decomposition of PSD matrices - which suits well for KernelPCA but can not be true in the context of Isomap. Therefore Isomap has not accelerated implementation as of today.\n\nWe propose to introduce an additional approximate eigendecomposition method based on algorithm 5.3 from the same paper.\nThis method should offer a faster alternative to existing solvers (arpack, dense, etc.) while maintaining accuracy, and as opposed to randomized svd, is suitable to find eigenvalues for non-PSD matrices.\n\n### Describe your proposed solution\n\n- Implement `_randomized_eigsh(selection='value')`, that is left as [NotImplemented](https://github.com/scikit-learn/scikit-learn/pull/12069) today.\n- Integrate it as an alternate solver in `Isomap` and in `KernelPCA`.\n- Add tests comparing performance with existing solvers.\n- Provide benchmarks to evaluate speedup and accuracy.\n\n\n### Motivation\n\n- Improves scalability for large datasets.\n- Reduces computation time for eigen decomposition-based methods.\n\nNote: this solution could be used to accelerate all models relying on eigenvalue decomposition, including possibly https://github.com/scikit-learn/scikit-learn/pull/22330",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-04-24T17:05:03Z",
      "updated_at": "2025-04-28T12:09:18Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31246"
    },
    {
      "number": 31245,
      "title": "GradientBoostingClassifier does not have out-of-bag (OOB) score",
      "body": "### Describe the bug\n\nHi, the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) for Gradient boosting Classifier says that there is an out-of-bag score that can be retrieved by the `oob_score_` attribute. However, this attribute doesn't seem to exist in the latest version.\n\n\n\n### Steps/Code to Reproduce\n\nCopy-and-paste code to reproduce this:\n\n```python\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\nXs = np.random.randn(100, 10)\nys = np.random.randint(0, 2, 100)\n\ngbc = GradientBoostingClassifier()\ngbc.fit(Xs, ys)\ngbc.oob_score_\n```\n\n### Expected Results\n\nNo error is thrown. OOB score should be a float\n\n### Actual Results\n\n```\n$ conda create -n sklearn-env -c conda-forge scikit-learn\n$ conda activate sklearn-env\n(sklearn-env) $ python\nPython 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:30) [Clang 18.1.8 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sklearn; sklearn.__version__\n'1.6.1'\n>>> from sklearn.ensemble import GradientBoostingClassifier\n... import numpy as np\n...\n... Xs = np.random.randn(100, 10)\n... ys = np.random.randint(0, 2, 100)\n...\n... gbc = GradientBoostingClassifier()\n... gbc.fit(Xs, ys)\n... gbc.oob_score_\nTraceback (most recent call last):\n  File \"<python-input-0>\", line 9, in <module>\n    gbc.oob_score_\nAttributeError: 'GradientBoostingClassifier' object has no attribute 'oob_score_'\n```\n\n### Versions\n\n```shell\n>>> import sklearn; sklearn.show_versions()\n\nSystem:\n    python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:30) [Clang 18.1.8 ]\nexecutable: /Users/longyuxi/miniforge3/envs/sklearn-env/bin/python\n   machine: macOS-15.3.2-arm64-arm-64bit-Mach-O\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 79.0.1\n        numpy: 2.2.5\n        scipy: 1.15.2\n       Cython: None\n       pandas: None\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: ...",
      "labels": [
        "Bug",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-04-24T15:13:52Z",
      "updated_at": "2025-04-24T15:43:40Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31245"
    },
    {
      "number": 31244,
      "title": "Add the baseline corrected accuracy score for (multi-class) classification to sklearn.metrics",
      "body": "### Describe the workflow you want to enable\n\nWould it be possible to add a new score to `sklearn.metrics`, namely the baseline corrected accuracy score (BCAS) ([DOI:10.5281/zenodo.15262049](https://doi.org/10.5281/zenodo.15262049)). The proposed metric quantifies the model improvement w.r.t. the baseline, and represents a direct evaluation of classifier performance. See the proposed code below, which is label agnostic, and is suitable for both binary and multi-class classification.\n\n### Describe your proposed solution\n\n```\nimport numpy as np\n\ndef BCAS(y_true, y_pred):\n    \"\"\"Baseline corrected accuracy score (BCAS).\n\n    Parameters\n    ----------\n    y_true : Ground truth (correct) labels.\n\n    y_pred : Predicted labels.\n\n    Returns\n    -------\n    score : float\n    \"\"\"\n    label, count = np.unique(y_true, return_counts=True)\n    most_frequent_class = label[np.argmax(count)]\n    y_baseline = np.full(len(y_true), most_frequent_class)\n    as_baseline = np.mean(y_true == y_baseline)\n    as_predicted = np.mean(y_true == y_pred)\n    return (as_predicted - as_baseline)\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature"
      ],
      "state": "closed",
      "created_at": "2025-04-24T09:35:42Z",
      "updated_at": "2025-04-25T13:02:39Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31244"
    },
    {
      "number": 31235,
      "title": "MLP Classifier \"Logistic\" activation function providing ~constant prediction probabilities for all inputs when predicting quadratic function",
      "body": "### Describe the bug\n\nRepeatedly the sigmoid activation function produces very similar (multiple dp) outputs for the prediction probabilities, seemingly similar around the average of the predicted value, similar to a linear function. It works when predicting a linear function, but higher order tends to cause issues.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np\n\nnp.random.seed(1)\nData_X = (np.random.random((500,2)))\nData_Y = np.array([int((x[0] + ((2*(x[1]-0.5))**2  - 0.75))>=0) for x in Data_X])\n\nNN = MLPClassifier(hidden_layer_sizes = (20,20),activation = \"logistic\", random_state = 42)\nNN.fit(Data_X,Data_Y)\nprint(NN.predict(np.array(Data_X[:20])))\nprint(Data_Y[:20])\n```\n\n### Expected Results\n\nThe prediction does not resemble target data \n\n### Actual Results\n\n```\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n[0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\nexecutable: c:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: None\n        numpy: 2.2.3\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-43e11ff0749b8cbe0a615c9cf6737e0e.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: vcomp\n       filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\n        version: Non...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-04-21T16:16:37Z",
      "updated_at": "2025-04-25T14:07:23Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31235"
    },
    {
      "number": 31224,
      "title": "OneVsRestClassifier when all estimators predict a sample belongs to the other classes",
      "body": "### Describe the bug\n\nHello, I stumbled upon quite a funny case by accident.\n\nIn OneVsRestClassifier, each classifier predicts whether a sample belongs to a specific class, or to any of the other class. For instance, if you have 3 classes, you will have 3  binary classifiers:\n\n- the first one says if the sample belongs to class 1 or to one of the two other classes.\n- the second one says if the sample belongs to class 2 or to one of the two other classes.\n- the third one says if the sample belongs to class 3 or to one of the two other classes.\n\nHowever, it creates an edge case where all of the estimators of OneVsRestClassifier predict a specific sample belongs to the other classes:\n\n- calling .predict() will mark the sample as belonging to the last class (which is of course wrong since in our example, the 3rd estimator said the sample did not belong in that class).\n- calling .predict_proba() will return NaNs values.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.datasets import make_classification\n\nimport numpy as np\n\n\nclass MyDumbDumbBinaryClassifier(BaseEstimator, ClassifierMixin):\n    def fit(self, X, y):\n        self.classes_ = set(y)\n        return self\n\n    def predict(self, X):\n        return np.array([0 for _ in range(len(X))])\n\n    def predict_proba(self, X):\n        ones = np.ones((len(X), len(self.classes_)))\n        # the proba of being the positive class is always 0\n        ones[:, 1] = 0\n\n        return ones\n    \n\nclf = OneVsRestClassifier(MyDumbDumbBinaryClassifier())\n\nX, y = make_classification(n_classes=3, n_informative=5)\nclf.fit(X, y)\nclf.predict_proba(X)\n```\n\n### Expected Results\n\nI guess .predict() should return NaNs, and .predict_proba() should return a vector of 0s for that sample.\n\n### Actual Results\n\n```python\n>>> clf.predict(X)\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2,...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-04-18T09:11:32Z",
      "updated_at": "2025-08-15T04:37:22Z",
      "comments": 13,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31224"
    },
    {
      "number": 31223,
      "title": "Support orthogonal polynomial features (via QR decomposition) in `PolynomialFeatures`",
      "body": "### Describe the workflow you want to enable\n\nI want to introduce support for orthogonal polynomial features via QR decomposition in `PolynomialFeatures`, closely mirroring the behavior of R's `poly()` function.\n\nIn regression modeling, using orthogonal polynomials can often lead to improved numerical stability and reduced multi-collinearity among polynomial terms\n\nAs an example of what the difference looks like in R,\n<pre>\n#fits raw polynomial data without an orthogonal basis\nmodel_raw <- lm(y ~ I(x) + I(x^2) + I(x^3), data = data)\n#model_raw <- lm(y ~poly(x,3,raw=TRUE), data = data)\n\n#fits the same degree-3 polynomial using an orthogonal basis\nmodel_poly <- lm(y ~ poly(x, 3), data = data)\n</pre>\n\nThis behavior cannot currently be replicated with `scikit-learn`'s `PolynomialFeatures`, which only produces the raw monomial terms. As a result transitioning from R to Python often leads to discrepancies in model behavior and performance.\n\n\n### Describe your proposed solution\n\nI propose extending `PolynomialFeatures` with a new parameter:\n<pre>\nPolynomialFeatures(..., method=\"raw\")\n</pre>\nAccepted values:\n- `\"raw\"` (default): retains existing behavior, returning standard raw terms\n- `\"qr\"`: applies QR decomposition to each feature to generate orthogonal polynomial features.\n\nBecause R's `poly()` only operates on 1D input vectors, my thought was to apply QR decomposition feature by feature when the input is multi-dimensional. Each column is processed independently, mirroring R's approach.\n\nThis feature would interact with other parameters as follows:\n\n- `include_bias`: When `method=\"qr\"`, The orthogonal polynomial basis inherently includes a transformed first column. However, this column is not a plain column of ones. Therefore, the concept of `include_bias=True` (which appends a column of ones) becomes redundant or misleading in this context. One option is to always set  `include_bias=False` if `method=qr` and always return orthogonal columns only, or raise a warning.\n\n-...",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "closed",
      "created_at": "2025-04-18T04:56:26Z",
      "updated_at": "2025-05-27T19:43:54Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31223"
    },
    {
      "number": 31222,
      "title": "SVC Sigmoid sometimes ROC AUC from predict_proba & decision_function are each other's inverse",
      "body": "### Describe the bug\n\nUncertain if this is a bug or counter-intuitive expected behavior.\n\nUnder certain circumstances the ROC AUC calculated for `SVC` with the `sigmoid` kernel will not agree depending on if you use `predict_proba` or `decision_function`. In fact, they will be nearly `1-other_method_auc`.\n\nThis was noticed when comparing ROC AUC calculated using `roc_auc_score` with predictions from `predict_proba(X)[:, 1]` to using the scorer from `get_scorer('roc_auc')` which appears to be calling `roc_auc_score` with scores from `decision_function`. \n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, get_scorer\nfrom sklearn.model_selection import train_test_split\n\nn_samples = 100\nn_features = 100\nrandom_state = 123\nrng = np.random.default_rng(random_state)\n\nX = rng.normal(loc=0.0, scale=1.0, size=(n_samples, n_features))\ny = rng.integers(0, 2, size=n_samples)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n\nsvc_params = {\n    \"kernel\": \"sigmoid\",\n    \"probability\": True,\n    \"random_state\":random_state,\n}   \npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svc', SVC(**svc_params))\n])  \npipeline.fit(X_train, y_train)\ny_proba = pipeline.predict_proba(X_test)[:, 1]\ny_dec = pipeline.decision_function(X_test)\nroc_auc_proba = roc_auc_score(y_test, y_proba)\nroc_auc_dec = roc_auc_score(y_test, y_dec)\nauc_scorer = get_scorer('roc_auc')\nscorer_auc = auc_scorer(pipeline, X_test, y_test)\n\nprint(f\"AUC (roc_auc_score from predict_proba) = {roc_auc_proba:.4f}\")\nprint(f\"AUC (roc_auc_score from decision_function) = {roc_auc_dec:.4f}\")\nprint(f\"AUC (get_scorer) = {scorer_auc:.4f}\")\n```\n\n### Expected Results\n\nThe measures of ROC AUC agree\n\n### Actual Results\n\n```shell\nAUC (roc_auc_score from predict_proba) = 0.5833\nAUC (roc_auc_score from decision_function) = 0.4295\nAUC ...",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-04-17T20:58:26Z",
      "updated_at": "2025-05-28T11:00:11Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31222"
    },
    {
      "number": 31219,
      "title": "Add Categorical Feature Support to `IterativeImputer`",
      "body": "### Describe the workflow you want to enable\n\nI want to impute missing values in categorical columns using a similar approach to `IterativeImputer`, which currently works only for continuous data. Specifically, I want to enable the following workflow:\n\n- Identify and handle categorical columns in the dataset\n- Use classifier models (e.g., RandomForestClassifier) to impute missing values in categorical columns based on other features\n- Integrate with existing pipelines seamlessly, without needing to separate and impute categorical columns manually\n\n### Describe your proposed solution\n\nExtend the current `IterativeImputer` class (or create a new class, such as `IterativeCategoricalImputer`) to handle categorical data:\n\n- Detect categorical columns automatically (e.g., using `dtype='object'` or `category`) or accept them via a `categorical_features` parameter\n- Encode the categorical variables using an internal encoder (e.g., `LabelEncoder`)\n- Use a classifier model (e.g., `RandomForestClassifier`) instead of a regression model for those columns\n- Predict only the missing values, then inverse transform the predictions back to the original categories\n\nThis would enable more robust and automatic preprocessing for datasets that have numeric and categorical features.\n\n### Describe alternatives you've considered, if relevant\n\n- Manually encoding categorical variables, using a classifier-based imputation strategy outside of `IterativeImputer`\n- Using other libraries like `autoimpute` or `fancyimpute`, which support mixed-type imputation but lack full integration with Scikit-learn pipelines\n- Creating separate imputation steps for categorical and numeric features and merging them later, which adds complexity and can introduce data leakage risks\n\nNone of these are as clean or pipeline-friendly as a built-in solution.\n\n\n### Additional context\n\nThis feature would make `IterativeImputer` more powerful and suitable for real-world datasets that include both numeric and categorical ...",
      "labels": [
        "New Feature"
      ],
      "state": "open",
      "created_at": "2025-04-17T12:24:59Z",
      "updated_at": "2025-06-02T15:41:17Z",
      "comments": 5,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31219"
    },
    {
      "number": 31218,
      "title": "Add P4 classification metric",
      "body": "### Describe the workflow you want to enable\n\nHi, while working on a classification problem I found out there is no dedicated function to compute the P4 metric implemented in sklearn. As a reminder, P4 metrics is a binary classification metric that is commonly seen as an extension of the f_beta metrics because it takes into account all four True Positive, False Positive, True Negative and False Negative values, and because is it symmetrical unlike the f_beta metrics.\n\nP4 is defined as follows : P4 = 4 / ( 1/precision + 1/recall + 1/specificity + 1/NPV )\n\nWikipedia page right [here](https://en.wikipedia.org/wiki/P4-metric)\n\nMedium article right [there](https://medium.com/@thomas.vidori/better-than-the-f1-score-discover-the-p-4-score-903242e9545b)\n\n\n### Describe your proposed solution\n\nMy idea was to create a function `p4_support` similar to `precision_recall_fscore_support`. Since it is a binary metric, multiclass and multi-label inputs would be managed with `multilabel_confusion_matrix` so the arguments for `average` would be `'macro', 'samples', 'weighted', 'binary', None`.\nI would compute all necessaries values such as 1/precision, 1/recall, 1/specificity and 1/NPV using `_prf_divide`. If any of these four ratios are zero divisions, then P4 would also return the zero division argument. Indeed, for example if precision is null, then 1/precision is +inf and the whole denominator of the P4 is +inf which make P4 = 0 (Btw, this behavior is a reason why it is harder to achieve a high P4 score than f_score since all four ratios need to be 1 to have a P4 equals to 1.). The function would return the tuple (p4_value, support)\n\nA second function `p4_score` which would be the one actually used by users would return only the first element of the previously described `p4_support` function.\n\n### Describe alternatives you've considered, if relevant\n\nExtras : \n\nSince specificity and NVP are computed anyway, the `p4_support` function could return the tuple (specificity, NVP, p4_sco...",
      "labels": [
        "New Feature"
      ],
      "state": "closed",
      "created_at": "2025-04-17T09:02:01Z",
      "updated_at": "2025-04-25T08:44:42Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31218"
    },
    {
      "number": 31210,
      "title": "Issues with pairwise_distances(metric='euclidean') when used on the output of UMAP",
      "body": "### Describe the bug\n\nWhen using pairwise_distances with metric='euclidean' on the output of some data from a UMAP, a `RuntimeWarning: divide by zero encountered in matmul ret = a @ b` is raised. This warning is not raised if you just use pairwise_distances on some normally distributed values of the same dimension, it specifically happens when used on the output of UMAP. The warning is not raised if calling `scipy.pdist` on the same data. The warning doesn't come up with any other metric (other than euclidean family e.g. nan_euclidean etc)\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nimport umap.umap_ as umap \nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(42)\narr = np.random.normal(size = (300, 10))\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(arr)\n\n# this line produces the warning \ndist_mat = pairwise_distances(embedding, metric = 'euclidean')\n\n# no warning produced by this code\nsynthetic = np.random.normal(size = (300, 2))\ndist_mat_synth = pairwise_distances(synthetic, metric = 'euclidean')\n```\n\n### Expected Results\n\nWould expect to see no RuntimeWarning (FutureWarning is expected)\n\n### Actual Results\n\n``` \n\n[.../site-packages/sklearn/utils/deprecation.py:151]: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n[.../site-packages/sklearn/utils/extmath.py:203] RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n[.../site-packages/sklearn/utils/extmath.py:203]: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n[.../site-packages/sklearn/utils/extmath.py:203]: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.16 (main, Feb 25 2025, 09:29:51) [Clang 16.0.0 (clang-1600.0.26.6)]\nexecutable: .../bin/python\n   machine: macOS-15.4-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 65.5.0\n        numpy: 2.1.3\n        scipy: 1...",
      "labels": [
        "Bug",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-04-15T17:11:10Z",
      "updated_at": "2025-05-10T21:04:21Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31210"
    },
    {
      "number": 31206,
      "title": "Different Python version causes a different distribution of classification result",
      "body": "### Describe the bug\n\nRunning the same code using Python 3.10 and Python 3.13 with `n_jobs > 1` had a variety of result. Python 3.10 and Python 3.13 also has different distributions.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nimport random\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n\n# Control the randomness\nrandom.seed(0)  \nnp.random.seed(0)\n\niris = load_iris()  \nx, y = iris.data, iris.target\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n\n# Define and create a model\nmodel = RandomForestClassifier(\n    n_estimators=np.int64(101),\n    criterion='gini',\n    max_depth=np.int64(31),\n    min_samples_split=7.291122019556396e-304,\n    min_samples_leaf=np.int64(14876671),\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs= np.int64(255),\n    random_state=0,\n    verbose=np.int64(0),\n    warm_start=False,\n    class_weight='balanced_subsample',\n    ccp_alpha=0.0,\n    max_samples=None)\n\nmodel.fit(x_train, y_train)\n\n# Evaluate model\ny_pred = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test,\n                                    y_pred))\nprint(\"Recall:\",\n    recall_score(y_test, y_pred, average='micro'))\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n```\n\n### Expected Results\n\nIf `n_jobs` is 1, the result is:\n```\n    Accuracy:  0.43333333333333335\n    Recall: 0.43333333333333335\n    Confusion Matrix:\n    [[ 0 11  0]\n    [ 0 13  0]\n    [ 0  6  0]]\n```\n\n### Actual Results\n\nWhen the program is run 10,000 times:\n**n_jobs=255, Python 3.10** has two possible results:\n```\n    Group:\n    Accuracy:  0.43333333333333335\n    Recall: 0.43333333333333335\n    Confusion Matrix:\n    [[ 0 11  0]\n ...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-04-15T11:04:29Z",
      "updated_at": "2025-04-24T14:05:51Z",
      "comments": 6,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31206"
    },
    {
      "number": 31200,
      "title": "DOC Examples (imputation): add scaling when using k-neighbours imputation",
      "body": "### Describe the issue linked to the documentation\n\nTwo examples for missing-values imputation use k-neighbors imputation without scaling data first.\nAs a result, the approaches under-perform.\nThe examples are:\n\n1. https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py\n2. https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html\n\nIn the first example, the effect is quite small, adding scaling before calling k-neighbours imputer changes MSE for the california dataset for k-NN from 0.2987 ± 0.1469 to 0.2912 ± 0.1410 and for the diabetes dataset from 3314 ± 114  to 3323 ± 90.\n\nIn the second example (comparing iterative imputations), the change is more significant: before the change, iterative imputation with k-neighbors performed worse than imputation with mean, after the scaling -- it performs better than mean imputation.\n\nIn both cases, it is a better practice to scale data before using a k-neighbors approach which is based on distances between points.\n\n![Image](https://github.com/user-attachments/assets/167560c9-3011-425f-a29f-74548fc9e8bc)\n\n### Suggest a potential alternative/fix\n\nI will submit a patch to fix an issue.",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-04-14T12:17:24Z",
      "updated_at": "2025-06-12T09:11:13Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31200"
    },
    {
      "number": 31189,
      "title": "scikit-learn not included in conda env creation step for bleeding-edge install",
      "body": "### Describe the issue linked to the documentation\n\nOn the [Contributing](https://scikit-learn.org/stable/developers/contributing.html) page, under \"How to contribute\", Step 4 guides users to the \"Building from source\" section, which links to:\n\n [Advanced Installation – Install bleeding-edge](https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge)\n\nHowever, in Step 2 of that page (the conda environment creation command), the scikit-learn package itself is not mentioned or included.\n\n![Image](https://github.com/user-attachments/assets/d9ec66cb-7d2b-4708-8ce6-27dd6317f55f)\nbut Step 6 asks to Check that the installed scikit-learn has a version number ending with .dev0 which raises errors if scikit-learn is not installed in the virtual environment\n\n![Image](https://github.com/user-attachments/assets/4c04dec8-e112-441c-a941-e0d3bc1d0861)\n\n\n### Suggest a potential alternative/fix\n\n_No response_",
      "labels": [
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-04-13T18:26:37Z",
      "updated_at": "2025-04-14T07:45:23Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31189"
    },
    {
      "number": 31185,
      "title": "BUG:  examples\\applications\\plot_out_of_core_classification.py breaks with StopIteration error",
      "body": "### Describe the bug\n\nI was building the documentation from source, following [The contributing Tutorial](https://scikit-learn.org/stable/developers/contributing.html#documentation).\n\nWhen I ran the command `make html`, I noticed the following error from Sphinx build:\n\n```\nExtension error:\nHere is a summary of the problems encountered when running the examples:\n\nUnexpected failing examples (1):\n\n    ..\\examples\\applications\\plot_out_of_core_classification.py failed leaving traceback:\n\n    Traceback (most recent call last):\n      File \"C:\\Users\\vitor.pohlenz\\vpz\\scikit-learn\\examples\\applications\\plot_out_of_core_classification.py\", line 252, in <module>\n        X_test = vectorizer.transform(X_test_text)\n      File \"C:\\Users\\vitor.pohlenz\\vpz\\scikit-learn\\sklearn\\feature_extraction\\text.py\", line 878, in transform\n        X = self._get_hasher().transform(analyzer(doc) for doc in X)\n      File \"C:\\Users\\vitor.pohlenz\\vpz\\scikit-learn\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n        data_to_wrap = f(self, X, *args, **kwargs)\n      File \"C:\\Users\\vitor.pohlenz\\vpz\\scikit-learn\\sklearn\\feature_extraction\\_hash.py\", line 175, in transform\n        first_raw_X = next(raw_X)\n    StopIteration\n\n-------------------------------------------------------------------------------\n\nBuild finished. The HTML pages are in _build/html.\n```\n\n[Here is the full log of the build](https://gist.github.com/vitorpohlenz/2367944e20a0c05b29225631dbdaeb82)\n\nIt seems that the `raw_X` varible is empty.\n\nAlso when running the file `examples\\applications\\plot_out_of_core_classification.py` directly in the python environment we get the same error.\n\n### Steps/Code to Reproduce\n\nThe simple way to reproduce is just to run the file `plot_out_of_core_classification.py` in the sklearn-env.\n\n1. Activate sklearn-env\n2. Supposing that you are in the folder `scikit-learn`, run:\n`python examples\\applications\\plot_out_of_core_classification.py`\n\nAlternatively, you may enter the `doc` folder, and execute ...",
      "labels": [
        "Bug",
        "Documentation"
      ],
      "state": "closed",
      "created_at": "2025-04-11T21:08:08Z",
      "updated_at": "2025-04-12T16:34:57Z",
      "comments": 2,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31185"
    },
    {
      "number": 31183,
      "title": "Upper bound the build dependencies in `pyproject.toml` for release branches",
      "body": "### Describe the workflow you want to enable\n\nUpper bound the build dependencies on release branches makes it easier to build the wheel in the future. This has two benefits:\n\n- The wheels become easier to build when using the newest build dependency does not work. (Historically, I've seen issues with Cython)\n- If we wanted to backport a fix the wheel building is more stable.\n\n### Describe your proposed solution\n\nOn release branches, provide a upper bound to the build dependencies in `pyproject.toml`.\n\nSciPy does this already: https://github.com/scipy/scipy/blob/e3228cdfe42e403ed203db16e4db4822eb416797/pyproject.toml#L1-L13\n\n### Describe alternatives you've considered, if relevant\n\nLeave the build dependencies to be unbounded.",
      "labels": [
        "Build / CI",
        "Needs Decision"
      ],
      "state": "closed",
      "created_at": "2025-04-11T15:13:13Z",
      "updated_at": "2025-05-13T13:30:57Z",
      "comments": 7,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31183"
    },
    {
      "number": 31178,
      "title": "⚠️ CI failed on Wheel builder (last failure: Apr 11, 2025) ⚠️",
      "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14395159536)** (Apr 11, 2025)",
      "labels": [
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-04-11T04:48:44Z",
      "updated_at": "2025-04-12T04:34:19Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31178"
    },
    {
      "number": 31169,
      "title": "How is the progress of sklearn1.7?[help wanted]",
      "body": "### Describe the workflow you want to enable\n\nExcuse me, is sklearn1.7.dev0 available now? How to install it?  \n\n### Describe your proposed solution\n\nnone\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Triage"
      ],
      "state": "closed",
      "created_at": "2025-04-10T06:09:49Z",
      "updated_at": "2025-04-10T08:25:51Z",
      "comments": 1,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31169"
    },
    {
      "number": 31164,
      "title": "Fix ConvergenceWarning in `plot_gpr_on_structured_data.py` example",
      "body": "This issue is about addressing a `ConvergenceWarning` that occurs when running the `examples/gaussian_process/plot_gpr_on_structured_data.py `example in CI (also when building the documentation locally).\n\nThe example creates three plots. The last use case on a classification of DNA sequences throws a `ConvergenceWarning` related to the `baseline_similarity_bounds` defined in a custom kernel when fitting. It seems that the lower bound is pushed resulting in the lack of convergence.\n\nThis occurs with the setting `baseline_similarity_bounds=(1e-5, 1))` in the custom kernel.\n\nEven setting `baseline_similarity_bounds=(1e-40, 1)) ` results in the same warning:\n```\nConvergenceWarning: The optimal value found for dimension 0 of parameter baseline_similarity is close to the specified lower bound 1e-40. Decreasing the bound and calling fit again may find a better value.\n```\n\nLowering the bound further with `baseline_similarity_bounds=(1e-50, 1)) ` results in a different warning stemming from `lbfgs`:\n```\nConvergenceWarning: lbfgs failed to converge (status=2): ABNORMAL: .\nIncrease the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html\n```\n\nIt would be preferable to resolve this so the example can be build without displaying warnings.\n\n\nWhile being at the example, other small improvements are welcome (for instance fixing the typo in \"use of kernel functions that operates\" (the s in operates)).",
      "labels": [
        "help wanted",
        "Needs Investigation"
      ],
      "state": "open",
      "created_at": "2025-04-09T08:45:27Z",
      "updated_at": "2025-05-06T09:15:54Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31164"
    },
    {
      "number": 31158,
      "title": "Nearest neighbors Gaussian Process",
      "body": "### Describe the workflow you want to enable\n\nRecently I've been working on a Nearest Neighbor Gaussian Process Regressor as described in Datta 2016 [here](https://arxiv.org/abs/1406.7343). This kind of model exists in R, but not in scikit-learn. Nearest Neighbor Gaussian Process Regressor is a simple enhancement over standard GP that allows to use GP on large datasets. It also recently gained interest among the GPytorch package, see e.g. [here](https://arxiv.org/abs/2202.01694).\n\n\n\n### Describe your proposed solution\n\nI already have a scikit-learn-like implementation that I could bring to this project. This implementation becomes more convenient (uses less memory and less runtime) than classic Gaussian Process Regressor from a dataset size of approx 10k. It is based on Datta's work, so it's not as the one in the GPytorch package. If anyone deems this model interesting enough, I'm wiling to make a PR.\n\nHaving a baseline CPU-base implementation in scikit-learn could also server as a starting point for future GPU-based implementations, which is were this model really shines (e.g. inheriting from scikit-learn class and implementing in GPU the most time consuming operations). As an example, I also have a cupy-based implementation of Datta's NNGP which competes very well against GPytorch VNNGP.\n\n### Describe alternatives you've considered, if relevant\n\nAs mentioned above, a version of NNGP is implemented in GPytorch. GPytorch implementation however is not only based on Nearest Neighbors, but also on Variational method. The one from Datta's is simpler being only based on NN and can become competitive with more complex methods VNNGP when using GPUs.\n\n### Additional context\n\n_No response_",
      "labels": [
        "New Feature",
        "Needs Decision - Include Feature"
      ],
      "state": "open",
      "created_at": "2025-04-07T10:12:34Z",
      "updated_at": "2025-04-22T15:52:58Z",
      "comments": 4,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31158"
    },
    {
      "number": 31149,
      "title": "BUG: Build from source fails for scikit-learn v1.6.1 on Windows 11 with Visual Studio Build Tools 2022, Ninja subprocess error",
      "body": "### Describe the bug\n\nFirst of all, thank you guys for the fantastic job with Sklearn. \nI'm trying to build from source to start contributing to the project, but it ended with me bringing more issues to you. \nAfter struggling for some days with this problem, I'm seeking help. Maybe if you have some clue or workaround, I could open a Pull Request with the solution for this.\n\nI am following the guidelines for [Contributing with Scikit-learn](https://scikit-learn.org/stable/developers/contributing.html#contributing), and for that, it is necessary to [Build from source on Windows](https://scikit-learn.org/stable/developers/advanced_installation.html#windows), which recomends install  Build Tools for Visual Studio 2019, but nowadays is not possible to download the 2019 version just the [Build Tools for Visual Studio 2022 installer](https://aka.ms/vs/17/release/vs_buildtools.exe).\n\nThe installation of Build Tools for Visual Studio 2022 runs smoothly(and also the initialization of its Environment), as well as the creation of the Python virtual environment and the installation of the packages `wheel, numpy, scipy, cython, meson-python, ninja`.\n\nBut in the step of building from source using the `pip install --editable`, the build breaks when Compiling C objects after some [C4090 warnings](https://learn.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4090?view=msvc-170), throwing an `metadata-generation-failed error` from a subprocess of `ninja build`.\n\nThis seems related/similar to issue #31123. Despite not being the same problem, if we find a solution, it may work for both issues.\n\n### Steps/Code to Reproduce\n\nI have tried the steps using `pip install` and also `conda-forge` in different versions of Python: pip :{3.10.11, 3.12.7} conda:{ 3.13.2}  to check if it was a problem with Python/pip itself.\n\n1. **Environment Setup:**\n- OS:  Windows 11 Pro, Version 24H2, OS build 26100.3476\n- System type: 64-bit operating system, x64-based processor...",
      "labels": [
        "Bug"
      ],
      "state": "closed",
      "created_at": "2025-04-04T20:08:35Z",
      "updated_at": "2025-04-18T12:32:10Z",
      "comments": 19,
      "url": "https://github.com/scikit-learn/scikit-learn/issues/31149"
    }
  ]
}